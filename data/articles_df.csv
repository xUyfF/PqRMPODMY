id,title,venue,abstract,feild
519,Improving the performance of hierarchical classification with swarm intelligence,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics"," In this paper we propose a new method to improve the performance of hierarchical classification. We use a swarm intelligence algorithm to select the type of classification algorithm to be used at each “classifier node” in a classifier tree. These classifier nodes are used in a top-down divide and conquer fashion to classify the examples from hierarchical data sets. In this paper we propose a swarm intelligence based approach which attempts to mitigate a major drawback with a recently proposed local search-based, greedy algorithm. Our swarm intelligence based approach is able to take into account classifier interactions whereas the greedy algorithm is not. We evaluate our proposed method against the greedy method in four challenging bioinformatics data sets and find that, overall, there is a significant increase in performance. ",biology
520,Semantic Predications for Complex Information Needs in Biomedical Literature,Bioinformatics and Biomedicine,"Many complex information needs that arise in biomedical disciplines require exploring multiple documents in order to obtain information. While traditional information retrieval techniques that return a single ranked list of documents are quite common for such tasks, they may not always be adequate. The main issue is that ranked lists typically impose a significant burden on users to filter out irrelevant documents. Additionally, users must intuitively reformulate their search query when relevant documents have not been not highly ranked. Furthermore, even after interesting documents have been selected, very few mechanisms exist that enable document- to-document transitions. In this paper, we demonstrate the utility of assertions extracted from biomedical text (called semantic predications) to facilitate retrieving relevant documents for complex information needs. Our approach offers an alternative to query reformulation by establishing a framework for transitioning from one document to another. We evaluate this novel knowledge-driven approach using precision and recall metrics on the 2006 TREC Genomics Track.",biology
521,A collaborative filtering approach to assess individual condition risk based on patients' social network data,International Conference on Bioinformatics, ! §  Comorbidity refers to the phenomenon that conditions are correlated with each other. E.g. some persons may develop depression that is secondary to alcohol dependence [1]. ! ,biology
522,Faster Algorithms for Optimal Multiple Sequence Alignment Based on Pairwise Comparisons,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Multiple Sequence Alignment (MSA) is one of the most fundamental problems in computational molecular biology. The running time of the best known scheme for finding an optimal alignment, based on dynamic programming, increases exponentially with the number of input sequences. Hence, many heuristics were suggested for the problem. We consider a version of the MSA problem where the goal is to find an optimal alignment in which matches are restricted to positions in predefined matching segments. We present several techniques for making the dynamic programming algorithm more efficient, while still finding an optimal solution under these restrictions. We prove that it suffices to find an optimal alignment of the predefined sequence segments, rather than single letters, thereby reducing the input size and thus improving the running time. We also identify ""shortcuts"" that expedite the dynamic programming scheme. Empirical study shows that, taken together, these observations lead to an improved running time over the basic dynamic programming algorithm by 4 to 12 orders of magnitude, while still obtaining an optimal solution. Under the additional assumption that matches between segments are transitive, we further improve the running time for finding the optimal solution by restricting the search space of the dynamic programming algorithm. ÂŠ 2006 IEEE.",biology
523,Predicting Thermophilic Nucleotide Sequences Based on Chaos Game Representation Features and Support Vector Machine,International Conference on Bioinformatics and Biomedical Engineering,"Knowledge of thermophilic mechanisms about some organisms whose optimum growth temperature (OGT) range from 50 to 80 degree plays a major role for helping design stable proteins. How to predict a DNA sequence to be thermophilic is a long but not fairly resolved problem. After downloading 10586 thermophilic bacteria nucleotide sequences and 14261 mesophilic bacteria nucleotide sequences from NCBI database and eliminating the sequences with 95% homologous similarity by CD-HIT, 1638 thermophilic and 2996 mesophilic sequences are remained. Chaos game representation (CGR) can investigate the patterns hiding in DNA sequence, visually revealing previously unknown structure. In this paper, we convert every DNA sequence into a high dimensional vector by CGR algorithm, and predict the DNA sequence thermostability by these CGR features and support vector machine (SVM) with three group experiments: 16-dimensional vector, 64-dimensional vector and 256-dimensional vector, respectively. Each group is evaluated by resubstitution test and 10-fold cross-validation test. In the resubstitution test, the results of all three groups perform highly satisfactions, in which the accuracy achieves 0.9989 and MCC (Matthews Correlation Coefficient) achieves 0.9978. In 10-fold cross-validation test, 256-dimensional vector get the the best: the average accuracy is 0.9088 and average MCC is 0.8169. The results show the effectiveness of the new algorithm.",biology
524,Identifying inconsistencies in multiple clinical practice guidelines for a patient with co-morbidity,Bioinformatics and Biomedicine," - This paper describes a methodological approach to identifying inconsistencies when reconciling multiple clinical practice guidelines. The need to address these inconsistencies arises when a patient with co-morbidity has to be managed according to different treatment regimens. Starting with a wellknown flowchart representation we discuss how to create a formal guideline model that allows for easy manipulations of its components. For this model we present how to identify conflicting actions that are manifested by treatment-treatment and treatment-disease interactions, and how to reconcile these conflicting actions. ",biology
525,A Pattern Mining Approach for Classifying Multivariate Temporal Data,Bioinformatics and Biomedicine,"We study the problem of learning classification models from complex multivariate temporal data encountered in electronic health record systems. The challenge is to define a good set of features that are able to represent well the temporal aspect of the data. Our method relies on temporal abstractions and temporal pattern mining to extract the classification features. Temporal pattern mining usually returns a large number of temporal patterns, most of which may be irrelevant to the classification task. To address this problem, we present the minimal predictive temporal patterns framework to generate a small set of predictive and non-spurious patterns. We apply our approach to the real-world clinical task of predicting patients who are at risk of developing heparin induced thrombocytopenia. The results demonstrate the benefit of our approach in learning accurate classifiers, which is a key step for developing intelligent clinical monitoring systems.",biology
526,System for random access DNA sequence compression,Bioinformatics and Biomedicine,"DNA sequences are generally compressed by algorithms using approximate repeats that are found in most DNA sequences. The regions of DNA that are not part of a repeat are encoded by using arithmetic coder which estimates the probabilities for each symbol using a Markov model. Since arithmetic coding is used for compressing the bitstream, random access is very difficult in these methods as bthe itstream is tightly packed. Random access is a desirable feature as it enable to decompress only interesting regions in the sequence. This paper presents a system which uses the approximate repeats based compression algorithm and provides random access capability.",biology
527,Decision Tree Based Predictive Models for Breast Cancer Survivability on Imbalanced Data,International Conference on Bioinformatics and Biomedical Engineering,"Based on imbalanced data, the predictive models for 5-year survivability of breast cancer using decision tree are proposed. After data preprocessing from SEER breast cancer datasets, it is obviously that the category of data distribution is imbalanced. Under-sampling is taken to make up the disadvantage of the performance of models caused by the imbalanced data. The performance of the models is evaluated by AUC under ROC curve, accuracy, specificity and sensitivity with 10-fold stratified cross-validation. The performance of models is best while the distribution of data is approximately equal. Bagging algorithm is used to build an integration decision tree model for predicting breast cancer survivability.",biology
528,An Extended Kalman Filtering Approach to Modeling Nonlinear Dynamic Gene Regulatory Networks via Short Gene Expression Time Series,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Abstract-In this paper, the extended Kalman filter (EKF) algorithm is applied to model the gene regulatory network from gene time series data. The gene regulatory network is considered as a nonlinear dynamic stochastic model that consists of the gene measurement equation and the gene regulation equation. After specifying the model structure, we apply the EKF algorithm for identifying both the model parameters and the actual value of gene expression levels. It is shown that the EKF algorithm is an online estimation algorithm that can identify a large number of parameters (including parameters of nonlinear functions) through iterative procedure by using a small number of observations. Four real-world gene expression data sets are employed to demonstrate the effectiveness of the EKF algorithm, and the obtained models are evaluated from the viewpoint of bioinformatics. ÂŠ C 2009 IEEE.",biology
529,Pathological Image Analysis Using the GPU: Stroma Classification for Neuroblastoma,Bioinformatics and Biomedicine,"Neuroblastoma is one of the most malignant childhood cancers affecting infants mostly. The current prognosis is based on microscopic examination of slides by expert pathologists, a process that is error-prone, time consuming and may lead to inter- and intra-reader variations. Therefore, we are developing a Computer Aided Prognosis (CAP) system which provides computerized image analysis to assist pathologist in their prognosis. Since this system operates on relatively large- scale images and requires sophisticated algorithms, it takes a long time to process whole-slide images. In this paper, we propose a novel and efficient approach for the execution of a CAP system for neuroblastoma prognosis, using the graphics processing unit (GPU). By leveraging high memory bandwidth and strong floating point operation capabilities of the GPU, our goal is to achieve order of magnitude reduction in the overall execution time as compared to that on a CPU alone. The proposed approach was tested on a set of testing images with a promising accuracy of 99.4% and an execution performance gain factor up to 45 times compared to C++ code running on the CPU.",biology
530,Microarray Biclustering: A Novel Memetic Approach Based on the PISA Platform,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","In this paper, a new memetic approach that integrates a Multi- Objective Evolutionary Algorithm (MOEA) with local search for microarray biclustering is presented. The original features of this proposal are the consideration of opposite regulation and incorporation of a mechanism for tuning the balance between the size and row variance of the biclusters. The approach was developed according to the Platform and Programming Language Independent Interface for Search Algorithms (PISA) framework, thus achieving the possibility of testing and comparing several different memetic MOEAs. The performance of the MOEA strategy based on the SPEA2 performed better, and its resulting biclusters were compared with those obtained by a multi-objective approach recently published. The benchmarks were two datasets corresponding to Saccharomyces cerevisiae and human B-cells Lymphoma. Our proposal achieves a better proportion of coverage of the gene expression data matrix, and it also obtains biclusters with new features that the former existing evolutionary strategies can not detect. © Springer-Verlag Berlin Heidelberg 2009.",biology
531,A new method for measuring the semantic similarity on gene ontology,Bioinformatics and Biomedicine,"Semantic similarity defined on Gene Ontology (GO) aims to provide the functional relationship between different biological processes, molecular functions, or cellular components. In this paper, a novel method, namely the Shortest Path (SP) algorithm, for measuring the semantic similarity on GO is proposed based on both the GO structure information and the term's property. The proposed algorithm searches for the shortest path that connects two terms and uses the sum of weights on the shortest path to compute the semantic similarity for GO terms. A method for evaluating the nonlinear correlation between two variables is also introduced for validation. Extensive experiments conducted on two public gene expression datasets demonstrate the overall superiority of SP method over the other state-of-the-art methods evaluated.",biology
532,Detecting stairs and pedestrian crosswalks for the blind by RGBD camera,Bioinformatics and Biomedicine," A computer vision-based wayfinding and navigation aid can improve the mobility of blind and visually impaired people to travel independently. In this paper, we develop a new framework to detect and recognize stairs and pedestrian crosswalks using a RGBD camera. Since both stairs and pedestrian crosswalks are featured by a group of parallel lines, we first apply Hough transform to extract the concurrent parallel lines based on the RGB channels. Then, the Depth channel is employed to further recognize pedestrian crosswalks, upstairs, and downstairs using support vector machine (SVM) classifiers. Furthermore, we estimate the distance between the camera and stairs for the blind users. The detection and recognition results on our collected dataset demonstrate that the effectiveness and efficiency of our proposed framework. ",biology
533,An Optimized Algorithm for Task Scheduling Based on Activity Based Costing in Cloud Computing,International Conference on Bioinformatics and Biomedical Engineering,"In cloud computing, traditional way for task scheduling cannot measure the cost of cloud resources accurately by reason that each of the tasks on cloud systems is totally different between each other. There may be no relationship between the overhead application base and the way that different tasks cause overhead costs of resources in cloud systems. The traditional way for task scheduling cannot meet the cloud market well enough. This paper introduces an optimized algorithm for task scheduling based on ABC (activity based costing) in cloud computing and its implementation. Compared with the traditional methods of task scheduling, a new method with an optimized algorithm based on ABC algorithm was proposed in this paper.",biology
534,Graph-constrained discriminant analysis of functional genomics data,International Conference on Bioinformatics, https://hal-supelec.archives-ouvertes.fr/hal-00346450 ,biology
535,Association analysis techniques for analyzing complex biological data sets,International Conference on Bioinformatics,"Association analysis is one of the most popular analysis paradigms in data mining. In this paper, we present different types of association patterns and discuss some of their applications in bioinformatics. We present a case study showing the usefulness of association analysis-based techniques for pre-processing protein interaction networks. Finally, we discuss some of the challenges that need to be addressed to make association analysis-based techniques more applicable for bioinformatics.",biology
536,Fast graph approaches to measure influenza transmission across geographically distributed host types,International Conference on Bioinformatics,"Recent advances in next generation sequencing are provid- ing a number of large whole-genome sequence datasets stemming from globally distributed disease occurrences. This offers an unprecedented opportunity for epidemiological studies and the development of computationally efficient, robust tools for such studies. Here we present an analytic approach combining several existing tools that enables a quick, effective, and robust epidemiological analysis of large wholegenome datasets. In this report, our dataset contains over 4; 200 globally sampled Inuenza A virus isolates from multiple host type, subtypes, and years. These sequences are compared using an alignment-free method that runs in linear time. This enables us to generate a disease transmission network where sequences serve as nodes, and high-degree sequence similarity as edges. Mixing patterns are then used to examine statistical probabilities of edge formation among different host types from different global regions and from different localities within Southeast Asia. Our results reect notable amounts of inter-host and inter-regional transmission of Inuenza A viruCopyright 漏 2010 ACM.",biology
537,Research on the Classification of Brain Function Based on SVM,International Conference on Bioinformatics and Biomedical Engineering,"In order to improve the accuracy in classification of electroencephalographic (EEG) signals of different brain functions, the research of how to select the suitable classifier is carried out in the paper. Some experiments have been performed to select the suitable kernel function of support vector machine (SVM). Four different kernel functions are put into the comparison and the results show that the radial basis function (RBF) has the highest rate of correct classification (RCC). On the other hand, the EEG signals from different electrodes will lead different classification results. The study of selecting the suitable electrode has been done. It shows that the RCC of the signal which from the electrode near by the area where EEG signal of a certain brain function is generated is much higher than those far from. We increase the dimension of SVM through combine the signal of different channels, which the RCC is very low, to improve the RCC of the signal which far from the area of the certain brain function. The results of our experiments are satisfied. The RCC of the EEG signal can reached to 99%.",biology
538,Study on Metadata Applications for Proteomics Data Integration,International Conference on Bioinformatics and Biomedical Engineering,"The integration of complex, fast changing biological data repositories can be potentially supported by Grid computing to enable distributed data analysis. The ISPIDER proteomics Grid provides an architecture supporting the combined use of Grid data access, Grid distributed querying and data integration software tools, which support distributed, heterogeneous biologic data integration and analysis. This paper introduces the ISPIDER Proteomics Grid, and discusses what kind of metadata can be used for expressing the multiplicity of data models and the data transformation and integration processes in the proteomics integration Grid, and how this metadata can be further used for supporting other data integration activities.",biology
539,Inferring Adaptive Regulation Thresholds and Association Rules from Gene Expression Data through Combinatorial Optimization Learning,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"There is a need to design computational methods to support the prediction of gene regulatory networks (GRNs). Such models should offer both biologically meaningful and computationally accurate predictions which, in combination with other techniques, may improve large-scale integrative studies. This paper presents a new machine-learning method for the prediction of putative regulatory associations from expression data which exhibit properties never or only partially addressed by other techniques recently published. The method was tested on a Saccharomyces cerevisiae gene expression data set. The results were statistically validated and compared with the relationships inferred by two machine-learning approaches to GRN prediction. Furthermore, the resulting predictions were assessed using domain knowledge. The proposed algorithm may be able to accurately predict relevant biological associations between genes. One of the most relevant features of this new method is the prediction of adaptive regulation thresholds for the discretization of gene expression values, which is required prior to the rule association learning process. Moreover, an important advantage consists of its low computational cost to infer association rules. The proposed system may significantly support exploratory large-scale studies of automated identification of potentially relevant gene expression associations. © 2007 IEEE.",biology
540,A Monte Carlo EM Algorithm for De Novo Motif Discovery in Biomolecular Sequences,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Motif discovery methods play pivotal roles in deciphering the genetic regulatory codes (i.e., motifs) in genomes as well as in locating conserved domains in protein sequences. The Expectation Maximization (EM) algorithm is one of the most popular methods used in de novo motif discovery. Based on the position weight matrix (PWM) updating technique, this paper presents a Monte Carlo version of the EM motif-finding algorithm that carries out stochastic sampling in local alignment space to overcome the conventional EMs main drawback of being trapped in a local optimum. The newly implemented algorithm is named as Monte Carlo EM Motif Discovery Algorithm (MCEMDA). MCEMDA starts from an initial model, and then it iteratively performs Monte Carlo simulation and parameter update until convergence. A log-likelihood profiling technique together with the top-k strategy is introduced to cope with the phase shifts and multiple modal issues in motif discovery problem. A novel grouping motif alignment (GMA) algorithm is designed to select motifs by clustering a population of candidate local alignments and successfully applied to subtle motif discovery. MCEMDA compares favorably to other popular PWM-based and word enumerative motif algorithms tested using simulated (l, d)-motif cases, documented prokaryotic, and eukaryotic DNA motif sequences. Finally, MCEMDA is applied to detect large blocks of conserved domains using protein benchmarks and exhibits its excellent capacity while compared with other multiple sequence alignment methods. ÂŠ 2009 IEEE.",biology
541,Chaos Game Representation for Discriminating Thermophilic from Mesophilic Protein Sequences,International Conference on Bioinformatics and Biomedical Engineering,"Can sequence analysis tell us about the function of protein? A basic question in protein science is which kind of proteins extent thermostability. Chaos game representation (CGR) can investigate the patterns hiding in protein sequence, visually revealing previously unknown structure. In this paper, we convert every protein sequence into a 20-dimensional vector by CGR algorithm, and based on these vectors we discriminate thermophiles from mesophiles using support vector machine (SVM). The overall accuracy achieves 100% in resubstitution test, and 87.12% in Jackknife test. Moreover, Matthews correlation coefficients (MCC) is 0.745.",biology
542,Multi-Class SVM Classification of Surface EMG Signal for Upper Limb Function,International Conference on Bioinformatics and Biomedical Engineering,"Electromyography (EMG) signal is electrical manifestation of neuromuscular activation, that provides access to physiological processes which cause the muscle to generate force and produce movement and allow us to interact with the world. In this paper, an identification of six degree of freedom for evaluating and recording physiologic properties of muscles of the forearm at rest and while contracting is presented. The first step of this method is to analyze the surface EMG signal from the subject's forearm using wavelet packet transform and extract features using the singular value decomposition. In this way, a new feature space is generated from wavelet packet coefficients. The second step is to import the feature values into multi class support vector machine as a classifier, to identify six degree of freedom viz. open to close, close to open, supination, pronation, flexion and extension. The results showed that an accuracy of over 96% could be obtained for a six degree of freedom classification problem.",biology
543,3D Rotation Invariant Features for the Characterization of Molecular Density Maps,Bioinformatics and Biomedicine,"Cryo electron microscopy produces 3D density maps of large macromolecular structures. An important task lies in the efficient identification of structural similarities between different molecular density maps. Here we construct and test three different types of 3D rotation invariant features for template free similarity detection in molecular density maps. The density map comparison is based on feature vectors that describe the surrounding density distribution for a given map position. We propose Fast Fourier Transform based methods to speed up the computation of feature vectors. Previously, little is known about the discriminative power of rotation invariant features for noisy maps. Here, we test the three feature types with density maps at different noise levels. We assess the performance of our feature vectors by a classification experiment of protein density maps. Our results show that at low noise levels the three types of features perform equally well. However, at high noise levels the features that are constructed by a spherical harmonics decomposition of the density neighborhood is significantly more reliable and outperform the other two feature types, which are based on the moments and intensity histograms of the density distribution.",biology
544,Predicting the Risk Type of Human Papillomaviruses Based on Sequence-Derived Features,International Conference on Bioinformatics and Biomedical Engineering,"Human papillomaviruses (HPVs) are a group of viruses that are now recognized as one of the major causes of cervical cancer. But there are over 100 different types of HPV, so scientists have separated HPV types into those that are more likely to develop into cancer and those that are less likely. The so-called ""high risk"" HPV types are more likely to lead to the development of cancer, while ""low-risk"" viruses rarely develop into cancer. Therefore, how can we identify whether it is a risk type of HPVs is very useful and necessary to the diagnosis and the remedy of cervical cancer. To predict and to classify the risk types of HPV by bioinformatics analysis, we construct a HPV dataset from available databases. The classification was achieved on the basis of multitudinous physicochemical and statistical features from protein sequences using Fuzzy K nearest neighbor (FKNN) classifier. The overall predictive accuracy about 96% has been achieved through the rigorous leave-one-out cross-validation on the dataset. This indicates that our method can be a useful associated tool for risk type prediction of human papillomaviruses.",biology
545,Combining Hierarchical Inference in Ontologies with Heterogeneous Data Sources Improves Gene Function Prediction,Bioinformatics and Biomedicine,"The study of gene function is critical in various genomic and proteomic fields. Due to the availability of tremendous amounts of different types of protein data, integrating these datasets to predict function has become a significant opportunity in computational biology. In this paper, to predict protein function we (i) develop a novel Bayesian framework combining relational,hierarchical and structural information with improvement in data usage efficiency over similar methods, and (ii) propose to use it in conjunction with an integrative protein-protein association network, STRING (Search Tool for the Retrieval of INteracting Genes/proteins), which combines information from seven different sources. At the heart of our work is accomplishing protein data integration in a concerted fashion with respect to algorithm and data source. Method performance is assessed by a 5-fold cross-validation in yeast on selected terms from the Molecular Function ontology in the Gene Ontology database. Results show that our combined use of the proposed computational framework and the protein network from STRING offers substantial improvements in prediction. The benefits of using an aggressively integrative network, such as STRING, may derive from the fact that although it is likely that the ultimate gene interaction matrix (including but not limited to protein-protein, genetic, or regulatory interactions) will be sparse, presently it is still known only incompletely in most organisms, and thus the use of multiple distinct data sources is rewarded.",biology
546,Significance of Gene Ranking for Classification of Microarray Samples,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Many methods for classification and gene selection with microarray data have been developed. These methods usually give a ranking of genes. Evaluating the statistical significance of the gene ranking is important for understanding the results and for further biological investigations, but this question has not been well addressed for machine learning methods in existing works. Here, we address this problem by formulating it in the framework of hypothesis testing and propose a solution based on resampling. The proposed r-test methods convert gene ranking results into position p-values to evaluate the significance of genes. The methods are tested on three real microarray data sets and three simulation data sets with support vector machines as the method of classification and gene selection. The obtained position p-values help to determine the number of genes to be selected and enable scientists to analyze selection results by sophisticated multivariate methods under the same statistical inference paradigm as for simple hypothesis testing methods. ÂŠ 2006 IEEE.",biology
547,Gene clustering by structural prior based local factor analysis model under Bayesian Ying-Yang harmony learning,Bioinformatics and Biomedicine," We propose a clustering algorithm based on a structural prior based Local Factor Analysis (spLFA) model under the Bayesian Ying-Yang harmony learning, which automatically determines the hidden dimensionalities during parameter learning, reduces the number of free parameters by projecting the mean vectors onto a low dimensional manifold, imposes the sparseness by a Normal-Jeffreys prior. Experiments on the diagnostic research dataset show that BYY-spLFA outperforms the k-means clustering and single-link hierarchical clustering. The experiments on a lymphoma cancer datset further indicate the BYY-spLFA is able to uncover the number of phenotypes correctly and cluster the phenotypes more accurately. In addition, we modify BYY-spLFA to implement supervised learning and preliminarily demonstrate its effectiveness on a Leukemia data for classification. • INSPEC ◦ Controlled Indexing Bayes methods , bioinformatics , diseases , genetics , learning (artificial intelligence) , medical computing , patient diagnosis , pattern classification , pattern clustering ",biology
548,Hash Subgraph Pairwise Kernel for Protein-Protein Interaction Extraction,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Extracting protein-protein interaction (PPI) from biomedical literature is an important task in biomedical text mining (BioTM). In this paper, we propose a hash subgraph pairwise (HSP) kernel-based approach for this task. The key to the novel kernel is to use the hierarchical hash labels to express the structural information of subgraphs in a linear time. We apply the graph kernel to compute dependency graphs representing the sentence structure for protein-protein interaction extraction task, which can efficiently make use of full graph structural information, and particularly capture the contiguous topological and label information ignored before. We evaluate the proposed approach on five publicly available PPI corpora. The experimental results show that our approach significantly outperforms all-path kernel approach on all five corpora and achieves state-of-the-art performance. © 2012 IEEE.",biology
549,Querying Graphs in Protein-Protein Interactions Networks Using Feedback Vertex Set,IEEE/ACM Transactions on Computational Biology and Bioinformatics," Recent techniques increase rapidly the amount of our knowledge on interactions between proteins. The interpretation of these new information depends on our ability to retrieve known sub-structures in the data, the Protein-Protein Interactions (PPI) networks. In an algorithmic point of view, it is an hard task since it often leads to NP-hard problems. To overcome this difficulty, many authors have provided tools for querying patterns with a restricted topology, i.e. paths or trees in PPI networks. Such restriction leads to the development of fixed parameter tractable (FPT) algorithms, which can be practicable for restricted sizes of queries. Unfortunately, GRAPH HOMOMORPHISM is a W[1]-hard problem, and hence, no FPT algorithm can be found when patterns are in the shape of general graphs. However, Dost et al. [2] gave an algorithm (which is not implemented) to query graphs with a bounded treewidth in PPI networks (the treewidth of the query being involved in the time complexity). In this paper, we propose another algorithm for querying pattern in the shape of graphs, also based on dynamic programming and the color-coding technique. To transform graphs queries into trees without loss of informations, we use feedback vertex set coupled to a node duplication mecanism. Hence, our algorithm is FPT for querying graphs with a bounded size of their feedback vertex set. It gives an alternative to the treewidth parameter, which can be better or worst for a given query. We provide a python implementation which allows us to validate our implementation on real data. Especially, we retrieve some human queries in the shape of graphs into the fly PPI network. ",biology
550,seGOsa: Software environment for gene ontology-driven similarity assessment,Bioinformatics and Biomedicine,"In recent years there has been a growing trend towards the adoption of ontologies to support comprehensive, large-scale functional genomics research. This paper introduces seGOsa, a user-friendly cross-platform system to support large-scale assessment of Gene Ontology (GO)-driven similarity among gene products. Using information-theoretic approaches, the system exploits both topological features of the GO (i.e., between-term relationships in the hierarchy) and statistical features of the model organism databases annotated to the GO (i.e., term frequency) to assess functional similarity among gene products. Based on the assumption that the more information two terms share in common, the more similar they are, three GO-driven similarity measures (Resnik's, Lin's and Jiang's metrics) have been implemented to measure between-term similarity within each of the GO hierarchies. Meanwhile, seGOsa offers two approaches (simple and highest average similarity) to assessing the similarity between gene products based on the aggregation of between-term similarities. The program is freely available for non-profit use on request from the authors.",biology
551,Gene Expression Analysis Using Clustering,International Conference on Bioinformatics and Biomedical Engineering,"Data mining has become an important topic in effective analysis of gene expression data due to its wide application in the biomedical industry. In this paper, k-means clustering algorithm has been extensively studied for gene expression analysis. Since our purpose is to demonstrate the effectiveness of the k-means algorithm for a wide variety of data sets, we have chosen two pattern recognition data and thirteen microarray data sets with both overlapping and non-overlapping cluster boundaries, where the number of features/genes ranges from 4 to 7129 and number of sample ranges from 32 to 683. The number of clusters ranges from two to eleven. We use the clustering error rate (or, clustering accuracy) as evaluation metrics to measure the performance of k-means algorithm.",biology
552,Hilbert-Huang Transform for Analysis of Heart Rate Variability in Cardiac Health,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"This paper introduces a modified technique based on Hilbert-Huang transform (HHT) to improve the spectrum estimates of heart rate variability (HRV). In order to make the beat-to-beat (RR) interval be a function of time and produce an evenly sampled time series, we first adopt a preprocessing method to interpolate and resample the original RR interval. Then, the HHT, which is based on the empirical mode decomposition (EMD) approach to decompose the HRV signal into several monocomponent signals that become analytic signals by means of Hilbert transform, is proposed to extract the features of preprocessed time series and to characterize the dynamic behaviors of parasympathetic and sympathetic nervous system of heart. At last, the frequency behaviors of the Hilbert spectrum and Hilbert marginal spectrum (HMS) are studied to estimate the spectral traits of HRV signals. In this paper, two kinds of experiment data are used to compare our method with the conventional power spectral density (PSD) estimation. The analysis results of the simulated HRV series show that interpolation and resampling are basic requirements for HRV data processing, and HMS is superior to PSD estimation. On the other hand, in order to further prove the superiority of our approach, real HRV signals are collected from seven young health subjects under the condition that autonomic nervous system (ANS) is blocked by certain acute selective blocking drugs: atropine and metoprolol. The high-frequency power/total power ratio and low-frequency power/high-frequency power ratio indicate that compared with the Fourier spectrum based on principal dynamic mode, our method is more sensitive and effective to identify the low-frequency and high-frequency bands of HRV. ÂŠ 2011 IEEE.",biology
553,Hospital pricing estimation by Gaussian conditional random fields based regression on graphs,Bioinformatics and Biomedicine," - Accurate estimation of what a day in a hospital costs and what the hospital charges is of high interest to many parties, including health care providers, medical insurance companies, health researchers, and uninsured patients. The problem is complex, as the cost-to-charge ratio varies greatly from hospital to hospital and over time. In addition, the cost-tocharge ratio is often not reported, and in such cases group average values from similar hospitals are used. In this study we address the problem of estimating the cost-to-charge ratio at the hospital level by utilizing structured regression on a temporal graph of more than 4,000 hospitals, observed over 8 years, constructed from the National Inpatient Sample database. In the proposed approach, the cost-to-charge estimates at individual hospitals for a certain month obtained by an artificial neural network were used as unstructured components in the Gaussian Conditional Random Fields (GCRF) model. The diagnosis codes of treatments in each hospital were used to create a similarity metric that represents correlation among hospital specializations. The estimates of cost-to-charge ratio obtained using convex optimization of the GCRF parameters on the constructed graph were much better than those relying on group average based cost-to-charge estimates. In addition, cost-to-charge ratio estimates by our GCRF model outperformed regression by nonlinear artificial neural networks. ",biology
554,"Network-based inferring drug-disease associations from chemical, genomic and phenotype data",Bioinformatics and Biomedicine,"With the information of drug, disease phenotype and protein interactions accumulating rapidly, to investigate the relationships between drugs and diseases is a critical importance issue. Until recently, few studies attempt to discover drug-disease associations on a network basis. We integrate drug and phenotype information and protein interaction network together and apply a network propagation approach to infer and evaluate the likelihood of the probability between drug and disease based on gene expression profile. In the experiments, we adopt prostate cancer as our test data. We validate our results to the manually curated associations in Comparative Toxicogenomics Database. Our experimental studies show that our proposed method obtains high specificity and sensitivity (AUC=0.98) and clearly outperforms previous existing methods. Our proposed method discovers potential drug-disease associations that drew the attention of biologists and provides a new perspective for toxicogenomics and drug reposition evaluation.",biology
555,Sampling-based methods for a full characterization of energy landscapes of small peptides,Bioinformatics and Biomedicine,"Obtaining accurate representations of energy landscapes of biomolecules such as proteins and peptides is central to structure-function studies. Peptides are particularly interesting, as they exploit structural flexibility to modulate their biological function. Despite their small size, peptide modeling remains challenging due to the complexity of the energy landscape of such highly-flexible dynamic systems. Currently, only sampling-based methods can efficiently explore the conformational space of a peptide. In this paper, we suggest to combine two such methods to obtain a full characterization of energy landscapes of small yet flexible peptides. First, we propose a simplified version of the classical Basin Hopping algorithm to quickly reveal the meta-stable structural states of a peptide and the corresponding low-energy basins in the landscape. Then, we present several variants of a robotics-inspired algorithm, the Transition-based Rapidly-exploring Random Tree, to quickly determine transition state and transition path ensembles, as well as transition probabilities between meta-stable states. We demonstrate this combined approach on the terminally-blocked alanine.",biology
556,Guiding belief propagation using domain knowledge for protein-structure determination,International Conference on Bioinformatics,"A major bottleneck in high-throughput protein crystallography is producing protein-structure models from an electrondensity map. In previous work, we developed Acmi, a probabilistic framework for sampling all-atom protein-structure models. Acmi uses a fully connected, pairwise Markov random field to model the 3D location of each non-hydrogen atom in a protein. Since exact inference in this model is intractable, Acmi uses loopy belief propagation (BP) to calculate marginal probability distributions. In cases of approximation, BPs message-passing protocol becomes a crucial design decision. Previously, Acmi took a naive, round-robin protocol to sequentially process messages. Others have proposed informed methods for message scheduling by ranking messages based on the amount of new information they contain. These information-theoretic measures, however, fail in the highly connected, large output space domain of protein-structure inference. In this work, we develop a framework for using domain knowledge as a criterion for prioritizing messages in BP. Specifically, we show that using predictions of protein-disorder regions effectively guides BP in our task. Our results show that guiding BP using protein-disorder prediction improves the accuracy of marginal probability distributions and also produces more accurate, complete protein-structure models. Copyright ÂŠ 2010 ACM.",biology
557,SNOMED CT: Browsing the Browsers,International Conference on Bioinformatics," SNOMED CT is a complex ontology; sophisticated browsers are required to make it understandable and useful. We identified 23 SNOMED CT browsers that have been developed, and inspected 17. We enumerate and provide test criteria for a 'master list' of 143 browsing features supported by at least one inspected browser; future work will determine which of these features are implemented by individual browsers. Only 5 features were common to all 17 browsers; 89 were found in less than one third of browsers. We recommend that a core set of browsing features be defined and harmonized across browsers, particularly for text-to-concept search operations. ",biology
558,Minimum dominating sets in cell cycle specific protein interaction networks,Bioinformatics and Biomedicine,"Recently, scientists start to examine the dynamics of biological networks from a control theory perspective. Based on the determination of minimum dominating sets (MDSets), this paper investigated the properties associated with MDSet proteins in the context of the analysis of protein interaction networks specific to the yeast cell cycle. Statistically significant differences between MDSet and non-MDSet proteins were observed in terms of topological features, Gene Ontology-driven semantic similarities, and the number of protein domains associated with each protein. However, unlike previous studies, MDSet proteins were found to be enriched with essential genes. Furthermore, we constructed and analyzed a PPI network specific to the human cell cycle and highlighted that the distinction between MDSet and non-MDSet proteins is far more complex than that observed in yeast. The system used to determine a minimum dominating set in a protein interaction network was implemented as a user-friendly Java-based plugin for Cytoscape.",biology
559,Detecting privacy-sensitive events in medical text,International Conference on Bioinformatics," Recent US government initiatives have led to wide adoption of Electronic Health Records (EHRs). More and more health care institutions are storing patients' data in an electronic format. This emerging practice is posing several security-related risks because electronic data can easily be shared within and across institutions. So, it is important to design robust frameworks which will protect patients' privacy. In this report, we present a method to detect security-related (particularly drug abuse) events in medical text. Several applications can use this information to make the hospital systems more secure. For example, portions of the clinical reports which contain description of critical events can be encrypted so that it can be viewed only by selected individuals. ",biology
560,Robust peak detection and alignment of nanoLC-FT mass spectrometry data,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics", The following full text is an author's version which may differ from the publisher's version. For additional information about this publication click this link. http://hdl.handle.net/2066/84540 ,biology
561,Bayesian Segmental Models with Multiple Sequence Alignment Profiles for Protein Secondary Structure and Contact Map Prediction,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -In this paper, we develop a segmental semi-Markov model (SSMM) for protein secondary structure prediction which incorporates multiple sequence alignment profiles with the purpose of improving the predictive performance. The segmental model is a generalization of the hidden Markov model where a hidden state generates segments of various length and secondary structure type. A novel parameterized model is proposed for the likelihood function that explicitly represents multiple sequence alignment profiles to capture the segmental conformation. Numerical results on benchmark data sets show that incorporating the profiles results in substantial improvements and the generalization performance is promising. By incorporating the information from long range interactions in -sheets, this model is also capable of carrying out inference on contact maps. This is an important advantage of probabilistic generative models over the traditional discriminative approach to protein secondary structure prediction. The Web server of our algorithm and supplementary materials are available at http://public.kgi.edu/~wild/bsm.html. ",biology
562,PALMA: Perfect Alignments using Large Margin Algorithms,German Conference on Bioinformatics," Despite many years of research on how to properly align sequences in the presence of sequencing errors, alternative splicing and micro-exons, the correct alignment of mRNA sequences to genomic DNA is still a challenging task. We present a novel approach based on large margin learning that combines kernel based splice site predictions with common sequence alignment techniques. By solving a convex optimization problem, our algorithm - called PALMA - tunes the parameters of the model such that the true alignment scores higher than all other alignments. In an experimental study on the alignments of mRNAs containing artificially generated micro-exons, we show that our algorithm drastically outperforms all other methods: It perfectly aligns all 4358 sequences on an hold-out set, while the best other method misaligns at least 90 of them. Moreover, our algorithm is very robust against noise in the query sequence: when deleting, inserting, or mutating up to 50% of the query sequence, it still aligns 95% of all sequences correctly, while other methods achieve less than 36% accuracy. For datasets, additional results and a stand-alone alignment tool see http://www.fml.mpg.de/raetsch/projects/palma. ",biology
563,Automatic Summarization of Results from Clinical Trials,Bioinformatics and Biomedicine,"A central concern in Evidence Based Medicine (EBM) is how to convey research results effectively to practitioners. One important idea is to summarize results by key summary statistics that describe the effectiveness (or lack thereof) of a given intervention, specifically the absolute risk reduction (ARR) and number needed to treat (NNT). Manual summarization is slow and expensive, thus, with the exponential growth of the biomedical research literature, automated solutions are needed. In this paper, we present a novel method for automatically creating EBM-oriented summaries from research abstracts of randomly-controlled trials (RCTs). The system extracts descriptions of the treatment groups and outcomes, as well as various associated quantities, and then calculates summary statistics. Results on a hand-annotated corpus of research abstracts show promising, and potentially useful, results.",biology
564,Biclustering of Gene Expression Data Using PSO-GA Hybrid,International Conference on Bioinformatics and Biomedical Engineering,"The biclustering of gene expression data is an important technology for biologists and the biclustering problem is proven to be NP-hard. In this paper, a hybrid evolutionary optimization algorithm based on particle swarm and Genetic algorithms is presented to solve the biclustering problem. Additionally, this paper gives a comparison between hybrid algorithm, GA, and PSO. Experiments show that our method can beat other methods.",biology
565,The Austronesian Basic Vocabulary Database: From Bioinformatics to Lexomics,Evolutionary Bioinformatics," Phylogenetic methods have revolutionised evolutionary biology and have recently been applied to studies of linguistic and cultural evolution. However, the basic comparative data on the languages of the world required for these analyses is often widely dispersed in hard to obtain sources. Here we outline how our Austronesian Basic Vocabulary Database (ABVD) helps remedy this situation by collating wordlists from over 500 languages into one web-accessible database. We describe the technology underlying the ABVD and discuss the benefits that an evolutionary bioinformatic approach can provide. These include facilitating computational comparative linguistic research, answering questions about human prehistory, enabling syntheses with genetic data, and safe-guarding fragile linguistic information. ",biology
566,GoPubMed: ontology-based literature search applied to Gene Ontology and PubMed,German Conference on Bioinformatics," The biomedical literature grows at a tremendous rate, so that finding the relevant literature is becoming more and more difficult. To address this problem we introduce ontology-based literature search, which structures search results thorugh the categories of an ontology. We develop and implement GoPubMed, which submits keywords to PubMed, extracts GeneOntology-terms from the retrieved abstracts, and presents the relevant sub-ontology for browsing. For GoPubMed we develop a novel term extraction algorithm and evaluate its performance. GoPubMed is available at www.gopubmed.org ",biology
567,A Study of Hierarchical and Flat Classification of Proteins,IEEE/ACM Transactions on Computational Biology and Bioinformatics," - Automatic classification of proteins using machine learning is an important problem that has received significant attention in the literature. One feature of this problem is that expert-defined hierarchies of protein classes exist and can potentially be exploited to improve classification performance. In this article we investigate empirically whether this is the case for two such hierarchies. We compare multi-class classification techniques that exploit the information in those class hierarchies and those that do not, using logistic regression, decision trees, bagged decision trees, and support vector machines as the underlying base learners. In particular, we compare hierarchical and flat variants of ensembles of nested dichotomies. The latter have been shown to deliver strong classification performance in multi-class settings. We present experimental results for synthetic, fold recognition, enzyme classification, and remote homology detection data. Our results show that exploiting the class hierarchy improves performance on the synthetic data, but not in the case of the protein classification problems. Based on this we recommend that strong flat multi-class methods be used as a baseline to establish the benefit of exploiting class hierarchies in this area. ",biology
568,Margin-maximized redundancy-minimized SVM-RFE for diagnostic classification of mammograms,Bioinformatics and Biomedicine,"Classification techniques for digital mammography play an instrumental role in the diagnosis of breast cancer. Recent developments in the derivatives of support vector machines have shown to provide superior classification accuracy rates in comparison with other competing techniques. In this paper, we propose a new classification technique that is based on support vector machines with the additional properties of margin-maximization and redundancy-minimization in order to further increase the accuracy. We have conducted experiments on publicly available data set of mammograms and the empirical results indicated that our proposed technique performs superior to other previously proposed support vector machines-based techniques.",biology
569,"A Short Survey on Genetic Sequences, Chou’s Pseudo Amino Acid Composition and its Combination with Fuzzy Set Theory",The Open Bioinformatics Journal," The study of genetic sequences is of great importance in biology and medicine. Sequence analysis and taxonomy are two major fields of application of bioinformatics. In this survey, we present results concerning genetic sequences and Chou's pseudo amino acid composition as well as methodologies developed based on this concept along with elements of fuzzy set theory, and emphasize on fuzzy clustering and its application in analysis of genetic sequences. ",biology
570,Ensemble Learning Based on Active Example Selection for Solving Imbalanced Data Problem in Biomedical Data,Bioinformatics and Biomedicine,"The imbalanced data problem is popular in biomedical classification tasks. Since trained classifiers using imbalanced data are mostly derived from the majority class, their prediction performance is poor for the minority class. In this paper, we propose a novel ensemble learning method based on an active example selection algorithm to resolve the imbalanced data problem. To compensate a possible sub-optimal classifier, our proposed ensemble learning methods aggregates classifiers built by the active example selection algorithm. We implement this ensemble learning method based on the active example selection algorithm using incremental naive Bayes classifiers. Our empirical results show that we greatly improve the performance of classification models trained by five real world imbalanced biomedical data. The proposed ensemble learning methods outperforms other approaches by 0.03~0.15 in terms of AUC which solve imbalanced data problem.",biology
571,Using Global Sequence Similarity to Enhance Biological Sequence Labeling,Bioinformatics and Biomedicine,"Identifying functionally important sites from biological sequences, formulated as a biological sequence labeling problem, has broad applications ranging from rational drug design to the analysis of metabolic and signal transduction networks. In this paper, we present an approach to biological sequence labeling that takes into account the global similarity between biological sequences. Our approach combines unsupervised and supervised learning techniques. Given a set of sequences and a similarity measure defined on pairs of sequences, we learn a mixture of experts model by using spectral clustering to learn the hierarchical structure of the model and by using bayesian approaches to combine the predictions of the experts. We evaluate our approach on two important biological sequence labeling problems: RNA-protein and DNA-protein interface prediction problems. The results of our experiments show that global sequence similarity can be exploited to improve the performance of classifiers trained to label biological sequence data.",biology
572,A Max-Flow-Based Approach to the Identification of Protein Complexes Using Protein Interaction and Microarray Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The emergence of high-throughput technologies leads to abundant protein-protein interaction (PPI) data and microarray gene expression profiles, and provides a great opportunity for the identification of novel protein complexes using computational methods. By combining these two types of data, we propose a novel Graph Fragmentation Algorithm (GFA) for protein complex identification. Adapted from a classical max-flow algorithm for finding the (weighted) densest subgraphs, GFA first finds large (weighted) dense subgraphs in a protein-protein interaction network, and then, breaks each such subgraph into fragments iteratively by weighting its nodes appropriately in terms of their corresponding log-fold changes in the microarray data, until the fragment subgraphs are sufficiently small. Our tests on three widely used protein-protein interaction data sets and comparisons with several latest methods for protein complex identification demonstrate the strong performance of our method in predicting novel protein complexes in terms of its specificity and efficiency. Given the high specificity (or precision) that our method has achieved, we conjecture that our prediction results imply more than 200 novel protein complexes. © 2011 IEEE.",biology
573,Classification and Cluster Analysis of Complex Time-of-Flight Secondary Ion Mass Spectrometry for Biological Samples,International Conference on Bioinformatics," Identifying and separating subtly different biological samples is one of the most critical tasks in biological analysis. Time-of-flight secondary ion mass spectrometry (ToF-SIMS) is becoming a popular and important technique in the analysis of biological samples, because it can detect molecular information and characterize chemical composition. ToF-SIMS spectra of biological samples are enormously complex with large mass ranges and many peaks. As a result the classification and cluster analysis are challenging. This study presents a new classification algorithm, the most similar neighbor with a probability-based spectrum similarity measure (MSNPSSM), which uses all the information in the entire ToFSIMS spectra. MSN-PSSM is applied to automatically classify bacterial samples which are major causal agents of urinary tract infections. Experimental results show that MSN-PSSM is an accurate classification algorithm. It outperforms traditional techniques such as decision trees, principal component analysis (PCA) with discriminant function analysis (DFA), and soft independent modeling of class analogy (SIMCA). This study also applies a modern clustering algorithm, normalized spectral clustering, to automatically cluster the bacterial samples at the species level. Experimental results demonstrate that normalized spectral clustering is able to show accurate quantitative separations. It outperforms traditional techniques such as hierarchical clustering analysis, kmeans, and PCA with k-means. ",biology
574,Predicting Novel Human Gene Ontology Annotations Using Semantic Analysis,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The correct interpretation of many molecular biology experiments depends in an essential way on the accuracy and consistency of the existing annotation databases. Such databases are meant to act as repositories for our biological knowledge as we acquire and refine it. Hence, by definition, they are incomplete at any given time. In this paper, we describe a technique that improves our previous method for predicting novel GO annotations by extracting implicit semantic relationships between genes and functions. In this work, we use a vector space model and a number of weighting schemes in addition to our previous latent semantic indexing approach. The technique described here is able to take into consideration the hierarchical structure of the Gene Ontology (GO) and can weight differently GO terms situated at different depths. The prediction abilities of 15 different weighting schemes are compared and evaluated. Nine such schemes were previously used in other problem domains, while six of them are introduced in this paper. The best weighting scheme was a novel scheme, n2tn. Out of the top 50 functional annotations predicted using this weighting scheme, we found support in the literature for 84 percent of them, while 6 percent of the predictions were contradicted by the existing literature. For the remaining 10 percent, we did not find any relevant publications to confirm or contradict the predictions. The n2tn weighting scheme also outperformed the simple binary scheme used in our previous approach. © 2006 IEEE.",biology
575,Comparing Compressed Sequences for Faster Nucleotide BLAST Searches,IEEE/ACM Transactions on Computational Biology and Bioinformatics," Molecular biologists, geneticists, and other life scientists use the blast homology search package as their first step for discovery of information about unknown or poorly annotated genomic sequences. There are two main variants of blast: blastp for searching protein collections and blastn for nucleotide collections. Surprisingly, blastn has had very little attention; for example, the algorithms it uses do not follow those described in the 1997 blast paper (Altschul, Madden, Schaffer, Zhang, Zhang, Miller & Lipman 1997) and no exact description has been published. It is important that blastn is state-of-the-art: nucleotide collections such as GenBank dwarf the protein collections in size, they double in size almost yearly, and take many minutes to search on modern general-purpose workstations. This paper proposes significant improvements to the blastn algorithms. Each of our schemes is based on compressed bytepacked formats that allow queries and collection sequences to be compared four bases at a time, permitting very fast query evaluation using lookup tables and numeric comparisons. Our most significant innovations are two new, fast gapped alignment schemes that allow accurate sequence alignment without decompression of the collection sequences. Overall, our innovations more than double the speed of blastn with no effect on accuracy and have been integrated into our new version of blast that is freely available for download from http://www.fsa-blast.org/. ",biology
576,Personal genome privacy protection with feature-based hierarchical dual-stage encryption,International Conference on Bioinformatics,"Personal Genomic information is becoming increasingly important to both scientific research and clinical practice. However, security breach, misuse, or unintended disclosure of this information may result in severe privacy breaches. Traditional privacy preservation of personal genome information is implemented in an “all-or-none” manner, i.e., an entire genome being controlled as either fully accessible or fully inaccessible. In this paper, we propose a new fine-grained privacy protection method for flexible multi-level genome information protection and access. The method can make use of any user-defined hierarchical knowledge structure to define privacy levels and control cryptography-based hierarchical access. It also implements dual-stage encryptions to allow efficient definition, addition, and update of feature-based privacy protections. The experiments show that it can be effectively implemented to deal with real personal genome data sets in the future.",biology
577,Discovering Gene Networks with a Neural-Genetic Hybrid,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Recent advances in biology (namely, DNA arrays) allow an unprecedented view of the biochemical mechanisms contained within a cell. However, this technology raises new challenges for computer scientists and biologists alike, as the data created by these arrays is often highly complex. One of the challenges is the elucidation of the regulatory connections and interactions between genes, proteins and other gene products. In this paper, a novel method is described for determining gene interactions in temporal gene expression data using genetic algorithms combined with a neural network component. Experiments conducted on real-world temporal gene expression data sets confirm that the approach is capable of finding gene networks that fit the data. A further repeated approach shows that those genes significantly involved in interaction with other genes can be highlighted and hypothetical gene networks and circuits proposed for further laboratory testing. ÂŠ 2005 IEEE.",biology
578,A link prediction based unsupervised rank aggregation algorithm for informative gene selection,Bioinformatics and Biomedicine,"Informative Gene Selection is the process of identifying relevant genes that are significantly and differentially expressed in biological procedures. The microarray experiments conducted for this purpose usually implement only less than a hundred of samples to rank the relevance of over thousands of genes. Many irrelevant genes thus may gain statistical importance due to the randomness caused by the small sample problem, while relevant genes may lose focus in the same way. Overcoming such a problem goes beyond what a single microarray dataset can offer and stresses the use of multiple experiment results, which is defined as rank aggregation. In this paper, we propose a novel link prediction based rank aggregation algorithm for the purpose of informative gene selection. Each rank is transferred into a fully connected and weighted network, in which the nodes represent genes and the weights of links stand for priorities between connected nodes (genes). The integration of multiple gene ranks is then formulated as an optimization problem of link prediction on multiple networks, with criterion function favoring the maximization of weighted consensus among each network. We solve the problem through iterative estimation of weights and maximization of consensus among them. In the experimental evaluation, we demonstrate our method on the Prostate Cancer Dataset and compare it with other baseline methods. The results show that our link prediction based rank aggregation method remarkably outperforms all the compared methods, which proves the effectiveness of our framework in finding informative genes from multiple microarray experimental results.",biology
579,Binding Ontologies & Coding Systems to Electronic Health Records and Messages,International Conference on Bioinformatics," A major use of medical ontologies is to support coding systems for use in electronic healthcare records and messages. A key task is to define which codes are to be used where - to bind the terminology to the model of the medical record or message. To achieve this formally, it is necessary to recognise that the model of codes and information models are at a meta-level with respect to the underlying ontology. A methodology for defining a Terminology Binding Interface in OWL is presented which illustrates this point. It generalises methodologies that have been used in a successful test of the binding HL7 messages to SNOMED-CT codes. ",biology
580,GO for gene documents,International Conference on Bioinformatics,"Annotating genes and their products with Gene Ontology codes is an important area of research. One approach for doing this is to use the information available about these genes in the biomedical literature. Our goal, based on this approach, is to develop automatic methods for annotation that could supplement the expensive manual annotation processes currently in place. Using a set of Support Vector Machines (SVM) classifiers we were able to achieve Fscores of 0.48, 0.4 and 0.32 for codes of the molecular function, cellular component and biological process GO hierarchies respectively. We explore thresholding of SVM scores, the relationship of performance to hierarchy level and to the number of positives in the training sets. We find that hierarchy level is important especially for the molecular function and biological process hierarchies. We find that the cellular component hierarchy stands apart from the other two in many respects. This may be due to fundamental differences in link semantics. This research also exploits the hierarchical structures by defining and testing a relaxed criteria for classification correctness. Copyright 2006 ACM.",biology
581,Comparison of experimental and computed protein anisotropic temperature factors,Bioinformatics and Biomedicine,"Because of its appealing simplicity, the anisotropic network model (ANM) has been widely accepted and applied to study many molecular motion problems: such as the molecular mechanisms of GroEL-GroES function, allosteric changes in hemoglobin, ribosome motion, motor-protein motions, and conformational changes in general, etc. However, the validity of the ANM has not been closely examined. In this work, we use ANM to predict the anisotropic temperature factors of proteins. On the flip side, the rich, directional anisotropic temperature factor data available for hundreds of proteins in the protein data bank (PDB) are used as validation data to closely test the ANM model. The significance of this work is that it presents a timely, important evaluation of the model, shows the extent of its accuracy in reproducing experimental anisotropic temperature factors, and suggests ways to improve the model. An improved model will help us better understand the internal dynamics of proteins, which in turn can greatly expand the usefulness of the models, which has already been demonstrated in many applications.",biology
582,Iterative Dictionary Construction for Compression of Large DNA Data Sets,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Genomic repositories increasingly include individual as well as reference sequences, which tend to share long identical and near-identical strings of nucleotides. However, the sequential processing used by most compression algorithms, and the volumes of data involved, mean that these long-range repetitions are not detected. An order-insensitive, disk-based dictionary construction method can detect this repeated content and use it to compress collections of sequences. We explore a dictionary construction method that improves repeat identification in large DNA data sets. Our adaptation, Comrad, of an existing disk-based method identifies exact repeated content in collections of sequences with similarities within and across the set of input sequences. Comrad compresses the data over multiple passes, which is an expensive process, but allows Comrad to compress large data sets within reasonable time and space. Comrad allows for random access to individual sequences and subsequences without decompressing the whole data set. Comrad has no competitor in terms of the size of data sets that it can compress (extending to many hundreds of gigabytes) and, even for smaller data sets, the results are competitive compared to alternatives; as an example, 39 S. cerevisiae genomes compressed to 0.25 bits per base. © 2012 IEEE.",biology
583,Hierarchical Bayesian methods for integration of various types of genomics data,International Conference on Bioinformatics,"We propose methods to integrate data across several genomic platforms using a hierarchical Bayesian analysis framework that incorporates the biological relationships among the platforms to identify genes whose expression is related to clinical outcomes in cancer. This integrated approach combines information across all platforms, leading to increased statistical power in finding these predictive genes, and further provides mechanistic information about the manner of the effect on the outcome. We demonstrate the advantages of this approach (including improved estimation via effective estimate shrinkage) through a simulation, and finally we apply our method to a Glioblastoma Multiforme dataset and identify several genes significantly associated with patients' survival.",biology
584,Robust segmentation of biomedical figures for image-based document retrieval,Bioinformatics and Biomedicine,"Figures play an important role in illustrating concepts, methodology and results in biomedicai literature. However, figures in biomedicai literature are often composed of multiple subfigures (panels), which may illustrate diverse methodologies or results. Robust and accurate panel partitioning is crucial to support article categorization based on methods or experimental results and to provide the evidence source for derived assertions. But, it is a challenging task. In this paper, we present a comprehensive framework for harvesting multimodal panels in biomedicai literature, and demonstrate its application to protein-protein interaction (PPI)-related literature as a use case. A unique feature of our solution is that we combine pixel-level representations of images with figure captions. Our approach first analyzes figure captions to identify the label style used to mark panels. We then use pixel-level representations to partition a figure into a set of bounding boxes of connected components. We also perform a lexical analysis on the text within the figure to locate panel labels that match the caption analysis results. Finally, we estimate the optimal panel layout and use the layout to partition the figure. We tested our system on a dataset provided by the Molecular INTeraction database (MINT), and show that our approach surpasses pure caption-based and pure image-based approaches, achieving a 96.64% precision.",biology
585,Ontology for MicroRNA Target prediction in human cancer,International Conference on Bioinformatics,"The identification and characterization of important roles microRNAs (miRNAs) played in human cancer is an increasingly active area in medical informatics, and the prediction of miRNA target genes remains a challenging task to cancer researchers. We propose an innovative computing framework based on the Ontology for MicroRNA Target (OMIT) to facilitate knowledge acquisition from existing sources. The project aims to assist biologists in unraveling important roles of miRNAs in human cancer, and thus to help clinicians in making sound decisions when treating cancer patients. Copyright 2010 ACM.",biology
586,Spectral analysis of numerical exon and intron sequences,Bioinformatics and Biomedicine,Analysis of DNA sequences requires conversion of a base sequence to a numerical sequence. The choice of the numerical mapping of a DNA sequence affects how well its biological properties can be reflected in the numerical domain for the detection of regions of interest. This paper presents twelve one-sequence numerical representation methods and a discrete Fourier transform (DFT) based approach to extract the period-3 value of DNA sequences for classifying exon and intron sequences. Simulations are carried out using short sequences of the human genome; the results obtained indicate that the threshold value determined and consequently the classification performance is a function of the adopted numerical representation method and its window length.,biology
587,Cancer classification by sparse representation using microarray gene expression data,International Conference on Bioinformatics,"In this paper, a new method is proposed for cancer diagnosis using gene expression data by casting the classification problem as finding sparse representations of test samples with respect to training samples. The sparse representation is efficiently computed by lscr1-regularized least square. Numerical experiment shows that the new approach can match the best performance achieved by support vector machines (SVM). Sparse representation approach also has no need of model selection.",biology
588,Mining spatial gene expression data for association rules,Bioinformatics Research and Development," We analyse data from the Edinburgh Mouse Atlas GeneExpression Database (EMAGE) which is a high quality data source for spatio-temporal gene expression patterns. Using a novel process whereby generated patterns are used to probe spatially-mapped gene expression domains, we are able to get unbiased results as opposed to using annotations based predefined anatomy regions. We describe two processes to form association rules based on spatial configurations, one that associates spatial regions, the other associates genes. ",biology
589,Data Integration on Multiple Data Sets,Bioinformatics and Biomedicine,"A critical issue in dealing with voluminous records is that of data integration. Integration of data from two data bases has been studied well. For example, FEBRL is an excellent system for integrating two databases. Not much work has been conducted to integrate more than two databases. In practice, for example, health care networks have to often integrate many more databases than two. In this paper we offer hierarchical clustering based solutions to integrate multiple data sets. We also present experimental data that indicate that our algorithms perform well.",biology
590,A Hybrid Abbreviation Extraction Technique for Biomedical Literature,Bioinformatics and Biomedicine,"In this paper, we propose a novel technique to extract abbreviation combining natural language processing techniques and the Support Vector Machine (SVM) in biomedical literature. The proposed technique gives us the comparative advantages over others in the following aspects: 1) It incorporates lexical analysis techniques to supervised learning for extracting abbreviations. 2) It makes use of text chunking techniques to identify long forms of abbreviations. 3) It significantly improves Recall compared to other techniques. The experimental results show that our approach outperforms the leading abbreviation algorithms, Extract Abbrev, ALICE, and Acrophile, at least by 6% 13.9%, and 13.2% respectively, in both Precision and Recall on the Gold Standard Development corpus.",biology
591,Mutual Information Based Extrinsic Similarity for Microarray Analysis,International Conference on Bioinformatics,"Genes responding similarly to changing conditions are believed to be functionally related. Identification of such functional relations is crucial for annotation of unknown genes as well as the exploration of the underlying regulatory program. Gene expression profiling experiments provide noisy datasets about how cells respond to different experimental conditions. One way of analyzing these datasets is the identification of gene groups with similar expression patterns. A prevailing technique to find gene pairs with correlated expression profiles is to use linear measures like Pearson’s correlation coefficient or Euclidean distance. Similar genes are later compiled into a co-expression network to explore the system-level functionality of genes. However, the noise inherent in microarray datasets reduces the sensitivity of these measures and produces many spurious pairs with no real biological relevance. In this paper, we explore an extrinsic way of calculating similarity of two genes based on their relations with other genes. We show that ‘similar’ pairs identified by extrinsic measures overlap better with known biological annotations available in the Gene Ontology database. Our results also indicate that extrinsic measures are useful in enhancing the quality of co-expression networks and their functional subnetworks.",biology
592,A Hierarchical Classification Ant Colony Algorithm for Predicting Gene Ontology Terms,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","This paper proposes a novel Ant Colony Optimisation algorithm for the hierarchical problem of predicting protein functions using the Gene Ontology (GO). The GO structure represents a challenging case of hierarchical classification, since its terms are organised in a direct acyclic graph fashion where a term can have more than one parent- in contrast to only one parent in tree structures. The proposed method discovers an ordered list of classification rules which is able to predict all GO terms independently of their level. We have compared the proposed method against a baseline method, which consists of training classifiers for each GO terms individually, in five different ion-channel data sets and the results obtained are promising. ©Springer-Verlag Berlin Heidelberg 2009.",biology
593,Global Network Alignment in the Context of Aging,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Analogous to sequence alignment, network alignment (NA) can be used to transfer biological knowledge across species between conserved network regions. NA faces two algorithmic challenges: 1) Which cost function to use to capture similarities between nodes in different networks? 2) Which alignment strategy to use to rapidly identify high-scoring alignments from all possible alignments? We break down existing state-of-the-art methods that use both different cost functions and different alignment strategies to evaluate each combination of their cost functions and alignment strategies. We find that a combination of the cost function of one method and the alignment strategy of another method beats the existing methods. Hence, we propose this combination as a novel superior NA method. Then, since human aging is hard to study experimentally due to long lifespan, we use NA to transfer aging-related knowledge from well annotated model species to poorly annotated human. By doing so, we produce novel human aging-related knowledge, which complements currently available knowledge about aging that has been obtained mainly by sequence alignment. We demonstrate significant similarity between topological and functional properties of our novel predictions and those of known aging-related genes. We are the first to use NA to learn more about aging.",biology
594,Protein Design by Sampling an Undirected Graphical Model of Residue Constraints,IEEE/ACM Transactions on Computational Biology and Bioinformatics," - This paper develops an approach for designing protein variants by sampling sequences that satisfy residue constraints encoded in an undirected probabilistic graphical model. Due to evolutionary pressures on proteins to maintain structure and function, the sequence record of a protein family contains valuable information regarding position-specific residue conservation and coupling (or covariation) constraints. Representing these constraints with a graphical model provides two key benefits for protein design: a probabilistic semantics enabling evaluation of possible sequences for consistency with the constraints, and an explicit factorization of residue dependence and independence supporting efficient exploration of the constrained sequence space. We leverage these benefits in developing two complementary MCMC algorithms for protein design: constrained shuffling mixes wild-type sequences positionwise and evaluates graphical model likelihood, while component sampling directly generates sequences by sampling clique values and propagating to other cliques. We apply our methods to design WW domains. We demonstrate that likelihood under a model of wild-type WWs is highly predictive of foldedness of new WWs. We then show both theoretical and rapid empirical convergence of our algorithms in generating high-likelihood, diverse new sequences. We further show that these sequences capture the original sequence constraints, yielding a model as predictive of foldedness as the original one. ",biology
595,A two-stage machine learning approach for pathway analysis,Bioinformatics and Biomedicine,"Analysis of gene expression data has emerged as an important approach to discover active pathways related to biological phenotypes. Previous pathway analysis methods use all genes in a pathway for linking it to a particular phenotype. Using only a subset of informative genes, however, could better classify samples. Here, we propose a two-stage machine learning approach for pathway analysis. During the first stage, informative genes that can represent a pathway are selected using feature selection methods. These “representative genes” are mostly associated with the phenotype of interest. In the second stage, pathways are ranked based on their “representative genes” using classification methods. We applied our two-stage approach on three gene expression datasets. The results indicate our method does outperform methods that consider every gene in a pathway.",biology
596,Investigating Distance Metrics in Semi-supervised Fuzzy c-Means for Breast Cancer Classification,Computational Intelligence Methods for Bioinformatics and Biostatistics," In previous work, semi-supervised Fuzzy c-means (ssFCM) was used as an automatic classification technique to classify the Nottingham Tenovus Breast Cancer (NTBC) dataset as no method to do this currently exists. However, the results were poor when compared with semi-manual classification. It is known that the NTBC data is highly non-normal and it was suspected that this affected the poor results. This motivated a further investigation into alternative distance metrics to explore their effect on classification results. Mahalanobis, Euclidean and kernel-based distance metrics were used on 100 sets of randomly-selected labelled data. It was found that ssFCM with Euclidean distance successfully and automatically identified the six classes in close agreement with those of Soria et al. We showed that there is also high agreement in the key features that define the breast cancer classes with those of Soria et al. The superiority of Euclidean distance to Mahalanobis distance is unexpected as it can only generate spherical clusters while Mahalanobis distance can generate hyperellipsoidal ones including spherical ones. We expected Mahalanobis distance to generate the hyperellipsoidal clusters that would best fit NTBC data. ",biology
597,PairProSVM: Protein Subcellular Localization Based on Local Pairwise Profile Alignment and SVM,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The subcellular locations of proteins are important functional annotations. An effective and reliable subcellular localization method is necessary for proteomics research. This paper introduces a new method-PairProSVM-to automatically predict the subcellular locations of proteins. The profiles of all protein sequences in the training set are constructed by PSI-BLAST, and the pairwise profile alignment scores are used to form feature vectors for training a support vector machine (SVM) classifier. It was found that PairProSVM outperforms the methods that are based on sequence alignment and amino acid compositions even if most of the homologous sequences have been removed. PairProSVM was evaluated on Huang and Lis and Gardy et al.s protein data sets. The overall accuracies on these data sets reach 75.3 percent and 91.9 percent, respectively, which are higher than or comparable to those obtained by sequence alignment and composition-based methods. ÂŠ 2008 IEEE.",biology
598,Representing the MeSH in OWL: Towards a semi-automatic migration,International Conference on Bioinformatics," Due to the numerous health documents available on the Web, information retrieval remains problematic with existing tools. This paper is positioned within the context of the CISMeF project (acronym of Catalogue and Index of French-speaking Medical Sites) and of a future Semantic Web. In CISMeF the resources are described using a set of metadata based on a structured terminology which “encapsulates” the MeSH thesaurus in its French version. Now, the objective is to migrate the CISMeF terminology, and thus the MeSH thesaurus, to a formal ontology, so as to get a more powerful search tool. The paper presents the very first stage and results of this ongoing project, aiming at migrating the MeSH to OWL. It reports on the first steps, which have presently been done, concerning the automatic transformation of the terminology into OWLDL. First, the CISMeF terminology has been “formalized” in OWL. Then, the resulting OWL “ontology” has been imported under the Protégé editor which makes possible to check its consistency and its classification in using Racer. Finally, the paper concludes on the current results and encountered difficulties, and gives future work perspectives. ",biology
599,On DNA Numerical Representations for Period-3 Based Exon Prediction,International Conference on Bioinformatics,"Processing of DNA sequences using traditional digital signal processing methods requires their conversion from a character string into numerical sequences as a first step. Many representations introduced previously assign values to indicate the four DNA nucleotides A, C, G, and T that impose mathematical structures not present in the actual DNA sequence. In this paper, almost all existing methods are compared for the purpose of identifying protein coding regions, using the discrete Fourier transform (DFT) based spectral content measure to exploit period-3 behaviour in the exonic regions for the GENSCAN test set. False positive vs. sensitivity, receiver operating characteristic (ROC) curve and exonic nucleotides detected as false positive results all show that the two newly proposed numerical of DNA representations perform better than the well-known Z-curve, tetrahedron, and Voss representations, with 66-75% less processing. By comparison with Voss representation, the proposed paired numeric method can produce relative improvements of up to 12% in terms of prediction accuracy of exonic nucleotides at a 10% false positive rate using the GENSCAN test set.",biology
600,Uncovering Hidden Phylogenetic Consensus in Large Data Sets,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Many of the steps in phylogenetic reconstruction can be confounded by ""rogue"" taxa-taxa that cannot be placed with assurance anywhere within the tree, indeed, whose location within the tree varies with almost any choice of algorithm or parameters. Phylogenetic consensus methods, in particular, are known to suffer from this problem. In this paper, we provide a novel framework to define and identify rogue taxa. In this framework, we formulate a bicriterion optimization problem, the relative information criterion, that models the net increase in useful information present in the consensus tree when certain taxa are removed from the input data. We also provide an effective greedy heuristic to identify a subset of rogue taxa and use this heuristic in a series of experiments, with both pathological examples from the literature and a collection of large biological data sets. As the presence of rogue taxa in a set of bootstrap replicates can lead to deceivingly poor support values, we propose a procedure to recompute support values in light of the rogue taxa identified by our algorithm; applying this procedure to our biological data sets caused a large number of edges to move from ""unsupported"" to ""supported"" status, indicating that many existing phylogenies should be recomputed and reevaluated to reduce any inaccuracies introduced by rogue taxa. We also discuss the implementation issues encountered while integrating our algorithm into RAxML v7.2.7, particularly those dealing with scaling up the analyses. This integration enables practitioners to benefit from our algorithm in the analysis of very large data sets (up to 2,500 taxa and 10,000 trees, although we present the results of even larger analyses). © 2011 IEEE.",biology
601,Improved Protein-ligand Prediction Using Kernel Weighted Canonical Correlation Analysis,Ipsj Transactions on Bioinformatics," Protein-ligand interaction prediction plays an important role in drug design and discovery. However, wet lab procedures are inherently time consuming and expensive due to the vast number of candidate compounds and target genes. Hence, computational approaches became imperative and have become popular due to their promising results and practicality. Such methods require high accuracy and precision outputs for them to be useful, thus, the problem of devising such an algorithm remains very challenging. In this paper we propose an algorithm employing both support vector machines (SVM) and an extension of canonical correlation analysis (CCA). Following assumptions of recent chemogenomic approaches, we explore the effects of incorporating bias on similarity of compounds. We introduce kernel weighted CCA as a means of uncovering any underlying relationship between similarity of ligands and known ligands of target proteins. Experimental results indicate statistically significant improvement in the area under the ROC curve (AUC) and F-measure values obtained as opposed to those gathered when only SVM, or SVM with kernel CCA is employed, which translates to better quality of prediction. ",biology
602,A Log-Linear Graphical Model for inferring genetic networks from high-throughput sequencing data,Bioinformatics and Biomedicine,"Gaussian graphical models are often used to infer gene networks based on microarray expression data. Many scientists, however, have begun using high-throughput sequencing technologies to measure gene expression. As the resulting high-dimensional count data consists of counts of sequencing reads for each gene, Gaussian graphical models are not optimal for modeling gene networks based on this discrete data. We develop a novel method for estimating high-dimensional Poisson graphical models, the Log-Linear Graphical Model, allowing us to infer networks based on high-throughput sequencing data. Our model assumes a pair-wise Markov property: conditional on all other variables, each variable is Poisson. We estimate our model locally via neighborhood selection by fitting 1-norm penalized log-linear models. Additionally, we develop a fast parallel algorithm permitting us to fit our graphical model to high-dimensional genomic data sets. We illustrate the effectiveness of our methods for recovering network structure from count data through simulations and a case study on breast cancer microRNA networks. © 2012 IEEE.",biology
603,DNA Deformation Energy as an Indirect Recognition Mechanism in Protein-DNA Interactions,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Proteins that bind to specific locations in genomic DNA control many basic cellular functions. Proteins detect their binding sites using both direct and indirect recognition mechanisms. Deformation energy, which models the energy required to bend DNA from its native shape to its shape when bound to a protein, has been shown to be an indirect recognition mechanism for one particular protein, Integration Host Factor (IMF). This work extends the analysis of deformation to two other DNA-binding proteins, CRP and SRF, and two endonudeases, I-Crel and I-Ppol. Known binding sites for all five proteins showed statistically significant differences in mean deformation energy as compared to random sequences. Binding sites for the three DNA-binding proteins and one of the endonucleases had mean deformation energies lower than random sequences. Binding sites for I-Ppol had mean deformation energy higher than random sequences. Classifiers that were trained using the deformation energy at each base pair step showed good cross-validated accuracy when classifying unseen sequences as binders or nonbinders. These results support DNA deformation energy as an indirect recognition mechanism across a wider range of DNA-binding proteins. Deformation energy may also have a predictive capacity for the underlying catalytic mechanism of DNA-binding enzymes. ÂŠ 2007 IEEE.",biology
604,Automatic Time Gain Compensation in Ultrasound Imaging System,International Conference on Bioinformatics and Biomedical Engineering,"Many automatic time gain compensations are proposed, but most of them depart from the assumption that ultrasound signals are homogenous attenuated across examined tissues, which is often violated. In real ultrasound system, the large attenuation variation between different types of tissues confused the attenuation compensation. In this paper, considering the frequency shift effect, we propose the quadratic least square fitting to estimate the attenuation profile. Further, as for the large attenuation variations in image, especially around the anechoic regions, we proposed a new algorithm by adding two power functions to control the gain range and reduce the affluence of overwhelming gain value caused by noise and edges around anechoic regions. Test results of phantoms and ultrasound images show the new algorithm could effectively adapt to the large attenuation variation and provide a desired uniform image.",biology
605,An ontology-based MicroRNA knowledge sharing and acquisition framework,Bioinformatics and Biomedicine,"MicroRNAs (miRNAs) play important roles in various biological processes by regulating their target genes. Therefore, miRNAs are closely associated with development, diagnosis, and prognosis for many diseases. The prediction of miRNA targets remains a challenging task for biologists because it involves an extremely large amount of data sources to be explored: to manually integrate information of identified targets and related information from various sources is time-consuming and error-prone; most of all, it is subject to biologists' limited prior knowledge. In this paper we investigated an ontology-based knowledge sharing framework to assist biologists in unraveling important roles of miRNAs in human disease in an automated and more efficient manner, (i) We developed the very first domain-specific ontologies in the miRNA field, Ontology for MicroRNA Target (OMIT), (ii) According to the global metadata model defined in ontologies, heterogeneous data sources were annotated and seamlessly integrated and stored into a central Resource Description Framework (RDF) data repository, (iii) We then enabled ontology-based queries, instead of traditional SQL queries, by inferring new statements from RDF data triples. Consequently we were able to acquire hidden knowledge originally implicit and unclear, yet critical, to biologists.",biology
606,On the Classification of a Small Imbalanced Cytogenetic Image Database,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -Solving a multiclass classification task using a small imbalanced database of patterns of high dimension is difficult due to the curse-of-dimensionality and the bias of the training toward the majority classes. Such a problem has arisen while diagnosing genetic abnormalities by classifying a small database of fluorescence in situ hybridization signals of types having different frequencies of occurrence. We propose and experimentally study using the cytogenetic domain two solutions to the problem. The first is hierarchical decomposition of the classification task, where each hierarchy level is designed to tackle a simpler problem which is represented by classes that are approximately balanced. The second solution is balancing the data by up-sampling the minority classes accompanied by dimensionality reduction. Implemented by the naive Bayesian classifier or the multilayer perceptron neural network, both solutions have diminished the problem and contributed to accuracy improvement. In addition, the experiments suggest that coping with the smallness of the data is more beneficial than dealing with its imbalance. ",biology
607,An Ensemble Model for Mobile Device based Arrhythmia Detection,International Conference on Bioinformatics," Recent advances in smart mobile device technology have resulted in global availability of portable computing devices capable of performing many complex functions. With the ultimate intent of promoting human's well-being, mobile device based arrhythmia detection (MAD) has attracted lots of attention recently. Without any guidance or supervision from experts, the performance of arrhythmia detection is usually unsatisfactory. Supervised learning can learn from labeled cardiac cycles to detect arrhythmias for each mobile device user if enough training data is provided. However, it is time-consuming, costly and sometimes impossible to let experts annotate enough training data for each user. To tackle this problem, we take advantage of publicly available and well annotated data to infer knowledge which can be treated as experts for MAD. To reduce the space usage of the framework, we extract from each source of labeled data an expert model, which consists of a task-independent individual characteristic vector and a task-related preference vector. Multiple experts are then integrated into an ensemble model for arrhythmia detection. Both space and time complexities of this proposed approach are theoretically analyzed and experimentally examined. To evaluate the performance of the method, we implement it on the MIT-BIH Arrhythmia Dataset and compare it with seven state-of-the-art methods in the area. Extensive experimental results show that the proposed algorithm outperforms all the baseline methods, which validates the e ectiveness of the proposed algorithm in MAD. ",biology
608,Biclustering Algorithms for Biological Data Analysis: A Survey,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -A large number of clustering approaches have been proposed for the analysis of gene expression data obtained from microarray experiments. However, the results from the application of standard clustering methods to genes are limited. This limitation is imposed by the existence of a number of experimental conditions where the activity of genes is uncorrelated. A similar limitation exists when clustering of conditions is performed. For this reason, a number of algorithms that perform simultaneous clustering on the row and column dimensions of the data matrix has been proposed. The goal is to find submatrices, that is, subgroups of genes and subgroups of conditions, where the genes exhibit highly correlated activities for every condition. In this paper, we refer to this class of algorithms as biclustering. Biclustering is also referred in the literature as coclustering and direct clustering, among others names, and has also been used in fields such as information retrieval and data mining. In this comprehensive survey, we analyze a large number of existing approaches to biclustering, and classify them in accordance with the type of biclusters they can find, the patterns of biclusters that are discovered, the methods used to perform the search, the approaches used to evaluate the solution, and the target applications. ",biology
609,SideEffectPTM: an unsupervised topic model to mine adverse drug reactions from health forums,International Conference on Bioinformatics," Automatic discovery of medical knowledge using data mining has great potential benefit in improving population health and reducing healthcare cost. Discovering adverse drug reaction (ADR) is especially important because of the significant morbidity of ADRs to patients. Recently, more and more patients describe the ADRs they experienced and seek for help through online health forums, creating great opportunities for these forums to discover previously unknown ADRs. In this paper, we propose a novel unsupervised approach to tap into the increasingly available health forums to mine the side effect symptoms of drugs mentioned by forum users. Our approach is based on a novel probabilistic mixture model of symptoms, where the side effect symptoms and disease symptoms are explicitly modeled with two separate component models, and discovery of side effect symptoms can be achieved in an unsupervised way through fitting the mixture model to the forum data. Extensive experiments on online health forums demonstrate that our proposed model is effective for discovering the reported ADRs on forums in a completely unsupervised way. The mined knowledge using our model is directly useful for increasing our understanding of more challenging ADRs, such as long-term side effects, drug-drug interactions, and rare side effects. Since our approach is unsupervised, it can be applied to mining large amounts of growing forum data to discover new knowledge about ADRs, helping many patients become aware of possible ADRs. ",biology
610,Using rotation forest for protein fold prediction problem: an empirical study,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","Recent advancement in the pattern recognition field has driven many classification algorithms being implemented to tackle protein fold prediction problem. In this paper, a newly introduced method called Rotation Forest for building ensemble of classifiers based on bootstrap sampling and feature extraction is implemented and applied to challenge this problem. The Rotation Forest is a straight forward extension of bagging algorithms which aims to promote diversity within the ensemble through feature extraction by using Principle Component Analysis (PCA). We compare the performance of the employed method with other Meta classifiers that are based on boosting and bagging algorithms, such as: AdaBoost.M1, LogitBoost, Bagging and Random Forest. Experimental results show that the Rotation Forest enhanced the protein folding prediction accuracy better than the other applied Meta classifiers, as well as the previous works found in the literature. © 2010 Springer-Verlag Berlin Heidelberg.",biology
611,Feature Based Representation and Detection of Transcription Factor Binding Sites,German Conference on Bioinformatics," The prediction of transcription factor binding sites is an important problem, since it reveals information about the transcriptional regulation of genes. A commonly used representation of these sites are position speci c weight matrices which show weak predictive power. We introduce a feature-based modelling approach, which is able to deal with various kind of biological properties of binding sites and models them via Bayesian belief networks. The presented results imply higher model accuracy in contrast to the PSSM approach. ",biology
612,Integration of multiple annotators by aggregating experts and filtering novices,Bioinformatics and Biomedicine,"Learning from noisy labels obtained from multiple annotators and without access to any true labels is an increasingly important problem in bioinformatics and biomedicine. In our method, this challenge is addressed by iteratively filtering low-quality annotators and estimating the consensus labels based only on the remaining experts that provide higher-quality annotations. Experiments on biomedical text classification and CASP9 protein disorder prediction tasks provide evidence that the proposed algorithm is more accurate than the majority voting and previously developed multi-annotator approaches. The benefit of using the new method is particularly large when low-quality annotators dominate. Moreover, the new algorithm also suggests the most relevant annotators for each instance, thus paving the way for understanding the behaviors of each annotator and building more reliable predictive models for bioinformatics applications.",biology
613,Graphical Models of Residue Coupling in Protein Families,IEEE/ACM Transactions on Computational Biology and Bioinformatics," Identifying residue coupling relationships within a protein family can provide important insights into intrinsic molecular processes, and has significant applications in modeling structure and dynamics, understanding function, and designing new or modified proteins. We present the first algorithm to infer an undirected graphical model representing residue coupling in protein families. Such a model serves as a compact description of the joint amino acid distribution, and can be used for predictive (will this newly designed protein be folded and functional?), diagnostic (why is this protein not stable or functional?), and abductive reasoning (what if I attempt to graft features of one protein family onto another?). Unlike current correlated mutation algorithms that are focused on assessing dependence, which can conflate direct and indirect relationships, our algorithm focuses on assessing independence, which modularizes variation and thus enables efficient reasoning of the types described above. Further, our algorithm can readily incorporate, as priors, hypotheses regarding possible underlying mechanistic/energetic explanations for coupling. The resulting approach constitutes a powerful and discriminatory mechanism to identify residue coupling from protein sequences and structures. Analysis results on the G-protein coupled receptor (GPCR) and PDZ domain families demonstrate the ability of our approach to effectively uncover and exploit models of residue coupling. ",biology
614,Multilabel Learning via Random Label Selection for Protein Subcellular Multilocations Prediction,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Prediction of protein subcellular localization is an important but challenging problem, particularly when proteins may simultaneously exist at, or move between, two or more different subcellular location sites. Most of the existing protein subcellular localization methods are only used to deal with the single-location proteins. In the past few years, only a few methods have been proposed to tackle proteins with multiple locations. However, they only adopt a simple strategy, that is, transforming the multilocation proteins to multiple proteins with single location, which does not take correlations among different subcellular locations into account. In this paper, a novel method named random label selection (RALS) (multilabel learning via RALS), which extends the simple binary relevance (BR) method, is proposed to learn from multilocation proteins in an effective and efficient way. RALS does not explicitly find the correlations among labels, but rather implicitly attempts to learn the label correlations from data by augmenting original feature space with randomly selected labels as its additional input features. Through the fivefold cross-validation test on a benchmark data set, we demonstrate our proposed method with consideration of label correlations obviously outperforms the baseline BR method without consideration of label correlations, indicating correlations among different subcellular locations really exist and contribute to improvement of prediction performance. Experimental results on two benchmark data sets also show that our proposed methods achieve significantly higher performance than some other state-of-the-art methods in predicting subcellular multilocations of proteins. © 2004-2012 IEEE.",biology
615,Probabilistic topic modeling for genomic data interpretation,Bioinformatics and Biomedicine,"Recently, the concept of a species containing both core and distributed genes, known as the supra- or pangenome theory, has been introduced. In this paper, we aim to develop a new method that is able to analyze the genome-level composition of DNA sequences, in order to characterize a set of common genomic features shared by the same species and tell their functional roles. To achieve this end, we firstly apply a composition-based approach to break down DNA sequences into sub-reads called the `N-mer' and represent the sequences by N-mer frequencies. Then, we introduce the Latent Dirichlet Allocation (LDA) model to study the genome-level statistic patterns (a.k.a. latent topics) of the `N-mer' features. Each estimated latent topic represents a certain component of the whole genome. With the help of the BioJava toolkit, we access to the gene region information of reference sequences from the NCBI database. We use our data mining framework to investigate two areas: 1) do strains within species share similar core and distributed topics? and 2) do genes with similar functional roles contain similar latent topics? After studying the mutual information between latent topics and gene regions, we provide examples of each, where the BioCyc database is used to correlate pathway and reaction information to the genes. The examples demonstrate the effectiveness of proposed method.",biology
616,Ontology-Based MEDLINE Document Classification,International Conference on Bioinformatics,"An increasing and overwhelming amount of biomedical information is available in the research literature mainly in the form of free-text. Biologists need tools that automate their information search and deal with the high volume and ambiguity of free-text. Ontologies can help automatic information processing by providing standard concepts and information about the relationships between concepts. The Medical Subject Headings (MeSH) ontology is already available and used by MEDLINE indexers to annotate the conceptual content of biomedical articles. This paper presents a domain-independent method that uses the MeSH ontology inter-concept relationships to extend the existing MeSH-based representation of MEDLINE documents. The extension method is evaluated within a document triage task organized by the Genomics track of the 2005 Text REtrieval Conference (TREC). Our method for extending the representation of documents leads to an improvement of 17% over a non-extended baseline in terms of normalized utility, the metric defined for the task. The SVM light software is used to classify documents.",biology
617,Semi-supervised feature learning from clinical text,Bioinformatics and Biomedicine,"This paper is focused on the automated identification of the clinical free-text records that contain useful information (e.g. symptoms, modifiers, diagnosis, etc) of a certain disease. We introduce a novel semi-supervised machine learning algorithm to address this problem, by training the set covering machine in a bootstrapping procedure. The advantage of the proposed technique is that not only can it find the documents of interest more accurately than searching based on diagnostic codes, the features it learned could also be directly used as a knowledge representation of the given topic and to assist either further machine learning algorithms or manual post-processing and analysis.",biology
618,Classification of neonatal amplitude-integrated EEG using random forest model with combined feature,Bioinformatics and Biomedicine,"Amplitude integrated electroencephalogram (aEEG), a cerebral function monitoring method, is widely used in response to the clinical needs for continuous EEG monitoring. The focus work of this paper is presenting a novel combined feature set of aEEG and applying random forest (RF) method to identify the normal and abnormal aEEG tracing. To that end, a complete experimental evaluation was conducted on 282 aEEG tracing cases (209 normal and 73 abnormal infants). Instead of the traditional aEEG signal processing and diagnosing methods only based on linear features, we considered both statistical and non-linear features. In our experiments, we extracted and combined different types of features for integrated and segmented signals. The experiments examined the RF algorithmic issues including parameter optimization, segmentation of data and imbalanced datasets processing. The performance of the RF was compared to five commonly used classifiers. The result shows that classification accuracy of our method is up to 91.46%. This also indicates our combined feature set is effective for aEEG classification. Besides, the RF-based method can reach exceptional specificity. This novel method to automatically detect aEEG could help medical staff to monitor the progress of infants at all times.",biology
619,Graph-theoretic analysis of epileptic seizures on scalp EEG recordings,International Conference on Bioinformatics," physiological changes caused by the seizures. Clinical evaluation, identi cation of epileptic seizures, and localization of epileptic focus signi cantly rely on monitoring and analysis of long-term electroencephalographic (EEG) signals and bene t from intensive video-EEG monitoring. Large numbers of multi-channel EEG signals are visually analyzed by neurologists with a goal of understanding when and where the seizures start and how they propagate within the brain. However, there are two main disadvantages of visual analysis of EEG signals: it is time-consuming and prone to subjectivity. Therefore, automation of the detection of the underlying brain dynamics in EEG signals is signi cant in order to obtain fast and objective EEG analysis. ",biology
620,Gaussian quadrature formulae for arbitrary positive measures.,Evolutionary Bioinformatics," We present computational methods and subroutines to compute Gaussian quadrature integration formulas for arbitrary positive measures. For expensive integrands that can be factored into well-known forms, Gaussian quadrature schemes allow for efficient evaluation of high-accuracy and -precision numerical integrals, especially compared to general ad hoc schemes. In addition, for certain well-known density measures (the normal, gamma, log-normal, Student's t, inversegamma, beta, and Fisher's F) we present exact formulae for computing the respective quadrature scheme. Availability: Source code is freely available online as a C-linkable ISO C++ library under a BSD-style license from http://www.fernandes.org/gaussqr. The library may be built using single, double, or extended precision arithmetic. ",biology
621,Joint Classification and Pairing of Human Chromosomes,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"We reexamine the problems of computer-aided classification and pairing of human chromosomes, and propose to jointly optimize the solutions of these two related problems. The combined problem is formulated into one of optimal three-dimensional assignment with an objective function of maximum likelihood. This formulation poses two technical challenges: 1) estimation of the posterior probability that two chromosomes form a pair and the pair belongs to a class and 2) good heuristic algorithms to solve the three-dimensional assignment problem which is NP-hard. We present various techniques to solve these problems. We also generalize our algorithms to cases where the cell data are incomplete as often encountered in practice. ÂŠ 2005 IEEE.",biology
622,No-Reference Compression of Genomic Data Stored in FASTQ Format,Bioinformatics and Biomedicine,"In this paper, we propose a system to compress Next Generation Sequencing (NGS) information stored in a FASTQ file. A FASTQ file contains text, DNA read and quality information for millions or billions of reads. The proposed system first parses the FASTQ file into its component fields. In a partial first pass it gathers statistics which are then used to choose a representation for each field that can give the best compression. Text data is further parsed into repeating and variable components and entropy coding is used to compress the latter. Similarly, Markov encoding and repeat finding based methods are used for DNA read compression. Finally, we propose several run length based methods to encode quality data choosing the method that gives the best performance for a given set of quality values. The compression system provides features for lossless and nearly lossless compression as well as compressing only read and read + quality data. We compare its performance to bzip2 text compression utility and an existing benchmark algorithm. We observe that the performance of the proposed system is superior to that of both the systems.",biology
623,Metasample-Based Sparse Representation for Tumor Classification,IEEE/ACM Transactions on Computational Biology and Bioinformatics," A reliable and accurate identification of the type of tumors is crucial to the proper treatment of cancers. In recent years, it has been shown that sparse representation (SR) by l1 -norm minimization is robust to noise, outliers and even incomplete measurements, and SR has been successfully used for classification. This paper presents a new SR based method for tumor classification using gene expression data. A set of metasamples are extracted from the training samples, and then an input testing sample is represented as the linear combination of these metasamples by l1-regularized least square method. Classification is achieved by using a discriminating function defined on the representation coefficients. Since l1-norm minimization leads to a sparse solution, the proposed method is called metasample based SR classification (MSRC). Extensive experiments on publicly available gene expression datasets show that MSRC is efficient for tumor classification, achieving higher accuracy than many existing representative schemes. ",biology
624,Parallel Implementation of MAFFT on CUDA-Enabled Graphics Hardware,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Multiple sequence alignment (MSA) constitutes an extremely powerful tool for many biological applications including phylogenetic tree estimation, secondary structure prediction, and critical residue identification. However, aligning large biological sequences with popular tools such as MAFFT requires long runtimes on sequential architectures. Due to the ever increasing sizes of sequence databases, there is increasing demand to accelerate this task. In this paper, we demonstrate how graphic processing units (GPUs), powered by the compute unified device architecture (CUDA), can be used as an efficient computational platform to accelerate the MAFFT algorithm. To fully exploit the GPUs capabilities for accelerating MAFFT, we have optimized the sequence data organization to eliminate the bandwidth bottleneck of memory access, designed a memory allocation and reuse strategy to make full use of limited memory of GPUs, proposed a new modified-run-length encoding (MRLE) scheme to reduce memory consumption, and used high-performance shared memory to speed up I/O operations. Our implementation tested in three NVIDIA GPUs achieves speedup up to 11.28 on a Tesla K20m GPU compared to the sequential MAFFT 7.015.",biology
625,How Little Do We Actually Know? On the Size of Gene Regulatory Networks,IEEE/ACM Transactions on Computational Biology and Bioinformatics," Nowadays, we have whole-genome sequences for more than more than two thousand species available for download from the NCBI databases. Ongoing improvement of DNA sequencing technology will further feed this trend. However, the availability of sequence information is only the rst step in understanding how cells survive, reproduce and adjust their behavior. The molecular biological mechanisms, which control organized development and adaptation of complex organisms still remain widely undetermined. Transcriptional gene regulation is one of the key players here. The direct juxtaposition of the total number of sequenced species to the handful of model organisms with known regulations is astonishing. Recently, we investigated how little we even know about these model organisms. Our aim was to predict the sizes of the whole-organism regulatory networks of seven species. In particular, we provided a statistical lower bound for the expected number of regulations. For E. coli we estimate at most 37% of the expected gene regulatory interactions to be already discovered, 24% for B. subtilis, and < 3% for human respectively. We conclude that even for our best-researched model organisms we still lack substantial understanding of fundamental molecular control mechanisms, at least on a large scale. ",biology
626,Exploring Alternative Splicing Features Using Support Vector Machines,Bioinformatics and Biomedicine,"Alternative splicing is a mechanism for generating different gene transcripts (called isoforms) from the same genomic sequence. Finding alternative splicing events experimentally is both expensive and time consuming. Computational methods, in general, and machine learning algorithms,in particular, can be used to complement experimental methods in the process of identifying alternative splicing events. In this paper, we explore the predictive power of a rich set of features that have been experimentally shown to affect alternative splicing. We use these features to build support vector machine (SVM) classifiers for distinguishing between alternatively spliced exons and constitutive exons.Our results show that simple linear SVM classifiers built from a rich set of features give results comparable to those of more sophisticated SVM classifiers that use more basic sequence features. Furthermore, we use feature selection methods to identify computationally the most informative features for the prediction problem considered.",biology
627,"Scalable, updatable predictive models for sequence data",Bioinformatics and Biomedicine,"The emergence of data rich domains has led to an exponential growth in the size and number of data repositories, offering exciting opportunities to learn from the data using machine learning algorithms. In particular, sequence data is being made available at a rapid rate. In many applications, the learning algorithm may not have direct access to the entire dataset because of a variety of reasons such as massive data size or bandwidth limitation. In such settings, there is a need for techniques that can learn predictive models (e.g., classifiers) from large datasets without direct access to the data. We describe an approach to learn from massive sequence datasets using statistical queries. Specifically we show how Markov Models and Probabilistic Suffix Trees (PSTs) can be constructed from sequence databases that answer only a class of count queries. We analyze the query complexity (a measure of the number of queries needed) for constructing classifiers in such settings and outline some techniques to minimize the query complexity. We also show how some of the models can be updated in response to addition or deletion of subsets of sequences from the underlying sequence database.",biology
628,Implementation of a wearerable real-time system for physical activity recognition based on Naive Bayes classifier,International Conference on Bioinformatics,"In this paper, we implement a wearable real-time system on the Sun SPOT wireless sensors with Naive Bayes algorithm to recognize physical activity. Naive Bayes algorithm is demonstrated to work better than other algorithms both in accuracy performance and computational time in this particular application. 20 Hz is selected as the sampling rate. In terms of sensor location, one sensor attached to the thigh with 87.55% overall accuracy provides the most useful information than the shank or the chest. If two sensors are available, the combination of attaching them to the left thigh and the right thigh respectively is demonstrated to be optimal solution for recognizing physical activity, with 90.52% overall accuracy.",biology
629,Learning the Topological Properties of Brain Tumors,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"This work presents a graph-based representation (a.k.a., cell-graph) of histopathological images for automated cancer diagnosis by probabilistically assigning a link between a pair of cells (or cell clusters). Since the node set of a cell-graph can include a cluster of cells as well as individual ones, it enables working with low-cost, low-magnification photomicrographs. The contributions of this work are twofold. First, it is shown that without establishing a pairwise spatial relation between the cells (i.e., the edges of a cellgraph), neither the spatial distribution of the cells nor the texture analysis of the images yields accurate results for tissue level diagnosis of brain cancer called malignant glioma. Second, this work defines a set of global metrics by processing the entire cell-graph to capture tissue level information coded into the histopathological images. In this work, the results are obtained on the photomicrographs of 646 archival brain biopsy samples of 60 different patients. It is shown that the global metrics of cell-graphs distinguish cancerous tissues from noncancerous ones with high accuracy (at least 99 percent accuracy for healthy tissues with lower cellular density level, and at least 92 percent accuracy for benign tissues with similar high cellular density level such as nonneoplastic reactive/inflammatory conditions). © 2005 IEEE.",biology
630,Prediction of protein interactions on HIV-1-human PPI data using a novel closure-based integrated approach,International Conference on Bioinformatics, Discovering Protein-Protein Interactions (PPI) is a new interesting challenge in computational biology. Identifying interactions among proteins was shown to be useful for finding new drugs and preventing several kinds of diseases. The identification of interactions between HIV-1 proteins and Human proteins is a particular PPI problem whose study might lead to the discovery of drugs and important interactions responsible for AIDS. We present the FIST algorithm for extracting hierarchical bi-clusters and minimal covers of association rules in one process. This algorithm is based on the frequent closed itemsets framework to efficiently generate a hierarchy of conceptual clusters and non-redundant sets of association rules with supporting object lists. Experiments conducted on a HIV-1 and Human proteins interaction dataset show that the approach efficiently identifies interactions previously predicted in the literature and can be used to predict new interactions based on previous biological knowledge. ,biology
631,Cis-regulatory module detection using constraint programming,Bioinformatics and Biomedicine,"We propose a method for finding CRMs in a set of co-regulated genes. Each CRM consists of a set of binding sites of transcription factors. We wish to find CRMs involving the same transcription factors in multiple sequences. Finding such a combination of transcription factors is inherently a combinatorial problem. We solve this problem by combining the principles of itemset mining and constraint programming. The constraints involve the putative binding sites of transcription factors, the number of sequences in which they co-occur and the proximity of the binding sites. Genomic background sequences are used to assess the significance of the modules. We experimentally validate our approach and compare it with state-of-the-art techniques.",biology
632,Comparing Study of Nonlinear Model for Epileptic Preictal Prediction,International Conference on Bioinformatics and Biomedical Engineering,"Epilepsy is a group of disorders characterized by recurrent paroxysmal electrical discharges of the cerebral cortex that result in intermitted disturbances of brain function. The damage induced by seizure is severe, so it is significant to predict the preictal state of the epileptic seizure. The aim of this work is to compare and estimate the different nonlinear analysis methods in predicting of epileptic seizure, including approximate entropy, Lempel-Ziv complexity, spectral entropy and C0 complexity. The features of the epileptic EEG signals were extracted by an integrated nonlinear analysis system developed by LabVIEW. Through the experiments of these nonlinear analysis methods, it is concluded that all of them have a potential application for predicting epileptic seizure (t-test), but the each analysis model has obvious differences. The results indicate that approximate entropy and Lempel-Ziv complexity can distinguish preictal and ictal state with 99% confidence (t-test); spectral entropy achieve 96%, and 97% confidence is achieved by C0 complexity. Comparing with algorithms complexity, spectral entropy is simpler than the others, which computing speed is shorter.",biology
633,Systematic Evaluation of Scaling Methods for Gene Expression Data,Bioinformatics and Biomedicine,"Even after an experimentally prepared gene expression data set has been pre-processed to account for variations in the microarray technology, there may be inconsistencies between the scales of measurements in different conditions. This may happen for reasons such as the accumulation of gene expression data prepared by different laboratories into a single data set. A variety of scaling and transformation methods have been used for addressing these scale inconsistencies in different studies on the analysis of gene expression data sets. However, a quantitative estimation of their relative performance has been lacking. In this paper, we report an extensive evaluation of scaling and transformation methods for their effectiveness with respect to the important problem of protein function prediction. We consider several such commonly used methods for gene expression data, such as z-score scaling, quantile normalization, diff transformation, and two new scaling methods, sigmoid and double sigmoid, that have not been used previously in this domain to the best of our knowledge. We show that the performance of these methods can vary significantly across data sets, but Dsigmoid scaling and z-score transformation generally perform well for the two types of gene expression data, namely temporal and non-temporal, respectively.",biology
634,Decomposition of Flux Distributions into Metabolic Pathways,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Genome-scale reconstructions are often used for studying relationships between fundamental components of a metabolic system. In this study, we develop a novel computational method for analyzing predicted flux distributions for metabolic reconstructions. Because chemical reactions may have multiple reactants and products, a directed hypergraph where hyperarcs may have multiple tail vertices and head vertices is a more appropriate representation of the metabolic network than a conventional network. We use this view to represent predicted flux distributions by maximum generalized flows on hypergraphs. We then demonstrate that the generalized hyperflow problem may be transformed to an equivalent network flow problem with side constraints. This transformation allows a flux to be decomposed into chains of reactions. Subsequent analysis of these chains helps to characterize active pathways in a flux distribution. Such characterizations facilitate comparisons of flux distributions for different environmental conditions. The proposed method is applied to compare predicted flux distributions for Salmonella typhimurium to study changes in metabolism that cause enhanced virulence during a space flight. The differences between flux distributions corresponding to normal and enhanced virulence states confirm previous observations concerning infection mechanisms and suggest new pathways for exploration. © 2013 IEEE.",biology
635,CollHaps: A Heuristic Approach to Haplotype Inference by Parsimony,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Haplotype data play a relevant role in several genetic studies, e.g., mapping of complex disease genes, drug design, and evolutionary studies on populations. However, the experimental determination of haplotypes is expensive and time-consuming. This motivates the increasing interest in techniques for inferring haplotype data from genotypes, which can instead be obtained quickly and economically. Several such techniques are based on the maximum parsimony principle, which has been justified by both experimental results and theoretical arguments. However, the problem of haplotype inference by parsimony was shown to be NP-hard, thus limiting the applicability of exact parsimony-based techniques to relatively small data sets. In this paper, we introduce collapse rule, a generalization of the well-known Clarks rule, and describe a new heuristic algorithm for haplotype inference (implemented in a program called CollHaps), based on parsimony and the iterative application of collapse rules. The performance of CollHaps is tested on several data sets. The experiments show that CollHaps enables the user to process large data sets obtaining very parsimonious solutions in short processing times. They also show a correlation, especially for large data sets, between parsimony and correct reconstruction, supporting the validity of the parsimony principle to produce accurate solutions. © 2006 IEEE.",biology
636,"Divide, Align and Full-Search for Discovering Conserved Protein Complexes","Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","Advances in modern technologies for measuring protein-protein interaction (PPI) has boosted research in PPI networks analysis and comparison. One of the challenging problems in comparative analysis of PPI networks is the comparison of networks across species for discovering conserved modules. Approaches for this task generally merge the considered networks into one new weighted graph, called alignment graph, which describes how interaction between each pair of proteins is preserved in different networks. The problem of finding conserved protein complexes across species is then transformed into the problem of searching the alignment graph for subnetworks whose weights satisfy a given constraint. Because the latter problem is computationally intractable, generally greedy techniques are used. In this paper we propose an alternative approach for this task. First, we use a technique we recently introduced for dividing PPI networks into small subnets which are likely to contain conserved modules. Next, we perform network alignment on pairs of resulting subnets from different species, and apply an exact search algorithm iteratively on each alignment graph, each time changing the constraint based on the weight of the solution found in the previous iteration. Results of experiments show that this method discovers multiple accurate conserved modules, and can be used for refining state-of-the-art algorithms for comparative network analysis. © 2008 Springer-Verlag Berlin Heidelberg.",biology
637,Protein-Protein Interaction Network Alignment by Quantitative Simulation,Bioinformatics and Biomedicine,"We adapt a network simulation algorithm called quantitative simulation (QSim) for use in the alignment of biological networks. Unlike most network alignment methods, QSim finds local matches for one network in another, making it asymmetric, and takes full advantage of different edge types. We use QSim to simulate a protein-protein interaction (PPI) network from D. melanogaster using a PPI network from S. cerevisiae, and compare QSim's alignment to those from other methods using Gene Ontology (GO) biological process annotations as proxies for correct alignment matches. The best cross-species protein matches obtained from QSim have a higher agreement in GO biological process annotations than those from either BLAST or an alternative network alignment algorithm.",biology
638,Wavelet Analysis in Current Cancer Genome Research: A Survey,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"With the rapid development of next generation sequencing technology, the amount of biological sequence data of the cancer genome increases exponentially, which calls for efficient and effective algorithms that may identify patterns hidden underneath the raw data that may distinguish cancer Achilles heels. From a signal processing point of view, biological units of information, including DNA and protein sequences, have been viewed as one-dimensional signals. Therefore, researchers have been applying signal processing techniques to mine the potentially significant patterns within these sequences. More specifically, in recent years, wavelet transforms have become an important mathematical analysis tool, with a wide and ever increasing range of applications. The versatility of wavelet analytic techniques has forged new interdisciplinary bounds by offering common solutions to apparently diverse problems and providing a new unifying perspective on problems of cancer genome research. In this paper, we provide a survey of how wavelet analysis has been applied to cancer bioinformatics questions. Specifically, we discuss several approaches of representing the biological sequence data numerically and methods of using wavelet analysis on the numerical sequences. © 2013 IEEE.",biology
639,Identification of Regulatory Modules in Time Series Gene Expression Data Using a Linear Time Biclustering Algorithm,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Although most biclustering formulations are NP-hard, in time series expression data analysis, it is reasonable to restrict the problem to the identification of maximal biclusters with contiguous columns, which correspond to coherent expression patterns shared by a group of genes in consecutive time points. This restriction leads to a tractable problem. We propose an algorithm that finds and reports all maximal contiguous column coherent biclusters in time linear in the size of the expression matrix. The linear time complexity of CCC-Biclustering relies on the use of a discretized matrix and efficient string processing techniques based on suffix trees. We also propose a method for ranking biclusters based on their statistical significance and a methodology for filtering highly overlapping and, therefore, redundant biclusters. We report results in synthetic and real data showing the effectiveness of the approach and its relevance in the discovery of regulatory modules. Results obtained using the transcriptomic expression patterns occurring in Saccharomyces cerevisiae in response to heat stress show not only the ability of the proposed methodology to extract relevant information compatible with documented biological knowledge but also the utility of using this algorithm in the study of other environmental stresses and of regulatory modules in general. © 2006 IEEE.",biology
640,Comparative analysis of biclustering algorithms,International Conference on Bioinformatics,"Biclustering is a very popular method to identify hidden co-regulation patterns among genes. There are numerous biclustering algorithms designed to undertake this challenging task, however, a thorough comparison between these algorithms is even harder to accomplish due to lack of a ground truth and large variety in the search strategies and objectives of the algorithms. In this paper, we address this less studied, yet important problem and formally analyze several biclustering algorithms in terms of the bicluster patterns they attempt to discover. We systematically formulate the requirements for well-known patterns and show the constraints imposed by biclustering algorithms that determine their capacity to identify such patterns. We also give experimental results from a carefully designed testbed to evaluate the power of the employed search strategies. Furthermore, on a set of real datasets, we report the biological relevance of clusters identified by each algorithm. Copyright © 2010 ACM.",biology
641,Sequence-Only Based Prediction of β -Turn Location and Type Using Collocation of Amino Acid Pairs,The Open Bioinformatics Journal," Development of accurate -turn (beta-turn) type prediction methods would contribute towards the prediction of the tertiary protein structure and would provide useful insights/inputs for the fold recognition and drug design. Only one existing sequence-only method is available for the prediction of beta-turn types (for type I and II) for the entire protein chains, while the proposed method allows for prediction of type I, II, IV, VII, and non-specific (NS) beta-turns, filling in the gap. The proposed predictor, which is based solely on protein sequence, is shown to provide similar performance to other sequence-only methods for prediction of beta-turns and beta-turn types. The main advantage of the proposed method is simplicity and interpretability of the underlying model. We developed novel sequence-based features that allow identifying beta-turns types and differentiating them from non-beta-turns. The features, which are based on tetrapeptides (entire beta-turns) rather than a window centered over the predicted residues as in the case of recent competing methods, provide a more biologically sound model. They include 12 features based on collocation of amino acid pairs, focusing on amino acids (Gly, Asp, and Asn) that are known to be predisposed to form beta-turns. At the same time, our model also includes features that are geared towards exclusion of non-beta-turns, which are based on amino acids known to be strongly detrimental to formation of beta-turns (Met, Ile, Leu, and Val). ",biology
642,Robust Classification Method of Tumor Subtype by Using Correlation Filters,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Tumor classification based on Gene Expression Profiles (GEPs), which is of great benefit to the accurate diagnosis and personalized treatment for different types of tumor, has drawn a great attention in recent years. This paper proposes a novel tumor classification method based on correlation filters to identify the overall pattern of tumor subtype hidden in differentially expressed genes. Concretely, two correlation filters, i.e., Minimum Average Correlation Energy (MACE) and Optimal Tradeoff Synthetic Discriminant Function (OTSDF), are introduced to determine whether a test sample matches the templates synthesized for each subclass. The experiments on six publicly available data sets indicate that the proposed method is robust to noise, and can more effectively avoid the effects of dimensionality curse. Compared with many model-based methods, the correlation filter-based method can achieve better performance when balanced training sets are exploited to synthesize the templates. Particularly, the proposed method can detect the similarity of overall pattern while ignoring small mismatches between test sample and the synthesized template. And it performs well even if only a few training samples are available. More importantly, the experimental results can be visually represented, which is helpful for the further analysis of results. ÂŠ 2012 IEEE.",biology
643,A Survey on Methods for Modeling and Analyzing Integrated Biological Networks,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Understanding how cellular systems build up integrated responses to their dynamically changing environment is one of the open questions in Systems Biology. Despite their intertwinement, signaling networks, gene regulation and metabolism have been frequently modeled independently in the context of well-defined subsystems. For this purpose, several mathematical formalisms have been developed according to the features of each particular network under study. Nonetheless, a deeper understanding of cellular behavior requires the integration of these various systems into a model capable of capturing how they operate as an ensemble. With the recent advances in the ""omics"" technologies, more data is becoming available and, thus, recent efforts have been driven toward this integrated modeling approach. We herein review and discuss methodological frameworks currently available for modeling and analyzing integrated biological networks, in particular metabolic, gene regulatory and signaling networks. These include network-based methods and Chemical Organization Theory, Flux-Balance Analysis and its extensions, logical discrete modeling, Petri Nets, traditional kinetic modeling, Hybrid Systems and stochastic models. Comparisons are also established regarding data requirements, scalability with network size and computational burden. The methods are illustrated with successful case studies in large-scale genome models and in particular subsystems of various organisms. © 2011 IEEE.",biology
644,A Preview on Subspace Clustering of High Dimensional Data,International Conference on Bioinformatics," When clustering high dimensional data, traditional clustering methods are found to be lacking since they consider all of the dimensions of the dataset in discovering clusters whereas only some of the dimensions are relevant. This may give rise to subspaces within the dataset where clusters may be found. Using feature selection, we can remove irrelevant and redundant dimensions by analyzing the entire dataset. The problem of automatically identifying clusters that exist in multiple and maybe overlapping subspaces of high dimensional data, allowing better clustering of the data points, is known as Subspace Clustering. There are two major approaches to subspace clustering based on search strategy. Top-down algorithms find an initial clustering in the full set of dimensions and evaluate the subspaces of each cluster, iteratively improving the results. Bottom-up approaches start from finding low dimensional dense regions, and then use them to form clusters. Based on a survey on subspace clustering, we identify the challenges and issues involved with clustering gene expression data. ",biology
645,Fast Alignments of Metabolic Networks,Bioinformatics and Biomedicine,"Network alignments are extensively used for comparing, exploring, and predicting biological networks. Existing alignment tools are mostly based on isomorphic and homeomorphic embedding and require solving a problem that is NP-complete even when searching a match for a tree in acyclic networks. On the other hand, if the mapping of different nodes from the query network (pattern) into the same node from the text network is allowed, then trees can be optimally mapped into arbitrary networks in polynomial time.In this paper we present the first polynomial-time algorithm for finding the best matching pair consisting of a subtree in a given tree pattern and a subgraph in a given text (represented by an arbitrary network) when both insertions and deletions of degree-2 vertices are allowed on any path. Our dynamic programming algorithm is an order of magnitude faster than the previous network alignment algorithm when deletions are forbidden. The algorithm has been also generalized to pattern networks with cycles: with a modest increase in runtime it can handle patterns with the limited vertex feedback set.We have applied our algorithm to matching metabolic pathways of four organisms (E. coli, S. cerevisiae, B. subtilis and T. thermophilus species) and found a reasonably large set of statistically significant alignments. We show advantages of allowing pattern vertex deletions and give an example validating biological relevance of the pathway alignment.",biology
646,Pure Multiple RNA Secondary Structure Alignments: A Progressive Profile Approach,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"In functional, noncoding RNA, structure is often essential to function. While the full 3D structure is very difficult to determine, the 2D structure of an RNA molecule gives good clues to its 3D structure, and for molecules of moderate length, it can be predicted with good reliability. Structure comparison is, in analogy to sequence comparison, the essential technique to infer related function. We provide a method for computing multiple alignments of RNA secondary structures under the tree alignment model, which is suitable to cluster RNA molecules purely on the structural level, i.e., sequence similarity is not required. We give a systematic generalization of the profile alignment method from strings to trees and forests. We introduce a tree profile representation of RNA secondary structure alignments which allows reasonable scoring in structure comparison. Besides the technical aspects, an RNA profile is a useful data structure to represent multiple structures of RNA sequences. Moreover, we propose a visualization of RNA consensus structures that is enriched by the full sequence information.",biology
647,Cache-Oblivious Dynamic Programming for Bioinformatics,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"We present efficient cache-oblivious algorithms for some well-studied string problems in bioinformatics including the longest common subsequence, global pairwise sequence alignment and three-way sequence alignment (or median), both with affine gap costs, and RNA secondary structure prediction with simple pseudoknots. For each of these problems, we present cache-oblivious algorithms that match the best-known time complexity, match or improve the best-known space complexity, and improve significantly over the cache-efficiency of earlier algorithms. We present experimental results which show that our cache-oblivious algorithms run faster than software and implementations based on previous best algorithms for these problems. © 2006 IEEE.",biology
648,Classification and characterization of clinical finding expressions in medical literature,Bioinformatics and Biomedicine,"Text mining of clinical findings has been employed to extract clinical information contained in “electronic medical records” without the need for labor intensive work by medical experts. However, the automated building of disease ontology necessitates knowledge acquisition of clinical findings documented in “medical literature” that requires an independent strategy. This study performs a preliminary analysis of clinical finding expressions in medical literature to enable the automated acquisition of disease knowledge. To this end, we selected descriptions of 20 diseases in a free-text format and annotated the texts to extract expressions of clinical findings. This resulted in 1368 expressions with varying lengths and syntactic features, and 161 annotator comments. The comments suggested that certain types of expressions, which were further classified into 10 categories. Also, in-depth analyses of their syntactic and semantic characteristics were performed, resulting in the following observations. First, expressions of clinical findings have certain patterns, syntactic and semantic, which can be exploited for appropriate knowledge acquisition. Second, clinical knowledge may guide the knowledge acquisition process in a top-down manner. Third, natural language processing of medical literature requires specific considerations compared with the processing of health records, namely, i) distinction of subjects, ii) handling of generalized knowledge, and iii) processing of expressions for examination results. This preliminary survey on the expressions in medical literature provides helpful insights for future corpus design.",biology
649,Parameterized Algorithmics for Finding Connected Motifs in Biological Networks,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -We study the NP-hard LIST-COLORED GRAPH MOTIF problem the second and more recent view on graph motifs takes a which, given an undirected list-colored graph G = (V; E) and a more “functional approach” [9, 18, 24]. Here, topology is multiset M of colors, asks for maximum-cardinality sets S V of lesser importance, while the functionalities of network raensdpeMct0 to mMultsipulcichityth)at hteGc[Sol]oirss cinonMn e0.cLteIdSTa-CndOLcOonRtEaDinsGeRxAaPcHtlyM(OwTitIhF nodes (expressed by colors) form the governing principle. has applications in the analysis of biological networks. We study LIST- In this work, we concentrate on the second, functional COLORED GRAPH MOTIF with respect to three different parameterizations. view of graph motifs. Typically, functional categories For the parameters motif size jM j and solution size jSj we present fixed- that are associated to network vertices are represented parameter algorithms, whereas for the parameter jV j jM j we show W[1]- as “colors”. Hence, we consider list-colored graphs which hardness for general instances and achieve fixed-parameter tractability means that each vertex v is associated with a set of ffisoxpreeade-dsp-pauerpcaimhaeleucterairssetaiclgosoffrLoitIrhStmTh-esCsfOeoLraOplgaRorEarDimthGemtResAr,sPajHnMdMjaOapTnpIdlFi e.jdSWjt,ehdeimemvpeilnleomtpheeednctfeoudnrtthtehexert sctoulodriesdcoinl(vt)h.isAwfoorrkm, afloldloewfi ninitgioLnacorfotixheetmaal.in[2p4]r,oibsl:em of querying protein-interaction networks, demonstrating their usefulness for realistic instances. Furthermore, we show that extending the request for motif connectedness to stronger demands such as biconnectedness or bridge-connectedness leads to W[1]-hard problems when the parameter is the motif size jM j. ",biology
650,Curating a large-scale regulatory network by evaluating its consistency with expression datasets,Computational Intelligence Methods for Bioinformatics and Biostatistics," The analysis of large-scale regulatory models using data issued from genome-scale high-throughput experimental techniques is an actual challenge in the systems biology field. This kind of analysis faces three common problems: the size of the model, the uncertainty in the expression datasets, and the heterogeneity of the data. On that account, we propose a method that analyses large-scale networks with small - but reliable - expression datasets. Our method relates regulatory knowledge with heterogeneous expression datasets using a simple consistency rule. If a global consistency is found, we predict the changes in gene expression or protein activity of some components of the network. When the whole model is inconsistent, we highlight regions in the network where the regulatory knowledge is incomplete. Confronting our predictions with mRNA expression experiments allows us to determine the missing post-transcriptional interactions of our model. We tested this approach with the transcriptional network of E. coli. ",biology
651,A Uniform Projection Method for Motif Discovery in DNA Sequences,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Buhler and Tompa introduced the random projection algorithm for the motif discovery problem and demonstrated that this algorithm performs well on both simulated and biological samples. We describe a modification of the random projection algorithm, called the uniform projection algorithm, which utilizes a different choice of projections. We replace the random selection of projections by a greedy heuristic that approximately equalizes the coverage of the projections. We show that this change in selection of projections leads to improved performance on motif discovery problems. Furthermore, the uniform projection algorithm is directly applicable to other problems where the random projection algorithm has been used, including comparison of protein sequence databases.",biology
652,Human Disease-Gene Classification with Integrative Sequence-Based and Topological Features of Protein-Protein Interaction Networks,Bioinformatics and Biomedicine,"Medical implanted devices usually prompt foreign body reactions that lead to the formation of fibrotic capsule surrounding the implanted devices. Cascade phagocyte responses, such as phagocyte chemotaxis and phagocyte adherence, are believed important in pathogenesis of fibrotic responses. Therefore, in-depth understanding of phagocyte chemotaxis and adherence processes is important to successfully reduce the fibrotic responses to improve the stability and functionality of implanted medical devices. This paper hypothesizes the molecular mechanisms governing phagocyte chemotaxis and adherence. To support our hypotheses and to provide the prediction of un-measured dynamic features in phagocyte chemotaxis and adherence processes, mathematical models based on system and control theory are built. To maximize the prediction ability and to guide future experimental design, external controls are provided by the mathematical model. The effectiveness of the mathematical model is demonstrated by computer simulation.",biology
653,High-Precision Function Prediction using Conserved Interactions,German Conference on Bioinformatics," The recent availability of large data sets of protein- protein-interactions (PPIs) from various species offers new opportunities for functional genomics and proteomics. We describe a method for exploiting conserved and connected subgraphs (CCSs) in the PPI networks of multiple species for the prediction of protein function. Structural conservation is combined with functional conservation using a GeneOntologybased scoring scheme. We applied our method to the PPI networks of five species, i.e., E. coli, D. melanogaster, M. musculus, H. sapiens and S. cerevisiae. We detected surprisingly large CCSs for groups of three species but not beyond. A manual analysis of the biological coherence of exemplary subgraphs strongly supports a close relationship between structural and functional conservation. Based on this observation, we devised an algorithm for function prediction based on CCS. Using our method, for instance, we predict new functional annotations for human based on mouse proteins with a precision of 70%. ",biology
654,On the Role of Local Matching for Efficient Semi-supervised Protein Sequence Classification,Bioinformatics and Biomedicine,"Recent studies in protein sequence analysis have leveraged the power of unlabeled data. For example, the profile and mismatch neighborhood kernels have shown significant improvements over classifiers estimated under the fully supervised setting. In this study, we present a principled and biologically motivated framework that more effectively exploits the unlabeled data by only utilizing regions that are more likely to be biologically relevant for better prediction accuracy. As overly-represented sequences in large uncurated databases may bias kernel estimations that rely on unlabeled data, we also propose a method to remove this bias and improve performance of resulting classifiers.Combined with a computationally efficient sparse family of string kernels, our proposed framework achieves state-of-the-art accuracy in semi-supervised protein remote homology detection on three large unlabeled databases.",biology
655,A binary matrix factorization algorithm for protein complex prediction,Bioinformatics and Biomedicine," Background: Identifying biologically relevant protein complexes from a large protein-protein interaction (PPI) network, is essential to understand the organization of biological systems. However, high-throughput experimental techniques that can produce a large amount of PPIs are known to yield non-negligible rates of false-positives and false-negatives, making the protein complexes difficult to be identified. Results: We propose a binary matrix factorization (BMF) algorithm under the Bayesian Ying-Yang (BYY) harmony learning, to detect protein complexes by clustering the proteins which share similar interactions through factorizing the binary adjacent matrix of a PPI network. The proposed BYY-BMF algorithm automatically determines the cluster number while this number is pre-given for most existing BMF algorithms. Also, BYY-BMF's clustering results does not depend on any parameters or thresholds, unlike the Markov Cluster Algorithm (MCL) that relies on a so-called inflation parameter. On synthetic PPI networks, the predictions evaluated by the known annotated complexes indicate that BYY-BMF is more robust than MCL for most cases. On real PPI networks from the MIPS and DIP databases, BYY-BMF obtains a better balanced prediction accuracies than MCL and a spectral analysis method, while MCL has its own advantages, e.g., with good separation values. ",biology
656,Cascaded Bidirectional Recurrent Neural Networks for Protein Secondary Structure Prediction,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Protein secondary structure (PSS) prediction is an important topic in bioinformatics. Our study on a large set of nonhomologous proteins shows that long-range interactions commonly exist and negatively affect PSS prediction. Besides, we also reveal strong correlations between secondary structure (SS) elements. In order to take into account the long-range interactions and SS-SS correlations, we propose a novel prediction system based on a cascaded bidirectional recurrent neural network (BRNN). We compare the cascaded BRNN against two other BRNN architectures, namely, the original BRNN architecture used for speech recognition and Pollastris BRNN, which was proposed for PSS prediction. Our cascaded BRNN achieves an overall three-state accuracy Q3 of 74.38 percent and reaches a high Segment OVerlap (SOV) of 66.0455. It outperforms the original BRNN and Pollastris BRNN in both Q3 and SOV. Specifically, it improves the SOV score by 4-6 percent. ÂŠ 2007 IEEE.",biology
657,TreeDT: Tree Pattern Mining for Gene Mapping,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -We describe TreeDT, a novel association-based gene mapping method. Given a set of disease-associated haplotypes and a set of control haplotypes, TreeDT predicts likely locations of a disease susceptibility gene. TreeDT extracts, essentially in the form of haplotype trees, information about historical recombinations in the population: A haplotype tree constructed at a given chromosomal location is an estimate of the genealogy of the haplotypes. TreeDT constructs these trees for all locations on the given haplotypes and performs a novel disequilibrium test on each tree: Is there a small set of subtrees with relatively high proportions of disease-associated chromosomes, suggesting shared genetic history for those and a likely disease gene location? We give a detailed description of TreeDT and the tree disequilibrium tests, we analyze the algorithm formally, and we evaluate its performance experimentally on both simulated and real data sets. Experimental results demonstrate that TreeDT has high accuracy on difficult mapping tasks and comparisons to other methods (EATDT, HPM, TDT) show that TreeDT is very competitive. ",biology
658,Identification Algorithm of Winged Insects Based on Hybrid Moment Invariants,International Conference on Bioinformatics and Biomedical Engineering,"Automated species identification based on image processing and pattern recognition, as a small but growing domain, has been actively researched in recent years. Feature extraction is always the key step. This paper proposes a hybrid moment invariants method to extract the features of image. The main idea of this method is to combine Hu's invariants and blur- affine moment invariants to identify winged insects obtained under unknown illumination and blurring conditions and it is exemplified by left forewing images of dragonflies. The new method has more advantages such as insensitive to translation, scale change, rotation, as well as affine transform and blurring. The experimental results on a small set of samples showed that the method can achieve higher identification rate.",biology
659,Translational Control by RNA-RNA Interaction: Improved Computation of RNA-RNA Binding Thermodynamics,Bioinformatics Research and Development, FSI FSI NISTUE ,biology
660,Multi-objective Particle Swarm Optimization Biclustering of Microarray Data,Bioinformatics and Biomedicine,"With the advent of the DNA microarray technology,it is now possible to study the transcriptional response of a complete genome to different experimental conditions. Biclustering is a very useful data mining technique for analysis of those gene expression data.During biclustering several objectives in conflict with each other have to be optimized simultaneously, so multi-objective modeling is suitable for solving biclustering problem. This paper proposes a novel multi-objective particle swarm optimization biclustering (MOPSOB) algorithm to mine coherent patterns from microarray data. Experimental results on real datasets show that our approach can effectively find significant biclusters of high quality.",biology
661,Alternating Constraint Least Squares Parameter Estimation for S-System Models of Biological Networks,International Conference on Bioinformatics and Biomedical Engineering,"S-system models for biological systems are derived from the generalized mass action law and are typically a group of nonlinear differential equations. Estimation of parameters in these models from experimental measurements is thus a nonlinear problem. In principle, all algorithms for nonlinear optimization can be used to estimate parameters in molecular biological systems, for example, Gauss-Newton iteration method and its variants. However, these methods do not take the special structures of biological system models into account and thus are not efficient. In this paper, we propose an alternating constraint least squares method for estimating parameters in S-system model by taking use of their special structure and the biological meaning of parameters. To investigate its performance, the alternating constraint least squares method is applied to a biological system and is compared with other parameter estimation methods. Simulation results show the good performance of the proposed estimation method.",biology
662,Histological Image Feature Mining Reveals Emergent Diagnostic Properties for Renal Cancer,Bioinformatics and Biomedicine,"Computer-aided histological image classification systems are important for making objective and timely cancer diagnostic decisions. These systems use combinations of image features that quantify a variety of image properties. Because researchers tend to validate their diagnostic systems on specific cancer endpoints, it is difficult to predict which image features will perform well given a new cancer endpoint. In this paper, we define a comprehensive set of common image features (consisting of 12 distinct feature subsets) that quantify a variety of image properties. We use a data-mining approach to determine which feature subsets and image properties emerge as part of an ""optimal"" diagnostic model when applied to specific cancer endpoints. Our goal is to assess the performance of such comprehensive image feature sets for application to a wide variety of diagnostic problems. We perform this study on 12 endpoints including 6 renal tumor subtype endpoints and 6 renal cancer grade endpoints.",biology
663,Identification and quantification of abundant species from pyrosequences of 16S rRNA by consensus alignment,Bioinformatics and Biomedicine,"16S rRNA gene profiling has recently been boosted by the development of pyrosequencing methods. A common analysis is to group pyrosequences into Operational Taxonomic Units (OTUs), such that reads in an OTU are likely sampled from the same species. However, species diversity estimated from error-prone 16S rRNA pyrosequences may be inflated because the reads sampled from the same 16S rRNA gene may appear different, and current OTU inference approaches typically involve time-consuming pairwise/multiple distance calculation and clustering. I propose a novel approach Abun-dantOTU based on a Consensus Alignment (CA) algorithm, which infers consensus sequences, each representing an OTU, taking advantage of the sequence redundancy for abundant species. Pyrosequencing reads can then be recruited to the consensus sequences to give quantitative information for the corresponding species. As tested on 16S rRNA pyrosequence datasets from mock communities with known species, Abun-dantOTU rapidly reported identified sequences of the source 16S rRNAs and the abundances of the corresponding species. AbundantOTU was also applied to 16S rRNA pyrosequence datasets derived from real microbial communities and the results are in general agreement with previous studies.",biology
664,Detection of Gene Orthology Based on Protein-Protein Interaction Networks,Bioinformatics and Biomedicine,"Ortholog detection methods present a powerful approach for finding genes that participate in similar biological processes across different organisms, extending our understanding of interactions between genes across different pathways, and understanding the evolution of gene families. We exploit features derived from the alignment of protein-protein interaction networks to reconstruct KEGG orthologs for Drosophila melanogaster, Saccharomyces cerevisiae, Mus musculus and Homo sapiens protein-protein interaction networks extracted from the DIP repository for protein-protein interaction data using the decision tree, naive-Bayes and support vector machine classification algorithms. The performance of our classifiers in reconstructing KEGG orthologs is compared against a basic reciprocal BLAST hit approach. We provide implementations of the resulting algorithms as part of BiNA, an open source biomolecular network alignment toolkit.",biology
665,A novel dynamic graph-based computational model for predicting salivary gland branching morphogenesis,Bioinformatics and Biomedicine,"In this paper, we introduce a biologically motivated dynamic graph-based growth model to describe and predict the stages of cleft formation during the process of branching morphogenesis in the submandibular mouse gland (SMG) from 3 hrs after embryonic day E12 to 8 hrs after embryonic day E12, which can be considered as E12.5. Branching morphogenesis is the process by which many mammalian exocrine and endocrine glands undergo significant morphological transformations, from a primary bud to an adult organ. Although many studies have investigated the cellular and molecular mechanisms driving branching morphogenesis, it is not clear how the shape changes that are inherent to establishing organ structure are produced. Using morphological features extracted from sequential images of SMG organ cultures we were able to develop a dynamic graph-based predictive model that is able to mimic the process of cleft formation and predict the final state. In addition, we compare our model to a state-of-the-art Glazier-Graner-Hogeweg (GGH) simulative tool, and demonstrate that the dynamic graph-based predictive model has comparable accuracy in modeling growth of clefts across SMG developmental stages, as well as faster convergence to the target SMG morphology.",biology
666,A Novel Knowledge-Driven Systems Biology Approach for Phenotype Prediction upon Genetic Intervention,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Deciphering the biological networks underlying complex phenotypic traits, e.g., human disease is undoubtedly crucial to understand the underlying molecular mechanisms and to develop effective therapeutics. Due to the network complexity and the relatively small number of available experiments, data-driven modeling is a great challenge for deducing the functions of genes/proteins in the network and in phenotype formation. We propose a novel knowledge-driven systems biology method that utilizes qualitative knowledge to construct a Dynamic Bayesian network (DBN) to represent the biological network underlying a specific phenotype. Edges in this network depict physical interactions between genes and/or proteins. A qualitative knowledge model first translates typical molecular interactions into constraints when resolving the DBN structure and parameters. Therefore, the uncertainty of the network is restricted to a subset of models which are consistent with the qualitative knowledge. All models satisfying the constraints are considered as candidates for the underlying network. These consistent models are used to perform quantitative inference. By in silico inference, we can predict phenotypic traits upon genetic interventions and perturbing in the network. We applied our method to analyze the puzzling mechanism of breast cancer cell proliferation network and we accurately predicted cancer cell growth rate upon manipulating (anti)cancerous marker genes/proteins. © 2006 IEEE.",biology
667,Statistical Analysis of RNA Backbone,IEEE/ACM Transactions on Computational Biology and Bioinformatics, Phone: 612/624-6066 Fax: 612/626-7370 URL: http://www.ima.umn.edu ,biology
668,Discovering Coherent Biclusters from Gene Expression Data Using Zero-Suppressed Binary Decision Diagrams,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -The biclustering method can be a very useful analysis tool when some genes have multiple functions and experimental conditions are diverse in gene expression measurement. This is because the biclustering approach, in contrast to the conventional clustering techniques, focuses on finding a subset of the genes and a subset of the experimental conditions that together exhibit coherent behavior. However, the biclustering problem is inherently intractable, and it is often computationally costly to find biclusters with high levels of coherence. In this work, we propose a novel biclustering algorithm that exploits the zero-suppressed binary decision diagrams (ZBDDs) data structure to cope with the computational challenges. Our method can find all biclusters that satisfy specific input conditions, and it is scalable to practical gene expression data. We also present experimental results confirming the effectiveness of our approach. ",biology
669,Twin Removal in Genetic Algorithms for Protein Structure Prediction Using Low-Resolution Model,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -This paper presents the impact of twins and the measures for their removal from the population of genetic algorithm (GA) when applied to effective conformational searching. It is conclusively shown that a twin removal strategy for a GA provides considerably enhanced performance when investigating solutions to complex ab initio protein structure prediction (PSP) problems in low resolution model. Without twin removal, GA crossover and mutation operations can become ineffectual as generations lose their ability to produce significant differences which can lead to the solution stalling. The paper relaxes the definition of chromosomal twins in the removal strategy to not only encompass identical, but also highly-correlated chromosomes within the GA population, with empirical results consistently exhibiting significant improvements solving PSP problems. ",biology
670,Integer Programming Approaches to Haplotype Inference by Pure Parsimony,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"In 2003, Gusfield introduced the Haplotype Inference by Pure Parsimony (HIPP) problem and presented an integer program (IP) that quickly solved many simulated instances of the problem [1], Although it solved well on small instances, Gusfields IP can be of exponential size in the worst case. Several authors [2], [3] have presented polynomial-sized IPs for the problem. In this paper, we further the work on IP approaches to HIPP. We extend the existing polynomial-sized IPs by introducing several classes of valid cuts for the IP. We also present a new polynomial-sized IP formulation that is a hybrid between two existing IP formulations and inherits many of the strengths of both. Many problems that are too complex for the exponential-sized formulations can still be solved in our new formulation in a reasonable amount of time. We provide a detailed empirical comparison of these IP formulations on both simulated and real genotype sequences. Our formulation can also be extended in a variety of ways to allow errors in the input or model the structure of the population under consideration. ÂŠ 2006 IEEE.",biology
671,Knowledge Discovery in Clinical Performance of Cancer Patients,Bioinformatics and Biomedicine,"Our goal in this research is to construct predictive models for clinical performance of pancreatic cancer patients. Current predictive model design in medical oncology literature is dominated by linear and logistic regression techniques. We seek to show that novel machine learning methods can perform as well or better than these traditional techniques.We construct these predictive models via a clinical database we have developed for the University of Massachusetts Memorial Hospitalin Worcester, Massachusetts, USA. The database contains retrospective records of 91 patient treatments for pancreatic tumors.Classification and regression prediction targets include patient survival time, ECOG quality of life scores, surgical outcomes,and tumor characteristics. The predictive accuracy of various data mining models is described, and specific models are presented.",biology
672,Molecular Pattern Discovery Based on Penalized Matrix Decomposition,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"A reliable and precise identification of the type of tumors is crucial to the effective treatment of cancer. With the rapid development of microarray technologies, tumor clustering based on gene expression data is becoming a powerful approach to cancer class discovery. In this paper, we apply the penalized matrix decomposition (PMD) to gene expression data to extract metasamples for clustering. The extracted metasamples capture the inherent structures of samples belong to the same class. At the same time, the PMD factors of a sample over the metasamples can be used as its class indicator in return. Compared with the conventional methods such as hierarchical clustering (HC), self-organizing maps (SOM), affinity propagation (AP) and nonnegative matrix factorization (NMF), the proposed method can identify the samples with complex classes. Moreover, the factor of PMD can be used as an index to determine the cluster number. The proposed method provides a reasonable explanation of the inconsistent classifications made by the conventional methods. In addition, it is able to discover the modules in gene expression data of conterminous developmental stages. Experiments on two representative problems show that the proposed PMD-based method is very promising to discover biological phenotypes. ÂŠ 2011 IEEE.",biology
673,Data Mining and Predictive Modeling of Biomolecular Network from Biomedical Literature Databases,IEEE/ACM Transactions on Computational Biology and Bioinformatics," In this paper, we present a novel approach Bio-IEDM (Biomedical Information Extraction and Data Mining) to integrate text mining and predictive modeling to analyze biomolecular network from biomedical literature databases. Our method consists of two we discuss a semi-supervised efficient learning approach to automatically extract biological relationships such as protein-protein interaction, proteingene interaction from the biomedical literature databases to construct the biomolecular network. Our method automatically learns the patterns based on a few user seed tuples and then extracts new tuples from the biomedical literature based on the discovered patterns. The derived biomolecular network forms a large scale-free network graph. In phase 2, we present a novel clustering algorithm to analyze the biomolecular network graph to identify biologically meaningful subnetworks (communities). The clustering algorithm considers the characteristics of the scale-free network graphs and is based on the local density of the vertex and its neighborhood functions that can be used to find more meaningful clusters with different density level. The experimental results indicate our approach is very effective in extracting biological knowledge from a huge collection of biomedical literatures. The integration of data mining and information extraction provides a promising direction for analyzing the biomolecular network. ",biology
674,Registration to a Neuroanatomical Reference Atlas - Identifying Glomeruli in Optical Recordings of the Honeybee Brain,German Conference on Bioinformatics," An odorant stimulus given to a bee elicits a characteristic combinatorial pattern of activity in neuronal units called glomeruli. These patterns can be measured by optical imaging, however detecting and identifying the glomeruli is a laborious task and prone to errors. Here, we present an image analysis pipeline for the automatic detection and identification of glomeruli. It involves Independent Component Analysis (ICA) to detect glomeruli in CCD camera data, a filtering step to exclude nonglomerulus objects and a graph-matching approach to find the best projection of the observed brain region onto a reference atlas. We evaluate our method against a manual glomerulus identification performed by a human expert and show that we achieve reliable results. Employing our method, we are now able to screen multiple recordings with the same accuracy, yielding a homogeneous collection of glomerulus identity mappings. These will subsequently be used to extract activity patterns that can be compared between individuals. ",biology
675,SP-Dock: Protein-Protein Docking Using Shape and Physicochemical Complementarity,IEEE/ACM Transactions on Computational Biology and Bioinformatics," - In this paper, a framework for protein-protein docking is proposed, which exploits both shape and physicochemical complementarity to generate improved docking predictions. Shape complementarity is achieved by matching local surface patches. However, unlike existing approaches, which are based on single-patch or two-patch matching, we developed a new algorithm that compares simultaneously, groups of neighboring patches from the receptor with groups of neighboring patches from the ligand. Taking into account the fact that shape complementarity in protein surfaces is mostly approximate rather than exact, the proposed group-based matching algorithm fits perfectly to the nature of protein surfaces. This is demonstrated by the high performance that our method achieves especially in the case where the unbound structures of the proteins are considered. Additionally, several physicochemical factors, such as desolvation energy, electrostatic complementarity, hydrophobicity, Coulomb potential and Lennard-Jones potential are integrated using an optimized scoring function, improving geometric ranking in more than 60% of the complexes of Docking Benchmark 2.4. ",biology
676,Data-Fusion in Clustering Microarray Data: Balancing Discovery and Interpretability,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"While clustering genes remains one of the most popular exploratory tools for expression data, it often results in a highly variable and biologically uninformative clusters. This paper explores a data fusion approach to clustering microarray data. Our method, which combined expression data and Gene Ontology (GO)-derived information, is applied on a real data set to perform genome-wide clustering. A set of novel tools is proposed to validate the clustering results and pick a fair value of infusion coefficient. These tools measure stability, biological relevance, and distance from the expression-only clustering solution. Our results indicate that a data-fusion clustering leads to more stable, biologically relevant clusters that are still representative of the experimental data. © 2006 IEEE.",biology
677,Efficient Parameterized Algorithms for Biopolymer Structure-Sequence Alignment,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Computational alignment of a biopolymer sequence (e.g., an RNA or a protein) to a structure is an effective approach to predict and search for the structure of new sequences. To identify the structure of remote homologs, the structure-sequence alignment has to consider not only sequence similarity, but also spatially conserved conformations caused by residue interactions and, consequently, is computationally intractable. It is difficult to cope with the inefficiency without compromising alignment accuracy, especially for structure search in genomes or large databases. This paper introduces a novel method and a parameterized algorithm for structure-sequence alignment. Both the structure and the sequence are represented as graphs, where, in general, the graph for a biopolymer structure has a naturally small tree width. The algorithm constructs an optimal alignment by finding in the sequence graph the maximum valued subgraph isomorphic to the structure graph. It has the computational time complexity O(k tN 2) for the structure of N residues and its tree decomposition of width t. Parameter k, small in nature, is determined by a statistical cutoff for the correspondence between the structure and the sequence. This paper demonstrates a successful application of the algorithm to RNA structure search used for noncoding RNA identification. An application to protein threading is also discussed. © 2006 IEEE.",biology
678,Neuroinformatics for Genome-Wide 3-D Gene Expression Mapping in the Mouse Brain,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Large-scale gene expression studies in the mammalian brain offer the promise of understanding the topology, networks, and, ultimately, the function of its complex anatomy, opening previously unexplored avenues in neuroscience. High-throughput methods permit genome-wide searches to discover genes that are uniquely expressed In brain circuits and regions that control behavior. Previous gene expression mapping studies in model organisms have employed In situ hybridization (ISH), a technique that uses labeled nucleic acid probes to bind to specific mRNA transcripts In tissue sections. A key requirement for this effort is the development of fast and robust algorithms for anatomically mapping and quantifying gene expression for ISH. We describe a neuroinformatics pipeline for automatically mapping expression profiles of ISH data and Its use to produce the first genomic scale 3D mapping of gene expression In a mammalian brain. The pipeline is fully automated and adaptable to other organisms and tissues. Our automated study of more than 20,000 genes Indicates that at least 78.8 percent are expressed at some level In the adult C56BL/6J mouse brain. In addition to providing a platform for genomic scale search, high-resolution images and visualization tools for expression analysis are available at the Allen Brain Atlas web site (http://www.brain-map.org). © 2007 IEEE.",biology
679,Multistage Gene Normalization and SVM-Based Ranking for Protein Interactor Extraction in Full-Text Articles,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The interactor normalization task (INT) is to identify genes that play the interactor role in protein-protein interactions (PPIs), to map these genes to unique IDs, and to rank them according to their normalized confidence. INT has two subtasks: gene normalization (GN) and interactor ranking. The main difficulties of INT GN are identifying genes across species and using full papers instead of abstracts. To tackle these problems, we developed a multistage GN algorithm and a ranking method, which exploit information in different parts of a paper. Our system achieved a promising AUC of 0.43471. Using the multistage GN algorithm, we have been able to improve system performance (AUC) by 1.719 percent compared to a one-stage GN algorithm. Our experimental results also show that with full text, versus abstract only, INT AUC performance was 22.6 percent higher. ÂŠ 2006 IEEE.",biology
680,A Multiple-Filter-Multiple-Wrapper Approach to Gene Selection and Microarray Data Classification,IEEE/ACM Transactions on Computational Biology and Bioinformatics," Ieee/Acm Transactions On Computational Biology And Bioinformatics, 2010, v. 7 n. 1, p. 108-117 ",biology
681,A Trade-Off between Sample Complexity and Computational Complexity in Learning Boolean Networks from Time-Series Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"A key problem in molecular biology is to infer regulatory relationships between genes from expression data. This paper studies a simplified model of such inference problems in which one or more Boolean variables, modeling, for example, the expression levels of genes, each depend deterministically on a small but unknown subset of a large number of Boolean input variables. Our model assumes that the expression data comprises a time series, in which successive samples may be correlated. We provide bounds on the expected amount of data needed to infer the correct relationships between output and input variables. These bounds improve and generalize previous results for Boolean network inference and continuous-time switching network inference. Although the computational problem is intractable in general, we describe a fixed-parameter tractable algorithm that is guaranteed to provide at least a partial solution to the problem. Most interestingly, both the sample complexity and computational complexity of the problem depend on the strength of correlations between successive samples in the time series but in opposing ways. Uncorrelated samples minimize the total number of samples needed while maximizing computational complexity; a strong correlation between successive samples has the opposite effect. This observation has implications for the design of experiments for measuring gene expression. © 2006 IEEE.",biology
682,High Confidence Rule Mining for Microarray Analysis,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -We present an association rule mining method for mining high-confidence rules, which describe interesting gene relationships from microarray data sets. Microarray data sets typically contain an order of magnitude more genes than experiments, rendering many data mining methods impractical as they are optimized for sparse data sets. A new family of row-enumeration rule mining algorithms has emerged to facilitate mining in dense data sets. These algorithms rely on pruning infrequent relationships to reduce the search space by using the support measure. This major shortcoming results in the pruning of many potentially interesting rules with low support but high confidence. We propose a new row-enumeration rule mining method, MAXCONF, to mine highconfidence rules from microarray data. MAXCONF is a support-free algorithm that directly uses the confidence measure to effectively prune the search space. Experiments on three microarray data sets show that MAXCONF outperforms support-based rule mining with respect to scalability and rule extraction. Furthermore, detailed biological analyses demonstrate the effectiveness of our approach-the rules discovered by MAXCONF are substantially more interesting and meaningful compared with support-based methods. ",biology
683,Network-Based Drug-Target Interaction Prediction with Probabilistic Soft Logic,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Drug-target interaction studies are important because they can predict drugs unexpected therapeutic or adverse side effects. In silico predictions of potential interactions are valuable and can focus effort on in vitro experiments. We propose a prediction framework that represents the problem using a bipartite graph of drug-target interactions augmented with drug-drug and target-target similarity measures and makes predictions using probabilistic soft logic (PSL). Using probabilistic rules in PSL, we predict interactions with models based on triad and tetrad structures. We apply (blocking) techniques that make link prediction in PSL more efficient for drug-target interaction prediction. We then perform extensive experimental studies to highlight different aspects of the model and the domain, first comparing the models with different structures and then measuring the effect of the proposed blocking on the prediction performance and efficiency. We demonstrate the importance of rule weight learning in the proposed PSL model and then show that PSL can effectively make use of a variety of similarity measures. We perform an experiment to validate the importance of collective inference and using multiple similarity measures for accurate predictions in contrast to non-collective and single similarity assumptions. Finally, we illustrate that our PSL model achieves state-of-the-art performance with simple, interpretable rules and evaluate our novel predictions using online data sets.",biology
684,Development and evaluation of an open-ended computational evolution system for the genetic analysis of susceptibility to common human diseases,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","An important goal of human genetics is to identify DNA sequence variations that are predictive of susceptibility to common human diseases. This is a classification problem with data consisting of discrete attributes and a binary outcome. A variety of different machine learning methods based on artificial evolution have been developed and applied to modeling the relationship between genotype and phenotype. While artificial evolution approaches show promise, they are far from perfect and are only loosely based on real biological and evolutionary processes. It has recently been suggested that a new paradigm is needed where ""artificial evolution"" is transformed to ""computational evolution"" (CE) by incorporating more biological and evolutionary complexity into existing algorithms. It has been proposed that CE systems will be more likely to solve problems of interest to biologists and biomedical researchers. The goal of the present study was to develop and evaluate a prototype CE system for the analysis of human genetics data. We describe here this new open-ended CE system and provide initial results from a simulation study that suggests more complex operators result in better solutions. 漏 2008 Springer-Verlag Berlin Heidelberg.",biology
685,A random walk based approach for improving protein-protein interaction network and protein complex prediction,Bioinformatics and Biomedicine,"Recent advances in high-throughput technology have dramatically increased the quantity of available protein-protein interaction (PPI) data and stimulated the development of many methods for predicting protein complexes, which are important in understanding the functional organization of protein-protein interaction networks in different biological processes. However, automated protein complex prediction from PPI data alone is significantly hindered by the high level of noise, sparseness, and highly skewed degree distribution of PPI networks. Here we present a novel network topology-based algorithm to remove spurious interactions and recover missing ones by computational predictions, and to increase the accuracy of protein complex prediction by reducing the impact of hub nodes. The key idea of our algorithm is that two proteins sharing some high-order topological similarities, which are measured by a novel random walk-based procedure, are likely interacting with each other and may belong to the same protein complex. Applying our algorithm to a yeast protein-protein interaction network, we found that the interactions in the reconstructed PPI network have more significant biological relevance than the original network, assessed by multiple types of information, including gene ontology, gene expression, essentiality, conservation between species, and known protein complexes. Comparison with several existing methods show that the network reconstructed by our method has the highest quality. Finally, using two independent graph clustering algorithms, we found that the reconstructed network has resulted in significantly improved prediction accuracy of protein complexes.",biology
686,Two Approaches for Biomedical Text Classification,International Conference on Bioinformatics and Biomedical Engineering,"Automatic text classification systems can be especially valuable to biomedical researchers who seek to discover knowledge from terabyte-scale biomedical literatures. Different from the general domain, biomedical literatures contain a large number of named entities, complicated session structures and rich ontology resources. Taking these features into account, two approaches for biomedical text classification are presented, i.e., concept expansion and Meta-classification. Concept expansion is a method that introduces concept features using biomedical named entity recognition. Meta-classification is to combine the classification results of different parts of the full-text article and ontology resources using a Logistic regression model. The experiment results on the test set of TREC 2005 genomics track categorization task show that these techniques can improve the performance of the classification system consistently for all the classes.",biology
687,A Diagnostic Method Based on Tongue Imaging Morphology,International Conference on Bioinformatics and Biomedical Engineering,"The tongue morphology analysis is an important part of imaging diagnosis of tongue inspection. This paper studies and analyzes the tongue shape to establish a kind of tongue diagnosis method based on tongue images. CCD devices were employed to acquire the front and lateral images of tongue, and then measure the tongue's length (L), width (K) and height (H). Furthermore, in 500 testees with possible disorders, the coefficient of variation(CV) was corrected and adjusted to establish an optimum formula between the body surface area (Mt) and the sum of the width of the tongue (K) and height (H). Clinical studies show that this tongue judgment method is of high accuracy. The accuracy rate for fat tongue and thin tongue reaches 93.40% and 88.57% respectively. In addition, this method has been proven to be valuable in the diagnosis of diabetes mellitus, hypertension, chronic gastritis and hyperthyroidism.",biology
688,GenMiner: Mining Informative Association Rules from Genomic Data,Bioinformatics and Biomedicine,"GENMINER is a smart adaptation of closed itemsets based association rules extraction to genomic data. It takes advantage of the novel NORDI discretization method and of the CLOSE [27] algorithm to efficiently generate min- imal non-redundant association rules. GENMINER facili- tates the integration of numerous sources of biological in- formation such as gene expressions and annotations, and can tacitly integrate qualitative information on biological conditions (age, sex, etc.). We validated this approach ana- lyzing the microarray datasets used by Eisen et al. [10] with several sources of biological annotations. Extracted asso- ciations revealed significant co-annotated and co-expressed gene patterns, showing important biological relationships between genes and their features. Several of these relation- ships are supported by recent biological literature.",biology
689,Utilizing RNA-Seq data for cancer network inference,International Conference on Bioinformatics,"An important challenge in cancer systems biology is to uncover the complex network of interactions between genes (tumor suppressor genes and oncogenes) implicated in cancer. Next generation sequencing provides unparalleled ability to probe the expression levels of the entire set of cancer genes and their transcript isoforms. However, there are onerous statistical and computational issues in interpreting high-dimensional sequencing data and inferring the underlying genetic network. In this study, we analyzed RNA-Seq data from lymphoblastoid cell lines derived from a population of 69 human individuals and implemented a probabilistic framework to construct biologically-relevant genetic networks. In particular, we employed a graphical lasso analysis, motivated by considerations of the maximum entropy formalism, to estimate the sparse inverse covariance matrix of RNA-Seq data. Gene ontology, pathway enrichment and protein-protein path length analysis were all carried out to validate the biological context of the predicted network of interacting cancer gene isoforms.",biology
690,A Benign and Malignant Mass Classification Algorithm Based on an Improved Level Set Segmentation and Texture Feature Analysis,International Conference on Bioinformatics and Biomedical Engineering,"In this paper, we investigate the classification of masses with texture features. We propose an improved level set method to find the boundary of a mass, based on the initial contour provided by radiologists. After the boundary of a mass is found, texture features from Gray Level Co-occurrence Matrix (GLCM) are extracted from the surrounding area of the boundary of the mass. The extracted texture features are used as the input of Linear discriminant analysis and a support vector machine to classify the mass as benign or malignant. Mammography images from DDSM were used in the experiments and the classification accuracy was evaluated using the area under the receiver operating characteristic (ROC) curve. In the proposed method the area under the ROC curve is Az =0.7. The experimental result shows the effectiveness of the proposed method.",biology
691,Algorithm for DNA sequence compression based on prediction of mismatch bases and repeat location,Bioinformatics and Biomedicine,"For DNA sequence Compression, it has been observed that methods based on Markov modeling and repeats give best results. However, these methods tend to use uniform distribution assumption of mismatches for approximate repeats. We show that these replacements are not uniformly distributed and we can improve compression efficiency by using non uniform distribution for mismatches. We also propose a hash table based method to predict repeat location which works well for block based genomic sequence compression algorithms. The proposed methods give good compression gains. The method can be incorporated into any algorithm that uses approximate repeats to realize similar gains.",biology
692,Operon Prediction Using Chaos Embedded Particle Swarm Optimization,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Operons contain valuable information for drug design and determining protein functions. Genes within an operon are co-transcribed to a single-strand mRNA and must be coregulated. The identification of operons is, thus, critical for a detailed understanding of the gene regulations. However, currently used experimental methods for operon detection are generally difficult to implement and time consuming. In this paper, we propose a chaotic binary particle swarm optimization (CBPSO) to predict operons in bacterial genomes. The intergenic distance, participation in the same metabolic pathway and the cluster of orthologous groups (COG) properties of the Escherichia coli genome are used to design a fitness function. Furthermore, the Bacillus subtilis, Pseudomonas aeruginosa PA01, Staphylococcus aureus and Mycobacterium tuberculosis genomes are tested and evaluated for accuracy, sensitivity, and specificity. The computational results indicate that the proposed method works effectively in terms of enhancing the performance of the operon prediction. The proposed method also achieved a good balance between sensitivity and specificity when compared to methods from the literature. ÂŠ 2004-2012 IEEE.",biology
693,Molecular Function Prediction Using Neighborhood Features,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The recent advent of high-throughput methods has generated large amounts of gene interaction data. This has allowed the construction of genomewide networks. A significant number of genes in such networks remain uncharacterized and predicting the molecular function of these genes remains a major challenge. A number of existing techniques assume that genes with similar functions are topologically close in the network. Our hypothesis is that genes with similar functions observe similar annotation patterns in their neighborhood, regardless of the distance between them in the interaction network. We thus predict molecular functions of uncharacterized genes by comparing their functional neighborhoods to genes of known function. We propose a two-phase approach. First, we extract functional neighborhood features of a gene using Random Walks with Restarts. We then employ a KNN classifier to predict the function of uncharacterized genes based on the computed neighborhood features. We perform leave-one-out validation experiments on two S. cerevisiae interaction networks and show significant improvements over previous techniques. Our technique provides a natural control of the trade-off between accuracy and coverage of prediction. We further propose and evaluate prediction in sparse genomes by exploiting features from well-annotated genomes. ÂŠ 2006 IEEE.",biology
694,Prioritization of candidate genes based on disease similarity and protein's proximity in PPI networks,Bioinformatics and Biomedicine,"Identifying the genes causing a genetic disease is a key challenge in human health. Recently molecular interaction data has been used to prioritize candidate genes with respect to a particular disease. As a result, different methods have been implemented to rank genes which cause a given disease. However it has been suggested in literature that, to prioritize candidate genes it is necessary to consider disease similarity along with the protein's proximity to disease genes in a protein-protein interaction (PPI) network. This paper proposes a new algorithm called proximity disease similarity algorithm (ProSim) which considers both properties simultaneously. Prostate cancer, Alzheimer disease and diabetes mellitus type 2 case studies are then used to test the proposed method. Results in terms of leave-one-out cross validation and ROC curves indicate that the proposed approach outperforms existing methods.",biology
695,Learning from positive and unlabeled documents for automated detection of alternative splicing sentences in medline abstracts,Bioinformatics and Biomedicine,"Alternative splicing is considered to be a key factor underlying increased cellular and functional complexity in higher eukaryotes. With the advance of high-throughput genomics technologies, it becomes critical to mine alternative splicing knowledge from biological research literature. Meanwhile, there have been many papers published on DNA splicing and translation and it is time-consuming to find papers specifically relevant to alternative splicing. Observing that documents reporting alternative splicing can be obtained from existing knowledge bases recording literature evidences and also that a large number of unlabeled documents are freely available, we investigated learning from positive and unlabeled data (LPU) for retrieving papers relevant to alternative splicing. The positive documents are from Literature Support for Alternative Transcripts (LSAT) and unlabeled documents are obtained from Gene Reference Into Function (GeneRIF). We generated nine unlabeled datasets different in size or the way documents were sampled, and compared the performance of document classifiers built using different unlabeled datasets and machine learning algorithms. The study shows that LPU is a viable strategy to build document filtering system, while the performance of trained classifiers is affected by the choice of the unlabeled data set. Selection of machine learning algorithms and that of unlabeled documents would be critical in constructing an effective LPU-based system.",biology
696,Improving health records search using multiple query expansion collections,Bioinformatics and Biomedicine,"The increasing prevalence of electronic health records (EHR), along with the needs for enhanced clinical care, presents new challenges to information retrieval (IR). Many clinical decision-making tasks following the philosophy of Evidence-Based Medicine (EBM) rely on the ability to find relevant health records and gather sufficient clinical evidence under severe time constraints. In this work, we present a system built upon statistical IR methods for searching flat-text health records (i.e. the doctors' notes sections of EHR) for patients with particular conditions specified via a keyword query. In particular, we use multiple external repositories for query expansion, and introduce two novel model weighting methods. Cross-validation results show that our system improves a strong baseline by 30% on mean average precision (MAP), and has a promising overall performance when compared with a manual system doing the same task.",biology
697,Visual Exploration of Three-Dimensional Gene Expression Using Physical Views and Linked Abstract Views,IEEE/ACM Transactions on Computational Biology and Bioinformatics," During animal development, complex patterns of gene expression provide positional information within the embryo. To better understand the underlying gene regulatory networks, the Berkeley ",biology
698,Arlequin (version 3.0): An integrated software package for population genetics data analysis,Evolutionary Bioinformatics," Arlequin ver 3.0 is a software package integrating several basic and advanced methods for population genetics data analysis, like the computation of standard genetic diversity indices, the estimation of allele and haplotype frequencies, tests of departure from linkage equilibrium, departure from selective neutrality and demographic equilibrium, estimation or parameters from past population expansions, and thorough analyses of population subdivision under the AMOVA framework. Arlequin 3 introduces a completely new graphical interface written in C++, a more robust semantic analysis of input files, and two new methods: a Bayesian estimation of gametic phase from multi-locus genotypes, and an estimation of the parameters of an instantaneous spatial expansion from DNA sequence polymorphism. Arlequin can handle several data types like DNA sequences, microsatellite data, or standard multi-locus genotypes. A Windows version of the software is freely available on http://cmpg.unibe.ch/software/arlequin3. ",biology
699,A Bayesian graphical model for integrative analysis of TCGA data,International Conference on Bioinformatics,"We integrate three TCGA data sets including measurements on matched DNA copy numbers (C), DNA methylation (M), and mRNA expression (E) over 500+ ovarian cancer samples. The integrative analysis is based on a Bayesian graphical model treating the three types of measurements as three vertices in a network. The graph is used as a convenient way to parameterize and display the dependence structure. Edges connecting vertices infer specific types of regulatory relationships. For example, an edge between M and E and a lack of edge between C and E implies methylation-controlled transcription, which is robust to copy number changes. In other words, the mRNA expression is sensitive to methylational variation but not copy number variation. We apply the graphical model to each of the genes in the TCGA data independently and provide a comprehensive list of inferred profiles. Examples are provided based on simulated data as well.",biology
700,Virtual error: a new measure for evolutionary biclustering,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics"," Many heuristics used for finding biclusters in microarray data use the mean squared residue as a way of evaluating the quality of biclusters. This has led to the discovery of interesting biclusters. Recently it has been proven that the mean squared residue may fail to identify some interesting biclusters. This motivates us to introduce a new measure, called Virtual Error, for assessing the quality of biclusters in microarray data. In order to test the validity of the proposed measure, we include it within an evolutionary algorithm. Experimental results show that the use of this novel measure is effective for finding interesting biclusters, which could not have been discovered with the use of the mean squared residue. ",biology
701,Discovering Syndromes in Coronary Heart Disease by Cluster Algorithm Based on Random Neural Network,International Conference on Bioinformatics and Biomedical Engineering,"Integration of western medicine and Traditional Chinese Medicine (TCM) to cure Coronary Heart Disease (CHD) is taken by more and more Chinese. However, the gap between both medical theory systems is still wide. The goal of this contribution is to bridge the gap between them by standardizing syndromes of Traditional Chinese Medicine. We carry out a clinical epidemiology survey of Coronary Heart Disease and obtain 1069 cases. Each case is certainly a CHD case based on the evidence from Coronary Artery Angiography. It includes 78 symptoms and is diagnosed by TCM mentors as syndrome or syndrome combinations. We proposed an unsupervised cluster algorithm to partition 78 symptoms into several clusters. Each cluster is diagnosed by TCM mentor as syndrome and is clinically verified. The obtained seven clusters correspond to seven syndromes in TCM and the clinical verification consolidates the result. Each cluster is used as selected attributes to performe classification and the resulting accuracy is higher than 90%, which indicates that the cluster is successful and the data surveyed is of high quality. The investigation of the cluster algorithm to CHD data to retrieve syndromes in CHD successfully bridges gap between western medicine and TCM.",biology
702,A Blocking Strategy to Improve Gene Selection for Classification of Gene Expression Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Because of high dimensionality, machine learning algorithms typically rely on feature selection techniques in order to perform effective classification In microarray gene expression data sets. However, the large number of features compared to the number of samples makes the task of feature selection computationally hard and prone to errors. This paper interprets feature selection as a task of stochastic optimization, where the goal is to select among an exponential number of alternative gene subsets the one expected to return the highest generalization in classification. Blocking is an experimental design strategy which produces similar experimental conditions to compare alternative stochastic configurations in order to be confident that observed differences in accuracy are due to actual differences rather than to fluctuations and noise effects. We propose an original blocking strategy for improving feature selection which aggregates in a paired way the validation outcomes of several learning algorithms to assess a gene subset and compare it to others. This is a novelty with respect to conventional wrappers, which commonly adopt a sole learning algorithm to evaluate the relevance of a given set of variables. The rationale of the approach is that, by increasing the amount of experimental conditions under which we validate a feature subset, we can lessen the problems related to the scarcity of samples and consequently come up with a better selection. The paper shows that the blocking strategy significantly improves the performance of a conventional forward selection for a set of 16 publicly available cancer expression data sets. The experiments involve six different classifiers and show that improvements take place independent of the classification algorithm used after the selection step. Two further validations based on available biological annotation support the claim that blocking strategies in feature selection may improve the accuracy and the quality of the solution. The first validation is based on retrieving PubMEd abstracts associated to the selected genes and matching them to regular expressions describing the biological phenomenon underlying the expression data sets. The biological validation that follows Is based on the use of the Bioconductor package GoStats in order to perform Gene Ontology statistical analysis. © 2007 IEEE.",biology
703,Debugging SNOMED CT Using Axiom Pinpointing in the Description Logic EL,International Conference on Bioinformatics," Snomed ct is a large-scale medical ontology, which is developed using a variant of the inexpressive Description Logic E L. Description Logic reasoning can not only be used to compute subsumption relationships between Snomed concepts, but also to pinpoint the reason why a certain subsumption relationship holds by computing the axioms responsible for this relationship. This helps developers and users of Snomed ct to understand why a given subsumption relationship follows from the ontology, which can be seen as a first step toward removing unwanted subsumption relationships. In this paper, we describe a new method for axiom pinpointing in the Description Logic E L+, which is based on the computation of so-called reachabilitybased modules. Our experiments on Snomed ct show that the sets of axioms explaining subsumption are usually quite small, and that our method is fast enough to compute such sets on demand. ",biology
704,Protein Function Prediction withIncomplete Annotations,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Automated protein function prediction is one of the grand challenges in computational biology. Multi-label learning is widely used to predict functions of proteins. Most of multi-label learning methods make prediction for unlabeled proteins under the assumption that the labeled proteins are completely annotated, i.e., without any missing functions. However, in practice, we may have a subset of the ground-truth functions for a protein, and whether the protein has other functions is unknown. To predict protein functions with incomplete annotations, we propose a Protein Function Prediction method with Weak-label Learning (ProWL) and its variant ProWL-IF. Both ProWL and ProWL-IF can replenish the missing functions of proteins. In addition, ProWL-IF makes use of the knowledge that a protein cannot have certain functions, which can further boost the performance of protein function prediction. Our experimental results on protein-protein interaction networks and gene expression benchmarks validate the effectiveness of both ProWL and ProWL-IF. © 2004-2012 IEEE.",biology
705,On the Complexity of uSPR Distance,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"We show that subtree prune and regraft (uSPR) distance on unrooted trees is fixed parameter tractable with respect to the distance. We also make progress on a conjecture of Steel [9] on the preservation of uSPR distance under chain reduction, improving on lower bounds of Hickey et al. [7]. © 2006 IEEE.",biology
706,Discovering Temporal Associations among Significant Changes in Gene Expression,Bioinformatics and Biomedicine,"One of the most demanding problems in mining temporal data is to identify how multivariate change associations might be discovered and used to better understand data interactions and dependencies. This paper introduces a framework to mine associations among significant changes in multivariate time-series data. Building on statistical methods, we detect significant changes in time-series data and use marginal change rates to qualify the direction of change at significant change points. Furthermore, a propositional confirmation-guided rule discovery method is used to discover associations among these significant changes. We apply our approach to gene expression data measured in yeast cell cycles and demonstrate that our method can learn novel and high-quality significant change associations among different genes. Such associations can be used to cluster genes and build gene interaction networks.",biology
707,Initializing Partition-Optimization Algorithms,IEEE/ACM Transactions on Computational Biology and Bioinformatics," - Clustering datasets is a challenging problem needed in a wide array of applications. Partition-optimization approaches, such as k-means or expectation-maximization (EM) algorithms, are sub-optimal and find solutions in the vicinity of their initialization. This paper proposes a staged approach to specifying initial values by finding a large number of local modes and then obtaining representatives from the most separated ones. Results on test experiments are excellent. We also provide a detailed comparative assessment of the suggested algorithm with many commonlyu-sed initialization approaches in the literature. Finally, the methodology is applied to two datasets on diurnal microarray gene expressions and industrial releases of mercury. ",biology
708,Mining gene expression data focusing cancer therapeutics: a digest,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"An understanding towards genetics and epigenetics is essential to cope up with the paradigm shift which is underway. Personalized medicine and gene therapy will confluence the days to come. This review highlights traditional approaches as well as current advancements in the analysis of the gene expression data from cancer perspective. Due to improvements in biometric instrumentation and automation, it has become easier to collect a lot of experimental data in molecular biology. Analysis of such data is extremely important as it leads to knowledge discovery that can be validated by experiments. Previously, the diagnosis of complex genetic diseases has conventionally been done based on the non-molecular characteristics like kind of tumor tissue, pathological characteristics, and clinical phase. The microarray data can be well accounted for high dimensional space and noise. Same were the reasons for ineffective and imprecise results. Several machine learning and data mining techniques are presently applied for identifying cancer using gene expression data. While differences in efficiency do exist, none of the well-established approaches is uniformly superior to others. The quality of algorithm is important, but is not in itself a guarantee of the quality of a specific data analysis. ÂŠ 2014 IEEE.",biology
709,Mutagenic Primer Design for Mismatch PCR-RFLP SNP Genotyping Using a Genetic Algorithm,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Polymerase chain reaction-restriction fragment length polymorphism (PCR-RFLP) is useful in small-scale basic research studies of complex genetic diseases that are associated with single nucleotide polymorphism (SNP). Designing a feasible primer pair is an important work before performing PCR-RFLP for SNP genotyping. However, in many cases, restriction enzymes to discriminate the target SNP resulting in the primer design is not applicable. A mutagenic primer is introduced to solve this problem. GA-based Mismatch PCR-RFLP Primers Design (GAMPD) provides a method that uses a genetic algorithm to search for optimal mutagenic primers and available restriction enzymes from REBASE. In order to improve the efficiency of the proposed method, a mutagenic matrix is employed to judge whether a hypothetical mutagenic primer can discriminate the target SNP by digestion with available restriction enzymes. The available restriction enzymes for the target SNP are mined by the updated core of SNP-RFLPing. GAMPD has been used to simulate the SNPs in the human SLC6A4 gene under different parameter settings and compared with SNP Cutter for mismatch PCR-RFLP primer design. The in silico simulation of the proposed GAMPD program showed that it designs mismatch PCR-RFLP primers. The GAMPD program is implemented in JAVA and is freely available at http://bio.kuas.edu.tw/gampd/ ÂŠ 2006 IEEE.",biology
710,Lessons learned from aligning two representations of anatomy,International Conference on Bioinformatics," The objective of this study is to analyze the comparison, through their results, of two distinct methods applied to aligning two representations of anatomy. The same versions of FMA and GALEN were aligned by each method. 2199 concept matches were obtained by both methods. For matches identified by one method only (337 and 336 respectively), we analyzed the reasons that caused the other method to fail. Alignment 1 could be improved by addressing partial lexical matches and identifying matches based solely on structural similarity. Alignment 2 may be improved by taking into account synonyms in FMA and identifying semantic mismatches. However, both methods identify only a fraction of all possible matches and new approaches need to be explored in order to handle more complex matches. ",biology
711,Pattern Classification for Doppler Ultrasonic Wrist Pulse Signals,International Conference on Bioinformatics and Biomedical Engineering,"Wrist pulse signal contains important information about the health status of a person and it has been used in Traditional Chinese Medicine for a long time. In this work, digitalized wrist pulse signals from patients with different diseases as well as healthy persons are collected by a Doppler ultrasonic device. Two methods, namely, the wavelet method and the auto regressive prediction error (ARPE) method, are proposed to analyze the pulse signals and distinguish patients from healthy persons. Distinctive features are first extracted from the pulse signals and then the support vector machine (SVM) is used for classification. The applicability of the methods is investigated using wrist pulse signals collected from 50 healthy persons and 74 patients. The results illustrate a great promise of the proposed methods for computerized pulse signal analysis.",biology
712,The Development of a Schema for the Annotation of Terms in the Biocaster Disease Detecting/Tracking System,International Conference on Bioinformatics," Amid growing public concern about the spread of infectious diseases such as avian influenza and SARS, there is an increasing need for collecting timely and reliable information about disease outbreaks from natural language data such as online news articles. In this paper we introduce BioCaster, a text mining-based system for infectious disease detection and tracking currently being developed, and discuss the development of a domain ontology and schema for the annotation of terms. In particular we focus on the comparison between two approaches, 1) a traditional task-oriented approach with a simple schema that does not strictly follow ontological principles, and 2) a formal approach which is ontologically well-founded but adds extra requirements to the annotation schema. We report on several critical problems that were highlighted by an entity annotation experiment, attributable to the purely task-oriented ontology design. A second experiment based on a formally constructed ontology produced improved annotation results despite the apparent complexity of the annotation schema. ",biology
713,Incorporating Nonlinear Relationships in Microarray Missing Value Imputation,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Microarray gene expression data often contain missing values. Accurate estimation of the missing values is important for downstream data analyses that require complete data. Nonlinear relationships between gene expression levels have not been well-utilized in missing value imputation. We propose an imputation scheme based on nonlinear dependencies between genes. By simulations based on real microarray data, we show that incorporating nonlinear relationships could improve the accuracy of missing value imputation, both in terms of normalized root-mean-squared error and in terms of the preservation of the list of significant genes in statistical testing. In addition, we studied the impact of artificial dependencies introduced by data normalization on the simulation results. Our results suggest that methods relying on global correlation structures may yield overly optimistic simulation results when the data have been subjected to row (gene)-wise mean removal. ÂŠ 2011 IEEE.",biology
714,A Framework for Multiple Kernel Support Vector Regression and Its Applications to siRNA Efficacy Prediction,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The cell defense mechanism of RNA interference has applications in gene function analysis and promising potentials in human disease therapy. To effectively silence a target gene, it is desirable to select appropriate initiator siRNA molecules having satisfactory silencing capabilities. Computational prediction for silencing efficacy of siRNAs can assist this screening process before using them in biological experiments. String kernel functions, which operate directly on the string objects representing siRNAs and target mRNAs, have been applied to support vector regression for the prediction and improved accuracy over numerical kernels in multidimensional vector spaces constructed from descriptors of siRNA design rules. To fully utilize information provided by string and numerical data, we propose to unify the two in a kernel feature space by devising a multiple kernel regression framework where a linear combination of the kernels is used. We formulate the multiple kernel learning into a quadratically constrained quadratic programming (QCQP) problem, which although yields global optimal solution, is computationally demanding and requires a commercial solver package. We further propose three heuristics based on the principle of kernel-target alignment and predictive accuracy. Empirical results demonstrate that multiple kernel regression can improve accuracy, decrease model complexity by reducing the number of support vectors, and speed up computational performance dramatically. In addition, multiple kernel regression evaluates the importance of constituent kernels, which for the siRNA efficacy prediction problem, compares the relative significance of the design rules. Finally, we give insights into the multiple kernel regression mechanism and point out possible extensions. © 2006 IEEE.",biology
715,Tumor Clustering Based on Penalized Matrix Decomposition,International Conference on Bioinformatics and Biomedical Engineering,"A reliable and precise identification of the type of tumors is essential for effective treatment of cancer. In this paper, we proposed a novel method to cluster tumors using gene expression data. In this method, we use penalized matrix decomposition (PMD) to extract metasamples from gene expression data. Specially, a metasample can capture structures inherent in the samples in one class. In addition, we present how to use the factors of PMD to cluster the samples. Compared with traditional methods, such as HC, SOM and NMF etc., our method can find the samples with complex classes. At the same time, the number of clusters can be determined automatically.",biology
716,A hybrid approach to recognising discourse causality in the biomedical domain,Bioinformatics and Biomedicine,"Whilst current domain-specific information extraction systems represent an important resource for biomedical researchers, the increasing amount of knowledge published daily is still overwhelming them. As such, automatic discourse causality recognition can further improve the search for relevant information by suggesting possible causal connections. We describe here an approach to the automatic recognition of discourse causality in the biomedical domain using a combination of machine learning and rules. We test and evaluate our system on BioCause, a corpus containing gold standard annotations of causal relations. The best performance in identifying triggers is achieved by CRFs with 79.35% F-score. We then locate the arguments using naïve syntactic rules, achieving F-scores of around 90% in most cases. Determining which argument plays which role is performed by a group of machine learners with an F-score of 84.35%.",biology
717,Inference of time-varying gene networks using constrained and smoothed Kalman filtering,International Conference on Bioinformatics," -This paper tackles the problem of recovering timevarying gene networks from a series of undersampled and noisy observations. Gene regulatory networks evolve over time in response to functional requirements in the cell and environmental conditions. Collected genetic profiles from dynamic biological processes, such as cell development, cancer progression and treatment recovery, underlie genetic interactions that rewire over the course of time. We formulate the problem of estimating time-varying networks in a state-space framework. We show that, due to the small number of measurements, the system is unobservable; thus making the application of the standard Kalman filter ineffective. We remedy the problem by performing simultaneous compression and state estimation. The sparsity property of gene regulatory networks is incorporated as a constraint in the Kalman filter, leading to a compressed Kalman estimate and reducing the number of required observations for effective tracking of the network. Moreover, we improve the estimation accuracy by taking into account the entire sample set for each time instant estimate of the network through a forwardbackward smoothing procedure. The proposed constrained and smoothed Kalman filter is shown to yield good tracking results for varying small and medium-size networks. ",biology
718,Reproducibility-Optimized Test Statistic for Ranking Genes in Microarray Studies,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"A principal goal of microarray studies is to identify the genes showing differential expression under distinct conditions. In such studies, the selection of an optimal test statistic is a crucial challenge, which depends on the type and amount of data under analysis. Although previous studies on simulated or spike-in data sets do not provide practical guidance on how to choose the best method for a given real data set, we introduce an enhanced reproducibility-optimization procedure, which enables the selection of a suitable gene-ranking statistic directly from the data. In comparison with existing ranking methods, the reproducibility-optimized statistic shows good performance consistently under various simulated conditions and on Affymetrix spike-in data set. Further, the feasibility of the novel statistic is confirmed in a practical research setting using data from an in-house cDNA microarray study of asthma-related gene expression changes. These results suggest that the procedure facilitates the selection of an appropriate test statistic for a given data set without relying on a priori assumptions, which may bias the findings and their interpretation. Moreover, the general reproducibility-optimization procedure is not limited to detecting differential expression only but could be extended to a wide range of other applications as well. © 2008 IEEE.",biology
719,A method for fast approximate searching of polypeptide structures in the PDB,German Conference on Bioinformatics, The main contribution of this paper is a novel approach for fast searching in huge structural databases like the PDB. The data structure is based on an adaption of the generalized suffix tree and relies on an translation- and rotation-invariant representation of the protein backbone. The method was evaluated by applying structural queries to the PDB and comparing the results to the established tool SPASM. Our experiments show that the new method reduces the query time by orders of magnitude while producing comparable results. ,biology
720,Antibody-Specified B-Cell Epitope Prediction in Line with the Principle of Context-Awareness,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Context-awareness is a characteristic in the recognition between antigens and antibodies, highlighting the reconfiguration of epitope residues when an antigen interacts with a different antibody. A coarse binary classification of antigen regions into epitopes, or nonepitopes without specifying antibodies may not accurately reflect this biological reality. Therefore, we study an antibody-specified epitope prediction problem in line with this principle. This problem is new and challenging as we pinpoint a subset of the antigenic residues from an antigen when it binds to a specific antibody. We introduce two kinds of associations of the contextual awareness: 1) residues-residues pairing preference, and 2) the dependence between sets of contact residue pairs. Preference plays a bridging role to link interacting paratope and epitope residues while dependence is used to extend the association from one-dimension to two-dimension. The paratope/epitope residues relative composition, cooperativity ratios, and Markov properties are also utilized to enhance our method. A nonredundant data set containing 80 antibody-antigen complexes is compiled and used in the evaluation. The results show that our method yields a good performance on antibody-specified epitope prediction. On the traditional antibody-ignored epitope prediction problem, a simplified version of our method can produce a competitive, sometimes much better, performance in comparison with three structure-based predictors. ÂŠ 2011 IEEE.",biology
721,Understanding the GeneRank Model,International Conference on Bioinformatics and Biomedical Engineering,"GeneRank is a new mathematical model in Bioinformatics, that is used in generating prioritized gene lists by exploiting biological background information. It is a modification of PageRank in the popular search engine Google. Using the matrix decomposition technique we find in this paper that there two dominant portions in the output of GeneRank. One is the information from the microarray data, and the other is the previous knowledge about the biological function of genes. The solution of the GeneRank can be approximated well by the linear combination of the two parts. Numerical results show the significance of the dominant parts. Thus the results of this paper give an understanding of GeneRank model.",biology
722,Annotated Stochastic Context Free Grammars for Analysis and Synthesis of Proteins,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics"," An important step to understand the main functions of a specific family of proteins is the detection of protein features that could reveal how protein chains are constituted. To achieve this aim we treated amino acid sequences of proteins as a formal language, building a ContextFree Grammar annotated using an n-gram Bayesian classifier. This formalism is able to analyze the connection between protein chains and protein functions. In order to design new protein chains with the properties of the considered family we performed a rule clustering of the grammar to build an Annotated Stochastic Context Free Grammar. Our methodology was applied to a class of Antimicrobial Peptides (AmPs): the Frog antimicrobial peptides family. Through this case study, our approach pointed out some important aspects regarding the relationship between sequences and functional domains of proteins and how protein domain motifs are preserved by natural evolution in to the amino acid sequences. Moreover our results suggest that the synthesis of new proteins with a given domain architecture can be one of the fields where application of Annotated Stochastic Context Free Grammars can be useful. ",biology
723,Comparison of Centralities for Biological Networks,German Conference on Bioinformatics," The analysis of biological networks involves the evaluation of the vertices within the connection structure of the network. To support this analysis we discuss five centrality measures and demonstrate their applicability on two example networks, a protein-protein-interaction network and a transcriptional regulation network. We show that all five centrality measures result in different valuations of the vertices and that for the analysis of biological networks all five measures are of interest. ",biology
724,Pattern-based phylogenetic distance estimation and tree reconstruction,Evolutionary Bioinformatics," We have developed an alignment-free method that calculates phylogenetic distances using a maximum-likelihood approach for a model of sequence change on patterns that are discovered in unaligned sequences. To evaluate the phylogenetic accuracy of our method, and to conduct a comprehensive comparison of existing alignment-free methods (freely available as Python package decaf+py at http://www.bioinformatics.org.au), we have created a data set of reference trees covering a wide range of phylogenetic distances. Amino acid sequences were evolved along the trees and input to the tested methods; from their calculated distances we infered trees whose topologies we compared to the reference trees. We find our pattern-based method statistically superior to all other tested alignment-free methods. We also demonstrate the general advantage of alignment-free methods over an approach based on automated alignments when sequences violate the assumption of collinearity. Similarly, we compare methods on empirical data from an existing alignment benchmark set that we used to derive reference distances and trees. Our pattern-based approach yields distances that show a linear relationship to reference distances over a substantially longer range than other alignment-free methods. The pattern-based approach outperforms alignment-free methods and its phylogenetic accuracy is statistically indistinguishable from alignment-based distances. ",biology
725,Differential Predictive Modeling for Racial Disparities in Breast Cancer,Bioinformatics and Biomedicine,The primary objective of disparities research is to model the differences across multiple groups and identify the groups that behave significantly different from each other. Independently generating various decision trees for different subsets of the data will not allow us to study the impact of the various attributes on these different subgroups. We propose a novel technique for inducing similar decision trees for different subpopulations and also develop a new distance metric between two decision trees which measures the difference in the underlying data distributions of these subgroups. The proposed framework is evaluated by analyzing the racial disparities in breast cancer. Our method was able to rank different populations with respect to the disparity and detect the attributes that are most responsible for such differences.,biology
726,Biologically-Inspired Identification of Plankton Based on Hierarchical Shape Semantics Modeling,International Conference on Bioinformatics and Biomedical Engineering,"This paper describes a novel hierarchical framework for automatic identification of plankton images, which is motivated by the semantics description of planktons used in the biology textbooks. The framework discretizes the identification of plankton into the recognition of various high-level shape semantics features. The semantics features are modeled with some manual instructions. Distinct from the previous approaches, such as ""PCA+SVM"" and ""classifier stacking"", our algorithm is more similar to the recognition procedure used by the biology experts, and the extracted features are more efficient for identification. The approach is tested on a collection of more than 2000 plankton images. Results demonstrate that the proposed approach has a satisfying classification accuracy and robustness to different number of training samples.",biology
727,Compressed Pattern Matching in DNA Sequences Using Multithreaded Technology,International Conference on Bioinformatics and Biomedical Engineering,"Compressed pattern matching on large DNA sequences data is very important in bioinformatics. In this paper, in order to improve the performance by searching pattern in parallel time, multithreaded programming technique is used. Then, two novel multithreaded algorithms are proposed, named MTd-BM and MTd-Horspool. The first one is a mutation of d-BM algorithm, which is based on Boyer-Moore method. And the second one is designed in the similitude of MTd-BM, but using Horspool method as its foundation. The experimental results show that these two algorithms are nearly 2 times faster than the d-BM algorithm for long DNA pattern (length>50). Moreover, compression of DNA sequences gives a guaranteed space saving of 75%.",biology
728,Representing Clinical Information using SNOMED Clinical Terms with Different Structural Information Models,International Conference on Bioinformatics," Findings related to developing implementation specifications for the use of SNOMED Clinical Terms (SNOMED CT) in both HL7 and openEHR information models are summarized and compared. Common themes from this work, including overlaps between the expressivity of structure and terminology, are identified and discussed. Distinctions are made between aspects of meaning that are most readily represented by distinct structures, others where terminology offers greater flexibility and a 'gray-area' in which the relative merits are more balanced. Focusing on particular stages in the clinical information life cycle may suggest different points of balance and may lead to different approaches to integration. However, greater consistency is essential if clinical information is to be used effectively in electronic record systems. Consensus guidance documents of the type developed by the work described are only a first step. Mutually aware evolutionary refinement of structural and terminology standards is suggested as an enhancement to independent development. ",biology
729,Reconstruction of 3D Structures From Protein Contact Maps,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The prediction of the protein tertiary structure from solely its residue sequence (the so-called Protein Folding Problem) is one of the most challenging problems in Structural Bioinformatics. We focus on the protein residue contact map. When this map is assigned, it is possible to reconstruct the 3D structure of the protein backbone. The general problem of recovering a set of 3D coordinates consistent with some given contact map is known as a unit-disk-graph realization problem, and it has been recently proven to be NP-hard. In this paper, we describe a heuristic method (COMAR) that is able to reconstruct with an unprecedented rate (3-15 seconds) a 3D model that exactly matches the target contact map of a protein. Working with a nonredundant set of 1,760 proteins, we find that the scoring efficiency of finding a 3D model very close to the protein native structure depends on the threshold value adopted to compute the protein residue contact map. Contact maps whose threshold values range from 10 to 18 Angstroms allow reconstructing 3D models that are very similar to the proteins native structure. © 2008 IEEE.",biology
730,Information-Theoretic Inference of Gene Networks Using Backward Elimination.,International Conference on Bioinformatics," - Unraveling transcriptional regulatory networks is essential for understanding and predicting cellular responses in different developmental and environmental contexts. Information-theoretic methods of network inference have been shown to produce high-quality reconstructions because of their ability to infer both linear and non-linear dependencies between regulators and targets. In this paper, we introduce MRNETB an improved version of the previous information-theoretic algorithm, MRNET, which has competitive performance with state-of-the-art algorithms. MRNET infers a network by using a forward selection strategy to identify a maximally-independent set of neighbors for every variable. However, a known limitation of algorithms based on forward selection is that the quality of the selected subset strongly depends on the first variable selected. In this paper, we present MRNETB, an improved version of MRNET that overcomes this limitation by using a backward selection strategy followed by a sequential replacement. Our new variable selection procedure can be implemented with the same computational cost as the forward selection strategy. MRNETB was benchmarked against MRNET and two other information-theoretic algorithms, CLR and ARACNE. Our benchmark comprised 15 datasets generated from two regulatory network simulators, 10 of which are from the DREAM4 challenge, which was recently used to compare over 30 network inference methods. To assess stability of our results, each method was implemented with two estimators of mutual information. Our results show that MRNETB has significantly better performance than MRNET, irrespective of the mutual information estimation method. MRNETB also performs comparably to CLR and significantly better than ARACNE indicating that our new variable selection strategy can successfully infer high-quality networks. ",biology
731,Multi-instance learning for skin biopsy image features recognition,Bioinformatics and Biomedicine,"In this paper, a multi-instance learning framework is introduced to solve the problem of skin biopsy image features recognition. Previously reported methods for skin surface images were mostly based on color features extraction. They are incapable to be directly applied to skin biopsy image features recognition because biopsy images are often dyed and have obvious inner structures with different textures. Therefore, we regard skin biopsy images as multi-instance samples, whose instances are regions or structures captured by applying Normalized Cut. Texture feature extraction methods are used to express each region as a vectorial expression. Then two multi-instance learning algorithms reported successful in various image retrieval tasks were applied. Nine features were manually selected as target features to evaluate the proposed method on a skin disease diagnosis datasets of 6579 biopsy images from 2010 to 2011. The result showed that the proposed method is effective and medically acceptable.",biology
732,Biomedical concept extraction using concept graphs and ontology-based mapping,Bioinformatics and Biomedicine,"Assigning keywords to articles can be extremely costly. In this paper we propose a new approach to biomedical concept extraction using semantic features of concept graphs to help in automatic labeling of scientific publications. The proposed system extracts key concepts similar to author-provided keywords. We represent full-text documents by graphs and map biomedical terms to predefined ontology concepts. In addition to occurrence frequency weights, we use concept relation weights to rank potential key concepts. We compare our technique to that of KEA's, a state-of-the-art keyphrase extraction software. The results show that using the relations weight significantly improves the performance of concept extraction. The results also highlight the subjectivity of the concept extraction procedure as well as of its evaluation.",biology
733,Gradient-Based Optimization of Kernel-Target Alignment for Sequence Kernels Applied to Bacterial Gene Start Detection,IEEE/ACM Transactions on Computational Biology and Bioinformatics," Biological data mining using kernel methods can be improved by a task-specific choice of the kernel function. Oligo kernels for genomic sequence analysis have proven to have a high discriminative power and to provide interpretable results. Oligo kernels that consider subsequences of different lengths can be combined and parameterized to increase their flexibility. For adapting these parameters efficiently, gradient-based optimization of the kernel-target alignment is proposed. The power of this new, general model selection procedure and the benefits of fitting kernels to problem classes are demonstrated by adapting oligo kernels for bacterial gene start detection. ",biology
734,Correlation–based scatter search for discovering biclusters from gene expression data,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","Scatter Search is an evolutionary method that combines existing solutions to create new offspring as the well-known genetic algorithms. This paper presents a Scatter Search with the aim of finding biclusters from gene expression data. However, biclusters with certain patterns are more interesting from a biological point of view. Therefore, the proposed Scatter Search uses a measure based on linear correlations among genes to evaluate the quality of biclusters. As it is usual in Scatter Search methodology an improvement method is included which avoids to find biclusters with negatively correlated genes. Experimental results from yeast cell cycle and human B-cell lymphoma datasets are reported showing a remarkable performance of the proposed method and measure. © 2010 Springer-Verlag Berlin Heidelberg.",biology
735,A New Machine Learning Approach for Protein Phosphorylation Site Prediction in Plants,International Conference on Bioinformatics,"Protein phosphorylation is a crucial regulatory mechanism in various organisms. With recent improvements in mass spectrometry, phosphorylation site data are rapidly accumulating. Despite this wealth of data, computational prediction of phosphorylation sites remains a challenging task. This is particularly true in plants, due to the limited information on substrate specificities of protein kinases in plants and the fact that current phosphorylation prediction tools are trained with kinase-specific phosphorylation data from non-plant organisms. In this paper, we proposed a new machine learning approach for phosphorylation site prediction. We incorporate protein sequence information and protein disordered regions, and integrate machine learning techniques of k-nearest neighbor and support vector machine for predicting phosphorylation sites. Test results on the PhosPhAt dataset of phosphoserines in Arabidopsis and the TAIR7 non-redundant protein database show good performance of our proposed phosphorylation site prediction method.",biology
736,Predicting Protein-Protein Interactions from Protein Domains Using a Set Cover Approach,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"One goal of contemporary proteome research is the elucidation of cellular protein interactions. Based on currently available protein-protein interaction and domain data, we introduce a novel method, Maximum Specificity Set Cover (MSSC), for the prediction of protein-protein interactions. In our approach, we map the relationship between interactions of proteins and their corresponding domain architectures to a generalized weighted set cover problem. The application of a greedy algorithm provides sets of domain interactions which explain the presence of protein interactions to the largest degree of specificity. Utilizing domain and protein interaction data of S. cerevisiae. MSSC enables prediction of previously unknown protein interactions, links that are well supported by a high tendency of coexpression and functional homogeneity of the corresponding proteins. Focusing on concrete examples, we show that MSSC reliably predicts protein interactions in well-studied molecular systems, such as the 26S proteasome and RNA polymerase II of S. cerevisiae. We also show that the quality of the predictions is comparable to the Maximum Likelihood Estimation while MSSC is faster. This new algorithm and all data sets used are accessible through a Web portal at http://ppi.cse.nd.edu. © 2007 IEEE.",biology
737,A statistical model to correct systematic bias introduced by algorithmic thresholds in protein structural comparison algorithms,International Conference on Bioinformatics,"The identification of protein function is crucial to understanding cellular processes and selecting novel proteins as drug targets. However, experimental methods for determining protein function can be expensive and time-consuming. Protein partial structure comparison methods seek to guide and accelerate the process of function determination by matching characterized functional site representations, motifs, to substructures within uncharacterized proteins, matches. One common difficulty of all protein structural comparison techniques is the computational cost of obtaining a match. In an effort to maintain practical efficiency, some algorithms employ efficient geometric threshold-based searches to eliminate biologically irrelevant matches. Thresholds refine and accelerate the method by limiting the number of potential matches that need to be considered. However, because statistical models rely on the output of the geometric matching method to accurately measure statistical significance, geometric thresholds can also artificially distort the basis of statistical models, making statistical scores dependant on geometric thresholds and potentially causing significant reductions in accuracy of the functional annotation method. This paper proposes a point-weight based correction approach to quantify and model the dependence of statistical scores to account for the systematic bias introduced by heuristics. Using a benchmark dataset of 20 structural motifs, we show that the point-weight correction procedure accurately models the information lost during the geometric comparison phase, removing systematic bias and greatly reducing misclassification rates of functionally related proteins, while maintaining specificity.",biology
738,Robust RFCM algorithm for identification of co-expressed miRNAs,Bioinformatics and Biomedicine,"MicroRNAs (miRNAs) are short, endogenous RNAs having ability to regulate gene expression at the post-transcriptional level. Various studies have revealed that miRNAs tend to cluster on chromosomes. Members of a cluster that are at close proximity on chromosome are highly likely to be processed as cotranscribed units. Therefore, a large proportion of miRNAs are co-expressed. Expression profiling of miRNAs generates a huge volume of data. Complicated networks of miRNA-mRNA interaction create a big challenge for scientists to decipher this huge expression data. In order to extract meaningful information from expression data, this paper presents the application of robust rough-fuzzy c-means (rRFCM) algorithm to discover co-expressed miRNA clusters. The rRFCM algorithm comprises a judicious integration of rough sets, fuzzy sets, and c-means algorithm. The effectiveness of the rRFCM algorithm and different initialization methods, along with a comparison with other related methods, is demonstrated on three miRNA microarray expression data sets using Silhouette index, Davies-Bouldin index, Dunn index, β index, and gene ontology based analysis.",biology
739,Investigating Subsumption in DL-Based Terminologies: A Case Study in SNOMED CT,International Conference on Bioinformatics," Formalisms such as description logics (DL) are sometimes expected to help terminologies ensure compliance with sound ontological principles. The objective of this paper is to study the degree to which one DL-based biomedical terminology (SNOMED CT) complies with such principles. We defined seven ontological principles (for example: each class must have at least one parent, each class must differ from its parent) and examined the properties of SNOMED CT classes with respect to these principles. Our major results are: 31% of the classes have a single child; 27% have multiple parents; 51% do not exhibit any differentiae between the description of the parent and that of the child. The applications of this study to quality assurance for ontologies are discussed and suggestions are made for dealing with multiple inheritance. ",biology
740,An iterative approach to probe-design for compressive sensing microarrays,International Conference on Bioinformatics,"The compressive sensing microarrays design was proposed by Sheikh et. al as an efficient way of sensing organisms in a given environment such as air, water or soil sample. However, Sheik et. al probe candidates are extracted from the shortest sequences among any given group of organisms. This implies that they have a limited search space for the probe candidates. Probes picked in such a way must not be the most optimal probe candidates. In this paper, we introduce an alternative compressive sensing probe picking algorithm, which consider all possible hybridization affinities and chooses the best group identifier probe among all possible probe candidates from all the members of a group. More importantly, we built relatively larger compressive sensing microarrays systems that consist of four or five groups with the total number of 22 organisms. The system that we built can sense all these organisms effectively by using a nonlinear decoding algorithm known as Belief Propagation (BP).",biology
741,Empirical Investigations into Full-Text Protein Interaction Article Categorization Task (ACT) in the BioCreative II.5 Challenge,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The selection of protein interaction documents is one important application for biology research and has a direct impact on the quality of downstream BioNLP applications, i.e., information extraction and retrieval, summarization, QA, etc. The BioCreative II.5 Challenge Article Categorization task (ACT) involves doing a binary text classification to determine whether a given structured full-text article contains protein interaction information. This may be the first attempt at classification of full-text protein interaction documents in wide community. In this paper, we compare and evaluate the effectiveness of different section types in full-text articles for text classification. Moreover, in practice, the less number of true-positive samples results in unstable performance and unreliable classifier trained on it. Previous research on learning with skewed class distributions has altered the class distribution using oversampling and downsampling. We also investigate the skewed protein interaction classification and analyze the effect of various issues related to the choice of external sources, oversampling training sets, classifiers, etc. We report on the various factors above to show that 1) a full-text biomedical article contains a wealth of scientific information important to users that may not be completely represented by abstracts and/or keywords, which improves the accuracy performance of classification and 2) reinforcing true-positive samples significantly increases the accuracy and stability performance of classification. ÂŠ 2006 IEEE.",biology
742,Examining SNOMED from the Perspective of Formal Ontological Principles: Some Preliminary Analysis and Observations,International Conference on Bioinformatics," The Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT) is a terminological resource designed to support electronic applications in health and medicine. Its design has evolved over a period of more than thirty years, and continues to evolve. Recently several authors working on formal ontological theory have observed that applying certain principles and constraints to terminology construction may result in a more consistent and useful terminology. In this paper we report on a preliminary analysis of SNOMED CT by two of its developers, from the perspective of a few such formal ontological principles, giving examples of prior design decisions that appear to be supported by these principles as well as examples of prior design decisions that may be at variance with them. We believe that design changes suggested by formal ontological principles have great potential for improving consistency. Empirical evidence of usefulness should accompany theoretically-inspired moves towards more fine-tuned representations of reality. ",biology
743,Graph Kernel-Based Learning for Gene Function Prediction from Gene Interaction Network,Bioinformatics and Biomedicine,"Prediction of gene functions is a major challenge to biologists in the post-genomic era. Interactions between genes and their products compose networks and can be used to infer gene functions. Most previous studies used heuristic approaches based on either local or global information of gene interaction networks to assign unknown gene functions. In this study, we propose a graph kernel-based method that can capture the structure of gene interaction networks to predict gene functions. We conducted an experimental study on a test-bed of P53-related genes. The experimental results demonstrated better performance for our proposed method as compared with baseline methods.",biology
744,Sonography Images for Breast Cancer Texture Classification in Diagnosis of Malignant or Benign Tumors,International Conference on Bioinformatics and Biomedical Engineering,"This work aims at selecting useful features in critical angles and distances by Gray Level Co-occurrence Matrix (GLCM). In this project, images were labeled based on physician opinion in two groups (malignant or benign). These labeled images were used in classification analysis. Images were opened and read in Matlab software. The tumors were cropped in rectangular shape manually; then graycomatrix and GLCM have been calculated in 4 angels (0, 45, 90 and 135 degree) and 4 distances (1, 2, 3 and 4) for cropped tumor images. Since each angle and distance pair include 22 features, each image had 352 final features (22 features * 4 angles * 4 distances =352). At the final step, features were classified using Kmeans method into 2 classes of malignant and benign; then the confusion matrix was made and qualitative comparison was used to select important features and critical distances and angles in each one. Some special features, angels and distances which had the best classification result and high percentages of accuracy were selected as useful features. These finding suggested that texture parameters can be useful to help in distinguishing between malignant and benign breast tumors.",biology
745,Correlation between TCM Subjective Symptoms and Biomedical Parameters in 500 Hypertension Patients with Biostatistics Approach,International Conference on Bioinformatics and Biomedical Engineering,"In order to classify the TCM subjective symptoms of hypertension and explore the relationship between TCM symptoms and body weight, waist circumference, course of disease and blood fat, factor analysis, spearman analysis and T test were used to analyze the clinical data from 500 primary hypertension patients. The results showed that the different TCM symptom groups were related to different biomedical signs. The results suggest that it is reasonable to further classify the hypertension with TCM symptoms and make different therapies with further stratification of hypertension patients.",biology
746,Prediction and Informative Risk Factor Selection of Bone Diseases,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -With the booming of healthcare industry and the overwhelming amount of electronic health records (EHRs) shared by healthcare institutions and practitioners, we take advantage of EHR data to develop an effective disease risk management model that not only models the progression of the disease, but also predicts the risk of the disease for early disease control or prevention. Existing models for answering these questions usually fall into two categories: the expert knowledge based model or the handcrafted feature set based model. To fully utilize the whole EHR data, we will build a framework to construct an integrated representation of features from all available risk factors in the EHR data and use these integrated features to effectively predict osteoporosis and bone fractures. We will also develop a framework for informative risk factor selection of bone diseases. A pair of models for two contrast cohorts (e.g., diseased patients vs. non-diseased patients) will be established to discriminate their characteristics and find the most informative risk factors. Several empirical results on a real bone disease data set show that the proposed framework can successfully predict bone diseases and select informative risk factors that are beneficial and useful to guide clinical decisions. ",biology
747,A sparse integrative cluster analysis for understanding soybean phenotypes,Bioinformatics and Biomedicine," -Soybean is one of the most important crops for food, feed and bio-energy world-wide. The study of soybean phenotypic variation at different geographical locations can help the understanding of soybean domestication, population structure of soybean, and the conservation of soybean biodiversity. We investigate if soybean varieties can be identified that they differ from other varieties on multiple traits even when growing at different geographical locations. When a collection of traits are observed for the same soybean type at different locations (different views), joint analysis of the multipleview data is required in order to identify the same soybean clusters based on data from different locations. We employ a new multi-view singular value decomposition approach that simultaneously decomposes the data matrix gathered at each location into sparse singular vectors. This approach is able to group soybean samples consistently across the different locations and simultaneously identify the phenotypes at each location on which the soybean samples within a cluster are the most similar. Comparison with several latest multi-view co-clustering methods demonstrates the superior performance of the proposed approach. ",biology
748,Retinal Identification Based on Rotation Invariant Moments,International Conference on Bioinformatics and Biomedical Engineering,"In this paper, we propose a new identification method based on the retinal images. In this algorithm, we use rotation invariant features extracted from the retinal images. Then, we use them for identification. Experimental results on a database, including 60 retina images obtained from 40 subjects of DRIVE database and 20 subjects from STARE database, show the average true identification accuracy rate equal to 99.78 percent for the proposed algorithm.",biology
749,On clinical decision support,International Conference on Bioinformatics,"Recent interest in search tools for Clinical Decision Support (CDS) has dramatically increased. These tools help clinicians assess a medical situation by providing actionable information in the form of a select few highly relevant recent medical papers. Unlike traditional search, which is designed to deal with short queries, queries in CDS are long and narrative. We investigate the utility of applying pseudo-relevance feedback (PRF), a query expansion method that performs well in keyword-based medical literature search to CDS search. Using the optimum combination of PRF parameters we obtained statistically significant retrieval efficiency improvement in terms of nDCG, over the baseline.",biology
750,Multiseed Lossless Filtration,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"We study a method of seed-based lossless filtration for approximate string matching and related bioinformatics applications. The method is based on a simultaneous use of several spaced seeds rather than a single seed as studied by Burkhardt and Kärkkäinen [1]. We present algorithms to compute several important parameters of seed families, study their combinatorial properties, and describe several techniques to construct efficient families. We also report a large-scale application of the proposed technique to the problem of oligonucleotide selection for an EST sequence database. © 2005 IEEE.",biology
751,Data Mining on DNA Sequences of Hepatitis B Virus,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Extraction of meaningful information from large experimental data sets is a key element in bioinformatics research. One of the challenges is to identify genomic markers in Hepatitis B Virus (HBV) that are associated with HCC (liver cancer) development by comparing the complete genomic sequences of HBV among patients with HCC and those without HCC. In this study, a data mining framework, which includes molecular evolution analysis, clustering, feature selection, classifier learning, and classification, is introduced. Our research group has collected HBV DNA sequences, either genotype B or C, from over 200 patients specifically for this project. In the molecular evolution analysis and clustering, three subgroups have been identified in genotype C and a clustering method has been developed to separate the subgroups. In the feature selection process, potential markers are selected based on Information Gain for further classifier learning. Then, meaningful rules are learned by our algorithm called the Rule Learning, which is based on Evolutionary Algorithm. Also, a new classification method by Nonlinear Integral has been developed. Good performance of this method comes from the use of the fuzzy measure and the relevant nonlinear integral. The nonadditivity of the fuzzy measure reflects the importance of the feature attributes as well as their interactions. These two classifiers give explicit information on the importance of the individual mutated sites and their interactions toward the classification (potential causes of liver cancer in our case). A thorough comparison study of these two methods with existing methods is detailed. For genotype B, genotype C subgroups C1, C2, and C3, important mutation markers (sites) have been found, respectively. These two classification methods have been applied to classify never-seen-before examples for validation. The results show that the classification methods have more than 70 percent accuracy and 80 percent sensitivity for most data sets, which are considered high as an initial scanning method for liver cancer diagnosis. © 2011 IEEE.",biology
752,Using ant colony optimization-based selected features for predicting post-synaptic activity in proteins,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","Feature Extraction (FE) and Feature Selection (FS) are the most important steps in classification systems. One approach in the feature selection area is employing population-based optimization algorithms such as Particle Swarm Optimization (PSO)-based method and Ant Colony Optimization (ACO)-based method. This paper presents a novel feature selection method that is based on Ant Colony Optimization (ACO). This approach is easily implemented and because of use of a simple classifier in that, its computational complexity is very low. The performance of proposed algorithm is compared to the performance of standard binary PSO algorithm on the task of feature selection in Postsynaptic dataset. Simulation results on Postsynaptic dataset show the superiority of the proposed algorithm. ÂŠ 2008 Springer-Verlag Berlin Heidelberg.",biology
753,Transactional Database Transformation and Its Application in Prioritizing Human Disease Genes,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Binary (0,1) matrices, commonly known as transactional databases, can represent many application data, including gene-phenotype data where ""1 represents a confirmed gene-phenotype relation and ""0 represents an unknown relation. It is natural to ask what information is hidden behind these ""0s and ""1s. Unfortunately, recent matrix completion methods, though very effective in many cases, are less likely to infer something interesting from these (0,1)-matrices. To answer this challenge, we propose Ind Evi, a very succinct and effective algorithm to perform independent-evidence-based transactional database transformation. Each entry of a (0,1)-matrix is evaluated by ""independent evidence (maximal supporting patterns) extracted from the whole matrix for this entry. The value of an entry, regardless of its value as 0 or 1, has completely no effect for its independent evidence. The experiment on a gene-phenotype database shows that our method is highly promising in ranking candidate genes and predicting unknown disease genes. © 2011 IEEE.",biology
754,MetaCluster: unsupervised binning of environmental genomic fragments and taxonomic annotation,International Conference on Bioinformatics, Proceedings of the 1st ACM International Conference on Bioinformatics and Computational Biology. Copyright © Association for Computing Machinery. ,biology
755,A new unsupervised binning approach for metagenomic sequences based on N-grams and automatic feature weighting,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The rapid development of high-throughput technologies enables researchers to sequence the whole metagenome of a microbial community sampled directly from the environment. The assignment of these sequence reads into different species or taxonomical classes is a crucial step for metagenomic analysis, which is referred to as binning of metagenomic data. Most traditional binning methods rely on known reference genomes for accurate assignment of the sequence reads, therefore cannot classify reads from unknown species without the help of close references. To overcome this drawback, unsupervised learning based approaches have been proposed, which need not any known species reference genome for help. In this paper, we introduce a novel unsupervised method called MCluster for binning metagenomic sequences. This method uses N-grams to extract sequence features and utilizes automatic feature weighting to improve the performance of the basic K-means clustering algorithm. We evaluate MCluster on a variety of simulated data sets and a real data set, and compare it with three latest binning methods: AbundanceBin, MetaCluster 3.0, and MetaCluster 5.0. Experimental results show that MCluster achieves obviously better overall performance (F-measure) than AbundanceBin and MetaCluster 3.0 on long metagenomic reads (≥800 bp); while compared with MetaCluster 5.0, MCluster obtains a larger sensitivity, and a comparable yet more stable F-measure on short metagenomic reads (<300 bp). This suggests that MCluster can serve as a promising tool for effectively binning metagenomic sequences. © 2004-2012 IEEE.",biology
756,SimpleTrPPI: A simple method for transferring knowledge between interaction networks for PPI prediction,Bioinformatics and Biomedicine,"Maps of protein-protein interactions (PPIs) are essential to uncover cellular processes and metabolic processes in a cell. However, various high-throughput biological experiments are time-consuming and labor-intensive, resulting in interactions of high false positive and false negative rates. The fact that most interaction networks remain sparse and incomplete motivates scientists to develop computational methods to predict protein-protein interactions accurately and automatically. However, state-of-the-art prediction algorithms cannot make satisfactory predictions. In this paper, we propose a simple yet effective approach SimpleTrPPI, to improve the accuracy of predicting protein-protein interactions in the target PPI network with the aid of another source PPI network. We attempt to transfer and borrow useful knowledge from the source PPI network using similarities of protein nodes between two protein interaction networks. Similarities are computed taking both protein sequence similarities and topological structures of protein networks into account. Two protein-protein interaction networks, Helicobacter pylori (target network) and Human (source network), are used to verify the feasibility of our proposed method. Our experimental results show that SimpleTrPPI can achieve more than 5% accuracy improvement compared to the baseline methods.",biology
757,Coclustering of Human Cancer Microarrays Using Minimum Sum-Squared Residue Coclustering,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -It is a consensus in microarray analysis that identifying potential local patterns, characterized by coherent groups of genes and conditions, may shed light on the discovery of previously undetectable biological cellular processes of genes, as well as macroscopic phenotypes of related samples. In order to simultaneously cluster genes and conditions, we have previously developed a fast coclustering algorithm, Minimum Sum-Squared Residue Coclustering (MSSRCC), which employs an alternating minimization scheme and generates what we call coclusters in a “checkerboard” structure. In this paper, we propose specific strategies that enable MSSRCC to escape poor local minima and resolve the degeneracy problem in partitional clustering algorithms. The strategies include binormalization, deterministic spectral initialization, and incremental local search. We assess the effects of various strategies on both synthetic gene expression data sets and real human cancer microarrays and provide empirical evidence that MSSRCC with the proposed strategies performs better than existing coclustering and clustering algorithms. In particular, the combination of all the three strategies leads to the best performance. Furthermore, we illustrate coherence of the resulting coclusters in a checkerboard structure, where genes in a cocluster manifest the phenotype structure of corresponding specific samples and evaluate the enrichment of functional annotations in Gene Ontology (GO). ",biology
758,Using C-OWL for the Alignment and Merging of Medical Ontologies,International Conference on Bioinformatics," A number of sophisticated medical ontologies have been created over the past years. With their development the need for supporting the alignment of different ontologies is gaining importance. We proposed C-OWL, an extension of the Web Ontology Language OWL that supports alignment mappings between different, possibly incompatible ontologies on a semantic level. In this paper we report experiences from using C-OWL for the alignment of medical ontologies. We briefly review key concepts of the C-OWL semantics, explain the setting of the case study including some examples from the alignment and discuss the possibility of reasoning about the mapping based on the C-OWL semantics We conclude by arguing that C-OWL provides an adequate framework for aligning complex ontologies in the medical domain. ",biology
759,Detecting Nonlinear Properties of Snoring Sounds for Sleep Apnea Diagnosis,International Conference on Bioinformatics and Biomedical Engineering,"This paper investigates nonlinear properties of snoring sounds by a surrogate analysis which is generally used to verify the existence of nonlinearity in a time series. The ultimate goal of this study is to extract useful information from the nonlinear properties of snores so as to diagnose obstructive sleep apnea. For such purpose, many researchers have examined snoring sounds by linear frequency analysis such as Fourier Transform or Linear Predictive Coding, but the nonlinear properties of snores have not yet been clarified and the existence of nonlinearity has not been proved so far. The author adopts correlation integral to evaluate the geometrical nonlinear structure of snore attractors quantitatively. As a result of experiments, nonlinear properties are found in some kinds of waveform. But a complex waveform, in which no prominent peaks are found in the amplitude spectrum, does not have a nonlinear property.",biology
760,Evaluation of Weight Matrix Models in the splice junction recognition problem,Bioinformatics and Biomedicine,"The amount of data produced by the several genomic sequencing projects has increased dramatically in recent years. One of the main goals of bioinformatics is to analyze biological data aiming at identifying genes. The splice junction recognition problem is an important part of the gene detection problem. This work evaluates the performance of two classification models, derived from the weight matrix model, when applied to the splice junction recognition problem. Two splice junction data sets were used in this work and some measures of predictive accuracy were reported. Based on the experiments, classification thresholds were established, which can be useful for further implementation of an automatic gene detection system.",biology
761,Analysis of Protein Thermostability Enhancing Factors in Industrially Important Thermus Bacteria Species,Evolutionary Bioinformatics," Elucidation of evolutionary factors that enhance protein thermostability is a critical problem and was the focus of this work on Thermus species. Pairs of orthologous sequences of T. scotoductus SA-01 and T. thermophilus HB27, with the largest negative minimum folding energy (MFE) as predicted by the UNAFold algorithm, were statistically analyzed. Favored substitutions of amino acids residues and their properties were determined. Substitutions were analyzed in modeled protein structures to determine their locations and contribution to energy differences using PyMOL and FoldX programs respectively. Dominant trends in amino acid substitutions consistent with differences in thermostability between orthologous sequences were observed. T. thermophilus thermophilic proteins showed an increase in non-polar, tiny, and charged amino acids. An abundance of alanine substituted by serine and threonine, as well as arginine substituted by glutamine and lysine was observed in T. thermophilus HB27. Structural comparison showed that stabilizing mutations occurred on surfaces and loops in protein structures. ",biology
762,Extracting unrecognized gene relationships from the biomedical literature via matrix factorizations using a priori knowledge of gene relationships,International Conference on Bioinformatics," The construction of literature-based networks of gene-gene interactions is one of the most important applications of text mining in bioinformatics. Extracting potential gene relationships from the biomedical literature may be helpful in building biological hypotheses that can be explored further experimentally. In this paper, we explore the utility of singular value decomposition (SVD) and nonnegative matrix factorization (NMF) to extract unrecognized gene relationships from the biomedical literature by taking advantage of known gene relationships. We introduce a way to incorporate a priori knowledge of gene relationships into LSI/SVD and NMF. In addition, we propose a gene retrieval method based on NMF (GR/NMF), which shows comparable performance with latent semantic indexing based on SVD. ",biology
763,Normal breast identification in screening mammography: A study on 18 000 images,Bioinformatics and Biomedicine," -Through the years, several CAD systems have been developed to help radiologists in the hard task of detecting signs of cancer in the numerous screening mammograms. A more recent trend includes the development of pre-CAD systems aiming at identifying normal mammograms instead of detecting suspicious ones. Normal breasts are screened-out from the process, leaving radiologists more time to focus on more difficult cases. ",biology
764,A General Framework for Analyzing Data from Two Short Time-Series Microarray Experiments,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"We propose a general theoretical framework for analyzing differentially expressed genes and behavior patterns from two homogenous short time-course data. The framework generalizes the recently proposed Hilbert-Schmidt Independence Criterion (HSIC)-based framework adapting it to the time-series scenario by utilizing tensor analysis for data transformation. The proposed framework is effective in yielding criteria that can identify both the differentially expressed genes and time-course patterns of interest between two time-series experiments without requiring to explicitly cluster the data. The results, obtained by applying the proposed framework with a linear kernel formulation, on various data sets are found to be both biologically meaningful and consistent with published studies. © 2011 IEEE.",biology
765,Medical Image Segmentation Based on Watershed Transformation and Rough Sets,International Conference on Bioinformatics and Biomedical Engineering,"Traditional watershed algorithm often causes over-segmentation because of its high sensitivity to the weak edge and the noise. To overcome this drawback and in light of the characteristics of medical image, a new segmentation algorithm based on watershed transformation and rough set theory is proposed. The original image is partitioned into the edge-detail sub-image and smooth sub-image according to indiscernibility relation of rough set theory. Two enhancement methods are designed for the two sub-images, and watershed transformation is used for the further segmentation in the smooth sub-image. Finally, combine the two processed sub-images to obtain the segmentation result. The proposed algorithm has been executed on Magnetic Resonance Imaging (MRI) image, the analysis of compare between conventional watershed algorithm and the proposed algorithm is given. The experimental result shows that this method is efficient to restrain the over-segmentation, thus obtaining good segmentation results.",biology
766,Identifying gene functions using functional expression profiles obtained by voxelation,International Conference on Bioinformatics," Gene expression profiles have been widely used in functional genomic studies. However, not much work in traditional gene expression profiling takes into account the location information of a gene's expressions in the brain. Gene expression maps, which contain spatial information regarding the expression of genes in mice‟s brain, are obtained by combining voxelation and microarrays. Based on the idea that genes with similar gene expression maps may have similar gene functions, we propose an approach to identify gene functions. A gene function can potentially be associated with a specific gene expression profile. We name this specific gene expression profile, Functional Expression Profile (FEP). A functional expression profile can be obtained either by directly finding genes with a certain function, or by analyzing clusters of genes that have similar expression maps and similar functions. By taking advantage of the identified FEPs, we can annotate gene functions with high accuracy. Compared to the traditional K-nearest neighbor method, our approach shows higher accuracy in predicting functions. The images of FEPs are in good agreement with anatomical components of mice‟s brain, and provide valuable insight in terms of function prediction to biological scientists. ",biology
767,Graph Alignments: A New Concept to Detect Conserved Regions in Protein Active Sites,German Conference on Bioinformatics," We introduce the novel concept of graph alignment, a generalization of graph isomorphism that is motivated by the commonly used multiple sequence alignments. Graph alignments and graph isomorphisms are equivalent in the case of pairwise comparisons, but if many graphs should be analyzed simultaneously, graph alignments are more robust against noise and perturbations. Thus, graph alignments can detect conserved patterns in large sets of related graphs in a more reliable way. We study simple heuristics for the efficient calculation of approximate graph alignments and apply our approach to the structural analysis and functional classification of the active sites of proteins. Our approach is able to automatically detect even weakly conserved structural patterns in protein structures. Among other applications, these motifs can be used to classify proteins according to their function. ",biology
768,Constrained Fisher Scores Derived from Interaction Profile Hidden Markov Models Improve Protein to Protein Interaction Prediction,International Conference on Bioinformatics,"Protein-protein interaction plays critical roles in cellular functions. In this paper, we propose a computational method to predict protein-protein interaction by using support vector machines and the constrained Fisher scores derived from interaction profile hidden Markov models (ipHMM) that characterize domains involved in the interaction. The constrained Fisher scores are obtained as the gradient, with respect to the model parameters, of the posterior probability for the protein to be aligned with the ipHMM as conditioned on a specified path through the model state space, in this case we used the most probable path –as determined by the Viterbi algorithm. The method is tested by leave-one-out cross validation experiments with a set of interacting protein pairs adopted from the 3DID database. The prediction accuracy measured by ROC score has shown significant improvement as compared to the previous methods.",biology
769,Fast Structural Search in Phylogenetic Databases,Evolutionary Bioinformatics," As the size of phylogenetic databases grows, the need for efficiently searching these databases arises. Thanks to previous and ongoing research, searching by attribute value and by text has become commonplace in these databases. However, searching by topological or physical structure, especially for large databases and especially for approximate matches, is still an art. We propose structural search techniques that, given a query or pattern tree P and a database of phylogenies D, find trees in D that are sufficiently close to P . The “closeness” is a measure of the topological relationships in P that are found to be the same or similar in a tree D in D. We develop a filtering technique that accelerates searches and present algorithms for rooted and unrooted trees where the trees can be weighted or unweighted. Experimental results on comparing the similarity measure with existing tree metrics and on evaluating the efficiency of the search techniques demonstrate that the proposed approach is promising. ",biology
770,Identification of Essential Proteins Based on Edge Clustering Coefficient,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Identification of essential proteins is key to understanding the minimal requirements for cellular life and important for drug design. The rapid increase of available protein-protein interaction (PPI) data has made it possible to detect protein essentiality on network level. A series of centrality measures have been proposed to discover essential proteins based on network topology. However, most of them tended to focus only on the location of single protein, but ignored the relevance between interactions and protein essentiality. In this paper, a new centrality measure for identifying essential proteins based on edge clustering coefficient, named as NC, is proposed. Different from previous centrality measures, NC considers both the centrality of a node and the relationship between it and its neighbors. For each interaction in the network, we calculate its edge clustering coefficient. A nodes essentiality is determined by the sum of the edge clustering coefficients of interactions connecting it and its neighbors. The new centrality measure NC takes into account the modular nature of protein essentiality. NC is applied to three different types of yeast protein-protein interaction networks, which are obtained from the DIP database, the MIPS database and the BioGRID database, respectively. The experimental results on the three different networks show that the number of essential proteins discovered by NC universally exceeds that discovered by the six other centrality measures: DC, BC, CC, SC, EC, and IC. Moreover, the essential proteins discovered by NC show significant cluster effect. © 2012 IEEE.",biology
771,Quantitative Estimation of siRNAs Gene Silencing Capability by Random Forest Regression Model,International Conference on Bioinformatics and Biomedical Engineering,"Although the observations concerning the factors which influence the siRNA efficacy give clues to the mechanism of RNAi, the quantitative prediction of the siRNA efficacy is still a challenge task. In this paper, we introduced a novel non-linear regression method: random forest regression (RFR), to quantitatively estimate siRNAs efficacy values. Compared with an alternative machine learning regression algorithm, support vector machine regression (SVR) and four other score-based algorithms (Reynolds et al. (2004), Ui-Tei et al. (2004), Hsieh et al. (2004), Amarzguioui et al. (2004)) our RFR model achieved the best performance of all.",biology
772,Multi-topic Aspects in Clinical Text Classification,Bioinformatics and Biomedicine,"This paper investigates multi-topic aspects in automatic classification of clinical free text. In many practical situ- ations, we need to deal with documents overlapping with multiple topics. Automatic assignment of multiple ICD-9- CM codes to clinical free text in medical records is a typi- cal multi-topic text classification problem. In this paper, we facilitate two different views on multi-topics. The Closed Topic Assumption (CTA) regards an absence of topics for a document as an explicit declaration that this document does not belong to those absent topics. In contrast, the Open Topic Assumption (OTA) considers the missing topics as neutral topics. This paper compares performances of vari- ous interpretations of a multi-topic Text Classification prob- lem into a Machine Learning problem. Experimental results show that the characteristics of multi-topic assignments in the Medical NLP Challenge data is OTA-oriented.",biology
773,Symbolic modeling of structural relationships in the Foundational Model of Anatomy,International Conference on Bioinformatics, The need for a sharable resource that can provide deep anatomical knowledge and support inference for biomedical applications has recently been the driving force in the creation of biomedical ontologies. Previous attempts at the symbolic representation of anatomical relationships necessary for such ontologies have been largely limited to general partonomy and class subsumption. We propose an ontology of anatomical relationships beyond class assignments and generic part-whole relations and illustrate the inheritance of structural attributes in the Digital Anatomist Foundational Model of Anatomy. Our purpose is to generate a symbolic model that accommodates all structural relationships and physical properties required to comprehensively and explicitly describe the physical organization of the human body. ,biology
774,BioFrameNet: A Domain-Specific FrameNet Extension with Links to Biomedical Ontologies,International Conference on Bioinformatics," Biomedical domain ontologies could be better put to use for automatic semantic linguistic processing if we could map them to lexical resources that model the linguistic phenomena encountered in this domain, e.g., complex noun phrase structures that reference specific biological entity names and processes. In this paper, we introduce BioFrameNet  a domain-specific FrameNet extension. BioFrameNet uses Frame semantics to express the meaning of natural language, is augmented with domain-specific semantic relations, and links to biomedical ontologies like the Gene Ontology  all of which are expressed in the Description Logic (DL) variant of OWL. Thus, BioFrameNet annotations of natural-language text precisely map to biomedical ontologies, which in turn facilitates inference using DL reasoners. ",biology
775,Side Effect Prediction Using Cooperative Pathways,Bioinformatics and Biomedicine,"Drugs and biological experiments are designed to affect a particular target gene or pathway. However, they might inadvertently activate other pathways and cause side effects. Because of the existence of complex cellular mechanisms responding to stimuli, it is difficult to detect the presence of such side effects. Therefore, identification of pathways that function together under identical conditions would greatly help in anticipating these side effects before conducting these experiments. We develop a novel method to enumerate ""cooperative pathways"" defined as pathways that function together under identical conditions by combining pathway networks with comprehensive gene expression profiles. For finding cooperative pathways from whole pathways, we propose an efficient algorithm, CoopeRativE Pathway Enumerator (CREPE), which enumerates connected subpathways having common activate conditions and selects combinations of the subpathways sharing the conditions. We apply CREPE to a yeast stress dataset combined with the KEGG pathways. We observe that the starch and sucrose metabolism pathway cooperates with the pyruvate metabolism under heat shock stresses. It cooperates with the tricarboxylic acid (TCA) cycle under the stationary phases.",biology
776,Suppress False Arrhythmia Alarms of ICU Monitors Using Heart Rate Estimation Based on Combined Arterial Blood Pressure and Ecg Analysis,International Conference on Bioinformatics and Biomedical Engineering,"Intensive care unit (ICU) monitors generate a high rate of false alarms when physiological signals are severely corrupted by noise. To suppress the false life-threatening heart rate (HR)-related electrocardiogram arrhythmia alarms, data derived from arterial blood pressure (ABP) signal were used. A new ABP signal quality index (SQI) was designed based upon the combination of two previously reported signal quality measures. HR was then tracked based on beat detection from ABP and a Kalman filter with a SQI-modified update sequence. Using the SQI to reject noisy ABP data, false HR-related arrhythmia alarms were rejected by comparing the estimated ABP-derived HR to the monitor's HR threshold. The algorithm was evaluated on 707 episodes of extreme bradycardia and 1877 episodes of extreme tachycardia alarms produced by a commercial ICU monitoring system. The false alarm reduction rate was 74.13% and 53.81%, and the corresponding true alarm acceptance rate was 99.60% and 99.58% for extreme bradycardia and extreme tachycardia respectively. Combining ECG and ABP information therefore provides a significant reduction in false alarms with minimal impact on true alarms.",biology
777,Discovering temporal patterns of differential gene expression in microarray time series,German Conference on Bioinformatics," A wealth of time series of microarray measurements have become available over recent years. Several two-sample tests for detecting differential gene expression in these time series have been defined, but they can only answer the question whether a gene is differentially expressed across the whole time series, not in which intervals it is differentially expressed. In this article, we propose a Gaussian process based approach for studying these dynamics of differential gene expression. In experiments on Arabidopsis thaliana gene expression levels, our novel technique helps us to uncover that the family of WRKY transcription factors appears to be involved in the early response to infection by a fungal pathogen. ",biology
778,Predicting breast cancer recurrence using data mining techniques,International Conference on Bioinformatics,"In this study, we firstly take good advantage of SEER Public-Use Data to predict breast cancer recurrence using data mining techniques. The SEER Public-Use Data 2005 is used in this research. We presented a new data pre-classification method and firstly find a possible solution to discover the information of breast cancer recurrence of SEER data. After the preprocessing of the dataset, we investigate several algorithms. As a result, we found c5 algorithm has the best performance of accuracy.",biology
779,On the Adaptive Design Rules of Biochemical Networks in Evolution,Evolutionary Bioinformatics," Biochemical networks are the backbones of physiological systems of organisms. Therefore, a biochemical network should be sufficiently robust (not sensitive) to tolerate genetic mutations and environmental changes in the evolutionary process. In this study, based on the robustness and sensitivity criteria of biochemical networks, the adaptive design rules are developed for natural selection in the evolutionary process. This will provide insights into the robust adaptive mechanism of biochemical networks in the evolutionary process. We find that if a mutated biochemical network satisfies the robustness and sensitivity criteria of natural selection, there is a high probability for the biochemical network to prevail during natural selection in the evolutionary process. Since there are various mutated biochemical networks that can satisfy these criteria but have some differences in phenotype, the biochemical networks increase their diversities in the evolutionary process. The robustness of a biochemical network enables co-option so that new phenotypes can be generated in evolution. The proposed robust adaptive design rules of natural selection gain much insight into the evolutionary mechanism and provide a systematic robust biochemical circuit design method of biochemical networks for biotechnological and therapeutic purposes in the future. ",biology
780,Using unsupervised learning to determine risk level for left ventricular diastolic dysfunction,Bioinformatics and Biomedicine," -Left Ventricular Diastolic Dysfunction (LVDD) is a The E/A ratio describes the ratio between passive and active decompensatory change in the relaxation properties of the heart, blood flow across the mitral valve[7]. In a healthy heart, the E the risk for which increases with age. Currently, physicians use a velocity is higher than the A veloci[t1y0]. With aging, the left decision-tree-like algorithm to distinguish between discrete ventricular wall may become stiff, leading to decreased passive LVDD levels. This approach, based on cut-off thresholds, can filling capacity of the left ventricle and an increasing need for potentially lead to information loss and possibly to misdiagnosis. active filling[4]. The fact that both people with normal diastolic This paper aims to explore an alternative diagnostic method to function and patients with severe LVDD can have an E/A ratio determine LVDD risk level, taking into account a wide variety of higher than 1, makes early diagnosis of LVDD difficult. Thus attributes available in patient records, without pre-setting cut-off additional measures are recommended[18]: i) The E/Em ratio is tLhornegshitoulddes.StUusdinygofaAlgairngge(BdLatSaAse),t adnedriavdejdusftrinogmthtehedaBtaalftoimraogree used for estimating left ventricular (LV) filling pres[9s]u,re and gender, we employ the Chi Square test and the information derived from the movement speed of the ventricular tiss[u3]e. gain criterion to identify attributes that correlate well with the As elevated filling pressures are the main physiological consephysician-assigned grades; such attributes are referred to as quence of diastolic dysfunction (DD), identifying those presdistinguishing attributes. We then apply the expectation maximi- sures can help detect DD[8]. Typically an E/Em ratio above12 zation (EM) algorithm, as well as the K-Means, in order to cluster is associated with increased LV filling pressu[3r].esii) The records that are represented using distinguishing attributes. indexed Left Atrial Volume (LAVi) assessmen[1t1], calculated While clusters resulting from the K-Means are not stable, three by indexing the volume of the left atrium to body surface area, stable and tightly-formed clusters, which are obtained from the reflects the cumulative effects of filling pressures over tim[e5], EM algorithm, roughly correspond to the physician-assigned unlike E/A or E/Em ratios, which reflect filling pressures at the categories. Based on the results from the EM algorithm, we can time of measurement. LAVi is used to differentiate between compute a patient's probability to have low, high or no risk for LVDD grades 0 vs. 2 and 0 vs. 3, when the E/Em ratio is in the LVDD, and use this probability as a basis for defining a risk score intermediate range of 8-12. to determine the patient's LVDD severity. The threshold-based grading suffers several shortcomings, ",biology
781,A Novel Hybrid Approach to Selecting Marker Genes for Cancer Classification Using Gene Expression Data,International Conference on Bioinformatics and Biomedical Engineering,"Selecting a subset of marker genes from thousands of genes is an important topic in microarray experiments for diseases classification and prediction. In this paper, we proposed a novel hybrid approach that combines gene ranking, heuristic clustering analysis and wrapper method to select marker genes for tumor classification. In our method, we firstly employed gene filtering to select the informative genes; secondly, we extracted a set of prototype genes as the representative of the informative genes by heuristic K-means clustering; finally, employed SVM- RFE to find marker genes from the representative genes based on recursive feature elimination. The performance of our method was evaluated by AML/ALL microarray dataset. The experimental results revealed that our method could find very small subset of marker genes with minimum redundancy but got better classification accuracy.",biology
782,Analysis of Protein Protein Dimeric Interfaces,Bioinformatics and Biomedicine,"We analyzed the structural properties and the local surface environment of surface amino acid residues of proteins using a large, non-redundant dataset of 2383 protein chains in dimeric complexes from PDB. We compared the interface residues and non-interface residues based on six properties: side chain orientation, surface roughness, solid angle, ex value, hydrophobicity and interface cluster size. The results of our analysis show that interface residues have side chains pointing inward; interfaces are rougher, tend to be flat, moderately convex or concave and protrude more relative to non-interface surface residues. Interface residues tend to be surrounded by hydrophobic neighbors and tend to form clusters consisting of three or more interfaces residues. These findings are consistent with previous published studies using much smaller datasets, while allowing for more qualitative conclusions due to our larger dataset. Preliminary results suggest the possibility of using the six the properties to identify putative interface residues.",biology
783,Finding steady states of large scale regulatory networks through partitioning,International Conference on Bioinformatics,"Identifying steady states that characterize the long term outcome of regulatory networks is crucial in understanding important biological processes such as cellular differentiation. Finding all possible steady states of regulatory networks is a computationally intensive task as it suffers from state space explosion problem. Here, we propose a method for finding steady states of large-scale Boolean regulatory networks. Our method exploits scale-freeness and weak connectivity of regulatory networks in order to speed up the steady state search through partitioning. In the trivial case where network has more than one component such that the components are disconnected from each other, steady states of each component are independent of those of the remaining components. When the size of at least one connected component of the network is still prohibitively large, further partitioning is necessary. In this case, we identify weakly dependent components (i.e., two components that have a small number of regulations from one to the other) and calculate the steady states of each such component independently. We then combine these steady states by taking into account the regulations connecting them. We show that this approach is much more efficient than calculating the steady states of the whole network at once when the number of edges connecting them is small. Since regulatory networks often have small in-degrees, this partitioning strategy can be used effectively in order to find their steady states. Our experimental results on real datasets demonstrate that our method leverages steady state identification to very large regulatory networks.",biology
784,Motif Search in Graphs: Application to Metabolic Networks,IEEE/ACM Transactions on Computational Biology and Bioinformatics, The classic view of metabolism as a collection of metabolic pathways is being questioned with the currently available possibility of studying whole networks. Novel ways of decomposing the network into modules and motifs that could be considered as the building blocks of a network are being sug1 ,biology
785,A Fast Hierarchical Clustering Algorithm for Functional Modules Discovery in Protein Interaction Networks,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"As advances in the technologies of predicting protein interactions, huge data sets portrayed as networks have been available. Identification of functional modules from such networks is crucial for understanding principles of cellular organization and functions. However, protein interaction data produced by high-throughput experiments are generally associated with high false positives, which makes it difficult to identify functional modules accurately. In this paper, we propose a fast hierarchical clustering algorithm HC-PIN based on the local metric of edge clustering value which can be used both in the unweighted network and in the weighted network. The proposed algorithm HC-PIN is applied to the yeast protein interaction network, and the identified modules are validated by all the three types of Gene Ontology (GO) Terms: Biological Process, Molecular Function, and Cellular Component. The experimental results show that HC-PIN is not only robust to false positives, but also can discover the functional modules with low density. The identified modules are statistically significant in terms of three types of GO annotations. Moreover, HC-PIN can uncover the hierarchical organization of functional modules with the variation of its parameters value, which is approximatively corresponding to the hierarchical structure of GO annotations. Compared to other previous competing algorithms, our algorithm HC-PIN is faster and more accurate. © 2011 IEEE.",biology
786,Understanding user intents in online health forums,International Conference on Bioinformatics," Online health forums provide a convenient way for patients to obtain medical information and connect with physicians and peers outside of clinical settings. However, large quantities of unstructured and diversi ed content generated on these forums make it di cult for users to digest and extract useful information. Understanding user intents would enable forums to more accurately and e ciently nd relevant information by ltering out threads that do not match particular intents. In this paper, we derive a taxonomy of intents to capture user information needs in online health forums, and propose novel pattern based features for use with a multiclass support vector machine (SVM) classi er to classify original thread posts according to their underlying intents. Since no dataset existed for this task, we employ three annotators to manually label a dataset of 1,200 HealthBoards posts spanning four forum topics. Experimental results show that SVM with pattern based features is highly capable of identifying user intents in forum posts, reaching a maximum precision of 75%. Furthermore, comparable classi cation performance can be achieved by training and testing on posts from di erent forum topics (e.g. training on allergy posts, testing on depression posts). Finally, we run a trained classi er on a MedHelp dataset to analyze the distribution of intents of posts from di erent forum topics. ",biology
787,Utilizing Both Topological and Attribute Information for Protein Complex Identification in PPI Networks,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Many computational approaches developed to identify protein complexes in protein-protein interaction (PPI) networks perform their tasks based only on network topologies. The attributes of the proteins in the networks are usually ignored. As protein attributes within a complex may also be related to each other, we have developed a PCIA algorithm to take into consideration both such information and network topology in the identification process of protein complexes. Given a PPI network, PCIA first finds information about the attributes of the proteins in a PPI network in the Gene Ontology databases and uses such information for the identification of protein complexes. PCIA then computes a Degree of Association measure for each pair of interacting proteins to quantitatively determine how much their attribute values associate with each other. Based on this association measure, PCIA is able to discover dense graph clusters consisting of proteins whose attribute values are significantly closer associated with each other. PCIA has been tested with real data and experimental results seem to indicate that attributes of the proteins in the same complex do have some association with each other and, therefore, that protein complexes can be more accurately identified when protein attributes are taken into consideration.",biology
788,An Automatic System for Extracting Figures and Captions in Biomedical PDF Documents,Bioinformatics and Biomedicine,"Figures in biomedical articles often constitute direct evidence of experimental results. Image analysis methods can be coupled with text-based methods to improve knowledge discovery. However, automatically harvesting figures along with their associated captions from full-text articles remains challenging. In this paper, we present an automatic system for robustly harvesting figures from biomedical literature. Our approach relies on the idea that the PDF specification of the document layout can be used to identify encoded figures and figure boundaries within the PDF and enforce constraints among figure-regions. This allows us to harvest fragments of figures (subflgures), from the PDF, correctly identify subfigures that belong to the same figure, and identify the captions associated with each figure. Our method simultaneously recovers figures and captions and applies additional filtering process to remove irrelevant figures such as logos, to eliminate text passages that were incorrectly identified as captions, and to re-group subflgures to generate a putative figure. Finally, we associate figures with captions. Our preliminary experiments suggest that our method achieves an accuracy of 95% in harvesting figures-caption pairs from a set of 2,035 full-text biomedical documents from BioCreative III, containing 12,574 figures.",biology
789,Multiclass Cancer Classification Using Semisupervised Ellipsoid ARTMAP and Particle Swarm Optimization with Gene Expression Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"It is crucial for cancer diagnosis and treatment to accurately identify the site of origin of a tumor. With the emergence and rapid advancement of DMA microarray technologies, constructing gene expression profiles for different cancer types has already become a promising means for cancer classification. In addition to research on binary classification such as normal versus tumor samples, which attracts numerous efforts from a variety of disciplines, the discrimination of multiple tumor types is also important. Meanwhile, the selection of genes which are relevant to a certain cancer type not only improves the performance of the classifiers, but also provides molecular insights for treatment and drug development. Here, we use Semisupervised Ellipsoid ARTMAP (ssEAM) for multiclass cancer discrimination and particle swarm optimization for informative gene selection. ssEAM is a neural network architecture rooted in Adaptive Resonance Theory and suitable for classification tasks. ssEAM features fast, stable, and finite learning and creates hyperellipsoidal clusters, inducing complex nonlinear decision boundaries. PSO is an evolutionary algorithm-based technique for global optimization. A discrete binary version of PSO is employed to indicate whether genes are chosen or not. The effectiveness of ssEAM/PSO for multiclass cancer diagnosis is demonstrated by testing it on three publicly available multiple-class cancer data sets. ssEAM/PSO achieves competitive performance on all these data sets, with results comparable to or better than those obtained by other classifiers. © 2007 IEEE.",biology
790,Conditioning-Based Modeling of Contextual Genomic Regulation,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"A more complete understanding of the alterations in cellular regulatory and control mechanisms that occur in the various forms of cancer has been one of the central targets of the genomic and proteomic methods that allow surveys of the abundance and/or state of cellular macromolecules. This preference is driven both by the intractability of cancer to generic therapies, assumed to be due to the highly varied molecular etiologies observed in cancer, and by the opportunity to discern and dissect the regulatory and control interactions presented by the highly diverse assortment of perturbations of regulation and control that arise in cancer. Exploiting the opportunities for inference on the regulatory and control connections offered by these revealing system perturbations is fraught with the practical problems that arise from the way biological systems operate. Two classes of regulatory action in biological systems are particularly inimical to inference, convergent regulation, where a variety of regulatory actions result in a common set of control responses (crosstalk), and divergent regulation, where a single regulatory action produces entirely different sets of control responses, depending on cellular context (conditioning). We have constructed a coarse mathematical model of the propagation of regulatory influence in such distributed, context-sensitive regulatory networks that allows a quantitative estimation of the amount of crosstalk and conditioning associated with a candidate regulatory gene taken from a set of genes that have been profiled over a series of samples where the candidates activity varies. ÂŠ 2006 IEEE.",biology
791,Efficient Extraction of Protein-Protein Interactions from Full-Text Articles,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -Proteins and their interactions govern virtually all cellular processes, such as regulation, signaling, metabolism, and structure. Most experimental findings pertaining to such interactions are discussed in research articles, which in turn get curated by protein interaction databases. Authors, editors, and publishers benefit from efforts to alleviate the tasks of searching for relevant articles, evidence for physical interactions, and proper identifiers for each protein involved. The BioCreative II.5 community challenge addressed these tasks in a competition-style assessment, to evaluate and compare different methodologies, to make aware of the increasing accuracy of automated methods, and to guide future implementations. In this paper, we present our approaches for protein named entity recognition including normalization, and for extraction of proteinprotein interactions from full text. Our overall goal is to identify efficient individual components, and we compare various compositions to handle a single full-text article in between ten seconds and two minutes. We propose strategies to transfer document-level annotations to the sentence-level, which allows for the creation of a more fine-grained training corpus; we use this corpus to automatically derive around 5000 patterns. We rank sentences by relevance to the task of finding novel interactions with physical evidence, using a sentence classifier built from this training corpus. Heuristics for paraphrasing sentences help to further remove unnecessary information that might interfere with patterns, such as additional adjectives, clauses, or bracketed expressions. In BioCreative II.5, we achieved an f-score of 22% for finding protein interactions, and 43% for mapping proteins to UniProt IDs; disregarding species, f-scores are 30% and 55%, respectively. On average, our best-performing setup required around two minutes per full text. All data and pattern sets as well as Java classes that extend third-party software are available as supplementary information. ",biology
792,Separable Parameter Estimation Method for Nonlinear Biological Systems,International Conference on Bioinformatics and Biomedical Engineering,"Models for biological systems derived from the generalized mass action law are typically a group of nonlinear ordinary differential equations. However, parameters in such models can be separated into two groups: one group of parameters linear in the model and another group of parameters nonlinear in the model. This paper introduces a separable parameter estimation method to estimate the parameters in such models. The separable parameter estimation method has three steps: in the first step, parameters linear in a model are estimated by optimizing the objective function using linear least squares method, assuming all parameters nonlinear in the model are known. In the second step, substituting the estimated parameters in the first step into the objective function yields a new objective function which is only of parameters nonlinear in the model. Then parameters nonlinear in the model are estimated by proper nonlinear estimation methods. In the last step, the estimates of parameters linear in the model are calculated using the estimates of parameters in the second step. To investigate its performance, the separable estimation method is applied to a biological system and is compared with the conventional parameter estimation methods. Simulation results show the improvement of the separable estimation method.",biology
793,An Introduction to Metabolic Networks and Their Structural Analysis,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"There has been a renewed interest for metabolism in the computational biology community, leading to an avalanche of papers coming from methodological network analysis as well as experimental and theoretical biology. This paper is meant to serve as an initial guide for both the biologists interested in formal approaches and the mathematicians or computer scientists wishing to inject more realism into their models. This paper is focused on the structural aspects of metabolism only. The literature is vast enough already, and the thread through it is difficult to follow even for the more experienced worker in the field. We explain methods for acquiring data and reconstructing metabolic networks and review the various models that have been used for their structural analysis. Several concepts such as modularity are introduced, as well as the controversies that have beset the field these past few years, for instance, on whether metabolic networks are small world or scale-free and on which model better explains the evolution of metabolism. © 2008 IEEE.",biology
794,Relational clustering and Bayesian networks for linking gene expression profiles and drug activity patterns,Bioinformatics and Biomedicine,"The combined analysis of the microarray and drug-activity datasets has the potential of revealing valuable knowledge about various relations among gene expressions and drug activity patterns in tumor cells. However, the huge amount of biological data needs appropriate data mining models in order to extract interesting patterns and useful information. In this paper, the NCI60 dataset has been analyzed for the molecular pharmacology of cancer. In particular, we proposed a novel relational clustering algorithm joint with Bayesian network inference engine for linking gene expression profiles to drug activity patterns. Our analysis could be an initial step for predicting potential useful drugs according to the gene expression level of tumor tissues.",biology
795,Automated classification of radiology reports for acute lung injury: Comparison of keyword and machine learning based natural language processing approaches,Bioinformatics and Biomedicine,"This paper compares the performance of keyword and machine learning-based chest x-ray report classification for Acute Lung Injury (ALI). ALI mortality is approximately 30 percent. High mortality is, in part, a consequence of delayed manual chest x-ray classification. An automated system could reduce the time to recognize ALI and lead to reductions in mortality. For our study, 96 and 857 chest x-ray reports in two corpora were labeled by domain experts for ALI. We developed a keyword and a Maximum Entropy-based classification system. Word unigram and character n-grams provided the features for the machine learning system. The Maximum Entropy algorithm with character 6-gram achieved the highest performance (Recall=0.91, Precision=0.90 and F-measure=0.91) on the 857-report corpus. This study has shown that for the classification of ALI chest x-ray reports, the machine learning approach is superior to the keyword based system and achieves comparable results to highest performing physician annotators.",biology
796,A model to predict and analyze protein-protein interaction types using electrostatic energies,Bioinformatics and Biomedicine,Identification and analysis of types of protein-protein interactions (PPI) is an important problem in molecular biology because of their key role in many biological processes in living cells. We propose a model to predict and analyze protein interaction types using electrostatic energies as properties to distinguish between obligate and non-obligate interactions. Our prediction approach uses electrostatic energies for pairs of atoms and amino acids present in interfaces where the interaction occurs. Our results confirm that electrostatic energy is an important property to predict obligate and non obligate protein interaction types achieving accuracy of over 96% on two well known datasets. The classifiers used are support vector machines and linear dimensionality reduction.,biology
797,Chronic Rat Toxicity Prediction of Chemical Compounds Using Kernel Machines,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","A recently published study showed the feasibility of chronic rat toxicity prediction, an important task to reduce the number of animalexperiments using the knowledge of previous experiments. We benchmarked various kernel learning approaches for the prediction of chronic toxicity on a set of 565 chemical compounds, labeled with the Lowest Observed Adverse Effect Level, and achieved a prediction error close to the interlaboratory reproducibility. e-Support Vector Regression was used in combination with numerical molecular descriptors and the Radial Basis Function Kernel, as well as with graph kernels for molecular graphs, to train the models. The results show that a kernel approach improves the Mean Squared Error and the Squared Correlation Coefficient using leave-one-out cross-validation and a seeded 10-fold-cross-validation averaged over 10 runs. Compared to the state-of-the-art, the Mean Squared Error was improved up to MSE                         loo of 0.45 and MSE                         cv of 0.46 ± 0.09 which is close to the theoretical limit of the estimated interlaboratory reproducibility of 0.41. The Squared Empirical Correlation Coefficient was improved to Q                         2 loo of 0.58 and Q                         2 cv of 0.57 ± 0.10. The results show that numerical kernels and graph kernels are both suited for predicting chronic rat toxicity for unlabeled compounds. ©Springer-Verlag Berlin Heidelberg 2009.",biology
798,RN-Cluster: Discovering Coherent Biclusters Which is Robust to Noise,"Biocomputation, Bioinformatics, and Biomedical Technologies"," A bicluster is a subset of genes that show similar behavior within a subset of conditions. Biclustering algorithm is a useful tool to uncover groups of genes involved in the same cellular process and groups of conditions which take place in this process. We are proposing a polynomial time algorithm to identify functionally highly correlated biclusters. Our algorithm identifies 1) the gene set that follows additive, multiplicative, and combined patterns simultaneously that allow high level of noise, 2) the multiple, possibly overlapped, and diverse gene sets, 3) biclusters with negatively correlated as well as positively correlated gene set simultaneously, and 4) gene sets whose functional association is strongly high. We validated the level of functional association of our method, and compared with current methods using GO. ",biology
799,Incorporating semantic similarity into clustering process for identifying protein complexes from Affinity Purification/Mass Spectrometry data,Bioinformatics and Biomedicine,"This paper presents a framework for incorporating semantic similarities in the detection of protein complexes from Affinity Purification/Mass Spectrometry (AP-MS) data. AP-MS data is modeled as a bipartite network, where one set of nodes consist of bait proteins and the other set are prey proteins. Pair-wise similarities of bait proteins are computed by combining similarities based on topological features and functional semantic similarities. A hierarchical clustering algorithm is then applied to obtain `seed clusters' consisting of bait proteins. Starting from these `seed' clusters, an expansion process is developed to recruit prey proteins which are significantly associated with bait proteins, to produce final sets of identified protein complexes. In the application to real AP-MS datasets, we validate biological significance of predicted protein complexes by using curated protein complexes. Six statistical metrics have been applied. Results show that by integrating semantic similarities into the clustering process, the accuracy of identifying complexes has been greatly improved. Meanwhile, clustering results obtained by the proposed framework are better than those from several existent clustering methods.",biology
800,Superposition and Alignment of Labeled Point Clouds,IEEE/ACM Transactions on Computational Biology and Bioinformatics," Geometric objects are often represented approximately in terms of a finite set of points in threedimensional Euclidean space. In this paper, we extend this representation to what we call labeled point clouds. A labeled point cloud is a finite set of points, where each point is not only associated with a position in three-dimensional space, but also with a discrete class label that represents a specific property. This type of model is especially suitable for modeling biomolecules such as proteins and protein binding sites, where a label may represent an atom type or a physico-chemical property. Proceeding from this representation, we address the question of how to compare two labeled points clouds in terms of their similarity. Using fuzzy modeling techniques, we develop a suitable similarity measure as well as an efficient evolutionary algorithm to compute it. Moreover, we consider the problem of establishing an alignment of the structures in the sense of a one-to-one correspondence between their basic constituents. From a biological point of view, alignments of this kind are of great interest, since mutually corresponding molecular constituents offer important information about evolution and heredity, and can also serve as a means to explain a degree of similarity. In this paper, we therefore develop a method for computing pairwise or multiple alignments of labeled point clouds. To this end, we proceed from an optimal superposition of the corresponding point clouds and construct an alignment which is as much as possible in agreement with the neighborhood structure established by this superposition. We apply our methods to the structural analysis of protein binding sites. ",biology
801,Consensus Genetic Maps as Median Orders from Inconsistent Sources,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -A genetic map is an ordering of genetic markers calculated from a population of known lineage. Although, traditionally, a map has been generated from a single population for each species, recently, researchers have created maps from multiple populations. In the face of these new data, we address the need to find a consensus map-a map that combines the information from multiple partial and possibly inconsistent input maps. We model each input map as a partial order and formulate the consensus problem as finding a median partial order. Finding the median of multiple total orders (preferences or rankings) is a well-studied problem in social choice. We choose to find the median by using the weighted symmetric difference distance, which is a more general version of both the symmetric difference distance and the Kemeny distance. Finding a median order using this distance is NP-hard. We show that, for our chosen weight assignment, a median order satisfies the positive responsiveness, extended Condorcet, and unanimity criteria. Our solution involves finding the maximum acyclic subgraph of a weighted directed graph. We present a method that dynamically switches between an exact branch and bound algorithm and a heuristic algorithm and show that, for real data from closely related organisms, an exact median can often be found. We present experimental results by using seven populations of the crop plant Zea mays. ",biology
802,Automating risk of bias assessment for clinical trials,International Conference on Bioinformatics," In medicine, the publication of clinical trials now far outpaces clinicians' ability to read them. Systematic reviews, which aim to summarize the entirety of the available evidence on a speci c clinical question, have therefore become the linchpin of evidence-based decision making. A key task in systematic reviews is determining whether the results of included studies may be a ected by biases, e.g., poor randomization or blinding. This is called risk of bias assessment and is now standard practice. Standardized tools are used to perform these assessments; a notable example being the Cochrane risk of bias tool, which covers seven di erent types of potential biases and involves researchers extracting sentences from articles to support their bias assessments. These assessments are crucial in interpretating published evidence, but due to the exponential growth of the biomedical literature base, manually assessing the risk of bias in clinical trials has grown burdensome for clinical researchers. Aiming to mitigate this workload, we explore automating risk of bias assessment. We demonstrate that systematic reviews may be used to distantly supervise text mining models, obviating the need for manually annotated clinical trial reports. Speci cally, we leverage data from the Cochrane Database of Systematic Reviews (a large repository of systematic reviews), and link clinical trial reports to structured data from the same studies found in CDSR to produce a pseudo-annotated labeled corpus. We then develop a joint model which, using (the PDF of) a clinical trial report as input, predicts the risks of bias in each of the aforementioned seven areas while simultaneously extracting the text fragments supporting these assessments. ",biology
803,A Statistical Change Point Model Approach for the Detection of DNA Copy Number Variations in Array CGH Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Array comparative genomic hybridization (aCGH) provides a high-resolution and high-throughput technique for screening of copy number variations (CNVs) within the entire genome. This technique, compared to the conventional CGH, significantly improves the identification of chromosomal abnormalities. However, due to the random noise inherited in the imaging and hybridization process, identifying statistically significant DNA copy number changes in aCGH data is challenging. We propose a novel approach that uses the mean and variance change point model (MVCM) to detect CNVs or breakpoints in aCGH data sets. We derive an approximate p-value for the test statistic and also give the estimate of the locus of the DNA copy number change. We carry out simulation studies to evaluate the accuracy of the estimate and the p-value formulation. These simulation results show that the approach is effective in identifying copy number changes. The approach is also tested on fibroblast cancer cell line data, breast tumor cell line data, and breast cancer cell line aCGH data sets that are publicly available. Changes that have not been identified by the circular binary segmentation (CBS) method but are biologically verified are detected by our approach on these cell lines with higher sensitivity and specificity than CBS. © 2009 IEEE.",biology
804,A method for analyzing health behavior in online forums,International Conference on Bioinformatics," The prevalence of online social networks has enabled users to communicate, connect, and share content. Many of these networks serve as the de-facto Internet portal for millions of users. Due to the enormous popularity of these sites, the data about the users and their communications o er an enormous opportunity to analyze human behaviors on a large scale. It is important to analyze patterns within these records in order to more e ectively treat individuals. In this paper, a method is presented for identifying these themes and patterns within forum data. This methodology includes automatic extraction of the main themes or patterns in the data, quantify the similarities and di erences in the contents of di erent online forums, and nding similar documents based on user queries. We used data sets from four di erent forums. In this paper we describe a method that automatically di erentiates between online discussion groups related to di erent behavioral health challenges and identi es the most appropriate discussion forum for a given input. Finally, we evaluated the e cacy of our method by using cross-validation. ",biology
805,A Cluster Refinement Algorithm for Motif Discovery,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Finding Transcription Factor Binding Sites, i.e., motif discovery, is crucial for understanding the gene regulatory relationship. Motifs are weakly conserved and motif discovery is an NP-hard problem. We propose a new approach called Cluster Refinement Algorithm for Motif Discovery (CRMD). CRMD employs a flexible statistical motif model allowing a variable number of motifs and motif instances. CRMD first uses a novel entropy-based clustering to find complete and good starting candidate motifs from the DNA sequences. CRMD then employs an effective greedy refinement to search for optimal motifs from the candidate motifs. The refinement is fast, and it changes the number of motif instances based on the adaptive thresholds. The performance of CRMD is further enhanced if the problem has one occurrence of motif instance per sequence. Using an appropriate similarity test of motifs, CRMD is also able to find multiple motifs. CRMD has been tested extensively on synthetic and real data sets. The experimental results verify that CRMD usually outperforms four other state-of-the-art algorithms in terms of the qualities of the solutions with competitive computing time. It finds a good balance between finding true motif instances and screening false motif instances, and is robust on problems of various levels of difficulty. © 2006 IEEE.",biology
806,Mining association rules among gene functions in clusters of similar gene expression maps,Bioinformatics and Biomedicine,"Association rules mining methods have been recently applied to gene expression data analysis to reveal relationships between genes and different conditions and features. However, not much effort has focused on detecting the relation between gene expression maps and related gene functions. Here we describe such an approach to mine association rules among gene functions in clusters of similar gene expression maps on mouse brain. The experimental results show that the detected association rules make sense biologically. By inspecting the obtained clusters and the genes having the gene functions of frequent itemsets, interesting clues were discovered that provide valuable insight to biological scientists. Moreover, discovered association rules can be potentially used to predict gene functions based on similarity of gene expression maps.",biology
807,Towards a Reference Terminology for Ontology Research and Development in the Biomedical Domain,International Conference on Bioinformatics," Ontology is a burgeoning field, involving researchers from the computer science, philosophy, data and software engineering, logic, linguistics, and terminology domains. Many ontology-related terms with precise meanings in one of these domains have different meanings in others. Our purpose here is to initiate a path towards disambiguation of such terms. We draw primarily on the literature of biomedical informatics, not least because the problems caused by unclear or ambiguous use of terms have been there most thoroughly addressed. We advance a proposal resting on a distinction of three levels too often run together in biomedical ontology research: 1. the level of reality; 2. the level of cognitive representations of this reality; 3. the level of textual and graphical artifacts. We propose a reference terminology for ontology research and development that is designed to serve as common hub into which the several competing disciplinary terminologies can be mapped. We then justify our terminological choices through a critical treatment of the 'concept orientation' in biomedical terminology research. ",biology
808,Why Should We Care About Molecular Coevolution?,Evolutionary Bioinformatics," Non-independent evolution of amino acid sites has become a noticeable limitation of most methods aimed at identifying selective constraints at functionally important amino acid sites or protein regions. The need for a generalised framework to account for non-independence of amino acid sites has fuelled the design and development of new mathematical models and computational tools centred on resolving this problem. Molecular coevolution is one of the most active areas of research, with an increasing rate of new models and methods being developed everyday. Both parametric and nonparametric methods have been developed to account for correlated variability of amino acid sites. These methods have been utilised for detecting phylogenetic, functional and structural coevolution as well as to identify surfaces of amino acid sites involved in protein-protein interactions. Here we discuss and briefly describe these methods, and identify their advantages and limitations. ",biology
809,On accelerating iterative algorithms with CUDA: A case study on Conditional Random Fields training algorithm for biological sequence alignment,Bioinformatics and Biomedicine,"The accuracy of Conditional Random Fields (CRF) is achieved at the cost of huge amount of computation to train model. In this paper we designed the parallelized algorithm for the Gradient Ascent based CRF training methods for biological sequence alignment. Our contribution is mainly on two aspects: 1) We flexibly parallelized the different iterative computation patterns, and the according optimization methods are presented. 2) As for the Gibbs Sampling based training method, we designed a way to automatically predict the iteration round, so that the parallel algorithm could be run in a more efficient manner. In the experiment, these parallel algorithms achieved valuable accelerations comparing to the serial version.",biology
810,Guidelines to Select Machine Learning Scheme for Classification of Biomedical Datasets,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics"," Biomedical datasets pose a unique challenge to machine learning and data mining algorithms for classification because of their high dimensionality, multiple classes, noisy data and missing values. This paper provides a comprehensive evaluation of a set of diverse machine learning schemes on a number of biomedical datasets. To this end, we follow a four step evaluation methodology: (1) pre-processing the datasets to remove any redundancy, (2) classification of the datasets using six different machine learning algorithms; Naive Bayes (probabilistic), multi-layer perceptron (neural network), SMO (support vector machine), IBk (instance based learner), J48 (decision tree) and RIPPER (rule-based induction), (3) bagging and boosting each algorithm, and (4) combining the best version of each of the base classifiers to make a team of classifiers with stacking and voting techniques. Using this methodology, we have performed experiments on 31 different biomedical datasets. To the best of our knowledge, this is the first study in which such a diverse set of machine learning algorithms are evaluated on so many biomedical datasets. The important outcome of our extensive study is a set of promising guidelines which will help researchers in choosing the best classification scheme for a particular nature of biomedical dataset. ",biology
811,Detecting Pathway Cross-Talks by Analyzing Conserved Functional Modules across Multiple Phenotype-Expressing Organisms,Bioinformatics and Biomedicine,"Biological systems are organized hierarchically, starting from the protein level and expanding to pathway or even higher levels. Understanding interactions at lower levels (proteins interactions) in the hierarchy will help us understand interactions at higher levels (pathway cross-talks). Identifying cross-talks that are related to the expression of a particular- phenotype will be of interest to genetic engineers, because it will provide information on how different cellular subsystems could work together to express a phenotype. Current research has typically focused on identifying genotype-phenotype associations or pathway-phenotype associations. In contrast, we developed a method to identify phenotype-related pathway cross- talks by obtaining conserved groups of interacting proteins (functional modules). By applying our method to two groups of hydrogen producing organisms (light fermentation and dark fermentation), we have shown that our method effectively unearths known pathway cross-talks that are important to hydrogen production.",biology
812,Modelling the effect of genes on the dynamics of probabilistic spiking neural networks for computational neurogenetic modelling,Computational Intelligence Methods for Bioinformatics and Biostatistics," Computational neuro-genetic models (CNGM) combine two dynamic models - a gene regulatory network (GRN) model at a lower level, and a spiking neural network (SNN) model at a higher level to model the dynamic interaction between genes and spiking patterns of activity under certain conditions. The paper demonstrates that it is possible to model and trace over time the effect of a gene on the total spiking behavior of the SNN when the gene controls a parameter of a stochastic spiking neuron model used to build the SNN. Such CNGM can be potentially used to study neurodegenerative diseases or develop CNGM for cognitive robotics. ",biology
813,Robust Feature Selection for Microarray Data Based on Multicriterion Fusion,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Feature selection often aims to select a compact feature subset to build a pattern classifier with reduced complexity, so as to achieve improved classification performance. From the perspective of pattern analysis, producing stable or robust solution is also a desired property of a feature selection algorithm. However, the issue of robustness is often overlooked in feature selection. In this study, we analyze the robustness issue existing in feature selection for high-dimensional and small-sized gene-expression data, and propose to improve robustness of feature selection algorithm by using multiple feature selection evaluation criteria. Based on this idea, a multicriterion fusion-based recursive feature elimination (MCF-RFE) algorithm is developed with the goal of improving both classification performance and stability of feature selection results. Experimental studies on five gene-expression data sets show that the MCF-RFE algorithm outperforms the commonly used benchmark feature selection algorithm SVM-RFE. ÂŠ 2011 IEEE.",biology
814,Hypothesis Ranking Based on Semantic Event Similarities,Ipsj Transactions on Bioinformatics," Accelerated by the technological advances in the biomedical domain, the size of its literature has been growing very rapidly. As a consequence, it is not feasible for individual researchers to comprehend and synthesize all the information related to their interests. Therefore, it is conceivable to discover hidden knowledge, or hypotheses, by linking fragments of information independently described in the literature. In fact, such hypotheses have been reported in the literature mining community; some of which have even been corroborated by experiments. This paper mainly focuses on hypothesis ranking and investigates an approach to identifying reasonable ones based on semantic similarities between events which lead to respective hypotheses. Our assumption is that hypotheses generated from semantically similar events are more reasonable. We developed a prototype system called, Hypothesis Explorer, and conducted evaluative experiments through which the validity of our approach is demonstrated in comparison with those based on term frequencies, often adopted in the previous work. ",biology
815,Jointly Analyzing Gene Expression and Copy Number Data in Breast Cancer Using Data Reduction Models,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"With the growing surge of biological measurements, the problem of integrating and analyzing different types of genomic measurements has become an immediate challenge for elucidating events at the molecular level. In order to address the problem of integrating different data types, we present a framework that locates variation patterns in two biological inputs based on the generalized singular value decomposition (GSVD). In this work, we jointly examine gene expression and copy number data and iteratively project the data on different decomposition directions defined by the projection angle 6 in the GSVD. With the proper choice of 0, we locate similar and dissimilar patterns of variation between both data types. We discuss the properties of our algorithm using simulated data and conduct a case study with biologically verified results. Ultimately, we demonstrate the efficacy of our method on two genome-wide breast cancer studies to identify genes with large variation in expression and copy number across numerous cell line and tumor samples. Our method identifies genes that are statistically significant in both input measurements. The proposed method is useful for a wide variety of joint copy number and expression-based studies. Supplementary information is available online, including software implementations and experimental data. © 2006 IEEE.",biology
816,Functional Census of Mutation Sequence Spaces: The Example of p53 Cancer Rescue Mutants,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Many biomedical problems relate to mutant functional properties across a sequence space of interest, e.g., flu, cancer, and HIV. Detailed knowledge of mutant properties and function improves medical treatment and prevention. A functional census of p53 cancer rescue mutants would aid the search for cancer treatments from p53 mutant rescue. We devised a general methodology for conducting a functional census of a mutation sequence space by choosing informative mutants early. The methodology was tested in a double-blind predictive test on the functional rescue property of 71 novel putative p53 cancer rescue mutants iteratively predicted in sets of three (24 iterations). The first double-blind 15-point moving accuracy was 47 percent and the last was 86 percent; r = 0.01 before an epiphanic 16th iteration and r = 0.92 afterward. Useful mutants were chosen early (overall r = 0.80). Code and data are freely available (http://www.igb.uci.edu/research/research.html, corresponding authors: R.H.L. for computation and R.K.B. for biology). ÂŠ 2006 IEEE.",biology
817,Coupling Oriented Hidden Markov Random Field Model with Local Clustering for Segmenting Blood Vessels and Measuring Spatial Structures in Images of Tumor Microenvironment,Bioinformatics and Biomedicine,"Interactions between cancer cells and factors within the tumor microenvironment (mE) are essential for understanding tumor development. The spatial relationships between blood vessel cells and cancer cells, e.g. tumor initiating cells (TICs), are an important parameter. Accurate segmentation of blood vessel is necessary for the quantization of their spatial relationships. However, this remains an open problem due to uneven intensity and low signal to noise ratio (SNR). To overcome these challenges, we propose a novel approach that integrates an oriented hidden Markov random field model (Ori-HMRF) with local clustering. The local clustering delineates boundaries of blood vessel segments with low SNR. Then blood vessel segments are viewed as random variables in the Ori-HMRF and their spatial dependence is defined based on directional information. The Ori-HMRF model suppresses noise and generates accurate blood vessel segmentation results. Experimental validations were conducted on both normal mammary and breast cancer tissues.",biology
818,Fast Westfall-Young permutation procedure for combinatorial regulation discovery,Bioinformatics and Biomedicine," -Three or more transcription factors (TFs) often work together, and the combinatorial regulations are essential in cellular machinery. However, it is impossible to discover statistically significant sets of TF binding motifs due to the necessity of the multiple testing procedure. To improve the sensitivity of widely used Bonferroni correction or its modified methods, such as Holm procedure, Westfall-Young permutation procedure (WY-procedure) has often been applied. However, few studies have used WY-procedure for the discoveries of the combinatorial effects of the motifs because of the extremely large computational time. In this paper, we propose an efficient branch-and-bound algorithm to perform WY-procedure to enumerate statistically significant motif combinations. When we use WY-procedure for the combinatorial regulation discovery, finding the minimum P-value from each permuted dataset consumes an enormous amount of time. We show that a combination that has the possibility to achieve the minimum P-value appears with high frequency over the threshold in dataset. This property enables a frequent itemset mining algorithm to efficiently select the candidates to achieve the minimum P-value. Our demonstrations using yeast and human transcriptome datasets show that the proposed algorithm is orders-of-magnitude faster than WY-procedure, and can practically list statistically significant motif combinations even when any combinations are considered. ",biology
819,Ontology-based knowledge acquisition for neuromotor functional recovery in stroke,Bioinformatics and Biomedicine,"Hemiparesis is the most common impairment after stroke, the leading cause of adult disability in the United States. The initial severity of hemiparesis had been the strongest predictor of neuromotor functional recovery level. However, the intervention response of stroke survivors does not always correlate with their initial level of impairment. This implies the existence of other factors that may significantly affect stroke survivors' recovery process. In order to design targeting intervention therapy strategies, it is critical to consider these factors in a principled, comprehensive way so that physical rehabilitation (PR) researchers may predict which stroke survivors will respond best to therapy and subsequently, determine if a particular type of therapy is a more optimal match. Currently, such prediction is primarily a manual process and remains a challenging task to PR researchers and clinicians. We propose a computing framework based upon a domain-specific ontology. This framework aims to facilitate knowledge acquisition from existing sources via semantics-enhanced data mining (SEDM) techniques. As a result, it will assist PR researchers and clinicians in better predicting stroke survivors' neuromotor functional recovery level, and will help physical therapists customize most effective intervention therapy plans for individual stroke survivors.",biology
820,Application of Hilbert-Huang Transform to Heart Rate Variability Analysis,International Conference on Bioinformatics and Biomedical Engineering,"The paper introduces a new methodology of heart rate variability(HRV) in time-frequency analysis, which is based on Hilbert-Huang transform. We adopt the empirical mode decompostition(EMD) technique to decompose the R-R interval series into several mono-component signals which become analytic signal by means of Hilbert transform. So a novel algorithm based on Hilbert-Huang transform is proposed to extract the features of HRV signals. The numerical simulation results presented herein show the method can be used to identify the low-frequency and high-frequency bands of HRV more sharply and effectively through Hilbert spectrum than using the Fourier spectrum.",biology
821,A supervised learning approach to the unsupervised clustering of genes,Bioinformatics and Biomedicine,Clustering is a common step in the analysis of microarray data. Microarrays enable simultaneous high-throughput measurement of the expression level of genes. These data can be used to explore relationships between genes and can guide development of drugs and further research. A typical first step in the analysis of these data is to use an agglomerative hierarchical clustering algorithm on the correlation between all gene pairs. While this simple approach has been successful it fails to identify many genetic interactions that may be important for drug design and other important applications. We present an approach to the clustering of expression data that utilizes known gene-gene interaction data to improve results for already commonly used clustering techniques. The approach creates an ensemble similarity measure that can be used as input to common clustering techniques and provides results with increased biological significance while not altering the clustering approach at all.,biology
822,Strand : fast sequence comparison using mapreduce and locality sensitive hashing,International Conference on Bioinformatics," The Super Threaded Reference-Free Alignment-Free Nsequence Decoder (Strand) is a highly parallel technique for the learning and classi cation of gene sequence data into any number of associated categories or gene sequence taxonomies. Current methods, including the state-of-the-art sequence classi cation method RDP, balance performance by using a shorter word length. Strand in contrast uses a much longer word length, and does so e ciently by implementing a Divide and Conquer algorithm leveraging MapReduce style processing and locality sensitive hashing. Strand is able to learn gene sequence taxonomies and classify new sequences approximately 20 times faster than the RDP classi er while still achieving comparable accuracy results. This paper compares the accuracy and performance characteristics of Strand against RDP using 16S rRNA sequence data from the RDP training dataset and the Greengenes sequence repository. ",biology
823,SiS: Significant subnetworks in massive number of network topologies,Bioinformatics and Biomedicine,"Availability of abundant biological network data and its noisy nature necessitates extracting reliable subnetworks hidden in it. One key step towards that goal is to discover the subnetworks that appear frequently in a collection of networks. This paper presents a method, named SiS (Significant Subnetworks), to discover most probable subnetworks (i.e., subnetworks that have the highest chance to exist) in a large collection of biological networks where each node is labeled with the corresponding molecule (such as gene or protein). SiS builds a template network which summarizes the entire set of input networks. It then grows subnetworks that are most probable with the guidance of this template network. Our experiments demonstrate that our method scales to very large datasets and subnetworks easily. On the metabolic networks of the eukaryote organisms, our method runs from a few seconds to a few minutes depending on the subnetwork size. MULE, an existing method for the same problem, takes hours or does not complete for days on the same dataset. Our results also suggest that the most probable subnetworks are often the most frequent ones as well.",biology
824,A Novel Model for DNA Sequence Similarity Analysis Based on Graph Theory,Evolutionary Bioinformatics,"Determination of sequence similarity is one of the major steps in computational phylogenetic studies. As we know, during evolutionary history, not only DNA mutations for individual nucleotide but also subsequent rearrangements occurred. It has been one of major tasks of computational biologists to develop novel mathematical descriptors for similarity analysis such that various mutation phenomena information would be involved simultaneously. In this paper, different from traditional methods (eg, nucleotide frequency, geometric representations) as bases for construction of mathematical descriptors, we construct novel mathematical descriptors based on graph theory. In particular, for each DNA sequence, we will set up a weighted directed graph. The adjacency matrix of the directed graph will be used to induce a representative vector for DNA sequence. This new approach measures similarity based on both ordering and frequency of nucleotides so that much more information is involved. As an application, the method is tested on a set of 0.9-kb mtDNA sequences of twelve different primate species. All output phylogenetic trees with various distance estimations have the same topology, and are generally consistent with the reported results from early studies, which proves the new methods efficiency; we also test the new method on a simulated data set, which shows our new method performs better than traditional global alignment method when subsequent rearrangements happen frequently during evolutionary history. © the author(s), publisher and licensee Libertas Academica Ltd.",biology
825,Pure Parsimony Xor Haplotyping,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The haplotype resolution from xor-genotype data has been recently formulated as a new model for genetic studies [1]. The xor-genotype data is a cheaply obtainable type of data distinguishing heterozygous from homozygous sites without identifying the homozygous alleles. In this paper, we propose a formulation based on a well-known model used in haplotype inference: pure parsimony. We exhibit exact solutions of the problem by providing polynomial time algorithms for some restricted cases and a fixed-parameter algorithm for the general case. These results are based on some interesting combinatorial properties of a graph representation of the solutions. Furthermore, we show that the problem has a polynomial time k-approximation, where k is the maximum number of xor-genotypes containing a given single nucleotide polymorphisms (SNP). Finally, we propose a heuristic and produce an experimental analysis showing that it scales to real-world large instances taken from the HapMap project. © 2006 IEEE.",biology
826,Analyzing dynamical simulations of intrinsically disordered proteins using spectral clustering,International Conference on Bioinformatics,"Continuing improvements in algorithms and computer speeds promise that an increasing number of biomolecular phenomena can be simulated by molecular dynamics to produce accurate ldquotrajectoriesrdquo of their molecular motions on the nanosecond to microsecond time scale. An important target for such simulations will be non-equilibrium biochemical processes, such as protein folding, but existing tools for analyzing molecular dynamics trajectories are not well suited to non-equilibrium processes and progress will require improvements in tools for classifying the range and types of dynamics exhibited by these systems. An extreme example of a non-equilibrium biochemical process is the function of ldquointrinsically disorderedrdquo proteins - proteins that function without ever folding into a unique structure. In this paper, we demonstrate the use of spectral clustering methods to analyze the data produced from simulations of several forms from one class of intrinsically disordered proteins, the phenylalanine-glycine nucleoporins (FG-Nups). We explain why such methods are well-suited for the data produced by our simulations and show that clustering methods provide a direct, quantitative measure of how effectively single simulations independently sample regions of structural phase space. Moreover, our clustering results show distinct dynamical behavior in different forms of the FG-Nups, which may provide insights into their biological function.",biology
827,Probabilistic Biological Network Alignment,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Interactions between molecules are probabilistic events. An interaction may or may not happen with some probability, depending on a variety of factors such as the size, abundance, or proximity of the interacting molecules. In this paper, we consider the problem of aligning two biological networks. Unlike existing methods, we allow one of the two networks to contain probabilistic interactions. Allowing interaction probabilities makes the alignment more biologically relevant at the expense of explosive growth in the number of alternative topologies that may arise from different subsets of interactions that take place. We develop a novel method that efficiently and precisely characterizes this massive search space. We represent the topological similarity between pairs of aligned molecules (i.e., proteins) with the help of random variables and compute their expected values. We validate our method showing that, without sacrificing the running time performance, it can produce novel alignments. Our results also demonstrate that our method identifies biologically meaningful mappings under a comprehensive set of criteria used in the literature as well as the statistical coherence measure that we developed to analyze the statistical significance of the similarity of the functions of the aligned protein pairs. ÂŠ 2004-2012 IEEE.",biology
828,Protein Complexes Discovery Based on Protein-Protein Interaction Data via a Regularized Sparse Generative Network Model,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Detecting protein complexes from protein interaction networks is one major task in the postgenome era. Previous developed computational algorithms identifying complexes mainly focus on graph partition or dense region finding. Most of these traditional algorithms cannot discover overlapping complexes which really exist in the protein-protein interaction (PPI) networks. Even if some density-based methods have been developed to identify overlapping complexes, they are not able to discover complexes that include peripheral proteins. In this study, motivated by recent successful application of generative network model to describe the generation process of PPI networks and to detect communities from social networks, we develop a regularized sparse generative network model (RSGNM), by adding another process that generates propensities using exponential distribution and incorporating Laplacian regularizer into an existing generative network model, for protein complexes identification. By assuming that the propensities are generated using exponential distribution, the estimators of propensities will be sparse, which not only has good biological interpretation but also helps to control the overlapping rate among detected complexes. And the Laplacian regularizer will lead to the estimators of propensities more smooth on interaction networks. Experimental results on three yeast PPI networks show that RSGNM outperforms six previous competing algorithms in terms of the quality of detected complexes. In addition, RSGNM is able to detect overlapping complexes and complexes including peripheral proteins simultaneously. These results give new insights about the importance of generative network models in protein complexes identification. © 2006 IEEE.",biology
829,Toward Verified Biological Models,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The last several decades have witnessed a vast accumulation of biological data and data analysis. Many of these data sets represent only a small fraction of the systems behavior, making the visualization of full system behavior difficult. A more complete understanding of a biological system is gained when different types of data (and/or conclusions drawn from the data) are integrated into a larger-scale representation or model of the system. Ideally, this type of model is consistent with all available data about the system, and it is then used to generate additional hypotheses to be tested. Computer-based methods intended to formulate models that integrate various events and to test the consistency of these models with respect to the laboratory-based observations on which they are based are potentially very useful. In addition, in contrast to informal models, the consistency of such formal computer-based models with laboratory data can be tested rigorously by methods of formal verification. We combined two formal modeling approaches in computer science that were originally developed for non-biological system design. One is the inter-object approach using the language of live sequence charts (LSCs) with the Play-Engine tool, and the other is the intra-object approach using the language of statecharts and Rhapsody as the tool. Integration is carried out using InterPlay, a simulation engine coordinator. Using these tools, we constructed a combined model comprising three modules. One module represents the early lineage of the somatic gonad of C. elegans in LSCs, while a second more detailed module in statecharts represents an interaction between two cells within this lineage that determine their developmental outcome. Using the advantages of the tools, we created a third module representing a set of key experimental data using LSCs. We tested the combined statechart-LSC model by showing that the simulations were consistent with the set of experimental LSCs. This small-scale modular example demonstrates the potential for using similar approaches for verification by exhaustive testing of models by LSCs. It also shows the advantages of these approaches for modeling biology. ÂŠ 2006 IEEE.",biology
830,Unsupervised integration of multiple protein disorder predictors,Bioinformatics and Biomedicine,"Studies of intrinsically disordered proteins that lack a stable tertiary structure but still have important biological functions critically rely on computational methods that predict this property based on sequence information. Although a number of fairly successful models for prediction of protein disorder were developed over the last decade, the quality of their predictions is limited by available cases of confirmed disorders. To more reliably estimate protein disorder from protein sequences, an iterative algorithm is proposed that integrates predictions of multiple disorder models without relying on any protein sequences with confirmed disorder annotation. The iterative method alternately provides the maximum a posterior (MAP) estimation of disorder prediction and the maximum-likelihood (ML) estimation of quality of multiple disorder predictors. Experiments on data used at the Critical Assessment of Techniques for Protein Structure Prediction (CASP7 and CASP8) have shown the effectiveness of the proposed algorithm.",biology
831,A new method for computational drug repositioning using drug pairwise similarity,Bioinformatics and Biomedicine," -The traditional de novo drug discovery is known as a high cost and high risk process. In response, recently there is an increasing interest in discovering new indications for known drugs-a process known as drug repositioning-using computational methods. In this study, we present a new systematic approach for identifying potential new indications of an existing drug through its relation to similar drugs. Different from the previous similarity-based methods, we adapted a novel bipartite-graph based method when considering common drug targets and their interaction information. Furthermore, we added drug structure information into the calculation of drug pairwise similarity. In cross-validation experiments, our method achieved a sensitivity of 0.77 and specificity of 0.92 (AUC = 0.888) and compared favorably to the state of the art. When compared with a control group of drug uses, our drug repositioning results were found to be significantly enriched in both the biomedical literature and clinical trials. Our results indicate that combining chemical structure and drug target information results in better prediction performance and that the proposed approach successfully captures the implicit information between drug targets. ",biology
832,De Novo Protein Subcellular Localization Prediction by N-to-1 Neural Networks,Computational Intelligence Methods for Bioinformatics and Biostatistics,"Knowledge of the subcellular location of a protein provides valuable information about its function and possible interaction with other proteins. In the post-genomic era, fast and accurate predictors of subcellular location are required if this abundance of sequence data is to be fully exploited. We have developed a subcellular localization predictor (SCL-pred) which predicts the location of a protein into four classes for animals and fungi and five classes for plants (secretory pathway, cytoplasm, nucleus, mitochondrion and chloroplast) using high throughput machine learning techniques trained on large non-redundant sets of protein sequences. The algorithm powering SCL-pred is a novel Neural Network (N-to-1 Neural Network, or N1-NN) which is capable of mapping whole sequences into single properties (a functional class, in this work) without resorting to predefined transformations, but rather by adaptively compressing the sequence into a hidden feature vector. We benchmark SCL-pred against other publicly available predictors using two benchmarks including a new subset of Swiss-Prot release 57. We show that SCL-pred compares favourably to the other state-of-the-art predictors. Moreover, the N1-NN algorithm is fully general and may be applied to a host of problems of similar shape, that is, in which a whole sequence needs to be mapped into a fixed-size array of properties, and the adaptive compression it operates may even shed light on the space of protein sequences. The predictive systems described in this paper are publicly available at http://distill.ucd.ie/distill/. © 2011 Springer-Verlag Berlin Heidelberg.",biology
833,Leveraging hierarchy in medical codes for predictive modeling,International Conference on Bioinformatics,"ICD-9 codes are among the most important patient information recorded in electronic health records. They have been shown to be useful for predictive modeling of different adverse outcomes in patients, including diabetes and heart failure. An important characteristic of ICD-9 codes is the hierarchical relationships among different codes. Nevertheless, the most common feature representation used to incorporate ICD-9 codes in predictive models disregards the structural relationships. In this paper, we explore different methods to leverage the hierarchical structure in ICD-9 codes with the goal of improving performance of predictive models. We compare methods that leverage hierarchy by 1) incorporating the information during feature construction, 2) using a learning algorithm that addresses the structure in the ICD-9 codes when building a model, or 3) doing both. We propose and evaluate a novel feature engineering approach to leverage hierarchy, while simultaneously reducing feature dimensionality. Our experiments indicate that significant improvement in predictive performance can be achieved by properly exploiting ICD-9 hierarchy. Using two clinical tasks: predicting chronic kidney disease progression (Task-CKD), and predicting incident heart failure (Task-HF), we show that methods that use hierarchy outperform the conventional approach in F-score (0.44 vs 0.36 for Task-HF and 0.40 vs 0.37 for Task- CKD) and relative risk (4.6 vs 3.3 for Task-HF and 5.9 vs 3.8 for Task-CKD).",biology
834,Synthetic Protein Sequence Oversampling Method for Classification and Remote Homology Detection in Imbalanced Protein Data,Bioinformatics Research and Development," Many classifiers are designed with the assumption of wellbalanced datasets. But in real problems, like protein classification and remote homology detection, when using binary classifiers like support vector machine (SVM) and kernel methods, we are facing imbalanced data in which we have a low number of protein sequences as positive data (minor class) compared with negative data (major class). A widely used solution to that issue in protein classification is using a different error cost or decision threshold for positive and negative data to control the sensitivity of the classifiers. Our experiments show that when the datasets are highly imbalanced, and especially with overlapped datasets, the efficiency and stability of that method decreases. This paper shows that a combination of the above method and our suggested oversampling method for protein sequences can increase the sensitivity and also stability of the classifier. Synthetic Protein Sequence Oversampling (SPSO) method involves creating synthetic protein sequences of the minor class, considering the distribution of that class and also of the major class, and it operates in data space instead of feature space. We used G-proteincoupled receptors families as real data to classify them at subfamily and sub-subfamily levels (having low number of sequences) and could get better accuracy and Matthew's correlation coefficient than other previously published method. We also made artificial data with different distributions and overlappings of minor and major classes to measure the efficiency of our method. The method was evaluated by the area under the Receiver Operating Curve (ROC). ",biology
835,Investigating Topic Models' Capabilities in Expression Microarray Data Classification,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"In recent years a particular class of probabilistic graphical models-called topic models-has proven to represent an useful and interpretable tool for understanding and mining microarray data. In this context, such models have been almost only applied in the clustering scenario, whereas the classification task has been disregarded by researchers. In this paper, we thoroughly investigate the use of topic models for classification of microarray data, starting from ideas proposed in other fields (e.g., computer vision). A classification scheme is proposed, based on highly interpretable features extracted from topic models, resulting in a hybrid generative-discriminative approach; an extensive experimental evaluation, involving 10 different literature benchmarks, confirms the suitability of the topic models for classifying expression microarray data. © 2012 IEEE.",biology
836,DNA Motif Representation with Nucleotide Dependency,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The problem of discovering novel motifs of binding sites is important to the understanding of gene regulatory networks. Motifs are generally represented by matrices (position weight matrix (PWM) or position specific scoring matrix (PSSM)) or strings. However, these representations cannot model biological binding sites well because they fail to capture nucleotide interdependence. It has been pointed out by many researchers that the nucleotides of the DNA binding site cannot be treated independently, for example, the binding sites of zinc finger in proteins. In this paper, a new representation called Scored Position Specific Pattern (SPSP), which is a generalization of the matrix and string representations, is introduced, which takes into consideration the dependent occurrences of neighboring nucleotides. Even though the problem of discovering the optimal motif in SPSP representation is proved to be NP-hard, we introduce a heuristic algorithm called SPSP Finder, which can effectively find optimal motifs in most simulated cases and some real cases for which existing popular motif-finding software, such as Weeder, MEME, and AlignACE, fail. ÂŠ 2008 IEEE.",biology
837,"Phylogenetic Networks: Modeling, Reconstructibility, and Accuracy",IEEE/ACM Transactions on Computational Biology and Bioinformatics," -Phylogenetic networks model the evolutionary history of sets of organisms when events such as hybrid speciation and horizontal gene transfer occur. In spite of their widely acknowledged importance in evolutionary biology, phylogenetic networks have so far been studied mostly for specific data sets. We present a general definition of phylogenetic networks in terms of directed acyclic graphs (DAGs) and a set of conditions. Further, we distinguish between model networks and reconstructible ones and characterize the effect of extinction and taxon sampling on the reconstructibility of the network. Simulation studies are a standard technique for assessing the performance of phylogenetic methods. A main step in such studies entails quantifying the topological error between the model and inferred phylogenies. While many measures of tree topological accuracy have been proposed, none exist for phylogenetic networks. Previously, we proposed the first such measure, which applied only to a restricted class of networks. In this paper, we extend that measure to apply to all networks, and prove that it is a metric on the space of phylogenetic networks. Our results allow for the systematic study of existing network methods, and for the design of new accurate ones. ",biology
838,Resolving healthcare forum posts via similar thread retrieval,International Conference on Bioinformatics," Web communities such as healthcare web forums serve as popular platforms for users to get their complex medical queries resolved. A typical forum thread contains a query in its rst post, and a discussion around it in subsequent posts. However many users do not receive satisfactory responses from other members in the community, leaving them dissatis ed. We propose to help these users by exploiting an existing collection of discussion threads. Often many users su er from the same medical condition and start multiple discussion threads on very similar queries. In this paper we develop and evaluate a plethora of specialized search methods that treat an entire unresolved forum post as a query, and retrieve forum threads discussing similar problems to help resolve it. The task is more challenging than a traditional document retrieval problem, since forum posts can contain a lot of irrelevant background information. The discussion threads to be retrieved are also quite di erent from traditional unstructured text documents. We evaluate our results on a dataset comprising over 350K discussion threads and show that our proposed methods outperform state of the art retrieval methods for the task. In particular, method based on non-uniform weighting of thread posts and semantic analysis of the query text perform quite well. ",biology
839,Poisson-Based Self-Organizing Feature Maps and Hierarchical Clustering for Serial Analysis of Gene Expression Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Serial analysis of gene expression (SAGE) is a powerful technique for global gene expression profiling, allowing simultaneous analysis of thousands of transcripts without prior structural and functional knowledge. Pattern discovery and visualization have become fundamental approaches to analyzing such large-scale gene expression data. From the pattern discovery perspective, clustering techniques have received great attention. However, due to the statistical nature of SAGE data (i.e., underlying distribution), traditional clustering techniques may not be suitable for SAGE data analysis. Based on the adaptation and improvement of Self-Organizing Maps and hierarchical clustering techniques, this paper presents two new clustering algorithms, namely, PoissonS and PoissonHC, for SAGE data analysis. Tested on synthetic and experimental SAGE data, these algorithms demonstrate several advantages over traditional pattern discovery techniques. The results indicate that, by incorporating statistical properties of SAGE data, PoissonS and PoissonHC, as well as a hybrid approach (neuro-hierarchical approach) based on the combination of PoissonS and PoissonHC, offer significant improvements in pattern discovery and visualization for SAGE data. Moreover, a user-friendly platform, which may improve and accelerate SAGE data mining, was implemented. The system is freely available on request from the authors for nonprofit use. ÂŠ 2007 IEEE.",biology
840,Detection DNA Point Mutation with Rolling-Circle Amplification Chip,International Conference on Bioinformatics and Biomedical Engineering,"We present a protocol with isothermal rolling-circle amplification (RCA) to detect DNA point mutation on chip. The basic principle of the method is an allele-specific oligonucleotide circularization mediated by special DNA ligase. The probe is circularized when perfect complementary sequences between the probe oligonucleotide and target DNA. Mismatches around the ligation site can prevent probe circularization. The circularized probe (C-probe) can be amplified by rolling circle amplification to generate multimeric singlestranded DNA (ssDNA) Under isothermal condition. There are sequence regions to bind respectively with fluorescent probe, solid probe and Artificial template in the C-probe which we designed. These ssDNA products are hybridized with fluorescent probe and immobilized on a glass slide composing a regular microarray pattern. The signal of fluorescence can be monitored by a scanner in the presence of nucleic acids templates, whereas the probe cannot be circularized and signal of fluorescence cannot be found. The stringency discrimination of the molecular templates are up to 102-103 folds between matched and mismatched sequences. The development of C-probe-based technologies offers a promising prospect for situ detection, microarray, molecular diagnosis, single nucleotide polymorphism, and whole genome amplification.",biology
841,Biomedical Relationship Extraction from Literature Based on Bio-semantic Token Subsequences,Bioinformatics and Biomedicine,"Relationship extraction (RE) from biomedical literature is an important and challenging problem in both text mining and bioinformatics. Although various approaches have been proposed to extract protein-protein interaction types, their accuracy rates leave a large room for further exploration of more effective methods. In this paper, two supervised learning algorithms based on newly-defined ldquobio-semantic token subsequencerdquo are proposed for multi-class biomedical relationship extraction. The first approach calculates a ldquobio-semantic token subsequence kernelrdquo, while the second one explicitly extracts weighted features from bio-semantic token subsequences. The proposed structure called ldquobio-semantic token subsequencerdquo is able to capture semantic features from natural language sentences for biomedical RE. Two supervised learning algorithms based on the proposed structure outperform the state-of-the-art biomedical RE methods on multi-class protein-protein interaction extraction.",biology
842,Network propagation models for gene selection,International Conference on Bioinformatics,"In this paper, we explore several network propagation methods for gene selection from microarray gene expression datasets. The network propagation methods capture gene co-expression and differential expression with unified machine learning frameworks. Large scale experiments on five breast cancer datasets validated that the network propagation methods are capable of selecting genes that are more biologically interpretable and more consistent across multiple datasets, compared with the existing approaches.",biology
843,Analyzing Gene Expression Time-Courses,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Measuring gene expression over time can provide important insights into basic cellular processes. Identifying groups of genes with similar expression time-courses is a crucial first step in the analysis. As biologically relevant groups frequently overlap, due to genes having several distinct roles in those cellular processes, this is a difficult problem for classical clustering methods. We use a mixture model to circumvent this principal problem, with hidden Markov models (HMMs) as effective and flexible components. We show that the ensuing estimation problem can be addressed with additional labeled data - partially supervised learning of mixtures - through a modification of the Expectation-Maximization (EM) algorithm. Good starting points for the mixture estimation are obtained through a modification to Bayesian model merging, which allows us to learn a collection of initial HMMs. We infer groups from mixtures with a simple information-theoretic decoding heuristic, which quantifies the level of ambiguity in group assignment. The effectiveness is shown with high-quality annotation data. As the HMMs we propose capture asynchronous behavior by design, the groups we find are also asynchronous. Synchronous subgroups are obtained from a novel algorithm based on Viterbi paths. We show the suitability of our HMM mixture approach on biological and simulated data and through the favorable comparison with previous approaches. A software implementing the method is freely available under the GPL from http://ghmm.org/gql. © 2005 IEEE.",biology
844,A New Metric to Measure Gene Product Similarity,Bioinformatics and Biomedicine,"The widespread use of microarray technology and sequencing of genomes has made it increasingly possible to study the cellular sub-systems of organisms. Computational techniques applied to sequence data annotated with ontologies such as the gene ontology (GO) aid in understanding regulatory networks of genes. An important related problem is the estimation of the similarity between gene products based on their annotations. We present an approach to compute gene product similarity that takes into account both the hierarchical nature of GO and the co-occurrence of GO terms in annotations. It also accounts for differences in the cardinality of annotations and differences in the frequency of usage of GO terms. We demonstrate the validity of the metric by computing the similarity between gene products in several different contexts. These include the analysis of similarity within a specific signaling pathway, between proteins constituting a sequence family and the comparative evaluation of different clusterings of microarray data.",biology
845,Robust Fisher's Test for Periodicity Detection in Noisy Biological Time Series,International Conference on Bioinformatics,"Periodicity detection in time series measurements is a usual application of signal processing in studying biological data. The reasons for detecting periodically behaving biological events are many, e.g. periodicity in gene expression time series could suggest cell cycle control over the gene expression. In this paper we present a robust version of the Fisher's test for detecting hidden periodicities in uniformly sampled time series data. The robust test performs better than the original test in case the data is not truly Gaussianly distributed. The proposed robust method is nearly as fast to evaluate as the original Fisher's test.",biology
846,Classification of Left and Right Hand Motor Imagery Tasks Based on EEG Frequency Component Selection,International Conference on Bioinformatics and Biomedical Engineering,"In this paper, a method based on the time-frequency analysis of EEG frequency spectral Fisher-ratio is proposed to pre-select the most relevant movement-related EEG features. Within this method, combining EEG spectral time-frequency distribution with Fisher criterion, the detailed separability information of the frequency components between two classes of EEG patterns in time-frequency plane over C3, C4, Cz are well characterized, which provides a good guide for selecting the most relevant EEG frequency components. According to Fisher-ratio distribution of EEG spectrum by Matching Pursuit (MP) with high frequency resolution, the matched method Morlet wavelet filter is applied to extract the most relevant EEG frequency components. With the optimized EEG features, two classes of EEG patterns during left and right hand motor imagery are discriminated. Here, BCI competition data are analyzed offline and the satisfactory classification results are obtained, which verify the effectiveness of the proposed method in selecting the most relevant EEG spectral components.",biology
847,Predicting MHC-II Binding Affinity Using Multiple Instance Regression,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Reliably predicting the ability of antigen peptides to bind to major histocompatibility complex class II (MHC-II) molecules is an essential step in developing new vaccines. Uncovering the amino acid sequence correlates of the binding affinity of MHC-II binding peptides is important for understanding pathogenesis and immune response. The task of predicting MHC-II binding peptides is complicated by the significant variability in their length. Most existing computational methods for predicting MHC-II binding peptides focus on identifying a nine amino acids core region in each binding peptide. We formulate the problems of qualitatively and quantitatively predicting flexible length MHC-II peptides as multiple instance learning and multiple instance regression problems, respectively. Based on this formulation, we introduce MHCMIR, a novel method for predicting MHC-II binding affinity using multiple instance regression. We present results of experiments using several benchmark data sets that show that MHCMIR is competitive with the state-of-the-art methods for predicting MHC-II binding peptides. An online web server that implements the MHCMIR method for MHC-II binding affinity prediction is freely accessible at http://ailab.cs.iastate.edu/mhcmir. © 2011 IEEE.",biology
848,An Efficient Algorithm for Haplotype Inference on Pedigrees with Recombinations and Mutations,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Haplotype Inference (HI) is a computational challenge of crucial importance in a range of genetic studies. Pedigrees allow to infer haplotypes from genotypes more accurately than population data, since Mendelian inheritance restricts the set of possible solutions. In this work, we define a new HI problem on pedigrees, called Minimum-Change Haplotype Configuration (MCHC) problem, that allows two types of genetic variation events: recombinations and mutations. Our new formulation extends the Minimum-Recombinant Haplotype Configuration (MRHC) problem, that has been proposed in the literature to overcome the limitations of classic statistical haplotyping methods. Our contribution is twofold. First, we prove that the MCHC problem is APX-hard under several restrictions. Second, we propose an efficient and accurate heuristic algorithm for MCHC based on an L-reduction to a well-known coding problem. Our heuristic can also be used to solve the original MRHC problem and can take advantage of additional knowledge about the input genotypes. Moreover, the L-reduction proves for the first time that MCHC and MRHC are O(nm/log nm)-approximable on general pedigrees, where n is the pedigree size and m is the genotype length. Finally, we present an extensive experimental evaluation and comparison of our heuristic algorithm with several other state-of-the-art methods for HI on pedigrees. © 2012 IEEE.",biology
849,A GPU-Based multi-swarm PSO method for parameter estimation in stochastic biological systems exploiting discrete-time target series,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","Parameter estimation (PE) of biological systems is one of the most challenging problems in Systems Biology. Here we present a PE method that integrates particle swarm optimization (PSO) to estimate the value of kinetic constants, and a stochastic simulation algorithm to reconstruct the dynamics of the system. The fitness of candidate solutions, corresponding to vectors of reaction constants, is defined as the point-to-point distance between a simulated dynamics and a set of experimental measures, carried out using discrete-time sampling and various initial conditions. A multi-swarm PSO topology with different modalities of particles migration is used to account for the different laboratory conditions in which the experimental data are usually sampled. The whole method has been specifically designed and entirely executed on the GPU to provide a reduction of computational costs. We show the effectiveness of our method and discuss its performances on an enzymatic kinetics and a prokaryotic gene expression network. © 2012 Springer-Verlag.",biology
850,Gene Selection Using Locality Sensitive Laplacian Score,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Gene selection based on microarray data, is highly important for classifying tumors accurately. Existing gene selection schemes are mainly based on ranking statistics. From manifold learning standpoint, local geometrical structure is more essential to characterize features compared with global information. In this study, we propose a supervised gene selection method called locality sensitive Laplacian score (LSLS), which incorporates discriminative information into local geometrical structure, by minimizing local within-class information and maximizing local between-class information simultaneously. In addition, variance information is considered in our algorithm framework. Eventually, to find more superior gene subsets, which is significant for biomarker discovery, a two-stage feature selection method that combines the LSLS and wrapper method (sequential forward selection or sequential backward selection) is presented. Experimental results of six publicly available gene expression profile data sets demonstrate the effectiveness of the proposed approach compared with a number of state-of-the-art gene selection methods.",biology
851,Inexact Local Alignment Search over Suffix Arrays,Bioinformatics and Biomedicine,"We describe an algorithm for finding approximate seeds for DNA homology searches. In contrast to previous algorithms that use exact or spaced seeds, our approximate seeds may contain insertions and deletions. We present a generalized heuristic for finding such seeds efficiently and prove that the heuristic does not affect sensitivity. We show how to adapt this algorithm to work over the memory efficient suffix array with provably minimal overhead in running time. We demonstrate the effectiveness of our algorithm on two tasks: whole genome alignment of bacteria and alignment of the DNA sequences of 177 genes that are orthologous in human and mouse. We show our algorithm achieves better sensitivity and uses less memory than other commonly used local alignment tools.",biology
852,Mining Frequent Dense Subgraphs based on Extending Vertices from Unbalanced PPI Networks,International Conference on Bioinformatics and Biomedical Engineering,"The prediction of protein function is one of the problems arising in the recent progress in bioinformatics. A common used approach is to derive clusters from PPI dataset. However, such results often contain false positives. In this study, we propose a novel algorithm, EVDENSE, to efficiently mine frequent dense subgraphs from PPI networks. Instead of using summary graph, EVDENSE produces frequent dense patterns by extending vertices. Due to the unbalance character of PPI network, we also propose to generate frequent patterns using relative support. Through dealing with the 4 PPI datasets, the experiments show our method is efficiently. With the help of relative support, more frequent dense functional interaction patterns in the PPI networks can be identified.",biology
853,Classifying protein complexes from candidate subgraphs using fuzzy machine learning model,Bioinformatics and Biomedicine,"Many computational methods have been applied to identify protein complexes from experimentally obtained protein-protein interaction (PPI) networks. Because of the presence of unreliable interactions in PPI networks, multi-functionality of proteins, and complex connectivity of the PPI network, the task is very challenging. In this study, we tackle the presence of unreliable interactions in protein complex using Genetic-Algorithm Fuzzy Naïve Bayes (GAFNB) which takes unreliability into consideration. Many existing methods can provide lots of candidate subgraphs. So we focused on how to classify the protein complexes from the subgraphs by considering the fuzzy attribute of PPI. We experimented with two datasets of size 10,371 and 986, each containing 493 positive protein complexes from MIPS and TAP-MS datasets. We compared the performance of GAFNB with Naïve Bayes (NB). Results show that GAFNB performed better which indicates that a fuzzy model is more suitable when unreliability is present. It is necessary to consider the unreliability in identifying protein complexes.",biology
854,Multiobjective Optimization in Bioinformatics and Computational Biology,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -This paper reviews the application of multiobjective optimization in the fields of bioinformatics and computational biology. A survey of existing work, organized by application area, forms the main body of the review, following an introduction to the key concepts in multiobjective optimization. An original contribution of the review is the identification of five distinct “contexts,” giving rise to multiple objectives: These are used to explain the reasons behind the use of multiobjective optimization in each application area and also to point the way to potential future uses of the technique. Typically, the objectives may estimate very different aspects of the solutions, aspects that are, therefore, incommensurable and often (partially or wholly) in conflict. A general (unconstrained) multiobjective optimization problem (MOP) can be defined mathematically as: 00minimize00 z ¼ f ðxÞ ¼ ðf1ðxÞ; f2ðxÞ; . . . ; fmðxÞÞ ",biology
855,FoldRate: A Web-Server for Predicting Protein Folding Rates from Primary Sequence,The Open Bioinformatics Journal," With the avalanche of gene products in the postgenomic age, the gap between newly found protein sequences and the knowledge of their 3D (three dimensional) structures is becoming increasingly wide. It is highly desired to develop a method by which one can predict the folding rates of proteins based on their amino acid sequence information alone. To address this problem, an ensemble predictor, called FoldRate, was developed by fusing the folding-correlated features that can be either directly obtained or easily derived from the sequences of proteins. It was demonstrated by the jackknife cross-validation on a benchmark dataset constructed recently that FoldRate is at least comparable with or even better than the existing methods that, however, need both the sequence and 3D structure information for predicting the folding rate. As a user-friendly web-server, FoldRate is freely accessible to the public at www.csbio.sjtu.edu.cn/bioinf/FoldRate/, by which one can get the desired result for a query protein sequence in around 30 seconds. ",biology
856,Relationship between Vascular Elasticity and Human Pulse Waveform Based on FFT Analysis of Pulse Waveform with Different Age,International Conference on Bioinformatics and Biomedical Engineering,"This study proposed a method using the value of amplitude of FFT frequency to evaluate the vascular elasticity (VE). Although personally different features of subject and accidentalia in the measurements were existing, the result of the experiment reflected the relationship between VE and the pulse waveform comparatively well, and the relationship between the VE and age indirectly. The value of amplitude of FFT frequency could reflect the subtle variation of the pulse waveform better compared with the K value evaluating the VE reported in the published literature. This method showed potential in the respect of non-invasive VE detecting and put forward a new and doable suggestion for the study of VE.",biology
857,Underlying Principles of Natural Selection in Network Evolution: Systems Biology Approach,Evolutionary Bioinformatics," Systems biology is a rapidly expanding field that integrates diverse areas of science such as physics, engineering, computer science, mathematics, and biology toward the goal of elucidating the underlying principles of hierarchical metabolic and regulatory systems in the cell, and ultimately leading to predictive understanding of cellular response to perturbations. Because post-genomics research is taking place throughout the tree of life, comparative approaches offer a way for combining data from many organisms to shed light on the evolution and function of biological networks from the gene to the organismal level. Therefore, systems biology can build on decades of theoretical work in evolutionary biology, and at the same time evolutionary biology can use the systems biology approach to go in new uncharted directions. In this study, we present a review of how the post-genomics era is adopting comparative approaches and dynamic system methods to understand the underlying design principles of network evolution and to shape the nascent field of evolutionary systems biology. Finally, the application of evolutionary systems biology to robust biological network designs is also discussed from the synthetic biology perspective. ",biology
858,A Propagation-based Algorithm for Inferring Gene-Disease Assocations,German Conference on Bioinformatics," A fundamental challenge in human health is the identification of diseasecausing genes. Recently, several studies have tackled this challenge via a two-step approach: first, a linkage interval is inferred from population studies; second, a computational approach is used to prioritize genes within this interval. State-of-the-art methods for the latter task are based on the observation that genes causing the same or similar diseases tend to lie close to one another in a network of protein-protein or functional interactions. However, most of these approaches use only local network information in the inference process. Here we provide a global, network-based method for prioritizing disease genes. The method is based on formulating constraints on the prioritization function that relate to its smoothness over the network and usage of prior information. A propagation-based method is used to compute a function satisfying the constraints. We test our method on gene-disease association data in a cross-validation setting, and compare it to extant prioritization approaches. We show that our method provides the best overall performance, ranking the true causal gene first for 29% of the 1,369 diseases with a known gene in the OMIM knowledgebase. ",biology
859,Microarray Time-Series Data Clustering Using Rough-Fuzzy C-Means Algorithm,Bioinformatics and Biomedicine,"Clustering is one of the important analysis in functional genomics that discovers groups of co-expressed genes from microarray data. In this paper, the application of rough-fuzzy c-means (RFCM) algorithm is presented to discover co-expressed gene clusters. One of the major issues of the RFCM based microarray data clustering is how to select initial prototypes of different clusters. To overcome this limitation, a method is proposed to select initial cluster centers. It enables the RFCM algorithm to converge to an optimum or near optimum solutions and helps to discover co-expressed gene clusters. A method is also introduced based on Dunn's cluster validity index to identify optimum values of different parameters of the initialization method and the RFCM algorithm. The effectiveness of the RFCM algorithm, along with a comparison with other related methods, is demonstrated on five yeast gene expression time-series data sets using Silhouette index, Davies-Bouldin index, and gene ontology based analysis.",biology
860,BioPM:An Efficient Algorithm for Protein Motif Mining,International Conference on Bioinformatics and Biomedical Engineering,"Protein sequence analysis is one of frontier research issues in Bioinformatics. Sequential patterns in protein sequences can provide useful information for protein research, such as protein family identification. Sequential patterns mining is becoming one of important tasks in protein analysis. In this paper, a novel pattern mining algorithm called BioPM is presented based on prefix projection method. BioPM can mine conserved motifs across protein sequences efficiently. Experiments on Pfam database show that BioPM improves performance. The satisfactory experimental results suggest that BioPM may be applied to protein sequences analysis.",biology
861,Integrative machine learning approach for multi-class SCOP protein fold classification,German Conference on Bioinformatics," Classification and prediction of protein structure has been a central research theme in structural bioinformatics. Due to the imbalanced distribution of proteins over multi SCOP classification, most discriminative machine learning suffers the well-known 'False Positives' problem when learning over these types of problems. We have devised eKISS, an ensemble machine learning specifically designed to increase the coverage of positive examples when learning under multiclass imbalanced data sets. We have applied eKISS to classify 25 SCOP folds and show that our learning system improved over classical learning methods. Learning the similarities (or differences) between protein structures is very important in understanding the relationship between protein sequence, structure and function, and for the analysis of possible evolutionary relationships. Several new initiatives (e.g. structural genomics) and improvement of the methods for structure determination will result in a rapid increase in the number of structures. These highthroughput structure determination projects will produce structural data on proteins for which very little is known about their biology. Thus sophisticated computational methods are needed to detect, search for and compare remote protein homology in the hope that knowledge can be transferred to the new unknown protein (e.g. inference about function). Machine learning is one such approach that has been widely used in the development of automatic protein structure classification and prediction (Turcotte et al, 2001; Selbig and Argos, 1998; King et al., 1994). One of the aims of structural genomics is to enhance the understanding of the relationships between amino acid sequence and its corresponding protein fold. Hence, one of the advantages of using symbolic machine learning approaches for this purpose is to generate human understandable classifiers (rules) from some biological background knowledge that can explain the current protein folds in the Protein Data Bank (PDB). The SCOP database (Lo Conte et al, 2002) is a comprehensive hierarchical human classification of known protein structures, according to their evolutionary and structural relationships. The SCOP database is divided into 4 hierarchical levels: Class, Fold, Superfamily and Family. For SCOP 1.61 (Sept 2002), the 44327 protein domains were classified into 701 folds, resulting in an average of 64 domains per fold. The number of domains per fold varies in SCOP, where some of the folds are highly populated (e.g. TIM barrels) while some of the folds only contain a few examples (e.g. the HSP40/DnaJ peptide-binding fold only contains one protein). Thus, in order to perform learning over the SCOP folds, the common one-againstothers approach (two-class problem) will result in learning with an imbalanced data set. This imbalanced proportion of examples in each class contributes to the poor performance of standard machine learning techniques (e.g. decision trees). Existing machine learning approaches tend to produce a strong discriminatory classifier (high accuracy) with very low sensitivity (also called completeness) when learning on these types of problems. ",biology
862,Development of a Fall Detecting System for the Elderly Residents,International Conference on Bioinformatics and Biomedical Engineering,"Accidental falls are common causes of serious injury and health threats in the elder population. To deliver adequate medical support, the robust and immediate falls detection is important. Since the fall detection in the elderly remains a major challenge in the public health domain, effective fall-detection will provide urgent support and dramatically reduce the cost of medical care. In this work, we propose a fall-detecting system placing an accelerometer on the head level and using an algorithm to distinguish between falls and daily activities. The experimental results have demonstrated the proposed scheme with high reliability and sensitivity on fall detection. The system is not only cost effectively but also potable. It fulfills the requirements of fall detection.",biology
863,Inferring Connectivity of Genetic Regulatory Networks Using Information-Theoretic Criteria,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Recently, the concept of mutual information has been proposed for infering the structure of genetic regulatory networks from gene expression profiling. After analyzing the limitations of mutual information in inferring the gene-to-gene interactions, this paper introduces the concept of conditional mutual information and based on it proposes two novel algorithms to infer the connectivity structure of genetic regulatory networks. One of the proposed algorithms exhibits a better accuracy while the other algorithm excels in simplicity and flexibility. By exploiting the mutual information and conditional mutual information, a practical metric is also proposed to assess the likeliness of direct connectivity between genes. This novel metric resolves a common limitation associated with the current inference algorithms, namely the situations where the gene connectivity is established in terms of the dichotomy of being either connected or disconnected. Based on the data sets generated by synthetic networks, the performance of the proposed algorithms is compared favorably relative to existing state-of-the-art schemes. The proposed algorithms are also applied on realistic biological measurements, such as the cutaneous melanoma data set, and biological meaningful results are inferred. ÂŠ 2006 IEEE.",biology
864,Named Entity Recognition from Biomedical Text Using SVM,International Conference on Bioinformatics and Biomedical Engineering,"Nowadays biomedical research is developing rapidly. A large number of biomedical knowledge exists in the form of unstructured text documents in various files. Named Entity Recognition (NER) from biomedical text is one of the basic task s of biomedical text mining, of which purpose is to recognize the name of the specified type from the collection of biomedical text. NER result is usually the processing object of other text mining. NER from biological text is the foundation of bioinformatics research. At present, the best f-measure of biological named entity recognition system has reached more than 80%, but is lower than general NER system which can reach about 90%. Here we use support vector machine (SVM), which is an effective and efficient tool to analyze data and recognize patterns, to recognize biomedical named entity. We get data set from GENIA corpus which is a collection of Medline abstracts. In the experiment, we get precision rate= 84.24% and recall rate=80.76% finally.",biology
865,Exploration of Evolutionary Relations between Protein Structures,Bioinformatics Research and Development," We describe a new method for the exploration of evolutionary relations between protein structures. The approach is based on the ESSM algorithm for detecting structural mutations, the output of which is then used for construction of fold space graphs. Fold space graphs can be regarded as a convenient tool for visualization and analysis of evolutionary relationships between protein structures, providing more information than traditional phylogenetic approaches. We have applied the method for analysis of evolutionary relations between CATH protein domains. The experiments allowed us to obtain estimates of the distribution of probabilities for different types of fold mutations, detect several chains of evolutionary related protein domains as well as to explore the most probable b-sheet extension scenarios. ",biology
866,Improved Gapped Alignment in BLAST,IEEE/ACM Transactions on Computational Biology and Bioinformatics," - Homology search is a key tool for understanding the role, structure, and biochemical function of genomic sequences. The most popular technique for rapid homology search is BLAST, which has been in widespread use within universities, research centres, and commercial enterprises since the early 1990s. In this paper, we propose a new step in the BLAST algorithm to reduce the computational cost of searching with negligible effect on accuracy. This new step - semi-gapped alignment - compromises between the efficiency of ungapped alignment and the accuracy of gapped alignment, allowing BLAST to accurately filter sequences with lower computational cost. In addition, we propose an heuristic - restricted insertion alignment - that avoids unlikely evolutionary paths with the aim of reducing gapped alignment cost with negligible effect on accuracy. Together, after including an optimisation of the local alignment recursion, our two techniques more than double the speed of the gapped alignment stages in BLAST. We conclude that our techniques are an important improvement to the BLAST algorithm. Source code for the alignment algorithms is available for download at http://www.bsg.rmit.edu.au/iga/. ",biology
867,Strong Association Rules Mining Without Using Frequent Items for Microarray Analysis,International Conference on Bioinformatics and Biomedical Engineering,"Microarray technology has created a revolution in the field of biological research. Association rules can not only group the similarly expressed genes but also discern relationships among genes. However, the efficiency of traditional method to generate association rules is not very well. We develop a novel algorithm, SAW, to generate strong association rules by combining the paired rules, which can avoid lots of unnecessary computing that traditional method often encounter. The experiments show our method is more efficiently than FARMER.",biology
868,A Calculated Methodology of Regional Contributions Based on MM5-CAMx in Typical City: A 2006 Case Study of SO2 and Sulfate,International Conference on Bioinformatics and Biomedical Engineering,"A calculated methodology of regional contributions based on the MM5-CAMx model was developed to analyze the impact of different emission regions on urban air quality. Meteorological fields of January, April, July and October in 2006 were obtained by running the MM5 modeling system. The SO2 source apportionment technology was used to investigate the influence of the regions on air quality in Tangshan. A K-means clustering algorithm was used to group these contribution rates into five clusters according to daily average concentration contribution of 120 days. Results indicate that the receptor-self as emission region has the important contribution to receptor sites, the pollution could be aggravated by local source emissions. Combined with the Tangshan emission inventory, the analysis shows that several industries are located in inappropriate sits. Additionally the SO2 and PSO4 source apportionment study reveals a few differences from the contributions of emission regions.",biology
869,Early Breast Cancer Detection with Hemi-Elliptical Configuration by UWB Imaging,International Conference on Bioinformatics and Biomedical Engineering,"Ultra-wideband (UWB) microwave imaging is a very promising method for the early breast cancer detection physically based on the large contrast of electromagnetic parameters between the malignant tumor and the normal breast tissues. In this study, the tumor detection in a two dimensional breast model with hemi-elliptical configuration is carried out numerically. The influences from the skin, breast gland and the chest wall are involved in the study. The dispersion characteristics of the breast organisms are taken into account to approach the actual properties of the human breast. Results show that the tumor could be clearly recognized from the reconstructed images created by the confocal algorithm after the appropriate signal processing.",biology
870,Multisample aCGH Data Analysis via Total Variation and Spectral Regularization,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -DNA copy number variation (CNV) accounts for a large proportion of genetic variation. One commonly used approach to detecting CNVs is array-based comparative genomic hybridization (aCGH). Although many methods have been proposed to analyze aCGH data, it is not clear how to combine information from multiple samples to improve CNV detection. In this paper, we propose to use a matrix to approximate the multisample aCGH data and minimize the total variation of each sample as well as the nuclear norm of the whole matrix. In this way, we can make use of the smoothness property of each sample and the correlation among multiple samples simultaneously in a convex optimization framework. We also developed an efficient and scalable algorithm to handle large-scale data. Experiments demonstrate that the proposed method outperforms the state-of-theart techniques under a wide range of scenarios and it is capable of processing large data sets with millions of probes. ",biology
871,Conditional random fields for morphological analysis of wireless ECG signals,International Conference on Bioinformatics," Thanks to advances in mobile sensing technologies, it has recently become practical to deploy wireless electrocardiograph sensors for continuous recording of ECG signals. This capability has diverse applications in the study of human health and behavior, but to realize its full potential, new computational tools are required to e ectively deal with the uncertainty that results from the noisy and highly nonstationary signals collected using these devices. In this work, we present a novel approach to the problem of extracting the morphological structure of ECG signals based on the use of dynamically structured conditional random eld (CRF) models. We apply this framework to the problem of extracting morphological structure from wireless ECG sensor data collected in a lab-based study of habituated cocaine users. Our results show that the proposed CRF-based approach signi cantly out-performs independent prediction models using the same features, as well as a widely cited open source toolkit. ",biology
872,Enhancing the computation of approximate solutions of the protein structure determination problem through global constraints for discrete crystal lattices,Bioinformatics and Biomedicine,This paper investigates alternative global constraints that can be introduced in a constraint solver over discrete crystal lattices. The objective is to enhance the efficiency of lattice solvers in dealing with the construction of approximate solutions of the protein structure determination problem. The paper discusses various alternatives and provides preliminary results concerning the computational properties of the different global constraints.,biology
873,Structure Motif Discovery and Mining the PDB,German Conference on Bioinformatics," Motivation: Many of the most interesting functional and evolutionary relationships among proteins are so ancient that they cannot be reliably detected through sequence analysis and are apparent only through a comparison of the tertiary structures. The conserved features can often be described as structural motifs consisting of a few single residues or Secondary Structure (SS) elements. Confidence in such motifs is greatly boosted when they are found in more than a pair of proteins. Results: We describe an algorithm for the automatic discovery of recurring patterns in protein structures. The patterns consist of individual residues having a defined order along the protein's backbone that come close together in the structure and whose spatial conformations are similar. The residues in a pattern need not be close in the protein's sequence. The work described in this paper builds on an earlier reported algorithm for motif discovery. This paper describes a significant improvement of the algorithm which makes it very efficient. The improved efficiency allows us to use it for doing unsupervised learning of patterns occurring in small subsets in a large set of structures, a non-redundant subset of the Protein Data Bank (PDB) database of all known protein structures. Availability: The program is freely available to academia, requests can be sent to Inge.Jonassen@ii.uib.no. Contact: Inge.Jonassen@ii.uib.no ",biology
874,Efficient and Scalable Indexing Techniques for Biological Sequence Data,Bioinformatics Research and Development," We investigate indexing techniques for sequence data, crucial in a wide variety of applications, where efficient, scalable, and versatile search algorithms are required. Recent research has focused on suffix trees (ST) and suffix arrays (SA) as desirable index representations. Existing solutions for very long sequences however provide either efficient index construction or efficient search, but not both. We propose a new ST representation, STTD64, which has reasonable construction time and storage requirement, and is efficient in search. We have implemented the construction and search algorithms for the proposed technique and conducted numerous experiments to evaluate its performance on various types of real sequence data. Our results show that while the construction time for STTD64 is comparable with current ST based techniques, it outperforms them in search. Compared to ESA, the best known SA technique, STTD64 exhibits slower construction time, but has similar space requirement and comparable search time. Unlike ESA, which is memory based, STTD64 is scalable and can handle very long sequences. ",biology
875,A Brain MR Images Segmentation Method Based on SOM Neural Network,International Conference on Bioinformatics and Biomedical Engineering,"Image segmentation is an indispensable process in the visualization of human tissues, particularly during clinical analysis of magnetic resonance (MR) images. In this paper, a novel brain MR images segmentation method is presented based on self-organizing map (SOM) neural network. The method comprises two main steps: feature extraction and pixel classification based on SOM neural network. In traditional techniques, neural network's input is the feature vector extracted from the intensity of the pixel and of its n nearest neighbors, which introduces dependency on the gray levels spatial distribution, and thus the final segmentation results are prone to be effected by noise. To enhance the robustness of the method, we perform statistical transformation to the traditional feature vector as neural network's input. Simulated brain MR images with different noise levels and intensity inhomogeneities are segmented to demonstrate the superiority of the proposed method compared to the traditional technique.",biology
876,Algorithms for Regularized Linear Discriminant Analysis,International Conference on Bioinformatics," This paper is focused on regularized versions of classification analysis and their computation for highdimensional data. A variety of regularized classification methods has been proposed and we critically discuss their computational aspects. We formulate several new algorithms for regularized linear discriminant analysis, which exploits a regularized covariance matrix estimator towards a regular target matrix. Numerical linear algebra considerations are used to propose tailor-made algorithms for specific choices of the target matrix. Further, we arrive at proposing a new classification method based on L2-regularization of group means and the pooled covariance matrix and accompany it by an efficient algorithm for its computation. ",biology
877,Internal Evaluation Measures as Proxies for External Indices in Clustering Gene Expression Data,Bioinformatics and Biomedicine," -Several external indices that use information not present in the dataset were shown to be useful for evaluation of representative based clustering algorithms. However, such supervised measures are not directly useful for construction of better clustering algorithms when class labels are not provided. We propose a method for identifying internal cluster evaluation measures that use only information present in the dataset and are related to given external indices. We utilize these internal measures for the construction of representative based clustering algorithms. Both identification and utilization steps of the proposed method are enabled by use of a component-based clustering algorithm design. Experiments on 432 algorithms using gene expression data sets provide evidence that some internal measures could be used as surrogates for external indices proposed in the literature. Moreover, the obtained results suggest that internal measures correlated to selected external indices can guide the algorithms toward significantly better cluster models. ",biology
878,Investigating the pattern of syndrome based on the difference of symptom network in depression,Bioinformatics and Biomedicine,"In TCM theory, the syndrome is crucial to diagnose diseases and treat patients. In syndrome identification, the relation of symptoms usually correlates with syndrome and represents the pattern of syndrome at symptomatic level. Hence, we learn models for classifying syndromes in depression using 4 different algorithms, which are naive Bayes, Bayes network, SVM and C4.5. From the results of classification, we find that the dependence of symptoms has something to do with the accuracies of syndrome classification. Then, 8 symptom networks corresponding to depression and 7 syndromes are constructed to explore the interaction profile of symptoms under syndrome. By comparing syndrome-specific symptom network to the base network of depression, we discover the enriched edges and different nodes to represent the pattern of each syndrome. Literature and symptom ranking by Fisher score demonstrate the correctness of the different nodes selected through network comparison. After all, the enriched edges and different nodes associated with a given syndrome reveal the pattern of that syndrome at symptomatic level.",biology
879,Density-based classification of protein structures using iterative TM-score,Bioinformatics and Biomedicine,"Finding similarity between a pair of protein structures is one of the fundamental tasks in many areas of bioinformatical research such as protein structure prediction, function mapping, etc. We propose a method for finding pairing of amino acids based on densities of the structures and we also propose a modification to the original TM-score rotation algorithm that assess similarity score to this alignment. Proposed modification is faster than TM and comparably robust according to non-optimal parts in the alignment. We measure the qualities of the algorithm in terms of SCOP classification accuracy. Regarding the accuracy, our solution outperforms the contemporary solutions at two out of three tested levels of the SCOP hierarchy.",biology
880,Network-assisted causal gene detection in genome-wide association studies: an improved module search algorithm,International Conference on Bioinformatics,"The recent success of genome-wide association (GWA) studies has greatly expanded our understanding of many complex diseases by delivering previously unknown loci and genes. A large number of GWAS datasets have already been made available, with more being generated. To explore the underlying moderate and weak signals, we recently developed a network-based dense module search (DMS) method for identification of disease candidate genes from GWAS datasets, leveraging on the joint effect of multiple genes. DMS is designed to dynamically search for the best nodes in a step-wise fashion and, thus, could overcome the limitation of pre-defined gene sets. Here, we propose an improved version of DMS, the topologically-adjusted DMS, to facilitate the analysis of complex diseases. Building on the previous version of DMS, we improved the randomization process by taking into account the topological character, aiming to adjust the bias potentially caused by high-degree nodes in the whole network. We demonstrated the topologically-adjusted DMS algorithm in a GWAS dataset for schizophrenia. We found the improved DMS strategy could effectively identify candidate genes while reducing the burden of high-degree nodes. In our evaluation, we found more candidate genes identified by the topologically-adjusted DMS algorithm have been reported in the previous association studies, suggesting this new algorithm has better performance than the unweighted DMS algorithm. Finally, our functional analysis of the top module genes revealed that they are enriched in immune-related pathways.",biology
881,Predicting Metal-Binding Sites from Protein Sequence,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -Prediction of binding sites from sequence can significantly help towards determining the function of uncharacterized proteins on a genomic scale. The task is highly challenging due to the enormous amount of alternative candidate configurations. Previous research has only considered this prediction problem starting from three-dimensional information. When starting from sequence alone, only methods that predict the bonding state of selected residues are available. The sole exception consists of pattern-based approaches, which rely on very specific motifs and cannot be applied to discover truly novel sites. We develop new algorithmic ideas based on structured output learning for determining transition-metal binding sites coordinated by cysteines and histidines. The inference step (retrieving the best-scoring output) is intractable for general output types (i.e. general graphs). However, under the assumption that no residue can coordinate more than one metal ion, we prove that metal binding has the algebraic structure of a matroid, allowing us to employ a very efficient greedy algorithm. We test our predictor in a highly stringent setting where the training set consists of protein chains belonging to SCOP folds different from the ones used for accuracy estimation. In this setting, our predictor achieves 56% precision and 60% recall in the identification of ligand-ion bonds. ",biology
882,An Overview of BioCreative II.5,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -We present the results of the BioCreative II.5 evaluation in association with the FEBS Letters experiment, where authors created Structured Digital Abstracts to capture information about protein-protein interactions. The BioCreative II.5 challenge evaluated automatic annotations from 15 text mining teams based on a gold standard created by reconciling annotations from curators, authors, and automated systems. The tasks were to rank articles for curation based on curatable protein-protein interactions; to identify the interacting proteins (using UniProt identifiers) in the positive articles (61); and to identify interacting protein pairs. There were 595 fulltext articles in the evaluation test set, including those both with and without curatable protein interactions. The principal evaluation metrics were the interpolated area under the precision/recall curve (AUC iP/R), and (balanced) F-measure. For article classification, the best AUC iP/R was 0.70; for interacting proteins, the best system achieved good macroaveraged recall (0.73) and interpolated area under the precision/recall curve (0.58), after filtering incorrect species and mapping homonymous orthologs; for interacting protein pairs, the top (filtered, mapped) recall was 0.42 and AUC iP/R was 0.29. Ensemble systems improved performance for the interacting protein task. ",biology
883,Non-negative matrix and tensor factorization based classification of clinical microarray gene expression data,Bioinformatics and Biomedicine,Non-negative information can benefit the analysis of microarray data. This paper investigates the classification performance of non-negative matrix factorization (NMF) over gene-sample data. We also extends it to higher-order version for classification of clinical time-series data represented by tensor. Experiments show that NMF and the higher-order NMF can achieve at least comparable prediction performance.,biology
884,Prediction of Protein Functions with Gene Ontology and Interspecies Protein Homology Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Accurate computational prediction of protein functions increasingly relies on network-inspired models for the protein function transfer. This task can become challenging for proteins isolated in their own network or those with poor or uncharacterized neighborhoods. Here, we present a novel probabilistic chain-graph-based approach for predicting protein functions that builds on connecting networks of two (or more) different species by links of high interspecies sequence homology. In this way, proteins are able to exchange functional information with their neighbors-homologs from a different species. The knowledge of interspecies relationships, such as the sequence homology, can become crucial in cases of limited information from other sources of data, including the protein-protein interactions or cellular locations of proteins. We further enhance our model to account for the Gene Ontology dependencies by linking multiple but related functional ontology categories within and across multiple species. The resulting networks are of significantly higher complexity than most traditional protein network models. We comprehensively benchmark our method by applying it to two largest protein networks, the Yeast and the Fly. The joint Fly-Yeast network provides substantial improvements in precision, accuracy, and false positive rate over networks that consider either of the sources in isolation. At the same time, the new model retains the computational efficiency similar to that of the simpler networks. ÂŠ 2011 IEEE.",biology
885,Automated Detection of Pulmonary Nodules in HRCT Images,International Conference on Bioinformatics and Biomedical Engineering,"Lung cancer's death rates have been the main cause of cancer deaths in the world, early detection and treatment of lung cancer can greatly improve the survival rate of patient. This paper presents an automatic computer-aided detection (CAD) scheme that can identify the pulmonary nodule at an early stage from CT images. The work is separated to several steps: the segmentation of lung parenchyma, trachea and main airway bronchi elimination, the filter of nodule candidates, the detection of nodule candidates, the feature extraction and classification, three-dimensional visualization. The clinical testing for lots of lung cancer patients revealed that its high detection sensitivity of pulmonary nodules can meet basically the requirement of clinical diagnosis.",biology
886,An efficient motif finding algorithm for large DNA data sets,Bioinformatics and Biomedicine,"The planted (l, d) motif discovery has been successfully used to locate transcription factor binding sites in dozens of promoter sequences over the past decade. However, there has not been enough work done in identifying (l, d) motifs in the next-generation sequencing (ChIP-seq) data sets, which contain thousands of input sequences and thereby bring new challenge to make a good identification in reasonable time. To cater this need, we propose a new planted (l, d) motif discovery algorithm named MCES, which identifies motifs by mining and combining emerging substrings. Specially, to handle larger data sets, we design a MapReduce-based strategy to mine emerging substrings distributedly. Experimental results on the simulated data show that i) MCES is able to identify (l, d) motifs efficiently and effectively in thousands to millions of input sequences, and runs faster than the state-of-the-art (l, d) motif discovery algorithms, such as F-motif and TraverStringsR; ii) MCES is able to identify motifs without known lengths, and has a better identification accuracy than the competing algorithm CisFinder. Also, the validity of MCES is tested on real data sets.",biology
887,Class Conditional Distance Metric for 3D Protein Structure Classification,International Conference on Bioinformatics and Biomedical Engineering,"Considering that proteins with similar 3D structures have similar functions or biological actions, classification of proteins based on 3D structure can lead biologists to the investigation of new protein's structural, evolutionary, and functional relatedness. However, 3D protein structure remains a hard task, because different protein classes may have different discriminant features and unbalanced data distribution. This means that standard pairwise distance computation like Euclidean distance does not always have equal strength on features in the feature space. Most existing 3D protein structure classification methods relies on a K-Nearest Neighbor (KNN) classifier with a Euclidean distance, which do not consider the above factors and results on low classification accuracy. To improve the KNN for 3D protein structure classification, we propose the Class Conditional Distance Metric (CCDM), which takes into account the within-class neighborhood distribution of the protein descriptors and iteratively estimates distance update terms, thereby modifying the within-class neighborhood structure. Experimental results show that our approach gives significantly better results than a standard KNN classifier and is comparable the state of the art in terms of accuracy on the FSSP/DALI protein data set.",biology
888,Using silver and semi-gold standard corpora to compare open named entity recognisers,Bioinformatics and Biomedicine,"Ontologies have become a central resource for defining biomedical concepts but linkage to and from textual data is still an unresolved technology. In this paper we approach the task of concept recognition in text by comparing four extant systems (cTAKES, NCBO Annotator, BeCAS and Metamap) with default parameter settings. The systems are compared on benchmark data consisting of 2,163 scientific abstracts and 906 clinical trial reports using an automatically constructed “silver” standard and a random semi-gold standard evaluation methodology. Furthermore, evaluation is conducted on the basis of specific concept identifiers. Experimental results show: (i) Generally higher levels of concept recognition on clinical trial reports than on scientific abstracts; (ii) The best performing system we observed on the silver standard was cTAKES on both the abstract and clinical trial corpora, however NCBO Annotator performed stronger when considering only the selected broad semantic types; (iii) BeCAS and Metamap had a tendency to annotate coarser-grained annotations; (iv) the random semi-gold evaluation places an upper bound on the performance of systems. This shows broad agreement with the silver standard evaluation but highlights areas where the silver standard methodology might be improved.",biology
889,Finding Informative Genes from Multiple Microarray Experiments: A Graph-based Consensus Maximization Model,Bioinformatics and Biomedicine,"With the rapid advancement of biology technology, many microarray experiments are conducted towards the same problem of finding informative genes. Therefore, it is important to find a set of informative genes integrating multiple microarray experiments that achieves maximal consensus. Most previous re- searches formulated this problem as a rank aggregation problem. In this paper, we propose a novel Graph-based Consensus Maximization (GCM) model to estimate the conditional probability of each gene being informative, then the genes are ranked by this probability. The estimation of the probabilities is formulated as an optimization problem on a bipartite graph, where the criterion function favors the smoothness of the prediction over the graph and penalizes deviations from the initial input ranked lists from microarray experiments. We solve this problem through iterative propagation of probability estimates among neighboring nodes. In addition, when certain genes have already been identified to be informative, it has never been explored in the literature how to take advantage of such information to improve the consensus result. Our proposed GCM model can be naturally extended to incorporate such information, thus increasing the quality of the predicted result. In the experimental evaluation, we conducted experiments on the five prostate cancer microarray studies. The results showed that our model outperformed other baseline methods in finding informative genes. Furthermore, by adding only one piece of information that some gene is informative, our model yielded a significantly better result. The experimental evaluation demonstrates that the proposed GCM model is effective and superior in finding informative genes from multiple microarray experiments.",biology
890,Identification of biomarkers in breast cancer metastasis by integrating protein-protein interaction network and gene expression data,International Conference on Bioinformatics,"Identification of biomarkers for breast cancer metastasis is a well studied problem. Recently, several large-scale studies used gene expression data to identify markers related to metastatic process. However, it was shown that these gene expression based markers often have low reproducibility across different data sets, for a number of reasons. These include small sample sizes compared to the number of genes, gene expression variations between individuals that do not contribute to the metastasis process, and the limitation for microarray technology being unable to detect changes beyond transcriptional level. Here a graph-theoretical approach based on the topology of protein-protein interaction (PPI) networks is proposed for biomarker discovery. The idea is to identify a set of genes that give connectivity to differentially expressed (DE) genes in a PPI network, based on the key observation that biomarkers may provide functional linkage to DE genes in PPI networks. Our approach is applied to two breast cancer microarray datasets for biomarker discovery. Those biomarkers have a significant number of known cancer susceptibility genes among them and are significantly enriched in biological processes and pathways that are involved in carcinogenic process. Furthermore, markers selected by our method have a higher stability across the two datasets than in the previous studies. Therefore, the approach described in this study is a new way to identify novel biomarkers for cancer metastasis and can potentially improve the understanding of carcinogenesis dynamics.",biology
891,Biclustering Models for Structured Microarray Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Microarrays have become a standard tool for investigating gene function and more complex microarray experiments are increasingly being conducted. For example, an experiment may involve samples from several groups or may investigate changes in gene expression over time for several subjects, leading to large three-way data sets. In response to this increase in data complexity, we propose some extensions to the plaid model, a biclustering method developed for the analysis of gene expression data. This model-based method lends itself to the incorporation of any additional structure such as external grouping or repeated measures. We describe how the extended models may be fitted and illustrate their use on real data. © 2005 IEEE.",biology
892,Data driven knowledge acquisition method for domain knowledge enrichment in the healthcare,Bioinformatics and Biomedicine,"Semantic computing technologies have matured to be applicable to many critical domains, such as life sciences and health care. However, the key to their success is the rich domain knowledge which consists of domain concepts and relationships, whose creation and refinement remains a challenge. In this paper, we develop a technique for enriching domain knowledge, focusing on populating the domain relationships. We determine missing relationships between the domain concepts by validating domain knowledge against real world data sources. We evaluate our approach in the healthcare domain using Electronic Medical Record(EMR) data, and demonstrate that semantic techniques can be used to semi-automate labour intensive tasks without sacrificing fidelity of domain knowledge.",biology
893,Bases of Motifs for Generating Repeated Patterns with Wild Cards,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -Motif inference represents one of the most important areas of research in computational biology, and one of its oldest ones. Despite this, the problem remains very much open in the sense that no existing definition is fully satisfying, either in formal terms, or in relation to the biological questions that involve finding such motifs. Two main types of motifs have been considered in the literature: matrices (of letter frequency per position in the motif) and patterns. There is no conclusive evidence in favor of either, and recent work has attempted to integrate the two types into a single model. In this paper, we address the formal issue in relation to motifs as patterns. This is essential to get at a better understanding of motifs in general. In particular, we consider a promising idea that was recently proposed, which attempted to avoid the combinatorial explosion in the number of motifs by means of a generator set for the motifs. Instead of exhibiting a complete list of motifs satisfying some input constraints, what is produced is a basis of such motifs from which all the other ones can be generated. We study the computational cost of determining such a basis of repeated motifs with wild cards in a sequence. We give new upper and lower bounds on such a cost, introducing a notion of basis that is provably contained in (and, thus, smaller) than previously defined ones. Our basis can be computed in less time and space, and is still able to generate the same set of motifs. We also prove that the number of motifs in all bases defined so far grows exponentially with the quorum, that is, with the minimal number of times a motif must appear in a sequence, something unnoticed in previous work. We show that there is no hope to efficiently compute such bases unless the quorum is fixed. ",biology
894,A Bayesian-based prediction model for personalized medical health care,Bioinformatics and Biomedicine,"In this paper, we present a Bayesian-based Personalized Laboratory Tests prediction (BPLT) model to solve a real world medical problem: how to recommend laboratory tests to a group of patients? Given a patient who has conducted several laboratory tests, BPLT model recommends further laboratory tests that are the most related to this patient. We regard this laboratory test prediction problem as a special classification problem, where a new laboratory test belongs to either a ""taken"" or ""not-taken"" class. Our goal is to find the laboratory tests with high probability of ""taken"" and low probability of ""not taken"". Based on Bayesian method, the BPLT model builds a weighting function to investigate the correlations among laboratory tests and generate the rank of laboratory tests. In order to evaluate the proposed BPLT model, we further propose a novel evaluation metric to subjectively measure the accuracy of BPLT model. Experimental results show that BPLT model achieves good performance on the real data sets and provides a good solution to our real world application.",biology
895,Identification of Individual Walking Patterns Using Gait Acceleration,International Conference on Bioinformatics and Biomedical Engineering,"This paper presents an improved approach on identifying users based on three-dimensional gait acceleration signal characteristics produced by walking. When the user carries the wearable gait acceleration acquiring system, acceleration signals are registered by the accelerometer. Through dividing the signals into gait cycles, gait feature code which represents the walking pattern of the user can be extracted. Recognition is based on the general idea of template matching. We use dynamic time warping (DTW) algorithm for matching so that non-linear time normalization could be used to dispose the problems resulted from naturally occurring changes in walking speed. Experiments were performed on 35 healthy subjects walking on their normal speed; Equal Error Rate of 6.7% was achieved. Our preliminary experiments confirm the possibility of recognizing users based on their gait acceleration.",biology
896,Gene Mapping and Marker Clustering Using Shannon's Mutual Information,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Finding the causal genetic regions underlying complex traits is one of the main aims in human genetics. In the context of complex diseases, which are believed to be controlled by multiple contributing loci of largely unknown effect and position, it is especially important to develop general yet sensitive methods for gene mapping. We discuss the use of Shannons information theory for population-based gene mapping of discrete and quantitative traits and for marker clustering. Various measures of mutual information were employed in order to develop a comprehensive framework for gene mapping analyses. An algorithm aimed at finding so-called relevance chains of causal markers is proposed. Moreover, entropy measures are used in conjunction with multidimensional scaling to visualize clusters of genetic markers. The relevance chain algorithm successfully detected the two causal regions in a simulated scenario. The approach has also been applied to a published clinical study on autoimmune (Graves) disease. Results were consistent with those of standard statistical methods, but identified an additional locus of interest in the promotor region of the associated gene CTLA4. The developed software is freely available at http://www.Int.ei.tum.de/download/InfoGeneMap/. © 2006 IEEE.",biology
897,Fast Structured Motif Search in DNA Sequences,Bioinformatics Research and Development," We study the problem of structured motif search in DNA sequences. This is a fundamental task in bioinformatics which contributes to better understanding of genome characteristics and properties. We propose an efficient algorithm for Exact Match, Overlapping Structured motif search (EMOS), which uses a suffix tree index we proposed earlier and runs on a typical desktop computer. We have conducted numerous experiments to evaluate EMOS and compared its performance with the best known solution, SMOTIF1 [1]. While in some cases the search time of EMOS is comparable to SMOTIF1, it is on average 5 to 6 times faster. ",biology
898,Detecting Protein Complexes Basedon Uncertain Graph Model,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Advanced biological technologies are producing large-scale protein-protein interaction (PPI) data at an ever increasing pace, which enable us to identify protein complexes from PPI networks. Pair-wise protein interactions can be modeled as a graph, where vertices represent proteins and edges represent PPIs. However most of current algorithms detect protein complexes based on deterministic graphs, whose edges are either present or absent. Neighboring information is neglected in these methods. Based on the uncertain graph model, we propose the concept of expected density to assess the density degree of a subgraph, the concept of relative degree to describe the relationship between a protein and a subgraph in a PPI network. We develop an algorithm called DCU (detecting complex based on uncertain graph model) to detect complexes from PPI networks. In our method, the expected density combined with the relative degree is used to determine whether a subgraph represents a complex with high cohesion and low coupling. We apply our method and the existing competing algorithms to two yeast PPI networks. Experimental results indicate that our method performs significantly better than the state-of-the-art methods and the proposed model can provide more insights for future study in PPI networks. ÂŠ 2004-2012 IEEE.",biology
899,A Naïve Bayes classifier for differential diagnosis of Long QT Syndrome in children,Bioinformatics and Biomedicine,"This study examined disease models most indicative of risk of Long QT Syndrome (LQTS) in children. Data mined from electronic health records of children confirmed with (n=248) and without (n=101) a diagnosis of LQTS were used to develop a patient profile for LQTS. The profile consisted of 44 distinct features, 17 of which were enriched in LQTS patients. Notably, 66.9% of subjects with a diagnosis of LQTS fell into a category of “low” (22.6%) or “intermediate” (44.3%) risk using a current LQTS risk assessment standard. We developed and trained a machine learning process for LQTS classification by applying a Naïve Bayes model to our LQTS cohort. The model classified patients with a sensitivity of 91.1% and a specificity of 73.3%. These results suggest that data mining of clinical data in conjunction with a Bayesian modeling approach can lead to a diagnostic system for prediction of LQTS in children.",biology
900,The Latent Process Decomposition of cDNA Microarray Data Sets,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -We present a new computational technique (a software implementation, data sets, and supplementary information are available at http://www.enm.bris.ac.uk/lpd/) which enables the probabilistic analysis of cDNA microarray data and we demonstrate its effectiveness in identifying features of biomedical importance. A hierarchical Bayesian model, called Latent Process Decomposition (LPD), is introduced in which each sample in the data set is represented as a combinatorial mixture over a finite set of latent processes, which are expected to correspond to biological processes. Parameters in the model are estimated using efficient variational methods. This type of probabilistic model is most appropriate for the interpretation of measurement data generated by cDNA microarray technology. For determining informative substructure in such data sets, the proposed model has several important advantages over the standard use of dendrograms. First, the ability to objectively assess the optimal number of sample clusters. Second, the ability to represent samples and gene expression levels using a common set of latent variables (dendrograms cluster samples and gene expression values separately which amounts to two distinct reduced space representations). Third, in constrast to standard cluster models, observations are not assigned to a single cluster and, thus, for example, gene expression levels are modeled via combinations of the latent processes identified by the algorithm. We show this new method compares favorably with alternative cluster analysis methods. To illustrate its potential, we apply the proposed technique to several microarray data sets for cancer. For these data sets it successfully decomposes the data into known subtypes and indicates possible further taxonomic subdivision in addition to highlighting, in a wholly unsupervised manner, the importance of certain genes which are known to be medically significant. To illustrate its wider applicability, we also illustrate its performance on a microarray data set for yeast. ",biology
901,Selecting informative genes by Lasso and Dantzig selector for linear classifiers,Bioinformatics and Biomedicine,"Automatically selecting a subset of genes with strong discriminative power is a very important step in classification problems based on gene expression data. Lasso and Dantzig selector are known to have automatic variable selection ability in linear regression analysis. This paper employs Lasso and Dantzig selector to select most informative genes for representing the class label as a linear function of gene expression data. The selected genes are further used to fit linear classifiers for cancer classification. On 3 publicly available cancer datasets, the experimental results show that in general, Lasso is more capable than Dantzig selector in selecting informative genes for classification.",biology
902,A Blind Deconvolution Method of Single-Channel the EEG Convolution Mixed Signals Based on the Linear Predication Error Analysis,International Conference on Bioinformatics and Biomedical Engineering,"EEG signal has the chaotic character. Based on the chaotic characteristics and the linear predication error analysis, a deconvolution filter for single-input and single-output chaotic convolution mixed signal is discussed. Then corrected estimated error from the output data of this filter by the singular spectrum analysis. So the blind deconvolution of the source signals and the transmission function in the single-channel chaotic convolution system can be realized. This method takes advantage of the chaotic physical features and the compensatory technique that is based on the phase space reconstruction chaos dynamic equation. And EEG signal simulation result verified the effectiveness of this proposed method.",biology
903,Information flow in intensive care narratives,Bioinformatics and Biomedicine,"Fluent patient information flow is a prerequisite for clinical decision making. Our purpose is to identify unmet information needs in the flow of Finnish intensive care narratives in order to focus the development of natural language processing methods for this domain. Our data set consists of 516 authentic electronic patient records. First, we assess statistically the amount of narratives. We find that the amount is substantial: elective admission type and high nursing intensity contribute this. Second, we perform a content analysis. We observe that notes relevant for a given topic are scattered over the narratives, headings are inconsistent, and the flow from earlier narratives is fragmented. Consequently, support for gaining topical overviews is needed. Meeting this clinical need holds the promise of making narratives better accessible throughout a patient's stay and thereby improving clinical decision making and outcomes of care.",biology
904,Reciprocal Rank-Based Comparison of Ordered Gene Lists,International Conference on Bioinformatics,"There is growing interest in using rank-ordered gene lists to avoid excessive dependence on measured gene expression levels, which can vary strongly across experiments, platforms, or analysis methods. As a useful tool for working with these lists, this paper describes two extensions of an ordered list comparison measure, recently proposed for comparing Internet search engines: the use of random permutations to assess the significance of differences between ordered lists, and a graphical extension that highlights the items responsible for the main differences between two lists. The method is illustrated for a prostate cancer example from the genomics literature.",biology
905,Selection of negative examples in learning gene regulatory networks,Bioinformatics and Biomedicine,"Supervised learning methods have been recently exploited to learn gene regulatory networks from gene expression data. They consist of building a binary classifier from feature vectors composed by expression levels of a set of known regulatory connections, available in public databases (eg. RegulonDB, TRRD, Transfac, IPA), and using such a classifier to predict new unknown connections. The input to a binary supervised classifier consists normally of positive and negative examples, but usually the only available information are a partial set of gene regulations, i.e. positive examples, and unlabeled data which could include both positive and negative examples. A fundamental challenge is the choice of negative examples from such unlabeled data to make the classifier able to learn from data. We exploit the known topology of a gene network to select such negative examples and show whether such an assumption benefits the performance of a classifier.",biology
906,Exploiting Fast Classification of SNOMED CT for Query and Integration of Health Data,International Conference on Bioinformatics," By constructing local extensions to SNOMED we aim to enrich existing medical and related data stores, simplify the expression of complex queries, and establish a foundation for semantic integration of data from multiple sources. Specifically, a local extension can be constructed from the controlled vocabulary(ies) used in the medical data. In combination with SNOMED, this local extension makes explicit the implicit semantics of the terms in the controlled vocabulary. By using SNOMED as a base ontology we can exploit the existing knowledge encoded in it and simplify the task of reifying the implicit semantics of the controlled vocabulary. Queries can now be formulated using the relationships encoded in the extended SNOMED rather than embedding them ad-hoc into the query itself. Additionally, SNOMED can then act as a common point of integration, providing a shared set of concepts for querying across multiple data sets. Key to practical construction of a local extension to SNOMED is appropriate tool support including the ability to compute subsumption relationships very quickly. Our implementation of the polynomial algorithm for E L+ in Java is able to classify SNOMED in under 1 minute. ",biology
907,Figure Classification in Biomedical Literature towards Figure Mining,Bioinformatics and Biomedicine,"Biomedical papers contain large amounts of figures. Since they provide important information about research outcomes, mining techniques targeting them have attracted a great deal of attention. Our final goal is to develop a figure finding system, FigFinder, to retrieve figures relevant to a userpsilas query by mining information contained in figures, their legends, and the main text in an integrative manner. In this study, we worked on figure classification to choose those representing signaling or metabolic pathways, based on textual information contained in biomedical papers, as the first step to develop FigFinder. We took several supervised machine learning methods, and could confirm that the use of main text combined with figure legends was quite effective. Although many groups have considered figure legends, this is the first attempt to address figure classification task by utilizing figure legends together with main text to our knowledge.",biology
908,Axioms for parthood and containment relations in bio-ontologies,International Conference on Bioinformatics," To fix the semantics of different kinds of parthood relations we require axioms which go beyond those characterizing partial orderings. I formulate such axioms and show their implications for bio-ontologies. Specifically, I discuss parthood relations among masses, for example among body substances such as blood and portions thereof, and among components of complexes, for example between your stomach and your gastro-intestinal system. I contrast these with the relation of being contained in (as your lungs are contained in your thorax). The axioms considered are rooted in mereology, the formal theory of parts and wholes. By making explicit the differences between the different kinds of relations they support different kinds of data integration in bioinformatics. ",biology
909,A Metric for Phylogenetic Trees Based on Matching,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Comparing two or more phylogenetic trees is a fundamental task in computational biology. The simplest outcome of such a comparison is a pairwise measure of similarity, dissimilarity, or distance. A large number of such measures have been proposed, but so far all suffer from problems varying from computational cost to lack of robustness; many can be shown to behave unexpectedly under certain plausible inputs. For instance, the widely used Robinson-Foulds distance is poorly distributed and thus affords little discrimination, while also lacking robustness in the face of very small changesreattaching a single leaf elsewhere in a tree of any size can instantly maximize the distance. In this paper, we introduce a new pairwise distance measure, based on matching, for phylogenetic trees. We prove that our measure induces a metric on the space of trees, show how to compute it in low polynomial time, verify through statistical testing that it is robust, and finally note that it does not exhibit unexpected behavior under the same inputs that cause problems with other measures. We also illustrate its usefulness in clustering trees, demonstrating significant improvements in the quality of hierarchical clustering as compared to the same collections of trees clustered using the Robinson-Foulds distance. ÂŠ 2012 IEEE.",biology
910,A novel reinforcement learning framework for online adaptive seizure prediction,Bioinformatics and Biomedicine,"Epileptic seizure prediction is still a very challenging and unsolved problem for medical professionals. The current bottleneck of seizure prediction techniques is the lack of flexibility for different patients with an incredible variety of epileptic seizures. This study proposes a novel self-adaptation mechanism which successfully combines reinforcement learning, online monitoring and adaptive control theory for seizure prediction. The proposed method eliminates a sophisticated threshold-tuning/optimization process, and has a great potential of flexibility and adaptability to a wide range of patients with various types of seizures. The proposed prediction system was tested on five patients with epilepsy. With the best parameter settings, it achieved an averaged accuracy of 71.34%, which is considerably better than a chance model. The autonomous adaptation property of the system offers a promising path towards development of practical online seizure prediction techniques for physicians and patients.",biology
911,Probabilistic Mixture Regression Models for Alignment of LC-MS Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -A novel framework of a probabilistic mixture regression model (PMRM) is presented for alignment of liquid chromatography-mass spectrometry (LC-MS) data with respect to retention time (RT) points. The expectation maximization algorithm is used to estimate the joint parameters of spline-based mixture regression models and prior transformation density models. The latter accounts for the variability in RT points and peak intensities. The applicability of PMRM for alignment of LC-MS data is demonstrated through three data sets. The performance of PMRM is compared with other alignment approaches including dynamic time warping, correlation optimized warping, and continuous profile model in terms of coefficient variation of replicate LC-MS runs and accuracy in detecting differentially abundant peptides/proteins. ",biology
912,CTGR-Span: Efficient mining of cross-timepoint gene regulation sequential patterns from microarray datasets,Bioinformatics and Biomedicine,"Sequential pattern mining techniques have been widely used in different topics of interest, such as mining customer purchasing sequences from a transactional database. Notably, observation of gene expressions to discover gene regulations during biological or clinical progression via microarray approaches has become the dominant trend. By converting microarray datasets into the format of transactional databases, sequential patterns implying gene regulations could be identified. However, there exists no effective method in current studies that can handle such kind of dataset as every transaction may contain too many items/genes and the resultant patterns are very susceptible to item order. We propose a new method called CTGR-Span (Cross-Timepoint Gene Regulation Sequential Patterns) to efficiently mine CTGR-SPs (cross-timepoint gene regulation sequential patterns). The proposed method was experimented with two publicly available human time course microarray datasets and it outperformed traditional methods over 2,000 times in terms of the execution efficiency. Furthermore, via a Gene Ontology enrichment analysis, the resultant patterns are more meaningful biologically compared to previous literature reports. Hence, it could provide biologists more insights into the mechanisms of novel gene regulations in certain disease progressions.",biology
913,DNA copy number selection using robust structured sparsity-inducing norms,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -Array comparative genomic hybridization (aCGH) is a newly introduced method for the detection of copy number abnormalities associated with human diseases with special focus on cancer. Specific patterns in DNA copy number variations (CNVs) can be associated with certain disease types and can facilitate prognosis and progress monitoring of the disease. Machine learning techniques have been used to model the problem of tissue typing as a classification problem. Feature selection is an important part of the classification process, because many biological features are not related to the diseases and confuse the classification tasks. Multiple feature selection methods have been proposed in the different domains where classification has been applied. In this work, we will present a new feature selection method based on structured sparsity-inducing norms to identify the informative aCGH biomarkers which can help us classify different disease subtypes. To validate the performance of the proposed method, we experimentally compare it with existing feature selection methods on four publicly available aCGH datasets. In all empirical results, the proposed sparse learning based feature selection method consistently outperforms other related approaches. More important, we carefully investigate the aCGH biomarkers selected by our method, and the biological evidences in literature strongly support our results. ",biology
914,Feature Selection for Gene Expression Using Model-Based Entropy,IEEE/ACM Transactions on Computational Biology and Bioinformatics," Gene expression data usually contain a large number of genes, but a small number of samples. Feature selection for gene expression data aims at finding a set of genes that best discriminate biological samples of different types. Using machine learning techniques, traditional gene selection based on empirical mutual information suffers the data sparseness issue due to the small number of samples. To overcome the sparseness issue, we propose a model-based approach to estimate the entropy of class variables on the model, instead of on the data themselves. Here, we use multivariate normal distributions to fit the data, because multivariate normal distributions have maximum entropy among all real-valued distributions with specified mean and standard deviation, and are widely used to approximate various distributions. Given that the data follow a multivariate normal distribution, since the conditional distribution of class variables given the selected features is normal distribution, its entropy can be computed with the log-determinant of its covariance matrix. Because of the large number of genes, the computation of all possible log-determinants is not efficient. We propose several algorithms to largely reduce the computational cost. The experiments on seven gene datasets and the comparison with other five approaches show the accuracy of the multivariate Gaussian generative model for feature selection, and the efficiency of our algorithms. ",biology
915,Learning Gaussian graphical models of gene networks with false discovery rate control,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","In many cases what matters is not whether a false discovery is made or not but the expected proportion of false discoveries among all the discoveries made, i.e. the so-called false discovery rate (FDR). We present an algorithm aiming at controlling the FDR of edges when learning Gaussian graphical models (GGMs). The algorithm is particularly suitable when dealing with more nodes than samples, e.g. when learning GGMs of gene networks from gene expression data. We illustrate this on the Rosetta compendium [8]. © 2008 Springer-Verlag Berlin Heidelberg.",biology
916,Modeling Protein Interaction Networks with Answer Set Programming,Bioinformatics and Biomedicine,"In this paper we propose the use of answer set programming (ASP) to model protein interaction networks. We argue that this declarative formalism rivals the popular boolean networks in terms of ease of use, while at the same time being more expressive. As we demonstrate for the particular case of a fission yeast network, all information present in a boolean network, as well as relevant background assumptions,can be expressed explicitly in an answer set program. Moreover, readily available answer set solvers can then be used to find the stable states of the network.",biology
917,A clinical outcome evaluation model with local sample selection: A study on efficacy of acupuncture for cervical spondylosis,Bioinformatics and Biomedicine,"Local learning is a special learning framework that considers training samples located in a small region concentric of the query sample. In many applications the concept label of query sample can be evaluated effectively only by similar training samples, such as the famous K-nearest neighbors (KNN) classifier. The metric of locality or similarity is essential in local learning, which is often application oriented and implied in local geometry of input space. In this paper, we propose to apply local learning to the task of outcome assessment and evaluation on acupuncture for cervical spondylosis (CS) in a multi-center clinical trial. The analytic data are measures of three questionnaires which are recognized tools for subjective patient-reported outcomes (PROs) evaluation. We propose a similarity evaluation method based on both Euclidean distance and the therapy effect of recent records. A Non-Dominated Sort (NDS) based methods is applied to obtain a ranking of therapy effect. A WEKA implementation decision tree classifier is applied as the main learner in our work, with comparison to two base line methods. The result shows that the proposed local learning method dramatically outperforms the global version in both classification accuracy and computational costs.",biology
918,M-Finder: Functional association mining from protein interaction networks weighted by semantic similarity,Bioinformatics and Biomedicine,"Protein-protein interactions (PPIs) play a key role in understanding functional behavior of genes. Discovering association patterns from PPI networks is crucial for functional characterization on a system level. We present a novel approach to discover the functional association pattern of a query gene from the genome-wide PPI networks. This approach consists of two major components. First, we transform the PPI network to a weighted graph representation by measuring semantic similarity. Three enhanced semantic similarity methods are proposed to estimate functional closeness of each interacting pair. Second, we apply a dynamic propagation algorithm to detect the functional association pattern of a gene, represented as a sub-network. The size of the sub-networks is flexibly determined by user-specific parameters. In this paper, we also introduce an interactive web application, called M-Finder, to visualize the functional association pattern of a gene entered by a user. The semantic similarity measures and the dynamic propagation algorithm are embedded in this tool to run on up-to-date PPI networks of model species. M-Finder allows users to carry out further systematic analysis for functional characterization on the genomic scale.",biology
919,Protein Sequence Classification Using Feature Hashing,Bioinformatics and Biomedicine,"Recent advances in next-generation sequencing technologies have resulted in an exponential increase in protein sequence data. The k-gram representation, used for protein sequence classification, usually results in prohibitively high dimensional input spaces, for large values of k. Applying data mining algorithms to these input spaces may be intractable due to the large number of dimensions. Hence, using dimensionality reduction techniques can be crucial for the performance and the complexity of the learning algorithms. We study the applicability of feature hashing to protein sequence classification, where the original high-dimensional space is ""reduced"" by mapping features to hash keys, such that multiple features can be mapped (at random) to the same key, and ""aggregating"" their counts. We compare feature hashing with the ""bag of k-grams"" and feature selection approaches. Our results show that feature hashing is an effective approach to reducing dimensionality on protein sequence classification tasks.",biology
920,Deep graph search based disease related knowledge summarization from biomedical literature,Bioinformatics and Biomedicine,"In this paper, we present an approach to automatically construct disease related knowledge summarization from biomedical literature. In this approach, first Kullback-Leibler divergence combined with mutual information metric is used to extract disease salient information. Then deep search based on depth first search (DFS) is applied to find hidden relations between biomedical entities. Finally random walk algorithm is exploited to filter out the weak relations. The experimental results show that our approach achieves a precision of 60% and a recall of 61% on salient information extraction, and outperforms the method of Combo. In addition, the method of deep search obtains more hidden relations than the original correlation extraction methods.",biology
921,Inference of Biological S-System Using the Separable Estimation Method and the Genetic Algorithm,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Reconstruction of a biological system from its experimental time series data is a challenging task in systems biology. The S-system which consists of a group of nonlinear ordinary differential equations (ODEs) is an effective model to characterize molecular biological systems and analyze the system dynamics. However, inference of S-systems without the knowledge of system structure is not a trivial task due to its nonlinearity and complexity. In this paper, a pruning separable parameter estimation algorithm (PSPEA) is proposed for inferring S-systems. This novel algorithm combines the separable parameter estimation method (SPEM) and a pruning strategy, which includes adding an ℓ1 regularization term to the objective function and pruning the solution with a threshold value. Then, this algorithm is combined with the continuous genetic algorithm (CGA) to form a hybrid algorithm that owns the properties of these two combined algorithms. The performance of the pruning strategy in the proposed algorithm is evaluated from two aspects: the parameter estimation error and structure identification accuracy. The results show that the proposed algorithm with the pruning strategy has much lower estimation error and much higher identification accuracy than the existing method. © 2012 IEEE.",biology
922,Effect of commuting on the detection and characterization performance of the Bayesian Aerosol Release Detector,International Conference on Bioinformatics,"The Bayesian Aerosol Release Detector (BARD) is a system designed to detect and characterize disease outbreaks caused by aerosol releases of B. anthracis. The detection algorithm of BARD requires, among other things, an accurate estimation of the number of spores that would be inhaled under a specific release scenario. This is a challenging problem, in part due to the lack of fine-grained data on the mobility patterns of the exposed population. Indeed, the only type of spatial information routinely contained in biosurveillance databases is the residential administrative unit-such as the home zip code-of each case. The current version of BARD detector deals with this challenge by making the simplifying assumption that exposure to anthrax would occur at onepsilas residential unit. This paper presents an experimental study to assess how BARDpsilas performance would be impacted by incorporation of a commuting model in outbreak simulation. Our results show that incorporation of commuting in simulation leads to statistically and practically significant changes in BARDpsilas detection and characterization performance.",biology
923,Speech recognition in a dialog system for patient health monitoring,Bioinformatics and Biomedicine,"We describe CARDIAC, a prototype for an intelligent conversational assistant that provides health monitoring for chronic heart failure patients. CARDIAC supports user initiative through its ability to understand natural language and connect it to intention recognition. The spoken language interface allows patients to interact with CARDIAC without special training. We present speech recognition results obtained during an evaluation with fourteen chronic heart failure patients.",biology
924,Partitioning biological networks into highly connected clusters with maximum edge coverage,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"A popular clustering algorithm for biological networks which was proposed by Hartuv and Shamir identifies nonoverlapping highly connected components. We extend the approach taken by this algorithm by introducing the combinatorial optimization problem Highly Connected Deletion, which asks for removing as few edges as possible from a graph such that the resulting graph consists of highly connected components. We show that Highly Connected Deletion is NP-hard and provide a fixed-parameter algorithm and a kernelization. We propose exact and heuristic solution strategies, based on polynomial-time data reduction rules and integer linear programming with column generation. The data reduction typically identifies 75 percent of the edges that are deleted for an optimal solution; the column generation method can then optimally solve protein interaction networks with up to 6,000 vertices and 13,500 edges within five hours. Additionally, we present a new heuristic that finds more clusters than the method by Hartuv and Shamir. © 2004-2012 IEEE.",biology
925,Domain content based protein function prediction using incomplete GO annotation information,Bioinformatics and Biomedicine,"Given the essential role of protein in life processes, computational assignment of protein functions has become one of the most important tasks in the area of bioinformatics. While Gene Ontology (GO) has been widely used in functional annotation, new approaches to address the problem of annotation incompleteness, which can leverage the support of the GO framework, are imminently required. In this paper, two new models are proposed to predict GO terms from domain content: a Correlation Coefficient based model (CC-M) and a Support Vector Machine (SVM) based model (SVM-M). We have developed our models in the form of predictors for all GO terms with manually curated annotations. In comparison with the Bayesian probabilistic approach published previously [Forslund et al., 2008], our methods are demonstrated to have better capability in dealing with incomplete training data. In particular, the CC-M method is suitable for GO terms with extremely low occurrence frequency, and the SVM-M method for the remaining GO terms. Therefore, CC-M and SVM-M are subsequently integrated into a single model (CC-SVM), with their respective advantages combined.",biology
926,Matching Split Distance for Unrooted Binary Phylogenetic Trees,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The reconstruction of evolutionary trees is one of the primary objectives in phylogenetics. Such a tree represents the historical evolutionary relationship between different species or organisms. Tree comparisons are used for multiple purposes, from unveiling the history of species to deciphering evolutionary associations among organisms and geographical areas. In this paper, we propose a new method of defining distances between unrooted binary phylogenetic trees that is especially useful for relatively large phylogenetic trees. Next, we investigate in detail the properties of one example of these metrics, called the Matching Split distance, and describe how the general method can be extended to nonbinary trees. © 2012 IEEE.",biology
927,A Semi-supervised Learning Approach to Disease Gene Prediction,Bioinformatics and Biomedicine," Discovering human disease-causing genes (disease genes in short) is one of the most challenging problems in bioinformatics and biomedicine, as most diseases are related in some way to our genes. Various methods have been proposed to exploit existing data sources for solving the problem. We aim to develop a novel method to predict disease genes that takes into account the imbalance between known disease genes and unknown disease genes. To this end, our method makes the best of semi-supervised learning, integrating data of human protein-protein interactions and various biological data extracted from multiple proteomic/genomic databases. Experimental evaluation shows high performance of our proposed method. Also, a considerable number of potential disease genes were discovered. Supplementary materials are now available from http://www.jaist.ac.jp/∼s0560205/DiseaseGenes/. ",biology
928,Gene Selection and Tissue Classification Based on Support Vector Machine and Genetic Algorithm,International Conference on Bioinformatics and Biomedical Engineering,"The main problems in tissue classification by using DNA Microarray data are selecting genes relevant for a given tumor and constructing the optimized classifiers. This paper proposes a new gene selection and tissue classification method based on support vector machines and genetic algorithm. Firstly, the Wilcoxon-test is used as a coarse gene selection method to remove most of the irrelevant genes. Then the fine selection on the basis of its classification ability of a single gene with support vector machine is conducted to get the final gene subset. Finally, genetic algorithm is used to optimize the parameters of support vector machine to find out the best parameters with the gene subset. The experimental results with the Leukemia and breast cancer dataset show that the proposed strategy is more effective and competitive to the previous methods.",biology
929,Shorelines of Islands of Tractability: Algorithms for Parsimony and Minimum Perfect Phylogeny Haplotyping Problems,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The problem Parsimony Haplotyping (PH) asks for the smallest set of haplotypes which can explain a given set of genotypes, and the problem Minimum Perfect Phylogeny Haplotyping (MPPH) asks for the smallest such set which also allows the haplotypes to be embedded in a perfect phylogeny, an evolutionary tree with biologically-motivated restrictions. For PH, we extend recent work by further mapping the interface between easy and hard instances, within the framework of (k,l)-bounded instances where the number of 2s per column and row of the input matrix is restricted. By exploring, in the same way, the tractability frontier of MPPH we provide the first concrete, positive results for this problem. In addition, we construct for both PH and MPPH polynomial time approximation algorithms, based on properties of the columns of the input matrix. © 2006 IEEE.",biology
930,An Intelligent Two-Stage Evolutionary Algorithm for Dynamic Pathway Identification From Gene Expression Profiles,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -From gene expression profiles, it is desirable to rebuild cellular dynamic regulation networks to discover more delicate and substantial functions in molecular biology, biochemistry, bioengineering, and pharmaceutics. The S-system model is suitable to characterize biochemical network systems and capable of analyzing the regulatory system dynamics. However, the inference of an S-system model of N-gene genetic networks has 2NðN þ 1Þ parameters in a set of nonlinear differential equations to be optimized. This paper proposes an intelligent two-stage evolutionary algorithm (iTEA) to efficiently infer the S-system models of genetic networks from time-series data of gene expression. To cope with the curse of dimensionality, the proposed algorithm consists of two stages, where each uses a divide-and-conquer strategy. The optimization problem is first decomposed into N subproblems having 2ðN þ 1Þ parameters each. At the first stage, each subproblem is solved using a novel intelligent genetic algorithm (IGA) with intelligent crossover based on an orthogonal experimental design (OED). At the second stage, the obtained N solutions to the N subproblems are combined and refined using an OED-based simulated annealing algorithm for handling noisy gene expression profiles. The effectiveness of iTEA is evaluated using simulated expression patterns with and without noise running on a single-processor PC. It is shown that 1) IGA is efficient enough to solve subproblems, 2) IGA is significantly superior to the existing method GA with simplex crossover (SPXGA), and 3) iTEA performs well in inferring S-system models for dynamic pathway identification. ",biology
931,A hybrid random subspace classifier fusion approach for protein mass spectra classification,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","Classifier fusion strategies have shown great potential to enhance the performance of pattern recognition systems. There is an agreement among researchers in classifier combination that the major factor for producing better accuracy is the diversity in the classifier team. Re-sampling based approaches like bagging, boosting and random subspace generate multiple models by training a single learning algorithm on multiple random replicates or sub-samples, in either feature space or the sample domain. In the present study we proposed a hybrid random subspace fusion scheme that simultaneously utilizes both the feature space and the sample domain to improve the diversity of the classifier ensemble. Experimental results using two protein mass spectra datasets of ovarian cancer demonstrate the usefulness of this approach for six learning algorithms (LDA, 1-NN, Decision Tree, Logistic Regression, Linear SVMs and MLP). The results also show that the proposed strategy outperforms three conventional re-sampling based ensemble algorithms on these datasets. ÂŠ 2008 Springer-Verlag Berlin Heidelberg.",biology
932,Modular neural network model based foetal state classification,Bioinformatics and Biomedicine,"Cardiotocography (CTG) is a simultaneous recording of foetal heart rate (FHR) and uterine contractions (UC) and it is one of the most common diagnostic techniques to evaluate maternal and foetal well-being during pregnancy and before delivery. Assessment of the foetal state can be verified only after delivery using the foetal (newborn) outcome data. One of the most important features defining the abnormal foetal outcome is low birth weight. This paper proposes a multi-class classification algorithm using Modular neural network (MNN) models. It tries to boost two conflicting main objectives of multi-class classifiers: a high correct classification rate level and a high classification rate for each class. Using a Cardiotocography database of normal, suspect and pathological cases, we trained MNN classifiers with 23 real valued diagnostic features collected from total 2126 foetal CTG signal recordings data from UCI Machine Learning Repository. We used the classification in a detection process. The proposed methodology is presented, which then is tested on UCI Cardiotocography unseen testing data sets. Experimental results are promising paving the way for further research in that direction.",biology
933,SPR distance computation for unrooted trees.,Evolutionary Bioinformatics," The subtree prune and regraft distance (dSPR) between phylogenetic trees is important both as a general means of comparing phylogenetic tree topologies as well as a measure of lateral gene transfer (LGT). Although there has been extensive study on the computation of dSPR and similar metrics between rooted trees, much less is known about SPR distances for unrooted trees, which often arise in practice when the root is unresolved. We show that unrooted SPR distance computation is NP-Hard and verify which techniques from related work can and cannot be applied. We then present an efficient heuristic algorithm for this problem and benchmark it on a variety of synthetic datasets. Our algorithm computes the exact SPR distance between unrooted tree, and the heuristic element is only with respect to the algorithm's computation time. Our method is a heuristic version of a fixed parameter tractability (FPT) approach and our experiments indicate that the running time behaves similar to FPT algorithms. For real data sets, our algorithm was able to quickly compute dSPR for the majority of trees that were part of a study of LGT in 144 prokaryotic genomes. Our analysis of its performance, especially with respect to searching and reduction rules, is applicable to computing many related distance measures. ",biology
934,ECG Signal Processing Using Dyadic Wavelet for Mental Stress Assessment,International Conference on Bioinformatics and Biomedical Engineering,"This paper presents the evaluation of mental stress assessment using heart rate variability. The heart rate signals are processed first using Fourier transform, then it is applied to wavelet transform. The activity of the autonomic nervous system is noninvasively studied by means of autoregressive (AR) frequency analysis of the heart-rate variability (HRV) signal. Spectral decomposition of the Heart Rate Variability during whole night recordings was obtained, in order to assess the characteristic fluctuations in the heart rate. This paper presents a novel method of HRV analysis for mental stress assessment using fuzzy clustering and robust identification techniques. The approach consists of 1)online monitoring of heart rate signals, 2) signal processing using the Dyadic wavelet.",biology
935,The Impact of Gene Selection on Imbalanced Microarray Expression Data,International Conference on Bioinformatics,"Microarray experiments usually output small volumes but high dimensional data. Selecting a number of genes relevant to the tasks at hand is usually one of the most important steps for the expression data analysis. While numerous researches have demonstrated the effectiveness of gene selection from different perspectives, existing endeavors, unfortunately, ignore the data imbalance reality, where one type of samples (e.g., cancer tissues) may be significantly fewer than the other (e.g., normal tissues). In this paper, we carry out a systematic study to investigate the impact of gene selection on imbalanced microarray data. Our objective is to understand that if gene selection is applied to imbalanced expression data, what kind of consequences it may bring to the final results? For this purpose, we apply five gene selection measures to eleven microarray datasets, and employ four learning methods to build classification models from the data containing selected genes only. Our study will bring important findings and draw numerous conclusions on (1) the impact of gene selection on imbalanced data, and (2) behaviors of different learning methods on the selected data.",biology
936,Registration in Practice: Comparing Free-Text and Compositional Terminological System Based Registration of ICU Reasons for Admission,International Conference on Bioinformatics," Reusability of patient data for clinical research or quality assessment relies on structured, coded data. Terminological systems (TS) are meant to support this. It is hardly known how compositional TS-based registration affects the correctness and specificity of information, as compared to free-text registration. In this observational study free-text reasons for admission (RfA) in intensive care were compared to RfAs that were composed using a compositional TS. Both RfAs were registered in the Patient Data Management System by clinicians during care practice. Analysis showed that only 11% of the concepts matched exactly, 79% of the concepts matched partially and 10% of the concepts did not match. TS-based registration results in more details for almost half of the partial matches and in less details for the other half. This study demonstrates that the quality of TS-based registration is influences by the terminological system's content, its interface, and the registration practice of the users. ",biology
937,A Framework for Semisupervised Feature Generation and Its Applications in Biomedical Literature Mining,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Feature representation is essential to machine learning and text mining. In this paper, we present a feature coupling generalization (FCG) framework for generating new features from unlabeled data. It selects two special types of features, i.e., example-distinguishing features (EDFs) and class-distinguishing features (CDFs) from original feature set, and then generalizes EDFs into higher-level features based on their coupling degrees with CDFs in unlabeled data. The advantage is: EDFs with extreme sparsity in labeled data can be enriched by their co-occurrences with CDFs in unlabeled data so that the performance of these low-frequency features can be greatly boosted and new information from unlabeled can be incorporated. We apply this approach to three tasks in biomedical literature mining: gene named entity recognition (NER), protein-protein interaction extraction (PPIE), and text classification (TC) for gene ontology (GO) annotation. New features are generated from over 20 GB unlabeled PubMed abstracts. The experimental results on BioCreative 2, AIMED corpus, and TREC 2005 Genomics Track show that 1) FCG can utilize well the sparse features ignored by supervised learning. 2) It improves the performance of supervised baselines by 7.8 percent, 5.0 percent, and 5.8 percent, respectively, in the tree tasks. 3) Our methods achieve 89.1, 64.5 F-score, and 60.1 normalized utility on the three benchmark data sets. © 2011 IEEE.",biology
938,Generating linkage disequilibrium patterns in data simulations using genomeSIMLA,"Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics","Whole-genome association (WGA) studies are becoming a common tool for the exploration of the genetic components of common disease. The analysis of such large scale data presents unique analytical challenges, including problems of multiple testing, correlated independent variables, and large multivariate model spaces. These issues have prompted the development of novel computational approaches. Thorough, extensive simulation studies are a necessity for methods development work to evaluate the power and validity of novel approaches. Many data simulation packages exist, however, the resulting data is often overly simplistic and does not compare to the complexity of real data; especially with respect to linkage disequilibrium (LD). To overcome this limitation, we have developed genomeSIMLA. GenomeSIMLA is a forward-time population simulation method that can simulate realistic patterns of LD in both family-based and case-control datasets. In this manuscript, we demonstrate how LD patterns of the simulated data change under different population growth curve parameter initialization settings. These results provide guidelines to simulate WGA datasets whose properties resemble the HapMap. ÂŠ 2008 Springer-Verlag Berlin Heidelberg.",biology
939,NWE: Node-weighted expansion for protein complex prediction using random walk distances,Bioinformatics and Biomedicine,"Background Protein complexes are important entities to organize various biological processes in the cell, like signal transduction, gene expression, and molecular transmission. In most cases, proteins perform their intrinsic tasks in association with their specific interacting partners, forming protein complexes. Therefore, an enriched catalog of protein complexes in a cell could accelerate further research to elucidate the mechanisms underlying many biological processes. However, known complexes are still limited. Thus, it is a challenging problem to computationally predict protein complexes from protein-protein interaction networks, and other genome-wide data sets. Methods Macropol et al. proposed a protein complex prediction algorithm, called RRW, which repeatedly expands a current cluster of proteins according to the stationary vector of a random walk with restarts with the cluster whose proteins are equally weighted. In the cluster expansion, all the proteins within the cluster have equal influences on determination of newly added protein to the cluster. In this paper, we extend the RRW algorithm by introducing a random walk with restarts with a cluster of proteins, each of which is weighted by the sum of the strengths of supporting evidence for the direct physical interactions involving the protein. The resulting algorithm is called NWE (Node-Weighted Expansion of clusters of proteins). Those interaction data are obtained from the WI-PHI database. Results We have validated the biological significance of the results using curated complexes in the CYC2008 database, and compared our method to RRW and MCL (Markov Clustering), a popular clustering-based method, and found that our algorithm outperforms the other algorithms. Conclusions It turned out that it is an effective approach in protein complex prediction to expand a cluster of proteins, each of which is weighted by the sum of the strengths of supporting evidence for the direct physical interactions involving the protein. Background Protein complexes are important entities to organize various biological processes in the cell, like signal transduction, gene expression, and molecular transmission. In most cases, proteins perform their intrinsic tasks in association with their specific interacting partners, forming protein complexes. Therefore, an enriched catalog of protein complexes in a cell could accelerate further research to elucidate the mechanisms underlying many biological processes. However, known complexes are still limited. Thus, it is a challenging problem to computationally predict protein complexes from protein-protein interaction networks, and other genome-wide data sets. Methods Macropol et al. proposed a protein complex prediction algorithm, called RRW, which repeatedly expands a current cluster of proteins according to the stationary vector of a random walk with restarts with the cluster whose proteins are equally weighted. In the cluster expansion, all the proteins within the cluster have equal influences on determination of newly added protein to the cluster. In this paper, we extend the RRW algorithm by introducing a random walk with restarts with a cluster of proteins, each of which is weighted by the sum of the strengths of supporting evidence for the direct physical interactions involving the protein. The resulting algorithm is called NWE (Node-Weighted Expansion of clusters of proteins). Those interaction data are obtained from the WI-PHI database. Results We have validated the biological significance of the results using curated complexes in the CYC2008 database, and compared our method to RRW and MCL (Markov Clustering), a popular clustering-based method, and found that our algorithm outperforms the other algorithms. Conclusions It turned out that it is an effective approach in protein complex prediction to expand a cluster of proteins, each of which is weighted by the sum of the strengths of supporting evidence for the direct physical interactions involving the protein.",biology
940,A Short Proof that Phylogenetic Tree Reconstruction by Maximum Likelihood Is Hard,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Maximum likelihood is one of the most widely used techniques to infer evolutionary histories. Although it is thought to be intractable, a proof of its hardness has been lacking. Here, we give a short proof that computing the maximum likelihood tree is NP-hard by exploiting a connection between likelihood and parsimony observed by Tuffley and Steel. ÂŠ 2006 IEEE.",biology
941,Combining Boundaries and Ratings from Multiple Observers for Predicting Lung Nodule Characteristics,"Biocomputation, Bioinformatics, and Biomedical Technologies","We use the data collected by the Lung Image Database Consortium (LIDC) for modeling the radiologists  nodule interpretations based on image content of the nodule by using decision trees. Up to 4 radiologists delineated nodule boundaries and provided ratings for nine nodule characteristics (lobulation, margin, sphericity, etc). Therefore, there can be up to 4 instances per nodule in our data set. However, to learn a good predictive model, the data set should have only one instance per nodule. In this study, we investigate several approaches to combine delineated boundaries and ratings from multiple observers. From our experimental results, we learned that the thresholded p-map analysis approach with the probability threshold Pr>=0.75 provides the best predictive accuracies for the nodule characteristics. In the long run, we expect that the predictive model will improve radiologists  efficiency and reduce interreader variability. ÂŠ 2008 IEEE.",biology
942,Text Mining of Protein Phosphorylation Information Using a Generalizable Rule-Based Approach,International Conference on Bioinformatics,"Literature-based annotation of protein phosphorylation is the focus of many biological databases, as phosphorylation is a global regulator of cellular activity. To speed up manual curation of phosphorylation information, text mining technology has been utilized. In this paper, we report our ongoing effort to enhance RLIMS-P, a rule-based information extraction (IE) system to identify protein phosphorylation information in scientific literature. Despite the high accuracy attained by RLIMS-P, the use of elaborated patterns and rules resulted in a substantial effort for system development and maintenance. To mitigate this challenge, we redesigned RLIMS-P and integrated new natural language processing (NLP) techniques. It has also been adapted to mine full-text articles and generalized to be able to exploit common features for different post-translational modifications (PTMs). The updated RLIMS-P (version 2.0) was evaluated on abstracts in the publicly available BioNLP GENIA event extraction (GE) corpus, and achieved F-scores of 0.92 and 0.96 for phosphorylation substrate and site, respectively. On a full-text corpus developed in-house, it achieved F-scores of 0.91 and 0.92 for substrate and site, and 0.88 for kinase. The system was applied to the PubMed Central (PMC) Open Access Subset, and promising results have been obtained in mining the full-text articles. RLIMS-P focuses on protein phosphorylation information, but its new design would be generalizable for other PTM types. Copyright © 2007 by the Association for Computing Machinery.",biology
943,Graph-Kernels for the Comparative Analysis of Protein Active Sites,German Conference on Bioinformatics," Graphs are often used to describe and analyze the geometry and physicochemical composition of biomolecular structures, such as chemical compounds and protein active sites. A key problem in graph-based structure analysis is to define a measure of similarity that enables a meaningful comparison of such structures. In this regard, so-called kernel functions have recently attracted a lot of attention, especially since they allow for the application of a rich repertoire of methods from the field of kernel-based machine learning. Most of the existing kernel functions on graph structures, however, have been designed for the case of unlabeled and/or unweighted graphs. Since proteins are often more naturally and more exactly represented in terms of node-labeled and edge-weighted graphs, we propose corresponding extensions of existing graph kernels. Moreover, we propose an instance of the substructure fingerprint kernel suitability for the analysis of protein binding sites. The performance of these kernels is investigated by means of an experimental study in which graph kernels are used as similarity measures in the context of classification. ",biology
944,Novel image features for categorizing biomedical images,Bioinformatics and Biomedicine,"Images embedded in biomedical publications are richly informative. For example, they often concisely summarize key hypotheses, illustrate new methods, and highlight major experimental findings in a research article. Prior studies [1] suggested that images embedded in biomedical publications offer effective clues for retrieving and mining their source documents. To facilitate accessing such valuable imagery resources, image categorization can be helpful. Like many other image processing tasks, extracting discriminative image features is critical for the success of image categorization. For biomedical images, we notice that many of them are embedded with abundant annotation text. Observing this property, we introduce a set of novel image features that exploit the spatial distribution of text information inside an image as essential clues for categorizing biomedical images. Through results of our evaluation experiments, this paper demonstrates the effectiveness of the proposed novel features - compared with conventional image features, our new features can help categorize biomedical images with superior performance using a standard supervised learning based approach.",biology
945,Deep learning for healthcare decision making with EMRs,Bioinformatics and Biomedicine,"Computer aid technology is widely applied in decision-making and outcome assessment of healthcare delivery, in which modeling knowledge and expert experience is technically important. However, the conventional rule-based models are incapable of capturing the underlying knowledge because they are incapable of simulating the complexity of human brains and highly rely on feature representation of problem domains. Thus we attempt to apply a deep model to overcome this weakness. The deep model can simulate the thinking procedure of human and combine feature representation and learning in a unified model. A modified version of convolutional deep belief networks is used as an effective training method for large-scale data sets. Then it is tested by two instances: a dataset on hypertension retrieved from a HIS system, and a dataset on Chinese medical diagnosis and treatment prescription from a manual converted electronic medical record (EMR) database. The experimental results indicate that the proposed deep model is able to reveal previously unknown concepts and performs much better than the conventional shallow models.",biology
946,Parallel Clustering Algorithm for Large Data Sets with Applications in Bioinformatics,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Large sets of bioinformatical data provide a challenge in time consumption while solving the cluster identification problem, and that is why a parallel algorithm is so needed for identifying dense clusters in a noisy background. Our algorithm works on a graph representation of the data set to be analyzed. It identifies clusters through the identification of densely intraconnected subgraphs. We have employed a minimum spanning tree (MST) representation of the graph and solve the cluster identification problem using this representation. The computational bottleneck of our algorithm is the construction of an MST of a graph, for which a parallel algorithm is employed. Our high-level strategy for the parallel MST construction algorithm is to first partition the graph, then construct MSTs for the partitioned subgraphs and auxiliary bipartite graphs based on the subgraphs, and finally merge these MSTs to derive an MST of the original graph. The computational results indicate that when running on 150 CPUs, our algorithm can solve a cluster identification problem on a data set with 1,000,000 data points almost 100 times faster than on single CPU, indicating that this program is capable of handling very large data clustering problems in an efficient manner. We have implemented the clustering algorithm as the software CLUMP. © 2006 IEEE.",biology
947,An efficient sequential pattern mining algorithm for motifs with gap constraints,Bioinformatics and Biomedicine,"Mining biological data can provide insight into various realms of biology, such as finding co-occurring biosequences, which is essential for biological analyses and data mining. Sequential pattern mining reveals all-length implicit motifs, which have specific structures and are of functional significance in biological sequences. Traditional sequential pattern mining algorithms are inefficient for small alphabets and long sequences, such as DNA and protein sequences; therefore, it is necessary to move away from these algorithms. An approach called the Depth-First Spelling algorithm for mining sequential patterns (motifs) with Gap constraints in biological sequences (referred to as DFSG) is proposed in this work. In biological sequences, DFSG runtime is substantially shorter than that of GenPrefixSpan, where GenPrefixSpan is a method based on PrefixSpan (PrefixSpan is one of the fastest algorithms in traditional sequential pattern mining algorithms).",biology
948,RANGI: A Fast List-Colored Graph Motif Finding Algorithm,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Given a multiset of colors as the query and a list-colored graph, i.e., an undirected graph with a set of colors assigned to each of its vertices, in the NP-hard list-colored graph motif problem the goal is to find the largest connected subgraph such that one can select a color from the set of colors assigned to each of its vertices to obtain a subset of the query. This problem was introduced to find functional motifs in biological networks. We present a branch-and-bound algorithm named RANGI for finding and enumerating list-colored graph motifs. As our experimental results show, RANGIs pruning methods and heuristics make it quite fast in practice compared to the algorithms presented in the literature. We also present a parallel version of RANGI that achieves acceptable scalability. ÂŠ 2004-2012 IEEE.",biology
949,Estimating Missing Value in Microarray Data Using Fuzzy Clustering and Gene Ontology,Bioinformatics and Biomedicine,"Microarray experiments usually generate data sets with multiple missing expression values, due to several problems. In this paper, a new and robust method based on fuzzy clustering and gene ontology is proposed to estimate missing values in microarray data. In the proposed method, missing values are imputed with values generated from cluster centers. To determine the similar genes in clustering process, we have utilized the biological knowledge obtained from gene ontology as well as gene expression values. We have applied the proposed method on yeast cell cycle data with different percentage of missing entries. We compared the estimation accuracy of our method with some other methods. The experimental results indicate that the proposed method outperforms other methods in terms of accuracy.",biology
950,Text Categorization of Biomedical Data Sets Using Graph Kernels and a Controlled Vocabulary,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Recently, graph representations of text have been showing improved performance over conventional bag-of-words representations in text categorization applications. In this paper, we present a graph-based representation for biomedical articles and use graph kernels to classify those articles into high-level categories. In our representation, common biomedical concepts and semantic relationships are identified with the help of an existing ontology and are used to build a rich graph structure that provides a consistent feature set and preserves additional semantic information that could improve a classifiers performance. We attempt to classify the graphs using both a set-based graph kernel that is capable of dealing with the disconnected nature of the graphs and a simple linear kernel. Finally, we report the results comparing the classification performance of the kernel classifiers to common text-based classifiers. ÂŠ 2004-2012 IEEE.",biology
951,Solving the Problem of Trans-Genomic Query with Alignment Tables,IEEE/ACM Transactions on Computational Biology and Bioinformatics," - The trans-genomic query (TGQ) problem - enabling the free query of biological information, even across genomes - is a central challenge facing bioinformatics. Solutions to this problem can alter the nature of the field, moving it beyond the jungle of data integration and expanding the number and scope of questions that can be answered. ",biology
952,Alignments of RNA Structures,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"We describe a theoretical unifying framework to express the comparison of RNA structures, which we call alignment hierarchy. This framework relies on the definition of common supersequences for arc-annotated sequences and encompasses the main existing models for RNA structure comparison based on trees and arc-annotated sequences with a variety of edit operations. It also gives rise to edit models that have not been studied yet. We provide a thorough analysis of the alignment hierarchy, including a new polynomial-time algorithm and an NP-completeness proof. The polynomial-time algorithm involves biologically relevant edit operations such as pairing or unpairing nucleotides. It has been implemented in a software, called gardenia, which is available at the Web server http://bioinfo.lifl.fr/RNA/gardenia. © 2006 IEEE.",biology
953,KiWi: A Scalable Subspace Clustering Algorithm for Gene Expression Analysis,International Conference on Bioinformatics and Biomedical Engineering,"Subspace clustering has gained increasing popularity in the analysis of gene expression data. Among subspace cluster models, the recently introduced order-preserving sub-matrix (OPSM) has demonstrated high promise. An OPSM, essentially a pattern-based subspace cluster, is a subset of rows and columns in a data matrix for which all the rows induce the same linear ordering of columns. Existing OPSM discovery methods do not scale well to increasingly large expression datasets. In particular, twig clusters having few genes and many experiments incur explosive computational costs and are completely pruned off by existing methods. However, it is of particular interest to determine small groups of genes that are tightly coregulated across many conditions. In this paper, we present KiWi, an OPSM subspace clustering algorithm that is scalable to massive datasets, capable of discovering twig clusters and identifying negative as well as positive correlations. We extensively validate KiWi using relevant biological datasets and show that KiWi correctly assigns redundant probes to the same cluster, groups experiments with common clinical annotations, differentiates real promoter sequences from negative control sequences, and shows good association with cis-regulatory motif predictions.",biology
954,Network-based pathway enrichment analysis,Bioinformatics and Biomedicine,"Finding out the associations between an input gene set, such as genes associated with a certain phenotype, and annotated gene sets, such as known pathways, are a very important problem in modern molecular biology. The existing approaches mainly focus on the overlap between the two, and may miss important but subtle relationships between genes. In this paper, we propose a method, NetPEA, by combining the known pathways and high-throughput networks. Our method not only considers the shared genes, but also takes the gene interactions into account. It utilizes a protein-protein interaction network and a random walk procedure to identify hidden relationships between gene sets, and uses a randomization strategy to evaluate the significance for pathways to achieve such similarity scores. Compared with the over-representation based method, our method can identify more relationships. Compared with a state of the art network-based method, EnrichNet, our method not only provides a ranked list of pathways, but also provides the statistical significant information. Importantly, through independent tests, we show that our method likely has a higher sensitivity in revealing the true casual pathways, while at the same time achieve a higher specificity. Literature review of selected results indicates that some of the novel pathways reported by our method are biologically relevant and important.",biology
955,Parallel MRI Acceleration Using M-FOCUSS,International Conference on Bioinformatics and Biomedical Engineering,"Parallel magnetic resonance imaging (pMRI) cannot achieve its maximum reduction factor due to practical limitations. The combination of pMRI and distributed compressed sensing (DCS) for further acceleration is of great interest. In this paper, we propose a method to combine sensitivity encoding (SENSE), one of the standard methods for pMRI, and M-FOCUSS, an algorithm solving DCS reconstruction problem. The proposed method first employs M-FOCUSS algorithm to simultaneously reconstruct a set of aliased reduced field-of-view (FOV) images for each channel, and then applies cartesian SENSE to reconstruct the final image. The experimental results demonstrate that the proposed method outperforms the existing methods with the same reduction factor.",biology
956,Biological Data Outlier Detection Based on Kullback-Leibler Divergence,Bioinformatics and Biomedicine,"Outlier detection is imperative in biomedical data analysis to achieve reliable knowledge discovery. In this paper, a new outlier detection method based on Kullback-Leibler (KL) divergence is presented. The original concept of KL divergence was designed as a measure of distance between two distributions. Stemming from that, we extend it to biological sample outlier detection by forming sample sets composed of nearest neighbors. To handle the non-linearity during the KL divergence calculation and to tackle with the singularity problem due to small sample size, we map the original data into a higher feature space and apply kernel functions without resorting to a mapping function. A sample possessing the largest KL divergence is detected as an outlier. The proposed method is tested with one synthetic data, two public gene expression data sets, and our own mass spectrometry data generated for prostate cancer study.",biology
957,Characterization of Protein Interactions,German Conference on Bioinformatics," Available information on molecular interactions between proteins is currently incomplete with regard to detail and comprehensiveness. Although a number of repositories are already devoted to capture interaction data, only a small subset of the currently known interactions can be obtained that way. Besides further experiments, knowledge on interactions can only be complemented by applying text extraction methods to the literature. Currently, information to further characterize individual interactions can not be provided by interaction extraction approaches and is virtually nonexistent in repositories. We present an approach to not only confirm extracted interactions but also to characterize interactions with regard to four attributes such as activation vs. inhibition and protein-protein vs. protein-gene interactions. Here, training corpora with positional annotation of interacting proteins are required. As suitable corpora are rare, we propose an extensible curation protocol to conveniently characterize interactions by manual annotation of sentences so that machine learning approaches can be applied subsequently. We derived a training set by manually reading and annotating 269 sentences for 1090 candidate interactions; 439 of these are valid interactions, predicted via support vector machines at a precision of 83% and a recall of 87%. The prediction of interaction attributes from individual sentences on average yielded a precision of about 85% and a recall of 73%. ",biology
958,Notice of Retraction Characteristics of Nitrogen Isotopic Composition of Different Wheat Chromosome Ploidy,International Conference on Bioinformatics and Biomedical Engineering,"Notice of Retraction After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles. We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper. The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org. Plant nitrogen stable isotopic composition (δ15N) has been used in plant eco-physiology, and which can used to research plant nitrogen use efficiency and its relation to the surrounding environment. In the present study, we determined δ15N changes during evolution using three wheat evolution genotypes with different ploidy chromosome sets (T. monococcum: 2n=2x=14, AA; T. dicoccum: 2n=4x=28, AABB and T.aestivum : 2n=6x=42, AABBDD) under solution culture, and the results showed that: either wheat leaf and root or whole wheat plant, nitrogen isotopic composition was decreased linearity from T. monococcum (2x) to T. dicoccum (4x), to T. aestivum (6x), but the nitrogen content increased during wheat evolution. The reasons may be related to the ability of nitrogen uptake and nitrogen use efficiency. The idea proposed for us to research nitrogen isotope composition, nitrogen use efficiency and select of high nitrogen use efficiency of wheat genotypes.",biology
959,Using Chou’s Pseudo Amino Acid Composition and Machine LearningMethod to Predict the Antiviral Peptides,The Open Bioinformatics Journal," Traditional antiviral therapies are expensive, limitedly available, and cause several side effects. Currently, designing antiviral peptides is very important, because these peptides interfere with the key stage of virus life cycle. Most of the antiviral peptides are derived from viral proteins for example peptide derived from HIV-1 capsid protein. Because of the importance of these peptides, in this study the concept of pseudo-amino acid composition (PseAAC) and machine learning methods are used to classify or identify antiviral peptides. ",biology
960,Inferring regulatory networks through orthologous gene mapping,Bioinformatics and Biomedicine," -In recent years, constructing regulatory networks using gene expression data has received extensive attentions. From Boolean network, Bayesian network to Module network, a number of models have been applied in order to learn the regulatory networks more accurately. The statistical power of network modeling is directly affected by sample size of available expression data used as training data. However, training data are not always abundantly available, except a few well-studied model organisms. It is also infeasible to perform a large number of experiments which require a lot of resources and labor. How to learn a reliable network using minimal training data making use of well-characterized model organisms becomes an important problem with pressing needs. In this paper, we developed a method that infers regulatory sub-networks for a species with limited expression data by learning from a known reference network through orthologous gene mapping. Inspection of three predicted sub-networks confirms biological relevance of our predictions and demonstrates the ability of the method in extracting core regulatory relationships. ",biology
961,Automatic Detection of Large Dense-Core Vesicles in Secretory Cells and Statistical Analysis of Their Intracellular Distribution,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Analyzing the morphological appearance and the spatial distribution of large dense-core vesicles (granules) in the cell cytoplasm is central to the understanding of regulated exocytosis. This paper is concerned with the automatic detection of granules and the statistical analysis of their spatial locations in different cell groups. We model the locations of granules of a given cell as a realization of a finite spatial point process and the point patterns associated with the cell groups as replicated point patterns of different spatial point processes. First, an algorithm to segment the granules using electron microscopy images is proposed. Second, the relative locations of the granules with respect to the plasma membrane are characterized by two functional descriptors: the empirical cumulative distribution function of the distances from the granules to the plasma membrane and the density of granules within a given distance to the plasma membrane. The descriptors of the different cells for each group are compared using bootstrap procedures. Our results show that these descriptors and the testing procedure allow discriminating between control and treated cells. The application of these novel tools to studies of secretion should help in the analysis of diseases associated with dysfunctional secretion, such as diabetes. © 2006 IEEE.",biology
962,CEO a cloud epistasis computing model in GWAS,Bioinformatics and Biomedicine," -The 1000 Genome project has made available a large number of single nucleotide polymorphisms (SNPs) for genome-wide association studies (GWAS). However, the large number of SNPs has also rendered the discovery of epistatic interactions of SNPs computationally expensive. Parallelizing the computation offers a promising solution. In this paper, we propose a cloud-based epistasis computing (CEO) model that examines all k-locus SNPs combinations to find statistically significant epistatic interactions efficiently. Our CEO model uses the MapReduce framework which can be executed both on user's own clusters or on a cloud environment. Our cloud-based solution offers elastic computing resources to users, and more importantly, makes our approach affordable and available to all end-users. We evaluate our CEO model on a cluster of more than 40 nodes. Our experiment results show that our CEO model is computationally flexible, scalable and practical. ",biology
963,Relational Analysis of CpG Islands Methylation and Gene Expression in Human Lymphomas Using Possibilistic C-Means Clustering and Modified Cluster Fuzzy Density,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Heterogeneous genetic and epigenetic alterations are commonly found in human non-Hodgkins lymphomas (NHL). One such epigenetic alteration is aberrant methylation of gene promoter-related CpG islands, where hypermethylation frequently results in transcriptional inactivation of target genes, while a decrease or loss of promoter methylation (hypomethylation) is frequently associated with transcriptional activation. Discovering genes with these relationships in NHL or other types of cancers could lead to a better understanding of the pathobiology of these diseases. The simultaneous analysis of promoter methylation using Differential Methylation Hybridization (DMH) and its associated gene expression using Expressed CpG Island Sequence Tag (ECIST) microarrays generates a large volume of methylation-expression relational data. To analyze this data, we propose a set of algorithms based on fuzzy sets theory, in particular Possibilistic c-Means (PCM) and cluster fuzzy density. For each gene, these algorithms calculate measures of confidence of various methylation-expression relationships in each NHL subclass. Thus, these tools can be used as a means of high volume data exploration to better guide biological confirmation using independent molecular biology methods. ÂŠ 2007 IEEE.",biology
964,A Multi-task Feature Selection Filter for Microarray Classification,Bioinformatics and Biomedicine,"A major challenge in microarray classification and biomarker discovery is dealing with small-sample high-dimensional data where the number of genes used as features is typically orders of magnitude larger than the number of labeled microarrays. One way to address this challenge is by leveraging information from the publicly accessible repositories of microarray data. Following this idea, a multi-task feature selection filter is proposed that borrows strength from the auxiliary microarray classification data sets. The filter uses Kruskal-Wallis test on auxiliary data sets and ranks genes based on their aggregated p-values. Expressions of the top-ranked genes are used as features to build a classifier on the target data set. The proposed approach was evaluated on 9 microarray data sets related to 9 different types of cancers. Comparison of the classification accuracies reveals that the multi-task feature selection is superior to single-task feature selection. Furthermore, the results strongly suggest that multi-task algorithms could improve microarray classification by exploiting auxiliary data during feature selection and learning.",biology
965,Inference of Gene Pathways Using Gaussian Mixture Models,Bioinformatics and Biomedicine,"Identification of gene-gene interactions and complete characterization of gene pathways are critical in understanding the transcript processes underlying biological processes. Bayesian network is a powerful framework to infer gene pathways. We developed a novel Bayesian network, in which we use Gaussian mixture models to describe continuous gene expression data and learn gene pathways. Mixture parameters were estimated using an EM algorithm, while the optimal number of mixture component for each gene node and the optimal network topology best supported by the data were identified using the Bayesian Information criterion (BIC). We applied the proposed approach to a histone pathway in yeast and to a less explored circadian rhythm pathway in honeybee. The performance of the proposed approach was compared against alternative Bayesian network algorithms that either discretize the gene expression information or use single distribution instead of mixtures. Evaluation shows that our approach outperforms other approaches in terms of more accurate inference of the known network and can effectively predict gene pathways with different topology using continuous data. In addition, the estimated mixture model can facilitate an intuitive description of the gene node behavior, thus enhancing the interpretation of the inferred network.",biology
966,Multi-functional Protein Clustering in PPI Networks,Bioinformatics Research and Development," Protein-Protein Interaction (PPI) networks contain valuable information for the isolation of groups of proteins that participate in the same biological function. Many proteins play different roles in the cell by taking part to several processes, but isolating the different processes in which a protein is involved is often a difficult task. In this paper we present a method based on a greedy local search technique to detect functional modules in PPI graphs. The approach is conceived as a generalization of the algorithm PINCoC to generate overlapping clusters of the interaction graph in input. Due to this peculiarity, multi-facets proteins are allowed to belong to different groups corresponding to different biological processes. A comparison of the results obtained by our method with those of other well known clustering algorithms shows the capability of our approach to detect different and meaningful functional modules. ",biology
967,Drosophila Gene Expression Pattern Annotation through Multi-Instance Multi-Label Learning,IEEE/ACM Transactions on Computational Biology and Bioinformatics," The Berkeley Drosophila Genome Project (BDGP) has produced a large number of gene expression patterns, many of which have been annotated textually with anatomical and developmental terms. These terms spatially correspond to local regions of the images; however, they are attached collectively to groups of images, such that it is unknown which term is assigned to which region of which image in the group. This poses a challenge to the development of the computational method to automate the textual description of expression patterns contained in each image. In this paper, we show that the underlying nature of this task matches well with a new machine learning framework, Multi-Instance Multi-Label learning (MIML). We propose a new MIML support vector machine to solve the problems that beset the annotation task. Empirical study shows that the proposed method outperforms the state-of-the-art Drosophila gene expression pattern annotation methods. ",biology
968,CGM: A biomedical text categorization approach using concept graph mining,Bioinformatics and Biomedicine,Text Categorization is used to organize and manage biomedical text databases that are growing at an exponential rate. Feature representations for documents are a crucial factor for the performance of text categorization. Most of the successful existing techniques use a vector representation based on key entities extracted from the text. In this paper we investigate a new direction where we represent a document as a graph. In this representation we identify high level concepts and build a rich graph structure that contains additional concepts and relationships. We then use graph kernel techniques to perform text categorization. The results show a significant improvement in accuracy when compared to categorization based on only the extracted concepts.,biology
969,Relational operators for prioritizing candidate biomarkers in high-throughput differential expression data,International Conference on Bioinformatics," Recent developments in high-throughput proteomics technologies have made it possible to detect and identify low abundance proteins. These technologies provide a new window through which proteomes can be analyzed. Despite holding great promise, the contribution of mass spectrometry based proteomics in identifying novel diagnostic biomarkers has been disappointing. This failure has, in part, been attributed to the lack of effective strategies for determining candidate biomarkers that justify more expensive and timeconsuming validation studies. An approach that bridges the gap between unbiased experimental paradigm emphasizing comprehensive characterizations of proteins and a candidatedriven paradigm would overcome this limitation [33]. To this end, we have developed database operators that extend the database management systems to analyze highthroughput proteomics and genomics data. By analyzing differentially expressed genes and proteins using pathway databases, these operators take advantage of established expert domain knowledge in pathway annotation to prioritize candidate biomarkers. They provide a systematic way of bridging the gap between unbiased experimental paradigm and candidate-driven paradigm. To test the operators, we analyzed a dataset of salivary proteins differentially expressed between pre-malignant and malignant oral lesions. Six proteins are identified as candidate biomarkers worth of valida∗Accompanying software and supplementary data available at http://www.cs.umn.edu/~onsongo/acm/bcb2010.html username: onsongo password: acmbcb2010 †corresponding author tion studies. A literature search reveals these high priority candidate biomarkers interact with proteins implicated in cancer development highlighting their potential utility as biomarkers demonstrating the effectiveness of our operators. The developed operators will help overcome one of the main challenges of high-throughput computational techniques; provide a systematic way of bridging the gap between unbiased data driven approach and hypothesis driven approach to prioritize candidate biomarkers worth of more expensive and time consuming validation studies. ",biology
970,Protein Structure Comparison Based on Fold Evolution,German Conference on Bioinformatics," The paper presents a protein structure comparison algorithm that is capable to identify specific fold mutations between two proteins. The search for such mutations is based on structure evolution models suggesting that, similarly as sequences, protein folds (at least partially) evolve by a stepwise process, where each step comprises comparatively simple changes affecting few secondary structure elements. The particular fold mutations considered in this study are based on the work by Grishin [Gr01]. The algorithm uses structure representation by 3D graphs and is a modification of a method used in SSM structure alignment tool [KH04a]. Experiments demonstrate that our method is able automatically identify 85% of examples of fold mutations given by Grishin. Also a number of tests involving all-against-all comparisons of CATH structural domains have been performed in order to measure comparative frequencies of different types of fold mutations and some statistical estimations have been obtained. ",biology
971,Protein-protein interaction prediction via Collective Matrix Factorization,Bioinformatics and Biomedicine,"Protein-protein interactions (PPI) play an important role in cellular processes and metabolic processes within a cell. An important task is to determine the existence of interactions among proteins. Unfortunately, existing biological experimental techniques are expensive, time-consuming and labor-intensive. The network structures of many such networks are sparse, incomplete and noisy, containing many false positive and false negatives. Thus, state-of-the-art methods for link prediction in these networks often cannot give satisfactory prediction results, especially when some networks are extremely sparse. Noticing that we typically have more than one PPI network available, we naturally wonder whether it is possible to 'transfer' the linkage knowledge from some existing, relatively dense networks to a sparse network, to improve the prediction performance. Noticing that a network structure can be modeled using a matrix model, in this paper, we introduce the well-known Collective Matrix Factorization (CMF) technique to 'transfer' usable linkage knowledge from relatively dense interaction network to a sparse target network. Our approach is to establish the correspondence between a source and a target network via network similarities. We test this method on two real protein-protein interaction networks, Helicobacter pylori (as a target network) and Human (as a source network). Our experimental results show that our method can achieve higher and more robust performance as compared to some baseline methods.",biology
972,Estimating Genome-Wide Gene Networks Using Nonparametric Bayesian Network Models on Massively Parallel Computers,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"We present a novel algorithm to estimate genome-wide gene networks consisting of more than 20,000 genes from gene expression data using nonparametric Bayesian networks. Due to the difficulty of learning Bayesian network structures, existing algorithms cannot be applied to more than a few thousand genes. Our algorithm overcomes this limitation by repeatedly estimating subnetworks in parallel for genes selected by neighbor node sampling. Through numerical simulation, we confirmed that our algorithm outperformed a heuristic algorithm in a shorter time. We applied our algorithm to microarray data from human umbilical vein endothelial cells (HUVECs) treated with siRNAs, to construct a human genome-wide gene network, which we compared to a small gene network estimated for the genes extracted using a traditional bioinformatics method. The results showed that our genome-wide gene network contains many features of the small network, as well as others that could not be captured during the small network estimation. The results also revealed master-regulator genes that are not in the small network but that control many of the genes in the small network. These analyses were impossible to realize without our proposed algorithm. © 2011 IEEE.",biology
973,Supervised Posteriors for DNA-motif Classification,German Conference on Bioinformatics," Markov models have been proposed for the classification of DNA-motifs using generative approaches for parameter learning. Here, we propose to apply the discriminative paradigm for this problem and study two different priors to facilitate parameter estimation using the maximum supervised posterior. Considering seven sets of eukaryotic transcription factor binding sites we find this approach to be superior employing area under the ROC curve and false positive rate as performance criterion, and better in general using sensitivity. In addition, we discuss potential reasons for the improved performance. ",biology
974,Phylogenetic Super-Networks from Partial Trees,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"In practice, one is often faced with incomplete phylogenetic data, such as a collection of partial trees or partial splits. This paper poses the problem of Inferring a phylogenetic super-network from such data and provides an efficient algorithm for doing so, called the Z-closure method. Additionally, the questions of assigning lengths to the edges of the network and how to restrict the ""dimensionality"" of the network are addressed. Applications to a set of five published partial gene trees relating different fungal species and to six published partial gene trees relating different grasses illustrate the usefulness of the method and an experimental study confirms Its potential. The method Is implemented as a plug-in for the program SplitsTree4. © 2004 IEEE.",biology
975,Efficient and accurate clustering for large-scale genetic mapping,Bioinformatics and Biomedicine,"High-throughput “next generation” genome sequencing technologies are producing a flood of inexpensive genetic information that is invaluable to genomics research. Sequences of millions of genetic markers are being produced, providing genomics researchers with the opportunity to construct highresolution genetic maps for many complicated genomes. However, the current generation of genetic mapping tools were designed for the small data setting, and are now limited by the prohibitively slow clustering algorithms they employ in the genetic marker-clustering stage. In this work, we present a new approach to genetic mapping based on a fast clustering algorithm that exploits the geometry of the data. Our theoretical and empirical analysis shows that the algorithm can correctly recover linkage groups. Using synthetic and real-world data, including the grand-challenge wheat genome, we demonstrate that our approach can quickly process orders of magnitude more genetic markers than existing tools while retaining - and in some cases even improving - the quality of genetic marker clusters.",biology
976,A Co-Clustering Technique for Gene Expression Data Using Bi-Partite Graph Approach,International Conference on Bioinformatics and Biomedical Engineering,"Mining microarray data sets is vital in bioinformatics research and medical applications. There has been extensive research on co-clustering of gene expression data generated using cDNA microarrays. Co-clustering approach is an important analysis tool in gene expression measurement, when some genes have multiple functions and experimental conditions are diverse. In this paper, we introduce a new framework for microarray gene expression data co-clustering. The basis of this framework is a bipartite graph representation of 2-dimensional gene expression data. We have constructed this bipartite graph by partitioning the sample set into two disjoint sets. The key property of this representation is that, for a gene×sample matrix, it constructs the range bipartite graph, a compact representation of all similar value ranges between sample columns. In order to produce the set of co-clusters, it searches for constrained maximal cliques in this bipartite graph. Our method is scalable to practical gene expression data and can find some interesting co-clusters in real microarray datasets that meet specific input conditions.",biology
977,A Simple and Accurate Method for Rogue Taxon Identification,Bioinformatics and Biomedicine,"The summary of a phylogenetic analysis (typically a consensus tree) can be substantially biased by so-called rogue taxa (or briefly: rogues). Rogues assume varying phylogenetic positions in the tree collection that is used to build the consensus tree and thereby decrease the resolution of the consensus. We present an accurate and straight-forward algorithm for identifying rogues that assesses the effect on the consensus tree support values by removing one taxon at a time. Our approach improves the resolution of the consensus tree and, at the same time, increases the support values of existing relationships. We compare our algorithm to three competing methods (leaf stability index, taxonomic instability index, and Pattengale's algorithm) on a large number of real biological data sets. We show that it outperforms stability-based methods since rogue taxa are not necessarily the most unstable taxa with respect to stability measures. Our algorithm is more memory-efficient than Pattengale's approach while instances, where Pattengale's algorithm outperforms our approach, appear to be rare on real data. Finally, we find that, it is advisable to conduct a de novo bootstrap analysis after rogues have been removed from the sequence alignment.",biology
978,Network-Based Drug Ranking and Repositioning with Respect to DrugBank Therapeutic Categories,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Drug repositioning is a challenging computational problem involving the integration of heterogeneous sources of biomolecular data and the design of label ranking algorithms able to exploit the overall topology of the underlying pharmacological network. In this context, we propose a novel semisupervised drug ranking problem: prioritizing drugs in integrated biochemical networks according to specific DrugBank therapeutic categories. Algorithms for drug repositioning usually perform the inference step into an inhomogeneous similarity space induced by the relationships existing between drugs and a second type of entity (e.g., disease, target, ligand set), thus making unfeasible a drug ranking within a homogeneous pharmacological space. To deal with this problem, we designed a general framework based on bipartite network projections by which homogeneous pharmacological networks can be constructed and integrated from heterogeneous and complementary sources of chemical, biomolecular and clinical information. Moreover, we present a novel algorithmic scheme based on kernelized score functions that adopts both local and global learning strategies to effectively rank drugs in the integrated pharmacological space using different network combination methods. Detailed experiments with more than 80 DrugBank therapeutic categories involving about 1,300 FDA-approved drugs show the effectiveness of the proposed approach. © 2013 IEEE.",biology
979,A Biologically Inspired Validity Measure for Comparison of Clustering Methods over Metabolic Data Sets,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"In the biological domain, clustering is based on the assumption that genes or metabolites involved in a common biological process are coexpressed/ coaccumulated under the control of the same regulatory network. Thus, a detailed inspection of the grouped patterns to verify their memberships to well-known metabolic pathways could be very useful for the evaluation of clusters from a biological perspective. The aim of this work is to propose a novel approach for the comparison of clustering methods over metabolic data sets, including prior biological knowledge about the relation among elements that constitute the clusters. A way of measuring the biological significance of clustering solutions is proposed. This is addressed from the perspective of the usefulness of the clusters to identify those patterns that change in coordination and belong to common pathways of metabolic regulation. The measure summarizes in a compact way the objective analysis of clustering methods, which respects coherence and clusters distribution. It also evaluates the biological internal connections of such clusters considering common pathways. The proposed measure was tested in two biological databases using three clustering methods. ÂŠ 2006 IEEE.",biology
980,Fast Motif Selection for Biological Sequences,Bioinformatics and Biomedicine,"We consider the problem of identifying motifs, recurring or conserved patterns, in the sets of biological sequences. To solve this task, we present new deterministic and exact algorithms for finding patterns that are embedded as exact or inexact instances in all or most of the input strings. The proposed algorithms (1) improve search efficiency compared to existing exact algorithms by focusing search on a selected set of potential motif instances, and (2) scale well with the input length and the size of alphabet. Our algorithms are orders of magnitude faster than existing exact algorithms for common pattern identification. We evaluate our algorithms on benchmark motif finding problems and real applications in biological sequence analysis and show that they exhibit significant running time improvements compared to the state-of-the-art approaches.",biology
981,Predicting Future High-Cost Patients: A Real-World Risk Modeling Application,Bioinformatics and Biomedicine,"Health care data from patients in the Arizona Health Care Cost Containment System, Arizona's Medicaid program, provides a unique opportunity to exploit state-of-the-art data processing and analysis algorithms to mine the data and provide actionable results that can aid cost containment. This work addresses specific challenges in this real-life health care application to build predictive risk models for forecasting future high-cost users. Such predictive risk modeling has received attention in recent years with statistical techniques being the backbone of proposed methods. We survey the literature and propose a novel data mining approach customized for this potent application. Our empirical study indicates that this approach is useful and can benefit further research on cost containment in the health care industry.",biology
982,Structure identification and parameter estimation of biological s-systems,Bioinformatics and Biomedicine,"Reconstruction of a biological system from its experimental time series data is a challenging task in systems biology. The S-system which consists of a group of nonlinear ordinary differential equations is an effective model to characterize molecular biological systems and analyze the system dynamics. However, inference of S-systems without the knowledge of system structure is not a trivial task due to its nonlinearity and complexity. In this paper, a pruning separable parameter estimation algorithm is proposed for inferring S-systems. This novel algorithm combines the separable parameter estimation method and a pruning strategy, which includes adding an ℓ1 regularization term to the objective function and pruning the solution with a threshold value. The performance of the pruning strategy in the proposed algorithm is evaluated from two aspects: the parameter estimation error and structure identification accuracy. The proposed algorithm is applied to two S-systems with simulated data. The results show that the proposed algorithm has much lower estimation error and much higher identification accuracy than the existing method.",biology
983,Detection of copy number variation from next generation sequencing data with total variation penalized least square optimization,Bioinformatics and Biomedicine,"The detection of copy number variation is important to understand complex diseases such as autism, schizophrenia, cancer, etc. In this paper we propose a method to detect copy number variation from next generation sequencing data. Compared with conventional methods to detect copy number variation like array comparative genomic hybridization (aCGH), the next generation sequencing data provide higher resolution of genomic variations. There are a lot of methods to detect copy number variation from next sequencing data, and most of them are based on statistical hypothesis testing. In this paper, we consider this problem from an optimization point of view. The proposed method is based on optimizing a total variation penalized least square criterion, which involves ℓ-1 norm. Inspired by the analytical study of a statics system, we propose an iterative algorithm to find the optimal solution of this optimization problem. The comparative study with other existing methods on simulated data demonstrates that our method can detect relatively small copy number variants (low copy number and small single copy length) with low false positive rate.",biology
984,Analysis of Load Balancing Techniques in Cloud Computing,International Conference on Bioinformatics," Cloud Computing is an emerging computing paradigm. It aims to share data, calculations, and service transparently over a scalable network of nodes. Since Cloud computing stores the data and disseminated resources in the open environment. So, the amount of data storage increases quickly. In the cloud storage, load balancing is a key issue. It would consume a lot of cost to maintain load information, since the system is too huge to timely disperse load. Load balancing is one of the main challenges in cloud computing which is required to distribute the dynamic workload across multiple nodes to ensure that no single node is overwhelmed. It helps in optimal utilization of resources and hence in enhancing the performance of the system. A few existing scheduling algorithms can maintain load balancing and provide better strategies through efficient job scheduling and resource allocation techniques as well. In order to gain maximum profits with optimized load balancing algorithms, it is necessary to utilize resources efficiently. This paper discusses some of the existing load balancing algorithms in cloud computing and also their challenges. ",biology
985,Prediction of the pro-longevity or anti-longevity effect of Caenorhabditis Elegans genes based on Bayesian classification methods,Bioinformatics and Biomedicine,"The genetic mechanisms of ageing are mysterious and sophisticated issues that attract biologists' attention. With the help of data mining techniques, some findings relevant to the ageing problem can be revealed. This paper studies the performance of Bayesian network augmented naive Bayes classifier, naive Bayes classifier and proposed feature selection methods for naive Bayes on predicting a C. elegans gene's effect on the organism's longevity. The results show that due to the hierarchical structure of predictor attribute values (Gene Ontology terms), the Bayesian network augmented naive Bayes classifier performs better than the naive Bayes classifier, and the proposed feature selection methods for naive Bayes can effectively optimize the predictive performance of naive Bayes.",biology
986,On Lattice Protein Structure Prediction Revisited,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Protein structure prediction is regarded as a highly challenging problem both for the biology and for the computational communities. In recent years, many approaches have been developed, moving to increasingly complex lattice models and off-lattice models. This paper presents a Large Neighborhood Search (LNS) to find the native state for the Hydrophobic-Polar (HP) model on the Face-Centered Cubic (FCC) lattice or, in other words, a self-avoiding walk on the FCC lattice having a maximum number of H-H contacts. The algorithm starts with a tabu-search algorithm, whose solution is then improved by a combination of constraint programming and LNS. The flexible framework of this hybrid algorithm allows an adaptation to the Miyazawa-Jernigan contact potential, in place of the HP model, thus suggesting its potential for tertiary structure prediction. Benchmarking statistics are given for our method against the hydrophobic core threading program HPstruct, an exact method which can be viewed as complementary to our method. © 2011 IEEE.",biology
987,Improving the Computational Efficiency of Recursive Cluster Elimination for Gene Selection,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The gene expression data are usually provided with a large number of genes and a relatively small number of samples, which brings a lot of new challenges. Selecting those informative genes becomes the main issue in microarray data analysis. Recursive cluster elimination based on support vector machine (SVM-RCE) has shown the better classification accuracy on some microarray data sets than recursive feature elimination based on support vector machine (SVM-RFE). However, SVM-RCE is extremely time-consuming. In this paper, we propose an improved method of SVM-RCE called ISVM-RCE. ISVM-RCE first trains a SVM model with all clusters, then applies the infinite norm of weight coefficient vector in each cluster to score the cluster, finally eliminates the gene clusters with the lowest score. In addition, ISVM-RCE eliminates genes within the clusters instead of removing a cluster of genes when the number of clusters is small. We have tested ISVM-RCE on six gene expression data sets and compared their performances with SVM-RCE and linear-discriminant-analysis-based RFE (LDA-RFE). The experiment results on these data sets show that ISVM-RCE greatly reduces the time cost of SVM-RCE, meanwhile obtains comparable classification performance as SVM-RCE, while LDA-RFE is not stable. © 2011 IEEE.",biology
988,Modeling and Visualizing Uncertainty in Gene Expression Clusters Using Dirichlet Process Mixtures,IEEE/ACM Transactions on Computational Biology and Bioinformatics, DRAFT ,biology
989,A Study of Compression–Based Methods for the Analysis of Barcode Sequences,Computational Intelligence Methods for Bioinformatics and Biostatistics,"In this paper it is introduced a new methodology for the analysis of barcode sequences. Barcode DNA is a very short nucleotide sequence, corresponding for the animal kingdom to the mitochondrial gene cytochrome c oxidase subunit 1, that acts as a unique element for identification and taxonomic purposes. Traditional barcode analysis uses well consolidated bioinformatics techniques such as sequence alignment, computation of evolutionary distances and phylogenetic trees. The proposed alignment-free approach consists in the use of two different compression-based approximations of Universal Similarity Metric in order to compute dissimilarity matrices among barcode sequences of 20 datasets belonging to different species. From these matrices phylogenetic trees are computed and compared, in terms of topology and branch length, with trees built from evolutionary distance. The results show high similarity values between compression-based and evolutionary-based trees allowing us to consider the former methodology worth to be employed for the study of barcode sequences © 2013 Springer-Verlag.",biology
990,Aligning protein-protein interaction networks using random neural networks,Bioinformatics and Biomedicine,"We have developed RNNI, a global alignment method for protein-protein interaction networks between species, using a random neural network model (RNN) tailored for the alignment problem. The benchmark of the method in comparison with other available alignment approaches was performed using a range of measurements. The alignment results of the human and yeast pair showed that RNNI is capable of generating alignments with large conserved networks with functionally-related protein pairs while maintaining the closeness to the naive- sequence homology approach (BLAST).",biology
991,MultiFacTV: Finding modules from higher-order gene expression profiles with time dimension,Bioinformatics and Biomedicine,"Module detection is an important task in bioinformatics which aims at finding a set of cells/genes that interact together to be responsible for some biological functionalities. In this paper, we propose a novel tensor factorization approach to finding modules from higher-order gene expression profiles with the time dimension, e.g., gene × condition × time data. The main idea is to incorporate a total variation regularization term for the time dimension during the tensor factorization, and then use the factorization results to identify the modules. Experimental results on two real gene × condition × time datasets have shown the effectiveness of the proposed method.",biology
992,Markov clustering of protein interaction networks with improved balance and scalability,International Conference on Bioinformatics,"Markov Clustering (MCL) is a popular algorithm for clustering networks in bioinformatics such as protein-protein interaction networks and protein similarity networks. An important requirement when clustering protein networks is minimizing the number of big clusters, since it is generally understood that protein complexes tend not to have more than 15-30 nodes. Similarly, it is important to not output too many singleton clusters, since they do not provide much useful information. In this paper, we show how MCL may be modified so as to better respect these two requirements, while also taking the link structure in the graph into account. We design our algorithm on top of Regularized MCL (R-MCL) [16], a previously proposed modification of MCL. Our proposed variation computes a new regularization matrix at each iteration that penalizes big cluster sizes, with the size of the penalty being tunable using a balance parameter. This algorithm also naturally fits in a Multi level framework that allows great improvements in speed. We perform experiments on three real protein interaction networks and show significant improvements over MCL in quality, balance and execution speed. Copyright 2010 ACM.",biology
993,MR Image Segmentation Based on Modified Ant Colony Algorithm,International Conference on Bioinformatics and Biomedical Engineering,"Magnetic resonance imaging (MRI) is a widely used method to obtain high quality medical image of the brain. Post-processing MR images with segmentation algorithms enhances the visualization and measurement of soft tissues and lesions. However, the conventional algorithms are not perfect and there are still some regions which are not partitioned accurately. In this paper, a new ant colony algorithm is presented in accordance with the defect of previous variety and stagnation. The contrast experimental results show that the new algorithm is not only of higher segmentation quality but also of higher computational speed, and prove that the algorithm is efficient and superior.",biology
994,DEEN: A Simple and Fast Algorithm for Network Community Detection,Computational Intelligence Methods for Bioinformatics and Biostatistics," This paper introduces an algorithm for network community detection called DEEN (Delete Edges and Expand Nodes) consisting of two simple steps. First edges of the graph estimated to connect different clusters are detected and removed, next the resulting graph is used for generating communities by expanding seed nodes. DEEN uses as parameters the minimum and maximum allowed size of a cluster, and a resolution parameter whose value influences the number of removed edges. Application of DEEN to the budding yeast protein network for detecting functional protein complexes indicates its capability to identify clusters containing proteins with the same functional category, improving on MCL, a popular state-of-the-art method for functional protein complex detection. Moreover, application of DEEN to two popular benchmark networks results in the detection of accurate communities, substantiating the effectiveness of the proposed method in diverse domains. ",biology
995,Multiway Clustering for Creating Biomedical Term Sets,Bioinformatics and Biomedicine,"We present an EM-based clustering method that can be used for constructing or augmenting ontologies such as MeSH. Our algorithm simultaneously clusters verbs and nouns using both verb-noun and noun-noun co-occurrence pairs. This strategy provides greater coverage of words than using either set of pairs alone, since not all words appear in both datasets. We demonstrate it on data extracted from Medline and evaluate the results using MeSH and Wordnet.",biology
996,Quartet-Based Phylogeny Reconstruction with Answer Set Programming,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"In this paper, a new representation is presented for the Maximum Quartet Consistency (MQC) problem, where solving the MQC problem becomes searching for an ultrametric matrix that satisfies a maximum number of given quartet topologies. A number of structural properties of the MQC problem in this new representation are characterized through formulating into answer set programming, a recent powerful logic programming tool for modeling and solving search problems. Using these properties, a number of optimization techniques are proposed to speed up the search process. The experimental results on a number of simulated data sets suggest that the new representation, combined with answer set programming, presents a unique perspective to the MQC problem. © 2007 IEEE.",biology
997,Noise resistant generalized parametric validity index of clustering for gene expression data,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -Validity indices have been investigated for decades. However, since there is no study of noise-resistance performance of these indices in the literature, there is no guideline for determining the best clustering in noisy data sets, especially microarray data sets. In this paper, we propose a generalized parametric validity (GPV) index which employs two tunable parameters a and b to control the proportions of objects being considered to calculate the dissimilarities. The greatest advantage of the proposed GPV index is its noise-resistance ability, which results from the flexibility of tuning the parameters. Several rules are set to guide the selection of parameter values. To illustrate the noise-resistance performance of the proposed index, we evaluate the GPV index for assessing five clustering algorithms in two gene expression data simulation models with different noise levels and compare the ability of determining the number of clusters with eight existing indices. We also test the GPV in three groups of real gene expression data sets. The experimental results suggest that the proposed GPV index has superior noise-resistance ability and provides fairly accurate judgements. ",biology
998,Prioritizing human disease genes by multiple data integration,Bioinformatics and Biomedicine,"Now multiple types of data are available for prioritizing human disease genes, including gene-disease associations, disease phenotype similarities, locations of genes or their corresponding proteins in biological networks, etc. Integrating multiple types of data is expected to be effective for prioritizing human disease genes. In this paper, we propose a multiple data integration method based on the theory of Markov Random Field (MRF) and the method of Bayesian analysis for prioritizing human disease genes. The proposed method is not only flexible in easily incorporating different kinds of data, but also reliable in predicting candidate disease genes. Numerical experiments are carried out by integrating known gene-disease associations, protein complexes, protein-protein interactions and gene expression profiles. Predictions are evaluated by both the leave-one-out method and the fold enrichment method. The sensitivity and the specificity can reach at roughly 80% simultaneously. The method achieves 56.02-fold enrichment on average when integrating all those biological data in our experiments.",biology
999,Multiobjective optizition shuffled frog-leaping biclustering,Bioinformatics and Biomedicine,"Biclustering of DNA microarray data that can mine significant patterns to help in understanding gene regulation and interactions. This is a classical multi-objective optimization problem (MOP). Recently, many researchers have developed stochastic search methods that mimic the efficient behavior of species such as ants, bees, birds and frogs, as a means to seek faster and more robust solutions to complex optimization problems. The particle swarm optimization(PSO) is a heuristics-based optimization approach simulating the movements of a bird flock finding food. The shuffled frog leaping algorithm (SFLA) is a population-based cooperative search metaphor combining the benefits of the local search of PSO and the global shuffled of information of the complex evolution technique. This paper introduces SFL algorithm to solve biclustering of microarray data, and proposes a novel multi-objective shuffled frog leaping biclustering(MOSFLB) algorithm to mine coherent patterns from microarray data. Experimental results on two real datasets show that our approach can effectively find significant biclusters of high quality.",biology
1000,Learning Genetic Regulatory Network Connectivity from Time Series Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Recent experimental advances facilitate the collection of time series data that indicate which genes in a cell are expressed. This information can be used to understand the genetic regulatory network that generates the data. Typically, Bayesian analysis approaches are applied which neglect the time series nature of the experimental data, have difficulty in determining the direction of causality, and do not perform well on networks with tight feedback. To address these problems, this paper presents a method to learn genetic network connectivity which exploits the time series nature of experimental data to achieve better causal predictions. This method first breaks up the data into bins. Next, it determines an initial set of potential influence vectors for each gene based upon the probability of the genes expression increasing in the next time step. These vectors are then combined to form new vectors with better scores. Finally, these influence vectors are competed against each other to determine the final influence vector for each gene. The result is a directed graph representation of the genetic networks repression and activation connections. Results are reported for several synthetic networks with tight feedback showing significant improvements in recall and runtime over Yus dynamic Bayesian approach. Promising preliminary results are also reported for an analysis of experimental data for genes involved in the yeast cell cycle. © 2011 IEEE.",biology
1001,An O(N/sup 2/) algorithm for discovering optimal Boolean pattern pairs,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"We consider the problem of finding the optimal combination of string patterns, which characterizes a given set of strings that have a numeric attribute value assigned to each string. Pattern combinations are scored based on the correlation between their occurrences in the strings and the numeric attribute values. The aim is to find the combination of patterns which is best with respect to an appropriate scoring function. We present an O(N2) time algorithm for finding the optimal pair of substring patterns combined with Boolean functions, where N is the total length of the sequences. The algorithm looks for all possible Boolean combinations of the patterns, e.g., patterns of the form p Λ ¬q, which indicates that the pattern pair Is considered to occur in a given string s, if p occurs in s, AND q does NOT occur in s. An efficient Implementation using suffix arrays is presented, and we further show that the algorithm can be adapted to find the best k-pattern Boolean combination in O(Nk) time. The algorithm is applied to mRNA sequence data sets of moderate size combined with their turnover rates for the purpose of finding regulatory elements that cooperate, complement, or compete with each other in enhancing and/or silencing mRNA decay. © 2004 IEEE.",biology
1002,Protein 3D Structure Prediction by Improved Tabu Search in Off-Lattice AB Model,International Conference on Bioinformatics and Biomedical Engineering,"Tabu Search (TS) algorithm is one of optimization search methods and has been applied for a large number of combinatorial optimization problems. In the paper, an improved TS algorithm is proposed for protein 3D folding structure prediction in off-lattice AB model. Experimental results show that the lowest energies computed by the improved TS algorithm are better than those obtained by the previous methods. Given a protein sequence, its lowest-energy conformation obtained by our improved TS method forms a single hydrophobic core, which suggests that AB model in three dimensions appears to reflect the real protein reasonably. Compared with the previous heuristic approaches, the improved TS algorithm has higher performance and can be effectively used to predict 3D structure prediction of proteins.",biology
1003,Similarity Measures for Comparing Biclusterings,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The comparison of ordinary partitions of a set of objects is well established in the clustering literature, which comprehends several studies on the analysis of the properties of similarity measures for comparing partitions. However, similarity measures for clusterings are not readily applicable to biclusterings, since each bicluster is a tuple of two sets (of rows and columns), whereas a cluster is only a single set (of rows). Some biclustering similarity measures have been defined as minor contributions in papers which primarily report on proposals and evaluation of biclustering algorithms or comparative analyses of biclustering algorithms. The consequence is that some desirable properties of such measures have been overlooked in the literature. We review 14 biclustering similarity measures. We define eight desirable properties of a biclustering measure, discuss their importance, and prove which properties each of the reviewed measures has. We show examples drawn and inspired from important studies in which several biclustering measures convey misleading evaluations due to the absence of one or more of the discussed properties. We also advocate the use of a more general comparison approach that is based on the idea of transforming the original problem of comparing biclusterings into an equivalent problem of comparing clustering partitions with overlapping clusters.",biology
1004,Comparative study of GRNS inference methods based on feature selection by mutual information,International Conference on Bioinformatics,"Feature selection is a crucial topic in pattern recognition applications, especially in the genetic regulatory networks (GRNs) inference problem which usually involves data with a large number of variables and small number of observations. In this context, the application of dimensionality reduction approaches such as those based on feature selection becomes a mandatory step in order to select the most important predictor genes that can explain some phenomena associated with the target genes. Given its importance in GRN inference, many feature selection methods (algorithms and criterion functions) have been proposed. However, it is decisive to validate such results in order to better understand its significance. The present work proposes a comparative study of feature selection techniques involving information theory concepts, applied to the estimation of GRNs from simulated temporal expression data generated by an artificial gene network (AGN) model. Four GRN inference methods are compared in terms of global network measures. Some interesting conclusions can be drawn from the experimental results.",biology
1005,A physiological signal processing system for optimal engagement and attention detection,Bioinformatics and Biomedicine,"This paper proposes a computer aided system that aims to measure and interpret physiological signals so as to assess the attention/engagement level of a person during cognitive based activities. In this study, ECG (Electrocardiogram), HF (Heat Flux) and EEG (Electroencephalogram) signals were collected from 8 subjects. The subjects were made to watch a series of videos which demanded contrasting engagement levels. On the collected ECG data, Discrete Wavelet Transform (DWT) is applied to the raw signal and multiple features are extracted. Features from HF were also obtained. In EEG signals, different band components were first extracted upon which DWT is applied to extract numerous features. Finally machine learning techniques were employed to classify the extracted features into two categories of `attention' and `non-attention'. The results show success in distinguishing `attention' vs. `non-attention' cases by processing acquired physiological signals.",biology
1006,Applying Feature Coupling Generalization for Protein-Protein Interaction Extraction,Bioinformatics and Biomedicine,"We present the application of a recently proposed semi-supervised learning strategy - feature coupling generalization (FCG) - in the task of protein-protein interaction extraction from biomedical literatures. FCG is a framework that generates new features from relatedness of two special types of old features: example-distinguishing features (EDFs) and class-distinguishing features (CDFs). Their relatedness estimated from unlabeled data tends to capture indicative information not available in labeled data. For this task, we designed several EDFs and CDFs derived from the text patterns surrounding the co-occurrence proteins, and combined the new features generated by FCG with Boolean lexical features. The experimental results on AIMED corpus show that the new features yield significant improvement over a strong baseline, and the combined method achieves state-of-the-art performance without using any syntactic information.",biology
1007,Using semantic dependencies for consistency management of an ontology of brain-cortex anatomy,International Conference on Bioinformatics," In the context of the Semantic Web, ontologies have to be usable by software agents as well as by humans. Therefore, they must meet explicit representation and consistency requirements. This article describes a method for managing the semantic consistency of an ontology of brain-cortex anatomy. The methodology relies on the explicit identi cation of the relationship properties and of the dependencies that might exist among concepts or relationships. These dependencies have to be respected for insuring the semantic consistency of the model. We propose a method for automatically generating all the dependent items. As a consequence, knowledge base updates are easier and safer. Our approach is composed of three main steps: (1) providing a realistic representation, (2) ensuring the intrinsic consistency of the model and (3) checking its incremental consistency. The corner stone of ontological modeling lies in the expressiveness of the model and in the sound principles that structure it. This part de nes the ideal possibilities of the ontology and is called realism of representation. Regardless of how well a model represents reality, the intrinsic consistency of a model corresponds to its lack of contradiction. This step is particularly important as soon as dependencies between relationships or concepts have to be ful lled. Eventually, the incremental consistency encompasses the respect of the two previous criteria during the successive updates of the ontology. ",biology
1008,Multiple Peak Alignment in Sequential Data Analysis: A Scale-Space-Based Approach,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -In this paper, we address the multiple peak alignment problem in sequential data analysis with an approach based on the Gaussian scale-space theory. We assume that multiple sets of detected peaks are the observed samples of a set of common peaks. We also assume that the locations of the observed peaks follow unimodal distributions (e.g., normal distribution) with their means equal to the corresponding locations of the common peaks and variances reflecting the extension of their variations. Under these assumptions, we convert the problem of estimating locations of the unknown number of common peaks from multiple sets of detected peaks into a much simpler problem of searching for local maxima in the scale-space representation. The optimization of the scale parameter is achieved using an energy minimization approach. We compare our approach with a hierarchical clustering method using both simulated data and real mass spectrometry data. We also demonstrate the merit of extending the binary peak detection method (i.e., a candidate is considered either as a peak or as a nonpeak) with a quantitative scoring measure-based approach (i.e., we assign to each candidate a possibility of being a peak). ",biology
1009,Automated Hierarchical Density Shaving: A Robust Automated Clustering and Visualization Framework for Large Biological Data Sets,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"A key application of clustering data obtained from sources such as microarrays, protein mass spectroscopy, and phylogenetic profiles is the detection of functionally related genes. Typically, only a small number of functionally related genes cluster into one or more groups, and the rest need to be ignored. For such situations, we present Automated Hierarchical Density Shaving (Auto-HDS), a framework that consists of a fast hierarchical density-based clustering algorithm and an unsupervised model selection strategy. Auto-HDS can automatically select clusters of different densities, present them in a compact hierarchy, and rank individual clusters using an innovative stability criteria. Our framework also provides a simple yet powerful 2D visualization of the hierarchy of clusters that is useful for further interactive exploration. We present results on Gasch and Lee microarray data sets to show the effectiveness of our methods. Additional results on other biological data are included in the supplemental material. ÂŠ 2006 IEEE.",biology
1010,Analysis of the Free Energy in a Stochastic RNA Secondary Structure Model,IEEE/ACM Transactions on Computational Biology and Bioinformatics," There are two custom ways for predicting RNA secondary structures: minimizing the free energy of a conformation according to a thermodynamic model and maximizing the probability of a folding according to a stochastic model. In most cases stochastic grammars are used for the latter alternative applying the maximum likelihood principle for determining a grammar's probabilities. In this paper, building on such a stochastic model, we will analyze the expected minimum free energy of an RNA molecule according to Turner's energy rules. Even if the parameters of our grammar are chosen with respect to structural properties of native molecules only (and therefore independent of molecules' free energy), we prove formulae for the expected minimum free energy and the corresponding variance as functions of the molecule's size which perfectly t the native behavior of free energies. This gives proof for a high quality of our stochastic model making it a handy tool for further investigations. In fact, the stochastic model for RNA secondary structures presented in this work has for example been used as the basis of a new algorithm for the (non-uniform) generation of random RNA secondary structures. ",biology
1011,A Framework for Three-Dimensional Simulation of Morphogenesis,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -We present COMPUCELL3D, a software framework for three-dimensional simulation of morphogenesis in different organisms. COMPUCELL3D employs biologically relevant models for cell clustering, growth, and interaction with chemical fields. COMPUCELL3D uses design patterns for speed, efficient memory management, extensibility, and flexibility to allow an almost unlimited variety of simulations. We have verified COMPUCELL3D by building a model of growth and skeletal pattern formation in the avian (chicken) limb bud. Binaries and source code are available, along with documentation and input files for sample simulations, at http:// compucell.sourceforge.net. ",biology
1012,A Machine Learning Approach to Mass Spectra Classification with Unsupervised Feature Selection,Computational Intelligence Methods for Bioinformatics and Biostatistics," Mass spectrometry spectra are recognized as a screening tool for detecting discriminatory protein patterns. Mass spectra, however, are high dimensional data and a large number of local maxima (a.k.a. peaks) have to be analyzed; to tackle this problem we have developed a three-step strategy. After data pre-processing we perform an unsupervised feature selection phase aimed at detecting salient parts of the spectra which could be useful for the subsequent classification phase. The main contribution of the paper is the development of this feature selection and extraction procedure grounded on the theory of multi-scale spaces. Then we use support vector machines for classification. Results obtained by the analysis of a data set of tumor/healthy samples allowed us to correctly classify more than 95% of samples. ROC analysis has been also performed. ",biology
1013,Boosting Methods for Protein Fold Recognition: An Empirical Comparison,Bioinformatics and Biomedicine,"Protein fold recognition is the prediction of protein's tertiary structure (Fold) given the protein's sequence without relying on sequence similarity. Using machine learning techniques for protein fold recognition, most of the state-of-the-art research has focused on more traditional algorithms such as support vector machines (SVM), k-nearest neighbor (KNN) and neural networks (NN). In this paper, we present an empirical study of two variants of boosting algorithms - AdaBoost and LogitBoost for the problem of fold recognition. Prediction accuracy is measured on a dataset with proteins from 27 most populated folds from the SCOP database, and is compared with results from other literature using SVM, KNN and NN algorithms on the same dataset. Overall, boosting methods achieve 60% fold recognition accuracy on an independent test protein dataset which is the highest prediction achieved when compared with the accuracy values obtained with other methods proposed in the literature. Boosting algorithms have the potential to build efficient classification models in a very fast manner.",biology
1014,Chaos Game Representation of Genomes and their Simulation by Recurrent Iterated Function Systems,International Conference on Bioinformatics and Biomedical Engineering,"Chaos game representation (CGR) of DNA sequences and linked protein sequences from genomes was proposed by Jeffrey (Nucleic Acid Research 18 (1990) 2163-2170) and Yu et al. (J. Theor. Biol, 226 (2004) 341-348), respectively. In this paper, we consider the CGR of three kinds of sequences from complete genomes: whole genome DNA sequences, linked coding DNA sequences and linked protein sequences. Some fractal patterns are found in these CGRs. A recurrent iterated function systems (RIFS) model is proposed to simulate the CGRs of these sequences from genomes and their induced measures. Numerical results on 50 genomes show that the RIFS model can simulate very well the CGRs and their induced measures . The parameters estimated in the RIFS model reflect information on species classification .",biology
1015,Searching for Supermaximal Repeats in Large DNA Sequences,Bioinformatics Research and Development," We study the problem of finding supermaximal repeats in large DNA sequences. For this, we propose an algorithm called SMR which uses an auxiliary index structure (POL), which is derived from and replaces the suffix tree index STTD64 [1]. The results of our numerous experiments using the 24 human chromosomes data indicate that SMR outperforms the solution provided as part of the Vmatch [2] software tool. In searching for supermaximal repeats of size at least 10 bases, SMR is twice faster than Vmatch; for a minimum length of 25 bases, SMR is 7 times faster; and for repeats of length at least 200, SMR is about 9 times faster. We also study the cost of POL in terms of time and space requirements. ",biology
1016,Incorporation of biological pathway knowledge in the construction of priors for optimal Bayesian classification,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Small samples are commonplace in genomic/proteomic classification, the result being inadequate classifier design and poor error estimation. The problem has recently been addressed by utilizing prior knowledge in the form of a prior distribution on an uncertainty class of feature-label distributions. A critical issue remains: how to incorporate biological knowledge into the prior distribution. For genomics/proteomics, the most common kind of knowledge is in the form of signaling pathways. Thus, it behooves us to find methods of transforming pathway knowledge into knowledge of the feature-label distribution governing the classification problem. In this paper, we address the problem of prior probability construction by proposing a series of optimization paradigms that utilize the incomplete prior information contained in pathways (both topological and regulatory). The optimization paradigms employ the marginal log-likelihood, established using a small number of feature-label realizations (sample points) regularized with the prior pathway information about the variables. In the special case of a Normal-Wishart prior distribution on the mean and inverse covariance matrix (precision matrix) of a Gaussian distribution, these optimization problems become convex. Companion website: gsp.tamu.edu/Publications/supplementary/shahrokh13a. ÂŠ 2004-2012 IEEE.",biology
1017,Comparing SNOMED CT and the NCI Thesaurus through Semantic Web Technologies,International Conference on Bioinformatics," Objective: The objective of this study is to compare two large biomedical terminologies, SNOMED CT and the National Cancer Institute (NCI) Thesaurus, through Semantic Web technologies. Methods: The two terminologies are converted into the Resource Description Framework (RDF) and loaded into a common triple store. The Unified Medical Language System (UMLS) is used to identify correspondences between concepts across terminologies. Concepts common to both terminologies are compared based on shared relations to other concepts. Results: A total of 20,369 pairs of equivalent SNOMED CT and NCI Thesaurus concepts were identified through the UMLS. The highest proportion of shared relata is for the superclasses traversed recursively (75% of the concepts share at least one superclass). Slightly more than half of the concepts studied share at least one associative relation (direct relation or inherited from some ancestor). Conclusions: Overall, SNOMED CT and NCI Thesaurus concepts exhibit a relatively small proportion of shared relata. Semantic Web technologies, including RDF and triple stores, are suitable for comparing large biomedical ontologies, at least from a quantitative perspective. ",biology
1018,Discovery of Spatially Cohesive Itemsets in Three-Dimensional Protein Structures,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"In this paper we present a cohesive structural itemset miner aiming to discover interesting patterns in a set of data objects within a multidimensional spatial structure by combining the cohesion and the support of the pattern. We propose two ways to build the itemset miner, VertexOne and VertexAll, in an attempt to find a balance between accuracy and run-times. The experiments show that VertexOne performs better, and finds almost the same itemsets as VertexAll in a much shorter time. The usefulness of the method is demonstrated by applying it to find interesting patterns of amino acids in spatial proximity within a set of proteins based on their atomic coordinates in the protein molecular structure. Several patterns found by the cohesive structural itemset miner contain amino acids that frequently co-occur in the spatial structure, even if they are distant in the primary protein sequence and only brought together by protein folding. Further various indications were found that some of the discovered patterns seem to represent common underlying support structures within the proteins.",biology
1019,Nonlinear Numerical Optimization Technique Based on a Genetic Algorithm for Inverse Problems: Towards the Inference of Genetic Networks,German Conference on Bioinformatics," Nonlinear systems, such as metabolic pathways and genetic networks have richly complex structures and the details of the mechanism at the molecular level that govern interactions among system components are generally not well known. Estimation of the interaction mechanisms among system components via experimentally observed dynamic responses (time courses) of some of the system components is generally an inverse problem. The S system, which belongs to power law formalism, is one of the best representations to solve this type of inverse problem: the S system is rich enough in structure to capture all relevant dynamics. In the present paper, for the purpose of solving the inverse problem, we apply the Genetic Algorithm and propose an efficient procedure for the estimation of large numbers of parameters in the S system formalism. The proposed procedure enables genetic network architecture to be reconstructed based on experimentally observed time courses in gene expression. ",biology
1020,Use of ternary similarities in graph based clustering for protein structural family classification,International Conference on Bioinformatics,"Classification of proteins 3D structures into structural families is reformulated in terms of graph based clustering of objects which are modular as similarities between two 3D structures relies on the local similarities of their matching substructures. Similarities between 3D structures are then represented as edges connecting objectsin a graph. Applying clustering algorithms to such a graph results in the following drawback: subsets of more than two 3D structures belonging to the same cluster may share no similar substructure. To overcome this drawback we propose to introduce constraints about ternary similarities, i.e. constraints on triples of objects. The 3D structures graph is first transformed into its line graph, that represents the adjacencies between the graph edges. The ternary constraints are applied on the line graph, and a maximal line graph is then extracted from the modified line graph. The corresponding 3D structures graph now satisfies the above mentioned ternary constraints. In our experiments applying clustering on the new graph results in a more stable classification which is coherent with the expert classification SCOP. Copyright 20 ACM.",biology
1021,Mining Clinical Data with a Temporal Dimension: A Case Study,Bioinformatics and Biomedicine,"Clinical databases store large amounts of information about patients and their medical conditions. Data mining techniques can extract relationships and patterns holding in this wealth of data, and thus be helpful in understanding the progression of diseases and the efficacy of the associated therapies. A typical structure of medical data is a sequence of observations of clinical parameters taken at different time moments. In this kind of contexts, the temporal dimension of data is a fundamental variable that should be taken in account in the mining process and returned as part of the extracted knowledge. Therefore, the classical and well established framework of sequential pattern mining is not enough, because it only focuses on the sequentiality of events, without extracting the typical time elapsing between two particular events. Time-annotated sequences (IAS), is a novel mining paradigm that solves this problem. Recently defined in our laboratory together with an efficient algorithm for extracting them, IAS are sequential patterns where each transition between two events is annotated with a typical transition time that is found frequent in the data. In this paper we report a real-world medical case study, in which the IAS mining paradigm is applied to clinical data regarding a set of patients in the follow-up of a liver transplantation. The aim of the data analysis is that of assessing the effectiveness of the extracorporeal photopheresis (ECP) as a therapy to prevent rejection in solid organ transplantation. For each patient, a set of biochemical variables is recorded at different time moments after the transplantation. The IAS patterns extracted show the values of interleukins and other clinical parameters at specific dates, from which it is possible for the physician to assess the effectiveness of the ECP therapy. We believe that this case study does not only show the interestingness of extracting IAS patterns in this particular context but, more ambitiously, - it suggests a general methodology for clinical data mining, whenever the time dimension is an important variable of the problem in analysis.",biology
1022,Framework for a protein ontology,International Conference on Bioinformatics," Biological ontologies are part of caBIG initiatives critical for cancer data integration and sharing through the grid environment. A number of these ontologies describe the properties that can be attributed to proteins (See right figure). But the lacking of an appropriate ontology for protein objects and classes has impeded the genome/proteome annotation, data integration and analysis. PRotein Ontology (PRO) is designed to provide this ontological framework by: Providing a structure to support formal, computer-based inferences of shared attributes among homologous proteins Delineating the multiple protein forms of a gene locus Interconnecting existing OBO foundry ontologies Providing an enabling technology for knowledge discovery 1 phosphorylation Smad 2 P P Smad 4 P 2 complex formation Smad 4 P 3 nuclear translocation 4 DNA binding Transcription Regulation 1 phosphorylation Smad 2 P P Smad 4 P Transcription Regulation PRO:00019419 Smad2 sequence 1 phosphorylated in the loop region by Ca(2+)-calmodulin-dependent protein kinase I (CAMK2) participates_in GO:0007179 signal transduction [PMID: 11027280] llhoaaccska_stef_udfnu_cnintciotinon GGGOOO:::000000040563737331723cStrymatonaspdclarbsipimntidoi[nnPgMco[IPDaMc:t1IivD1a:0t12o1r702a28c7t0iv2]i8ty0][PMID: 11027280] has_modification MOD: 00046 O-phosphorylated L-serine UniProtKB: Q15796-1 (phosphorylated Ser-240/Ser-465/Ser-467) ",biology
1023,Using Gene Pair Combinations to Improve the Accuracy of the PAM Classifier,Bioinformatics and Biomedicine,"Various classification methods have been used to predict the class of tissue samples based on gene expression data. prediction analysis for microarrays (PAM) is one of the top classifiers that has been extensively used for cancer classification. In this paper a novel method of combining expression data from gene pairs is used to improve the overall accuracy of PAM. Recent studies suggest that deregulation of pathways, rather than individual genes, may be critical in triggering carcinogenesis. The pathway deregulation is often caused by the simultaneous deregulation of more than one genes in the pathway. Robust gene pair combinations may exploit these underlying bio-molecular reactions to provide better biomarkers for cancer, as compared to single genes. In this work, we used gene pair combinations, called doublets, to improve the accuracy of PAM. We validated the proposed approach with nine cancer datasets. The accuracy of PAM, using these doublets, increased consistently across these datasets, in some cases with a significant margin (13%).",biology
1024,Searching Genomes for Noncoding RNA Using FastR,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"The discovery of novel noncoding RNAs has been among the most exciting recent developments in biology. It has been hypothesized that there is, in fact, an abundance of functional noncoding RNAs (ncRNAs) with various catalytic and regulatory functions. However, the inherent signal for ncRNA is weaker than the signal for protein coding genes, making these harder to identify. We consider the following problem: Given an RNA sequence with a known secondary structure, efficiently detect all structural homologs in a genomic database by computing the sequence and structure similarity to the query. Our approach, based on structural filters that eliminate a large portion of the database while retaining the true homologs, allows us to search a typical bacterial genome in minutes on a standard PC. The results are two orders of magnitude better than the currently available software for the problem. We applied FastR to the discovery of novel riboswitches, which are a class of RNA domains found in the untranslated regions. They are of interest because they regulate metabolite synthesis by directly binding metabolites. We searched all available eubacterial and archaeal genomes for riboswitches from purine, lysine, thiamin, and riboflavin subfamilies. Our results point to a number of novel candidates for each of these subfamilies and include genomes that were not known to contain riboswitches. © 2005 IEEE.",biology
1025,A Study of GIS Based Risk Assessment of Meteorological Disasters in Tibet,International Conference on Bioinformatics and Biomedical Engineering,"Tibet is of broad expanse and greatly varied natural conditions. Due to the serious meteorological disasters, there are heavy disaster prevention and mitigation tasks. However, all places are of different disaster mitigation objectives therefore it's a basic principle to implement zone disaster mitigation measures. By using the disaster information from the 38 meteorological stations in Tibetan, utilizing the assessment model of meteorological disaster and thought of signal processing and based on the application of GIS technology to the risk index of disaster, the risk assessment and division of 7 meteorological disasters such as drought, flood, snow disaster, frost, hail, gale and thunderstorm are made. This improves the spatial density and accuracy of the risk assessment and division of disasters in Tibet and provides a solid foundation for the zonal risk mitigation measures of Tibet, which is also of significance to the cognition of the plateau climate and disasters.",biology
1026,Proximity Measures for Clustering Gene Expression Microarray Data: A Validation Methodology and a Comparative Analysis,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Cluster analysis is usually the first step adopted to unveil information from gene expression microarray data. Besides selecting a clustering algorithm, choosing an appropriate proximity measure (similarity or distance) is of great importance to achieve satisfactory clustering results. Nevertheless, up to date, there are no comprehensive guidelines concerning how to choose proximity measures for clustering microarray data. Pearson is the most used proximity measure, whereas characteristics of other ones remain unexplored. In this paper, we investigate the choice of proximity measures for the clustering of microarray data by evaluating the performance of 16 proximity measures in 52 data sets from time course and cancer experiments. Our results support that measures rarely employed in the gene expression literature can provide better results than commonly employed ones, such as Pearson, Spearman, and euclidean distance. Given that different measures stood out for time course and cancer data evaluations, their choice should be specific to each scenario. To evaluate measures on time-course data, we preprocessed and compiled 17 data sets from the microarray literature in a benchmark along with a new methodology, called Intrinsic Biological Separation Ability (IBSA). Both can be employed in future research to assess the effectiveness of new measures for gene time-course data. © 2013 IEEE.",biology
1027,Uncovering Genomic Reassortments among Influenza Strains by Enumerating Maximal Bicliques,Bioinformatics and Biomedicine,"The evolutionary histories of viral genomes have received significant recent attention due to their importance in understanding virulence and the corresponding ramifications to public health. We present a novel framework to detect reassortment events in influenza based on the comparison of two distributions of phylogenetic trees, rather than a pair of, possibly unreliable, consensus trees. We show how to detect all high-probability inconsistencies between two distributions of trees by enumerating maximal bicliques within a defined incompatibility graph. In the process, we give the first quadratic delay algorithm for enumerating maximal bicliques within general bipartite graphs. We demonstrate the utility of our approach by applying it to several sets of influenza genomes (both human- and avian-hosted) and successfully identify all known reassortment events and a few novel candidate reassortments. In addition, on simulated datasets, our approach correctly finds implanted reassortments and rarely detects reassortments where none were introduced.",biology
1028,Correlation between Gene Expression and GO Semantic Similarity,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"This research analyzes some aspects of the relationship between gene expression, gene function, and gene annotation. Many recent studies are implicitly based on the assumption that gene products that are biologically and functionally related would maintain this similarity both in their expression profiles as well as in their Gene Ontology (GO) annotation. We analyze how accurate this assumption proves to be using real publicly available data. We also aim to validate a measure of semantic similarity for GO annotation. We use the Pearson correlation coefficient and its absolute value as a measure of similarity between expression profiles of gene products. We explore a number of semantic similarity measures (Resnik, Jiang, and Lin) and compute the similarity between gene products annotated using the GO. Finally, we compute correlation coefficients to compare gene expression similarity against GO semantic similarity. Our results suggest that the Resnik similarity measure outperforms the others and seems better suited for use in Gene Ontology. We also deduce that there seems to be correlation between semantic similarity in the GO annotation and gene expression for the three GO ontologies. We show that this correlation is negligible up to a certain semantic similarity value; then, for higher similarity values, the relationship trend becomes almost linear. These results can be used to augment the knowledge provided by clustering algorithms and in the development of bioinformatic tools for finding and characterizing gene products. © 2005 IEEE.",biology
1029,Using Uncorrelated Discriminant Analysis for Tissue Classification with Gene Expression Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics," -The classification of tissue samples based on gene expression data is an important problem in medical diagnosis of diseases such as cancer. In gene expression data, the number of genes is usually very high (in the thousands) compared to the number of data samples (in the tens or low hundreds); that is, the data dimension is large compared to the number of data points (such data is said to be undersampled). To cope with performance and accuracy problems associated with high dimensionality, it is commonplace to apply a preprocessing step that transforms the data to a space of significantly lower dimension with limited loss of the information present in the original data. Linear Discriminant Analysis (LDA) is a well-known technique for dimension reduction and feature extraction, but it is not applicable for undersampled data due to singularity problems associated with the matrices in the underlying representation. This paper presents a dimension reduction and feature extraction scheme, called Uncorrelated Linear Discriminant Analysis (ULDA), for undersampled problems and illustrates its utility on gene expression data. ULDA employs the Generalized Singular Value Decomposition method to handle undersampled data and the features that it produces in the transformed space are uncorrelated, which makes it attractive for gene expression data. The properties of ULDA are established rigorously and extensive experimental results on gene expression data are presented to illustrate its effectiveness in classifying tissue samples. These results provide a comparative study of various state-of-the-art classification methods on well-known gene expression data sets. ",biology
1030,Algorithms to Detect Multiprotein Modularity Conserved during Evolution,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Detecting essential multiprotein modules that change infrequently during evolution is a challenging algorithmic task that is important for understanding the structure, function, and evolution of the biological cell. In this paper, we define a measure of modularity for interactomes and present a linear-time algorithm, Produles, for detecting multiprotein modularity conserved during evolution that improves on the running time of previous algorithms for related problems and offers desirable theoretical guarantees. We present a biologically motivated graph theoretic set of evaluation measures complementary to previous evaluation measures, demonstrate that Produles exhibits good performance by all measures, and describe certain recurrent anomalies in the performance of previous algorithms that are not detected by previous measures. Consideration of the newly defined measures and algorithm performance on these measures leads to useful insights on the nature of interactomics data and the goals of previous and current algorithms. Through randomization experiments, we demonstrate that conserved modularity is a defining characteristic of interactomes. Computational experiments on current experimentally derived interactomes for Homo sapiens and Drosophila melanogaster, combining results across algorithms, show that nearly 10 percent of current interactome proteins participate in multiprotein modules with good evidence in the protein interaction data of being conserved between human and Drosophila. © 2012 IEEE.",biology
1031,SequenceJuxtaposer: Fluid navigation for large-scale sequence comparison in context,German Conference on Bioinformatics," SequenceJuxtaposer supports interaction at 20 frames per second when browsing collections SequenceJuxtaposer is a sequence visualiza- of several hundred sequences that comprise over tion tool for the exploration and comparison of 1.7 million total base pairs. We show three exbiomolecular sequences. We use an information ample applications with large, publicly availvisualization technique called accordion draw- able datasets, and we are able to quickly observe ing that guarantees three key properties: con- many features that had previously required sigtext, visibility, and frame rate. We provide con- ni cant analysis to discover. text through the navigation metaphor of a rubber sheet that can be smoothly stretched to show Keywords: sequence analysis, motif and gene ndmore details in the areas of focus, while the ing, data visualization, Focus+Context surrounding regions of context are correspondingly shrunk. Landmarks, such as user specied motifs or differences between aligned base 1 Introduction pairs across multiple sequences, are guaranteed to be visible even if located in the shrunken ar- Biomolecular sequence comparisons are esseneas of context. Our graphics infrastructure for tial in understanding underlying genomic patprogressive rendering provides immediate re- terns. Current sequence browsers [14, 17, 34, sponsiveness to user interaction by guaranteeing 37] support examining the data at any level from that we redraw the scene at a target frame rate. a global overview down to a detailed view of a Our preprocessing algorithms are subquadratic: small section, where the interaction happens as O(nk) for k sequences of n base pairs each. a discrete jump from one level of magni cation All runtime rendering algorithms are sublinear to the next. At high levels of magni cation only in nk: they are O(v) where v is the number of a very small subset of the sequence is visible, so items visible onscreen at once, and v nk. it is easy to lose track of the current location and its position with respect to other areas of interDept of Computer Science, U. of British est. The cognitive load of maintaining a mental Columbia, Vancouver, BC V6T 1Z4, Canada, model of navigation history is very high, and hufjslack,hilde,tmmg@cs.ubc.ca. mans have a very limited capacity to do so [38]. yDept Media Systems, Bauhaus U., Weimar, Germany, Exploration often entails frequent backtracking krizsDtieapnt.ohfiMldaethbr&anCdo@mmepduiteenr.Sucniie-nwcee,iLmaerh.mdaen. College where people remind themselves of what they & the Graduate Center, City U. of New York, Bronx, NY have already seen and where they are now in re10468, USA, stjohn@lehman.cuny.edu. lation to previous viewpoints. ",biology
1032,A Novel Biologically and Psychologically Inspired Fuzzy Decision Support System: Hierarchical Complementary Learning,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"A computational intelligent system that models the human cognitive abilities may promise significant performance in problem learning because a human is effective in learning and problem solving. Functionally modeling the human cognitive abilities not only avoids the details of the underlying neural mechanisms performing the tasks but also reduces the complexity of the system. The complementary learning mechanism is responsible for human pattern recognition, that is, a human attends to positive and negative samples when making a decision. Furthermore, human concept learning is organized in a hierarchical fashion. Such hierarchical organization allows the divide-and-conquer approach to the problem. Thus, integrating the functional models of hierarchical organization and complementary learning can potentially improve the performance in pattern recognition. Hierarchical complementary learning (HCL) exhibits many of the desirable features of pattern recognition. It is further supported by the experimental results that verify the rationale of the integration and that the HCL system is a promising pattern recognition tool. © 2008 IEEE.",biology
1033,A multi-objective program for quantitative subtyping of clinically relevant phenotypes,Bioinformatics and Biomedicine,"Identifying genetic variations that underlie human disease is very important to advance our understanding of the disease's pathophysiology and promote its personalized treatment. However, many disease phenotypes have complex clinical manifestations and a complicated etiology. Gene finding efforts for complex diseases have had limited success to date. Research results suggest that one way to enhance these efforts is to differentiate subtypes of a complex multifactorial disease phenotype. Existing subtyping methods rely on cluster analysis using only clinical features of a disorder without guidance from genetic data, resulting in subtypes for which genotype association may be limited. In this work, we seek to derive a novel computational method based on multi-objective programming that is capable of clinically categorizing a disease phenotype so as to discover genetically different subtypes. Our approach optimizes two objectives: (1) the cluster-derived subtypes should differ significantly on clinical features; (2) these subtypes can be well separated using candidate genes. This work has been motivated by clinical studies of opioid dependence, a serious, prevalent disorder that is heterogeneous phenotypically. Analyses on a sample of 1,470 European American subjects aggregated from multiple genetic studies of opioid dependence show that the proposed algorithm is superior to existing subtyping methods.",biology
1034,Meta-analysis of Genomic and Proteomic Features to Predict Synthetic Lethality of Yeast and Human Cancer,International Conference on Bioinformatics," A major goal in cancer medicine is to find selective drugs with reduced side-effect. A pair of genes is called synthetic lethality (SL) if mutations of both genes will kill a cell while mutation of either gene alone will not. Hence, a gene in SL interactions with a cancer-specific mutated gene will be a promising drug target with anti-cancer selectivity. Wetlab screening approach is still so costly that even for yeast only a small fraction of gene pairs has been covered. Computational methods are therefore important for large-scale discovery of SL interactions. Most existing approaches focus on individual features or machine learning methods, which are prone to noise or overfitting. In this paper, we propose an approach of meta-analysis that integrates 17 genomic and proteomic features and the outputs of 10 classification methods. It thus combines the strengths of existing methods. It also adjusts relative contributions of multiple methods with weights learned from the training data. Running on a dataset of the yeast strain of S. cerevisiae, our method achieves AUC (area under ROC curve) of 87.2% the highest among all competitors. Moreover, through orthologous mapping from yeast to human genes, we predicted a list of SL pairs in human that contain top mutated genes in lung and breast cancers recently reported by The Cancer Genome Atlas (TCGA). Our method and predictions would shed light on mechanisms of SL and lead to discovery of novel anti-cancer drugs. ",biology
1035,Extracting Dynamics from Static Cancer Expression Data,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"Static expression experiments analyze samples from many individuals. These samples are often snapshots of the progression of a certain disease such as cancer. This raises an intriguing question: Can we determine a temporal order for these samples? Such an ordering can lead to better understanding of the dynamics of the disease and to the identification of genes associated with its progression. In this paper we formally prove, for the first time, that under a model for the dynamics of the expression levels of a single gene, it is indeed possible to recover the correct ordering of the static expression datasets by solving an instance of the traveling salesman problem (TSP). In addition, we devise an algorithm that combines a TSP heuristic and probabilistic modeling for inferring the underlying temporal order of the microarray experiments. This algorithm constructs probabilistic continuous curves to represent expression profiles leading to accurate temporal reconstruction for human data. Applying our method to cancer expression data we show that the ordering derived agrees well with survival duration. A classifier that utilizes this ordering improves upon other classifiers suggested for this task. The set of genes displaying consistent behavior for the determined ordering are enriched for genes associated with cancer progression. ÂŠ 2006 IEEE.",biology
1036,An algorithm to compute the full set of many-to-many stable matchings,Mathematical Social Sciences,"The paper proposes an algorithm to compute the full set of many-to-many stable matchings when agents have substitutable preferences. The algorithm starts by calculating the two optimal stable matchings using the deferred-acceptance algorithm. Then, it computes each remaining stable matching as the firm-optimal stable matching corresponding to a new preference profile, which is obtained after modifying the preferences of a previously identified sequence of firms. © 2003 Elsevier B.V. All rights reserved.",math
1037,Relative entropy maximization and directed diffusion equations,Mathematical Methods in The Applied Sciences,"Motivated by the concept of maximum entropy methods in signal and image processing, we introduce and discuss a class of ‘directed diffusion equations’ with suitable boundary conditions. The paradigmatic ‘directed diffusion equation’ is The relative entropy is rapidly increasing along solution trajectories of (i). This suggests that solving (i) will yield efficient procedures for entropy maximization. We also discuss the asymptotic behavior of solutions of (i)—this is readily done because (i) has a large family of Ljapunov functionals.",math
1038,On the secant varieties to the tangential varieties of a Veronesean,Proceedings of the American Mathematical Society,"We study the dimensions of the higher secant varieties to the tangent varieties of Veronese varieties. Our approach, generalizing that of Terracini, concerns 0-dimensional schemes which are the union of second infinitesimal neighbourhoods of generic points, each intersected with a generic double line. We find the deficient secant line varieties for all the Veroneseans and all the deficient higher secant varieties for the quadratic Veroneseans. We conjecture that these are the only deficient secant varieties in this family and prove this up to secant projective 4-spaces.",math
1039,"The $L(2,1)$-Labeling Problem on Graphs",SIAM Journal on Discrete Mathematics," 1. Introduction. The channel assignment problem is to assign a channel (nonnegative integer) to each radio transmitter so that interfering transmitters are assigned channels whose separation is not in a set of disallowed separations. Hale [11] formulated this problem into the notion of the T-coloring of a graph, and the T-coloring problem has been extensively studied over the past decade (see [4, 5, 7, 13, 14, 16, 17, 19]). Roberts [15] proposed a variation of the channel assignment problem in which ""close"" transmitters must receive different channels and ""very close"" transmitters must receive channels that are at least two channels apart. To formulate the problem in graphs, the transmitters are represented by the vertices of a graph; two vertices are ""very close"" if they are adjacent in the graph and ""close"" if they are of distance two in the graph. More precisely, an L(2, 1)-labeling of a graph G is a function f from the vertex set V(G) to the set of all nonnegative integers such that If(x)- f(Y)l >- 2 if d(x, y) 1 and If(x) f(Y)l -> 1 if d(x, y) 2. A k-L(2, 1)-labeling is an L(2, 1)labeling such that no label is greater than k. The L(2, 1)-labeling number of G, denoted by A(G), is the smallest number k such that G has a k-L(2, 1)-labeling. showed that i(Q_n) _< 2n + 1 for n _> 5. They also determined A(Q) for n _< 5 and Griggs and Yeh [10] and Yeh [21] determined the exact values of A(P), A(C), and A(W), where P is a path of n vertices, Cn is a cycle of n vertices, and Wn is an n-wheel obtained from Cn by adding a new vertex adjacent to all vertices in C. For the n-cube Q, Jonas [12] showed that n + 3 _< A(Q). Griggs and Yeh [10] conjectured that the lower bound n + 3 is the actual value of A(Q) for n _> 3. Using a coding theory method, Whittlesey, Georges, and Mauro [20] proved that /(Qn) 2k + 2k-q+1 2, where n _< 2k -q and 1 _< q _< k + 1. ",math
1040,Probabilities of causation: Bounds and identification,Annals of Mathematics and Artificial Intelligence," This paper deals with the problem of estimating the probability of causation, that is, the probability that one event was the real cause of another, in a given scenario. Starting from structural-semantical definitions of the probabilities of necessary or sufficient causation (or both), we show how to bound these quantities from data obtained in experimental and observational studies, under general assumptions concerning the data-generating process. In particular, we strengthen the results of Pearl [39] by presenting sharp bounds based on combined experimental and nonexperimental data under no process assumptions, as well as under the mild assumptions of exogeneity (no confounding) and monotonicity (no prevention). These results delineate more precisely the basic assumptions that must be made before statistical measures such as the excess-risk-ratio could be used for assessing attributional quantities such as the probability of causation. ",math
1041,Multiple Random Walks in Random Regular Graphs,SIAM Journal on Discrete Mathematics," We study properties of multiple random walks on a graph under various assumptions of interaction between the particles. To give precise results, we make the analysis for random regular graphs. The cover time of a random walk on a random r-regular graph was studied in [6], where it was shown with high probability (whp), that for r ≥ 3 the cover time is asymptotic to θrn ln n, where θr = (r − 1)/(r − 2). In this paper we prove the following (whp) results, arising from the study of multiple random walks on a random regular graph G. For k independent walks on G, the cover time CG(k) is asymptotic to CG/k, where CG is the cover time of a single walk. For most starting positions, the expected number of steps before any of the walks meet is θrn/ k2 . If the walks can communicate when meeting at a vertex, we show that, for most starting positions, the expected time for k walks to broadcast a single piece of information to each other is asymptotic to 2 ln k θrn, as k, n → ∞. k We also establish properties of walks where there are two types of particles, predator and prey, or where particles interact when they meet at a vertex by coalescing, or by annihilating each other. For example, the expected extinction time of k explosive particles (k even) tends to (2 ln 2)θrn as k → ∞. The case of n coalescing particles, where one particle is initially located at each vertex, corresponds to a voter model defined as follows: Initially each vertex has a distinct opinion, and at each step each vertex changes its opinion to that of a random neighbour. The expected time for a unique opinion to emerge is the same as the expected time for all the particles to coalesce, which is asymptotic to 2θrn. ",math
1042,A Simple but Realistic Model of Floating-Point Computation,ACM Transactions on Mathematical Software,"A model of floating-point computation, intended as a basis for efficient portable mathematical software, is presented. The model involves only simple familiar concepts, expressed in a small set of environment parameters. Using these, both a program and its documentation can tailor themselves to the host computer. First, the model numbers, a conventional four-parameter system of floating-point numbers, are introduced. The parameters are the base, the precision, and the minimum and maximum exponents. They must be chosen so that the result of a basic arithmetic operation on model numbers is no less accurate than the result that would be obtained by chopped arithmetic in the model system. Also, the result of a basic operation on operands between model numbers must be bounded by the results of the same operation on the neighboring model numbers. These ideas are summarized in a few fundamental axioms, which enable the machine-independent properties of numerical programs to be stated and proved. The main conclusion is that the model supports conventional, worst case, forward or backward error analyses with only minor qualifications. The model is simple in the sense that its axioms are more easily stated and understood than the detailed operational rules of most floating-point processors. It is realistic in the sense that real computers exhibit most of the forms of behavior (or misbehavior) that it allows, but nearly always conform to its rules. ÂŠ 1981, ACM. All rights reserved.",math
1043,Nonlinear filters for efficient shock computation,Mathematics of Computation, A new type of methods for the numerical approximation of hyperbolic conservation laws with discontinuous solution is introduced. The methods are based on standard finite difference schemes. The difference solution is processed with a nonlinear conservation form filter at every time level to eliminate spurious oscillations near shocks. It is proved that the filter can control the total variation of the solution and also produce sharp discrete shocks. The method is simpler and faster than many other high resolution schemes for shock calculations. Numerical examples in one and two space dimensions are presented. ,math
1044,An Algorithm for Finding Best Matches in Logarithmic Expected Time,ACM Transactions on Mathematical Software, An algorithm and data structure in part by U.S. Energy Research and Development under contract E(O43)515 ,math
1045,On the product of semi-groups of operators,Proceedings of the American Mathematical Society," 1. Introduction. We consider semi-groups of operators on a Banach space X, which are of class (Co) in the terminology of [3]. Such a semi-group is a family of bounded operators Tt, defined for all OO and satisfying the semi-group condition Let Tt be another semi-group erator fi'. If Tt and TI commute that of class (Co), with infinitesimal genfor all values of s and t, it is obvious condition generator is the operator is again a semi-group of class (Co), for any fixed positive a. If the commutativity does not hold, we may still attempt to define ""product"" semi-groups Sait by Theorem 1. If Tt and Ti commute for all values of t and s, then for any positive a, the closure of fi+afi' generates the semi-group Ua,t defined by (4). ",math
1046,The simplest cubic fields,Mathematics of Computation,"The cyclic cubic fields generated by x = ax + (a + 3)x + 1 are studied in detail. The regulators are relatively small and are known at once. The class numbers are always of the form A + 3B, are relatively large and easy to compute. The class groups are usually easy to determine since one has the theorem that if m is divisible only by primes = 2 (mod 3), then the m-rank of the class group is even. Fields with different 3-ranks are treated separately. ÂŠ 1974, American Mathematical Society.",math
1047,Models and Algorithms for Stochastic Online Scheduling,Mathematics of Operations Research," We introduce a model for non-preemptive scheduling under uncertainty. In this model, we combine the main characteristics of online and stochastic scheduling in a simple and natural way. Job processing times are assumed to be stochastic, but in contrast to traditional stochastic scheduling models, we assume that jobs arrive online, and there is no knowledge about the jobs that will arrive in the future. The particular setting we analyze is parallel machine scheduling, with the objective to minimize the total weighted completion times of jobs. We propose simple, combinatorial online scheduling policies for that model, and derive performance guarantees that match the currently best known performance guarantees for stochastic and online parallel machine scheduling. For processing times that follow NBUE distributions, we improve upon previously best known performance bounds from stochastic scheduling, even though we consider a more general setting. ",math
1048,New candidates welcome! Possible winners with respect to the addition of new candidates,Mathematical Social Sciences," In voting contexts, some new candidates may show up in the course of the process. In this case, we may want to determine which of the initial candidates are possible winners, given that a fixed number k of new candidates will be added. We give a computational study of this problem, focusing on scoring rules, and we provide a formal comparison with related problems such as control via adding candidates or cloning. In many real-life collective decision making situations, the set of candidates (or alternatives) may vary while the voting process goes on, and may change at any time before the decision is final: some new candidates may join, whereas some others may withdraw. This, of course, does not apply to situations where the vote takes place in a very short period of time (such as, typically, political elections in most countries), and neither does the addition of new candidates during the process apply to situations where the law forbids new candidates to be introduced after the voting process has started (which, again, is the case for most political elections). However, there are quite many practical settings where this may happen, especially situations where votes are sent by email during an extended period of time. This is typically the case when making a decision about the date and time of a meeting. In the course of the process, we may learn that the room is taken at a given time slot, making this time slot no longer a candidate. The opposite case also occurs frequently; we thought the room was taken on a given date and then we learn that it has become available, making this time slot a new candidate. The paper focuses on candidate addition only. More precisely, the class of situations we consider is the following. A set of voters have expressed their votes about a set of (initial) candidates. Then some new ",math
1049,Convex bodies with few faces,Proceedings of the American Mathematical Society," then the volume of the symmetric convex body whose boundary functionals are ±ux , ... , ±un , is bounded from below as \{xeKk: \{x, u¡)\ < 1 for every i}\l/k >\fyfpr. In [V], Vaaler proved that if Qn - [-\ , %]* is the central unit cube in R"" and U is a subspace of R"" then the volume \U (~Qln\, of the section of Qn by U is at least 1. This result may be reformulated as follows: if ux, ... ,un are vectors in R , I < k < n whose Euclidean lengths satisfy J2"" lw,l ^ k then |{a'gR*: |(x, ut)\ < 1 for every i}\x/k > 2. ",math
1050,On approximately convex functions,Proceedings of the American Mathematical Society," A real valued function f de ned on a real interval I is called (""; )-convex if it satis es for all x; y 2 D, t 2 [0; 1]. The basic result obtained by Hyers and Ulam states that if the underlying space X is of nite dimension, then f can be written as f = g + h, where g is a convex function and h is a bounded function whose supremum norm is not larger than kn , where the positive constant kn depends only on the dimension n of the underlying space X . Hyers and Ulam proved that kn (n(n + 3))=(4(n + 1)). Green [Gre52], Cholewa [Cho84] obtained much better estimations of kn showing that asymptotically kn is not bigger than (log2(n))=2. Laczkovich [Lac99] compared this constant to several other dimension-depending stability constants and proved that it is not less than (log2(n=2))=4. This result shows that there is no stability results for in nite dimensional spaces X . A counterexample in this direction was earlier constructed by Casini and Papini [CP93]. The stability aspects of -convexity are discussed by Ger [Ger94]. If t = 1=2 and (1) holds for all x; y 2 D, then f is called a -Jensen-convex function. There is no analogous decomposition for -Jensen-convex functions by Received by the editors April 2, 2001 and, in revised form, September 4, 2001. 2000 Mathematics Subject Classi cation. Primary 26A51, 26B25. ",math
1051,Decay rates for inverses of band matrices,Mathematics of Computation, Spectral theory and classical approximation theory are used to give a new proof of the exponential decay of the entries of the inverse of band matrices. The rate of decay oí A'1 can be bounded in terms of the (essential) spectrum of AA* for general A and in terms of the (essential) spectrum of A for positive definite A. In the positive definite case the bound can be attained. These results are used to establish the exponential decay for a class of generalized eigenvalue problems and to establish exponential decay for certain sparse but nonbanded matrices. We also establish decay rates for certain generalized inverses. ,math
1052,Some estimates of norms of random matrices,Proceedings of the American Mathematical Society, We show that for any random matrix (Xij ) with independent mean zero entries ,math
1053,NetNeg: A connectionist-agent integrated system for representing musical knowledge,Annals of Mathematics and Artificial Intelligence," Thesystempresented here showsthe feasibility of modcling the knowledgeinvolved in a complexmusical activity by integrating sub-symbolic and symbolic processes. This research focuses on the question of whether there is any advantagein integrating a neural network together with a distributed artificial intelligence approach within the music domain. Thesystem is designed to perform in real time and to be used for interactive computermusiccompositionor performance. The hybrid approach introduced in this workenables the musician to encodehis knowledgeand aesthetic taste into different modules.This is doneby applyingthree distinct functions: rules, fuzzy concepts, andlearning. As a case study, we began experimenting with first species two-part counterpoint melodies. Wehave developed a hybrid system composedof a connectionist moduleand an agent-based moduleto combinethe subsymbolicand symboliclevels to achieve this task. The technique presented here to represent musical knowledge constitutes a newapproach for composingpolyphonic music. ",math
1054,An Iterative Method of Solving a Game,Annals of Mathematics," Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at http://www.jstor.org/action/showPublisher?publisherCode=annals. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. ",math
1055,Deterministic Pivoting Algorithms for Constrained Ranking and Clustering Problems,Mathematics of Operations Research," criterion is due to Kemeny [7]). In rank aggregation, for We introduce new problems of nding minimum-cost rank- example, we are given k rankings of the same n objects, ings and clusterings which must be consistent with certain and want to combine these into one ranking that minconstraints (e.g. an input partial order in the case of ranking problems); we give deterministic approximation algo- imizes the sum over all pairs i; j such that i is ordered rithms for these problems. Randomized approximation al- before j of the number of input rankings that ordered gorithms for unconstrained versions of these problems were j before i. The partial order constraint allows us to aginvdenChbyariAkailron[1,].ChFainridkianrg, danetderNmeiwnimstaicn a[p2p]raonxdimbaytioAnilaoln- specify a priori information about the output ordering. gorithms for these problems answers an open question of In the clustering problems, we wish to partition a set Ailon et al. [2]. of objects into clusters, and are given (again, possibly straiInnedpawrteiicguhltaerd, wfeeegdibvaecdketaerrcmsinetistiinc atloguorrnitahmmesntfso,r ccoonn-- contradictory) information about the relation between strained correlation clustering, and constrained hierarchical any pair of objects. Consensus clustering is similar to clustering related to nding good ultrametrics. Our algo- rank aggregation in that we want to combine multiple raitphamrtsicfuollalorwvetrhteexpaarsadaigpmivootf aAnildonpaerttiatilo.n[2in] gofthcehogorsaipnhg input clusterings into a single clustering. In correlation according to the pivot; unlike their algorithms, we do not clustering, the input information consists of a `+' indichoose the pivot randomly but rather use an LP relaxation cating that a pair would prefer to be clustered together tuosecohfotohsee LaPgoreoldaxpaitviootn daelltoewrmsiunsisttoiciamllpy.oseAcdodnitsitornaainlltys, etahse- or ` ' indicating that a pair would prefer to be sepaily and analyze the results. In several cases we are able rated. We also can specify consistent information that to nd approximation factors for the constrained problems pairs of objects must be clustered together or must not tshtraatiniemdpcraovsees.onWtehealsfaocgtoivres athceoymobbitnaaitnoerdialfoarlgtohreithumncofonr- be clustered together in the nal clustering; this conconstrained weighted feedback arc set in tournaments with straint plays the same role as the input partial order in weights satisfying probability constraints. This algorithm the ranking problems. Hierarchical clustering is a genicmomprboivneastoornialthaelgobreistthmknsofworntfhaectuonrcognivsetnraibnyeddectaesrem.inistic eralization of correlation clustering. We want to nd a nested clustering of the elements, and are given in1 Introduction formation for every pair of elements and for every level ItccogppwTnhroforaiheneorsreeptihtbtacehairlposactaien:hsrnlomdcosriioppbtscaerar,naltsndeapoidkwmreiwreniryaernoise,)tgftdrhwahawioraaenanetbengfthkcjodggcioeermnroicncmevgitnlsingesunaisoan,pdisitftdmuteiaioaeetrtolirhnornzcisineennaseoagsgenotffbovbwtpiobroehnjmrurfieeectoacthaoodbltstnbsiltupiavehsjocmmrieiendospoctsnebatre.(odssrlentefitIlhstaimianntapintiseltsnaitdnovhfioortroeeprte(wrwdwpltmrraiieoaaoismttraesnneh.cstakdkaiidltWbioitinnihttnlesyyoggee--. iiitttrceonnnoahaonfngggnnangdmtsetckishinttetniaamehhnrsengna.esegucydirasslnfAruntcotbtaasoshrgbtiuntanaaeelkoWstetitriencnerainasnlaex,cugrbmtghtowssesh.ft,eseelneoteiefTrnorfovacoeitdhreracdutcliinshe.tvsthtepxheio,nsdpauaTgpsuriwmtteinheotatihcdbpieoshlviesillrfescveeereydrelem,ioetdakoicorsrwunrioieccnnatenmhabtdhlgsnousewuetsimrgnseaavloatdigaanlfavisinirnrtyennocpiotnohgetsrtbrusoopembeasebbaebgnlesiyeveergohtccimwtaiucleotnu-owitrsrsessamtoeeetspcabadeboadprrru[miii5iercansrssid]hesst----; ori beliefs about the output ordering (e.g. the top-level CornSeclhloUonlivoefrsOitpye,rIatthiaocnas, RNeYse1a4r8ch53a.nEdmIanidl:uastvrzi2a@lcEonrngeinlele.reindgu,. page of a website should be ordered before subpages of Supported by NSF grant CCF-0514628. the site). In an example from biology, the goal is to nd yMicrosoft Corporation, Redmond, WA. Email: classi cations of genes by integrating data from di erent rajneeshhegde@gmail.com. experiments [6]; again, the input constraints can re ect zMicrosoft Research, Redmond, WA 98052. Email: prior beliefs about the output classi cation. kamxaSlcjh@omoilcroosfofOt.pceorma.tions Research and Industrial Engi- We will model these problems as graphs, where neering, Cornell University, Ithaca, NY 14853. Email: each vertex represents an object, and the information dpw@cs.cornell.edu. Supported by NSF grant CCF-0514628. ",math
1056,On the shortest spanning subtree of a graph and the traveling salesman problem,Proceedings of the American Mathematical Society," Several years ago a typewritten translation (of obscure origin) of [l] raised some interest. This paper is devoted to the following theorem: If a (finite) connected graph has a positive real number attached to each edge (the length of the edge), and if these lengths are all distinct, then among the spanning1 trees (German: Gerüst) of the graph there is only one, the sum of whose edges is a minimum; that is, the shortest spanning tree of the graph is unique. (Actually in [l] this theorem is stated and proved in terms of the ""matrix of lengths"" of the graph, that is, the matrix \\aij\\ where a,;is the length of the edge connecting vertices i and /. Of course, it is assumed that a,j=ay,- and that a¿¿ = 0 for all i and/.) The proof in [l] is based on a not unreasonable method of constructing a spanning subtree of minimum length. It is in this construction that the interest largely lies, for it is a solution to a problem (Problem 1 below) which on the surface is closely related to one version (Problem 2 below) of the well-known traveling salesman problem. Problem 1. Give a practical method for constructing a spanning subtree of minimum length. Problem 2. Give a practical method for constructing an unbranched spanning subtree of minimum length. The construction given in [l] is unnecessarily elaborate. In the present paper I give several simpler constructions which solve Problem 1, and I show how one of these constructions may be used to prove the theorem of [l]. Probably it is true that any construction Received by the editors April 11, 1955. 1 A subgraph spans a graph if it contains all the vertices of the graph. ",math
1057,Identification of small inhomogeneities: Asymptotic factorization,Mathematics of Computation," We consider the boundary value problem of calculating the electrostatic potential for a homogeneous conductor containing ¯nitely many small insulating inclusions. We give a new proof of the asymptotic expansion of the electrostatic potential in terms of the background potential, the location of the inhomogeneities and their geometry, as the size of the inhomogeneities tends to zero. Such asymptotic expansions have already been used to design direct (i.e. non-iterative) reconstruction algorithms for the determination of the location of the small inclusions from electrostatic measurements on the boundary, e.g. MUSICtype methods. Our derivation of the asymptotic formulas is based on integral equation methods. It demonstrates the strong relation between factorization methods and MUSIC-type methods for the solution of this inverse problem. ",math
1058,Embedding and Automating Conditional Logics in Classical Higher-Order Logic,Annals of Mathematics and Artificial Intelligence, A sound and complete embedding of conditional logics into classical higher-order logic is presented. This embedding enables the application of off-the-shelf higher-order automated theorem provers and model finders for reasoning within and about conditional logics. ,math
1059,A mathematical framework for the semantics of symbolic languages representing periodic time,Annals of Mathematics and Artificial Intelligence,"In several areas, including Temporal DataBases (TDB), Presburger arithmetic has been chosen as a standard reference for the semantics of languages representing periodic time, and to study their expressiveness. On the other hand, the proposal of most symbolic languages in the AI literature has not been paired with an adequate semantic counterpart, making the task of studying the expressiveness of such languages and of comparing them a very complex one. In this paper, we first define a representation language which enables us to handle each temporal point as a complex object enriched with all the structure it is immersed in, and then we use it in order to provide a Presburger semantics for classes of symbolic languages coping with periodicity. Finally, we use the semantics to compare a few AI and TDB symbolic approaches. © Springer 2006.",math
1060,Evolutionary stability in extensive two-person games - correction and further development,Mathematical Social Sciences,The paper corrects a mistake concerning sufficient conditions for evolutionary stability in an earlier paper (MASS 5 (1983) 269-363). A new kind of decomposition of a symmetric extensive two-person game into a top and an abridgement is introduced. It is shown that regularity of a direct ESS imply a robustness property with respect to small payoff changes called essentiality. On the basis of the result on regularity and essentiality sufficient conditions for a regular limit ESS are obtained. These conditions make use of the decomposition into top and abridgement. Finally it is proved that the sufficient conditions are satisfied for the many-period models with ritual fights and escalated conflicts introduced in the earlier paper. © 1988.,math
1061,The Lanczos algorithm with partial reorthogonalization,Mathematics of Computation," The Lanczos algorithm is becoming accepted as a powerful tool for finding the eigenvalues and for solving linear systems of equations. Any practical implementation of the algorithm suffers however from roundoff errors, which usually cause the Lanczos vectors to lose their mutual orthogonality. In order to maintain some level of orthogonality, full reorthogonalization (FRO) and selective orthogonalization (SO) have been used in the past as a remedy. Here partial reorthogonalization (PRO) is proposed as a new method for maintaining semiorthogonality among the Lanczos vectors. PRO is based on a simple recurrence, which allows us to monitor the loss of orthogonality among the Lanczos vectors directly without computing the inner products. Based on the information from the recurrence, reorthogonalizations occur only when necessary. Thus substantial savings are made as compared to FRO. In some numerical examples we apply the Lanczos algorithm with PRO to the solution of large symmetric systems of linear equations and show that it is a robust and efficient algorithm for maintaining semiorthogonality among the Lanczos vectors. The results obtained compare favorably with the conjugate gradient method. ",math
1062,Computing in the Jacobian of a hyperelliptic curve,Mathematics of Computation," In this paper we present algorithms, suitable for computer use, for computation in the Jacobian of a hyperelliptic curve. We present a reduction algorithm which is asymptotically faster than that of Gauss when the genus g is very large. 1. Introduction. In [9], Shanks introduced the use of the class group of a quadratic number field as a tool in computational number theory and provided an efficient algorithm for multiplying (composing) ideal classes. A number of improvements have since occurred, and new algorithms using the class group have appeared. See, for example, Schnorr and Lenstra [7]. More recently, Lenstra [4] has shown how to use the group of points on an elliptic curve, defined over a finite field, in a factorization algorithm. Elliptic curves are the ""genus 1"" case of the Jacobian groups of hyperelliptic curves. The latter are the analogues of the class groups of quadratic number fields (henceforth we shall say simply ""class group""). While many explicit formulas for addition on an elliptic curve have appeared (for practical examples, see Chudnovsky and Chudnovsky [2], and Montgomery [5]), and numerous algorithms for computing in the class group of a quadratic number field and number fields of higher degree have appeared (see, for example, Lenstra [4], Shanks [9], and Williams, Dueck, and Schmid [10]), explicit formulas for addition in the Jacobian group of a hyperelliptic curve (henceforth we shall simply say Jacobian ), which are suitable for computation, do not appear to have been published. The purpose of this paper is to present such algorithms. While computation in the Jacobian is entirely analogous to computation in the class group and consists of ""composition"" followed by ""reduction"", we shall present formulas which are (asymptotically) more efficient than those used for the class group. In particular, our reduction procedure will use the Euclidean algorithm and be faster (when the genus is large) than the classical reduction procedure due to Gauss. A modification of it can be used for computation in the class group of an algebraic number field. By a hyperelliptic curve we shall mean, as usual, a curve C (with a model) of the form v2 = f(u), where /(«) is a polynomial of degree 2g + 1, with all roots distinct, and with coefficients in a field K of characteristic # 2; here g is a positive integer (the genus of the the curve C). ",math
1063,Resolution of singularities of convolutions with the Gaussian kernel,Proceedings of the American Mathematical Society," We present a complete classi cation of the zero set of a function which is a convolution with the Gaussian kernel. In the rst part, we calculate the Taylor expansion of the convolution in a critical point. In the second part, we resolve the singularity with the help of the general Newton process which yields the Puiseux expansions for the solutions. Finally, we describe the resolved singularity in terms of Hermite polynomials. ",math
1064,"Clocks, Dice and Processes",International Mathematics Research Notices," C l o c k s , D i c e a n d P r o c e s s e s ",math
1065,A prototyping environment for differential equations,ACM Transactions on Mathematical Software,"A system is presented to allow end users to solve nonlinear differential equations without need to write computer programs. The system treats nth order space 1992, first order time systems with initial and/or two point boundary value specification. Users of the system need only enter the problem in direct mathematical notation, and output is automatically presented as a solution graph. The system allows the user to alter this equations, in-situ, that is to computationally steer his model. Thus the system is suited for model prototyping. Implementation is based on an object-oriented paradigm, well established and robust numerical procedures, and distributed computing to supported needed resources for numerically intensive tasks. © 1992, ACM. All rights reserved.",math
1066,Attribute-incremental construction of the canonical implication basis,Annals of Mathematics and Artificial Intelligence,"We propose a new algorithm constructing the canonical implication basis of a formal context. Being incremental, the algorithm processes a single attribute of the context at a single step. Experimental results bear witness to its competitiveness. © 2007 Springer Science+Business Media B.V.",math
1067,Quasi-Newton methods and their application to function minimisation,Mathematics of Computation, solution of a set of n nonlinear simultaneous 1. Introduction.The may be written ,math
1068,Algorithm 666: Chabis: a mathematical software package for locating and evaluating roots of systems of nonlinear equations,ACM Transactions on Mathematical Software," CHABIS is a mathematical software package for the numerical solution of a system of n nonlinear equations in n variables. First, CHABIS locates at least one solution of the system within an ndimensional polyhedron. Then, it applies a new generalized method of bisection to this n-polyhedron in order to obtain an approximate solution of the system according to a predetermined accuracy. In this paper we briefly describe the user interface to CHABIS and present several details of its implementation, as well as an example of its usage. Categories and Subject Descriptors: systemsof equations;G.4 [Mathematics G.1.5 [Numerical of Computing]: Additional Key Words and Phrases: BLAS utilities, existence of the solution, generalized method of bisection, isolation of a root, localization of a root, solution of nonlinear systems, topological degree of a mapping ",math
1069,A remark on algorithm 643: FEXACT: an algorithm for performing Fisher's exact test in r x c contingency tables,ACM Transactions on Mathematical Software,"The network algorithm of Mehta and Patel [1986] m currently the best general algorithm for computing exact probabilities in r × c contingency tables with fixed marginals. Given here are some improvements to the network algorithm which speed Its computational performance; and thus increases the size of problems which can be handled, The new code also eliminates some programming restrictions in the old code, Implements the “hybrid” algorithm of Mehta and Patel [1986a], and demonstrates that the exact path length bounds of Joe [ 1988] should always be used in place of the approximate bounds of Mehta and Patel [1986]. The new code can be much faster than the old code m some problems. © 1993, ACM. All rights reserved.",math
1070,On the numerical solution of elliptic difference equations,Mathematics of Computation, requires These some properties of the upper bound properties are also available in von Neumann matrix matrix A and a vector = max ,math
1071,Skew symmetric additive utility with finite states,Mathematical Social Sciences,The skew symmetric additive utility model for decision under uncertainty generalizes Savages model by replacing his utility function on outcomes by a skew symmetric functional on ordered pairs of outcomes. This paper presents a preference axiomatization of the generalized model for finite state spaces that is based on a recent axiomatization for nontransitive additive conjoint measurement in multiattribute utility theory. The approach taken here is compared with previous approaches based on lottery acts and on the structure of Savages infinite-states formulation. © 1990.,math
1072,Graphoid properties of epistemic irrelevance and independence,Annals of Mathematics and Artificial Intelligence,"This paper investigates Walleys concepts of epistemic irrelevance and epistemic independence for imprecise probability models. We study the mathematical properties of irrelevance and independence, and their relation to the graphoid axioms. Examples are given to show that epistemic irrelevance can violate the symmetry, contraction and intersection axioms, that epistemic independence can violate contraction and intersection, and that this accords with informal notions of irrelevance and independence. © Springer 2005.",math
1073,Efficient Data Structures for Backtrack Search SAT Solvers,Annals of Mathematics and Artificial Intelligence," The implementation of efficient Propositional Satisfiability (SAT) solvers entails the utilization of highly efficient data structures, as illustrated by most of the recent state-of-the-art SAT solvers. However, it is in general hard to compare existing data structures, since different solvers are often characterized by fairly different algorithmic organizations and techniques, and by different search strategies and heuristics. This paper aims the evaluation of data structures for backtrack search SAT solvers, under a common unbiased SAT framework. In addition, advantages and drawbacks of each existing data structure are identified. Finally, new data structures are proposed, that are competitive with the most efficient data structures currently available, and that may be preferable for the next generation SAT solvers. ",math
1074,Finding optimal solutions to the graph partitioning problem with heuristic search,Annals of Mathematics and Artificial Intelligence,"As search spaces become larger and as problems scale up, an efficient way to speed up the search is to use a more accurate heuristic function. A better heuristic function might be obtained by the following general idea. Many problems can be divided into a set of subproblems and subgoals that should be achieved. Interactions and conflicts between unsolved subgoals of the problem might provide useful knowledge which could be used to construct an informed heuristic function. In this paper we demonstrate this idea on the graph partitioning problem (GPP). We first show how to format GPP as a search problem and then introduce a sequence of admissible heuristic functions estimating the size of the optimal partition by looking into different interactions between vertices of the graph. We then optimally solve GPP with these heuristics. Experimental results show that our advanced heuristics achieve a speedup of up to a number of orders of magnitude. Finally, we experimentally compare our approach to other states of the art graph partitioning optimal solvers on a number of classes of graphs. The results obtained show that our algorithm outperforms them in many cases. © Springer 2005.",math
1075,On Tikhonov’s method for ill-posed problems,Mathematics of Computation,"For Tikhonovs regularization of ill posed linear integral equations, numerical accuracy is estimated by a modulus of convergence, for which upper and lower bounds are obtained. Applications are made to the backward heat equation, to harmonic continuation, and to numerical differentiation. ÂŠ 1974, American Mathematical Society.",math
1076,Approximate Qualitative Temporal Reasoning,Annals of Mathematics and Artificial Intelligence," We partition the time-line in different ways, for example, into minutes, hours, days, etc. When reasoning about relations between events and processes we often reason about their location within such partitions. For example, happened yesterday and happened today, consequently and are disjoint. Reasoning about these temporal granularities so far has focussed on temporal units (relations between minute, hour slots). I shall argue in this paper that in our representations and reasoning procedures we need into account that events and processes often lie skew to the cells of our partitions (For example, 'happened yesterday' does not mean that started at 12 a. m. and ended 0 p. m.) This has the consequence that our descriptions of temporal location of events and processes are often approximate and rough in nature rather than exact and crisp. In this paper I describe representation and reasoning methods that take the approximate character of our descriptions and the resulting limits (granularity) of our knowledge explicitly into account. ",math
1077,Reconstruction algorithms in irregular sampling,Mathematics of Computation,"A constructive solution of the irregular sampling problem for bandlimited functions is given. We show how a band-limited function can be completely reconstructed from any random sampling set whose density is higher than the Nyquist rate, and give precise estimates for the speed of convergence of this iteration method. Variations of this algorithm allow for irregular sampling with derivatives, reconstruction of band-limited functions from local averages, and irregular sampling of multivariate band-limited functions. © 1992 American Mathematical Society.",math
1078,On the Complexity of Some Enumeration Problems for Matroids,SIAM Journal on Discrete Mathematics," We present an incremental polynomial-time algorithm for enumerating all circuits of a matroid or, more generally, all minimal spanning sets for a °at. We also show the NP-hardness of several related enumeration problems. ",math
1079,A theory of nonmonotonic rule systems I,Annals of Mathematics and Artificial Intelligence,"We introduce here the study of general nonmonotonic rule systems. These deal with situations where a conclusion is drawn from a ""system of beliefs""S (and seen to be in S), based both on some ""premises"" being in S and on some ""restraints"" not being in S. In the monotone systems of traditional logic there are no restraints, conclusions are drawn solely based on premises being in S. Nonmonotonic rule systems capture the essential syntactic, semantic, and algorithmic features of many nonmonotone systems such as default logic, negation as failure, truth maintenance, autoepistemic logic, and also important combinatorial questions from mathematics such as the marriage problem. This reveals semantics and syntax and proof procedures and algorithms for computing belief sets in many cases where none were previously available and entirely uniformly. In particular, we introduce and study deductively closed sets, extensions and weak extensions. Semantics of nonmonotonic rule systems is studied in part II of this paper and extensions to predicate classical, intuitionistic, and modal logics are left to a later paper. © 1990 J.C. Baltzer A.G. Scientific Publishing Company.",math
1080,A New Property and a Faster Algorithm for Baseball Elimination,SIAM Journal on Discrete Mathematics," In the baseball elimination problem, there is a league consisting of n teams. At some point during the season, team i has wi wins and gij games left to play against team j. A team is eliminated if it cannot possibly nish the season in rst place or tied for rst place. The goal is to determine exactly which teams are eliminated. The problem is not as easy as many sports writers would have you believe, in part because the answer depends not only on the number of games won and left to play but also on the schedule of remaining games. In the 1960's, Schwartz showed how to determine whether one particular team is eliminated using a maximum flow computation. This paper indicates that the problem is not as di cult as many mathematicians would have you believe. For each team i, let gi denote the number of games remaining. We prove that there exists a value W such that team i is eliminated if and only if wi + gi < W . Using this surprising fact, we can determine all eliminated teams in time proportional to a single maximum flow computation in a graph with n nodes; this improves upon the previous best known complexity bound by a factor of n. ",math
1081,On the level sets of a distance function in a Minkowski space,Proceedings of the American Mathematical Society,"Given a closed subset of an n-dimensional Minkowski space with a strictly convex or differentiable norm, then, for almost every r < 0, the r-level set (points whose distance from the closed set is r) contains an open subset which is an n - 1 dimensional Lipschitz manifold and whose complement relative to the level set has n - 1 dimensional Hausdorff measure zero. In case n = 2 and the norm is twice differentiable with bounded second derivative, almost every level set is a 1 manifold. ÂŠ 1972 American Mathematical Society.",math
1082,Algorithm 777: HOMPACK90: a suite of Fortran 90 codes for globally convergent homotopy algorithms,ACM Transactions on Mathematical Software," HOMPACK90 is a Fortran 90 version of the Fortran 77 package HOMPACK (Algorithm 652), a collection of codes for finding zeros or fixed points of nonlinear systems using globally convergent probability-one homotopy algorithms. Three qualitatively different algorithmsordinary differential equation based, normal flow, quasi-Newton augmented Jacobian matrix-are provided for tracking homotopy zero curves, as well as separate routines for dense and sparse Jacobian matrices. A high level driver for the special case of polynomial systems is also provided. Changes to HOMPACK include numerous minor improvements, simpler and more elegant interfaces, use of modules, new end games, support for several sparse matrix data structures, and new iterative algorithms for large sparse Jacobian matrices. Categories and Subject Descriptors: D.3.2 [Programming Languages]: Language Classifications-Fortran 90; G.1.5 [Numerical Analysis]: Roots of Nonlinear Equations-systems of equations; G.4 [Mathematics of Computing]: Mathematical Software Additional Key Words and Phrases: Chow-Yorke algorithm, curve tracking, fixed point, ",math
1083,A logic of argumentation for specification and verification of abstract argumentation frameworks,Annals of Mathematics and Artificial Intelligence," In this paper, we propose a logic of argumentation for the specification and verification (LA4SV) of requirements on Dung's abstract argumentation frameworks. We distinguish three kinds of decision problems for argumentation verification, called extension verification, framework verification, and specification verification respectively. For example, given a political requirement like “if the argument to increase taxes is accepted, then the argument to increase services must be accepted too,” we can either verify an extension of acceptable arguments, or all extensions of an argumentation framework, or all extensions of all argumentation frameworks satisfying a framework specification. We introduce the logic of argumentation verification to specify such requirements, and we represent the three verification problems of argumentation as model checking and theorem proving properties of the logic. Moreover, we recast the logic of argumentation verification in a modal framework, in order to express multiple extensions, and properties like transitivity and reflexivity of the attack relation. Finally, we introduce a logic of ",math
1084,A subclass of Horn CNFs optimally compressible in polynomial time,Annals of Mathematics and Artificial Intelligence,"The problem of Horn Minimization (HM) can be stated as follows: given a Horn CNF representing a Boolean function f, find a shortest possible (optimally compressed) CNF representation of f, i.e., a CNF representation of f which consists of the minimum possible number of clauses. This problem is the formalization of the problem of knowledge compression for speeding up queries to propositional Horn expert systems, and it is known to be NP-hard. There are two subclasses of Horn functions for which HM is known to be solvable in polynomial time: acyclic and quasi-acyclic Horn functions. In this paper we define a new class of Horn functions properly containing both of the known classes and design a polynomial time HM algorithm for this new class. © 2010 Springer Science+Business Media B.V.",math
1085,Representation theory for default logic,Annals of Mathematics and Artificial Intelligence," Default logic can be regarded as a mechanism to represent families of belief sets of a reasoning agent. As such, it is inherently second-order. In this paper, we study the problem of representability of a family of theories as the set of extensions of a default theory. We give a complete solution to the representability by means of normal default theories. We obtain partial results on representability by arbitrary default theories. In particular, we construct examples of denumerable families of nonincluding theories that are not representable. We also study the concept of equivalence between default theories. We show that for every normal default theory there exists a normal prerequisitefree theory with the same set of extensions. We derive a representation result connecting normal default logic with a version of CWA. ",math
1086,Some extremal 2-bases,Mathematics of Computation,"By means of a computer search, some extremal additive bases have been constructed which have heretofore been unknown. © 1978 American Mathematical Society.",math
1087,Restarts and Exponential Acceleration of the Davis–Putnam–Loveland–Logemann Algorithm: A Large Deviation Analysis of the Generalized Unit Clause Heuristic for Random 3-SAT,Annals of Mathematics and Artificial Intelligence,"An analysis of the hardness of resolution of random 3-SAT instances using the Davis-Putnam-Loveland-Logemann (DPLL) algorithm slightly below threshold is presented. While finding a solution for such instances demands exponential effort with high probability, we show that an exponentially small fraction of resolutions require a computation scaling linearly in the size of the instance only. We compute analytically this exponentially small probability of easy resolutions from a large deviation analysis of DPLL with the Generalized Unit Clause search heuristic, and show that the corresponding exponent is smaller (in absolute value) than the growth exponent of the typical resolution time. Our study therefore gives some quantitative basis to heuristic restart solving procedures, and suggests a natural cut-off cost (the size of the instance) for the restart.",math
1088,Evaluating trustworthiness from past performances: interval-based approaches,Annals of Mathematics and Artificial Intelligence,"In many multi-agent systems, especially in the field of e-commerce, the users have to decide whether they sufficiently trust an agent to achieve a certain goal. To help users to make such decisions, an increasing number of trust systems have been developed. By trust system, we mean a system that gathers information about an agent and evaluates its trustworthiness on the basis of this information. The aim of the present paper is to develop, and analyze from an axiomatic point of view, new trust systems based on intervals. More precisely, we assume that a set of grades describing the past performances of an agent is given. Then, the goal is to construct an interval that summarizes these grades. In our opinion, such an interval gives a good account of the trustworthiness of the agent. In addition, this kind of representation format overcomes certain limitations (at a certain cost) of the approaches that represent trustworthiness by a single number. We establish seven axioms that should be satisfied by a summarizing method. Next, we develop two new methods. The first one is based on the idea that certain concentrations of grades are strong enough to pull the bounds of the summarizing interval towards themselves. The second one represents data in the setting of possibility theory, and then computes lower and upper expected values. Finally, we check that our methods satisfy the axioms introduced before, which provide theoretical justifications for them. © 2012 Springer Science+Business Media B.V.",math
1089,On a regularity theorem for weak solutions to transmission problems with internal Lipschitz boundaries,Proceedings of the American Mathematical Society," We show that if « is a weak solution to div(AVu) = 0 on an open set Í2 containing a Lipschitz domain D , where A = IcIxd + IXci/d (^ > 0 , k ^ 1). Then, the nontangential maximal function of the gradient of u lies in L2(dD). This paper answers a question that was posed to us by A. Friedman in relation with a better regularity estimate for solutions to certain divergence form equations, which is necessary to prove some theorems of continuous dependence associated to some inverse problems [8]. In particular, we consider the operator Lu = div(A(X)Vu(X)) = 0, where A(X) = klXü(X) + IXaiû(X) (k>0, k ¿ 1), I denotes the identity matrix, X is the characteristic function of a set, il is an open set in R"" , and D denotes a Lipschitz domain contained in il (see the body of the paper for the relevant definitions). If u e Wx>2(il), the space of square integrable functions in il with distributional derivatives in L2(il), is a weak solution to Lu = 0 in il and the gradient of u has a restriction to the boundary of D, dD; then the following relation should hold, Received by the editors September 4, 1990 and, in revised form, February 2, 1991. 1991 Mathematics Subject Classification.Primary 35B65; Secondary 35C15, 35P15, 42B20, 45C05, 45E05. The first author was partially supported by NSF grant DMS 8421377-03, and the second author was partially supported by NSF grant DMS 8915413. ",math
1090,A Functional Description of ANALYZE: A Computer-Assisted Analysis System for Linear Programming Models,ACM Transactions on Mathematical Software," This paper describes the functions of a system, called ANALYZE, designed to assist analysts with their use of a linear programming model. It supplements a mathematical programming system to fullfdl some of the functions of Computer-Assisted Analysis (CAA): documentation/verification, debugging, interpreting results, simplification, and sensitivity analysis. These functions are described and illustrated with the ANALYZE commands used to assist the analysis. (The language of ANALYZE is interpretive, similar to its predecessor system, PERUSE.) Beginning with perspectives of the snatomy of a model, the ANALYZE aids for verification and documentation are described and applied to models used by the Energy Information Administration (EIA). Then, extensions of conventional sensitivity analysis in linear programming are described, based upon the digraph of a linear program's matrix. With the structural description that includes path tracing the next CAA function is model simplification. Two approaches are described: dimensional reduction and search for embedded structures, notably netforms. Throughout the paper the ANALYZE system is illustrated and its commands, listed in the appendix, are applied to real cases. The ANALYZE system works with the PERUSE data structures and, like PERUSE, is written in FORTRAN (not standard). I t is normally run under TSO, but one can execute batch mode. Categories and Subject Descriptors: D.l.l [ P r o g r a m m i n g Techniques]: Applicative (Functional) Programming; D.2.2 [Software Engineering]: Tools and Techniques-user interfaces; D.3.4 [Prog r a m m i n g Languages]: Processors-run-trme env~ronmentsG;.1.6 [Numerical Analysis]: Optimization-linear programming, G.2.2 [Discrete Mathematics]: Graph Theory-path a n d circuit problems; H . 1 . [Models a n d Principles]: User/Machine Systems-human factors; H.2.3 [Datab a s e Management]: Languages-query languages; H.2.4 [Database Management]: Systemsquery processzng, H.4.2 [Information S y s t e m s Applications]: Types of Systems-decision support; 5.4 [Social a n d Behavioral Sciences-economics; K.6.3 [Management of Computing a n d Information Systems]: Software Management-softuare development; K.6.4 [Management of Computing and Information Systems]: System Management-management audit ",math
1091,"A Comparison of the Sherali-Adams, Lovász-Schrijver, and Lasserre Relaxations for 0--1 Programming",Mathematics of Operations Research," Sherali and Adams (1990), Lov´asz and Schrijver (1991) and, recently, Lasserre (2001) have constructed hierarchies of successive linear or semidefinite relaxations of a 0 − 1 polytope P ⊆ Rn converging to P in n steps. Lasserre's approach uses results about representations of positive polynomials as sums of squares and the dual theory of moments. We present the three methods in a common elementary framework and show that the Lasserre construction provides the tightest relaxations of P . As an application this gives a direct simple proof for the convergence of the Lasserre's hierarchy. We describe applications to the stable set polytope and to the cut polytope. ",math
1092,Representation and computation of the pseudoinverse,Proceedings of the American Mathematical Society," I. Introduction. The theory of pseudoinverses, or generalized inverses, has been extensively developed over the last few years. A recent and comprehensive bibliography occurs in [2] which gives a short history also, and a brief survey of known results and computational methods. We consider here the case of a bounded operator with closed range between Hilbert spaces. Most of the information for this case, which is used in this paper can be found in [3], which is pithy and short. ",math
1093,Remark on “algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound constrained optimization”,ACM Transactions on Mathematical Software,This remark describes an improvement and a correction to Algorithm 778. It is shown that the performance of the algorithm can be improved significantly by making a relatively simple modification to the subspace minimization phase. The correction concerns an error caused by the use of routine dpmeps to estimate machine precision. © 2011 ACM 0098-3500/2011/11-ART7.,math
1094,Optimized code generation for finite element local assembly using symbolic manipulation,ACM Transactions on Mathematical Software," Automated code generators for finite element local assembly have facilitated exploration of alternative implementation strategies within generated code. However, even for a theoretical performance indicator such as operation count, an optimal strategy for local assembly is unknown. We explore a code generation strategy based on symbolic integration and polynomial common sub-expression elimination (CSE). We present our implementation of a local assembly code generator using these techniques. We systematically evaluate the approach, measuring operation count, execution time and numerical error using a benchmark suite of synthetic variational forms, comparing against the FEniCS Form Compiler (FFC). Our benchmark forms span complexities chosen to expose the performance characteristics of different code generation approaches. We show that it is possible with additional computational cost, to consistently achieve much of, and sometimes substantially exceed, the performance of alternative approaches without compromising precision. Although the approach of using symbolic integration and CSE for optimizing local assembly is not new, we distinguish our work through our strategies for maintaining numerical precision and detecting common sub-expressions. We discuss the benefits of the symbolic approach for inferring numerical relationships, and analyze the relationship to other proposed techniques which also have greater computational complexity than those of FFC. Categories and Subject Descriptors: G.1.8 [Numerical Analysis]: Partial Differential Equations - Finite element methods; G.4 [Mathematical Software]: Efficiency, reliability and robustness; I.1.4 [Symbolic and Algebraic Manipulation]: Applications c ACM, 2013. This is the author's version of the work. It is posted here by permission of ACM for your personal use. Not for redistribution. The definitive version was published in ACM Transactions on Mathematical Software, Volume 39, Issue 4, July 2013. http://dx.doi.org/10.1145/2491491.2491496 This work was supported by EPSRC under grant EP/I00677X/1 and a doctoral training studentship. The authors would like to acknowledge Florian Rathgeber and Chris Cantwell for their insightful feedback. Author's addresses: F. P. Russell & P. H. J. Kelly, Department of Computing, Imperial College London. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c 201Y ACM 0098-3500/201Y/-ARTA $10.00 DOI 10.1145/0000000.0000000 http://doi.acm.org/10.1145/0000000.0000000 ",math
1095,Extensive games with possibly unaware players,Mathematical Social Sciences,"Standard game theory assumes that the structure of the game is common knowledge among players. We relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. In particular, they may not be aware of moves that they and other agents can make. We show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. We provide a generalization of Nash equilibrium and show that every game with awareness has a generalized Nash equilibrium. Finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where players may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior. © 2012 Elsevier B.V.",math
1096,Duration Consistency Filtering for Qualitative Simulation,Annals of Mathematics and Artificial Intelligence," We present two new qualitative reasoning formalisms, and use them in the construction of a new type of filtering mechanism for qualitative simulators. Our new sign algebra, SR1*, facilitates reasoning about relationships among the signs of collections of real numbers. The comparison calculus, built on top of SR1*, is a general framework that can be used to qualitatively compare the behaviors of two dynamic systems or two excerpts of the behavior of a single dynamic system at different situations. These tools enable us to improve the predictive performance of qualitative simulation algorithms. We show that qualitative simulators can make better use of their input to deduce significant amounts of qualitative information about the relative lengths of the time intervals in their output behavior predictions. Simple techniques employing concepts like symmetry, periodicity, and comparison of the circumstances during multiple traversals of the same region can be used to build a list of facts representing the deduced information about relative durations. The duration consistency filter eliminates spurious behaviors leading to inconsistent combinations of these facts. Surviving behaviors are annotated with richer qualitative descriptions. Used in conjunction with other spurious behavior elimination methods, this approach would increase the ability of qualitative simulators to handle more complex systems. ",math
1097,Asymptotic properties of the residual bootstrap for Lasso estimators,Proceedings of the American Mathematical Society," In this article, we derive the asymptotic distribution of the bootstrapped Lasso estimator of the regression parameter in a multiple linear regression model. It is shown that under some mild regularity conditions on the design vectors and the regularization parameter, the bootstrap approximation converges weakly to a random measure. The convergence result rigorously establishes a previously known heuristic formula for the limit distribution of the bootstrapped Lasso estimator. It is also shown that when one or more components of the regression parameter vector are zero, the bootstrap may fail to be consistent. ",math
1098,Solution of Some Integral Equations Arising in Integral Geometry,Applied Mathematics Letters,"An analytical solution is given to some multidimensional integral equation arising in inverse problems. For example, we solve the following equation {A figure is presented} and similar equations. ÂŠ 1991.",math
1099,On the constructive orbit problem,Annals of Mathematics and Artificial Intelligence," Symmetry reduction techniques aim to combat the state-space explosion problem for model checking by restricting search to representative states from equivalence classes with respect to a group of symmetries. The standard approach to representative computation involves converting a state to its minimal image under a permutation group G, before storing the state. This is known as the constructive orbit problem (COP), and is NP hard. It may be possible to solve the COP efficiently if G is known to have certain structural properties: in particular if G is isomorphic to a full symmetry group, or G is a disjoint/wreath product of subgroups. We extend existing results on solving the COP efficiently for fully symmetric groups, and investigate the problem of automatically classifying an arbitrary permutation group as a disjoint/wreath product of subgroups. We also present an approximate COP strategy based on local search, and some computational group-theoretic optimisations to improve the basic approach of solving the COP by symmetry group enumeration. Experimental results using the TopSPIN symmetry reduction package, which interfaces with the computational group-theoretic system GAP, illustrate the effectiveness of our techniques. Alastair F. Donaldson is supported by EPSRC grant EP/G051100. Alice Miller is supported by EPSRC grant EP/E032354. ",math
1100,Every Monotone Graph Property Has A Sharp Threshold,Proceedings of the American Mathematical Society," In their seminal work which initiated random graph theory Erd¨os and Renyi discovered that many graph properties have sharp thresholds as the number of vertices tends to in nity. We prove a conjecture of Linial that every monotone graph property has a sharp threshold. This follows from the following theorem. Let Vn(p) = f0; 1gn denote the Hamming space endowed with the probability measure p de ned by p( 1; 2; : : : ; n) = pk (1 − p)n−k, where k = 1 + 2 + + n. Let A be a monotone subset of Vn. We say that A is symmetric if there is a transitive permutation group Γ on f1; 2; : : : ; ng such that A is invariant under Γ. Theorem. For every symmetric monotone A, if p(A) > then q(A) > 1for q = p + c1 log(1=2 )= log n. (c1 is an absolute constant.) 1. Graph properties A graph property is a property of graphs which depends only on their isomorphism class. Let P be a monotone graph property; that is, if a graph G satis es P then every graph H on the same set of vertices, which contains G as a subgraph satis es P as well. Examples of such properties are: G is connected, G is Hamiltonian, G contains a clique (=complete subgraph) of size t, G is not planar, the clique number of G is larger than that of its complement, the diameter of G is at most s, etc. For a property P of graphs with a xed set of n vertices we will denote by p(P ) the probability that a random graph on n vertices with edge probability p satis es P . The theory of random graphs was founded by Erd¨os and Renyi [8, 4], and one of their signi cant discoveries was the existence of sharp thresholds for various graph properties; that is, the transition from a property being very unlikely to it being very likely is very swift. Many results on various aspects of this phenomenon have appeared since then. In what follows c1; c2; etc. are universal constants. Theorem 1.1. Let P be any monotone property of graphs on n vertices. If p(P ) > then q(P ) > 1 − for q = p + c1 log(1=2 )= log n. ",math
1101,Bounds on positive integral solutions of linear Diophantine equations,Proceedings of the American Mathematical Society,"Assuming the existence of a solution, we find bounds for small solutions x of the finite matrix equation Ax = B, where each entry of A, B is an integer, and x is a nontrivial column vector with nonnegative integer entries. ÂŠ 1976 American Mathematical Society.",math
1102,Data model descriptions and translation signatures in a multi-model framework,Annals of Mathematics and Artificial Intelligence,"We refer to the problem of translating schemas from a data model to another, in a multi-model framework. Specifically, we consider an approach where translations are specified as Datalog-like programs. In this context we show how it is possible to reason on models and schemas involved as input and output for a translation. The various notions are formalized: (i) concise descriptions of models in terms of sets of constructs, with associated propositional formulas; (ii) a notion of signature for translation rules (with the property that signatures can be automatically computed out of rules); (iii) the application of signatures to models. The main result is that the target model of a translation can be completely characterized given the description of the source model and the signatures of the rules. This result is being exploited in the framework of a tool that implements model generic translations, as the basis for the automatic generation of translations out of a library of elementary ones. © 2012 Springer Science+Business Media B.V.",math
1103,Q-Learning and Enhanced Policy Iteration in Discounted Dynamic Programming,Mathematics of Operations Research," We consider the classical nite-state discounted Markovian decision problem, and we introduce a new policy iteration-like algorithm for nding the optimal Q-factors. Instead of policy evaluation by solving a linear system of equations, our algorithm requires (possibly inexact) solution of a nonlinear system of equations, involving estimates of state costs as well as Q-factors. This is Bellman's equation for an optimal stopping problem that can be solved with simple Q-learning iterations, in the case where a lookup table representation is used; it can also be solved with the Q-learning algorithm of Tsitsiklis and Van Roy [TsV99], in the case where feature-based Q-factor approximations are used. In exact/lookup table representation form, our algorithm admits asynchronous and stochastic iterative implementations, in the spirit of asynchronous/modi ed policy iteration, with lower overhead and more reliable convergence advantages over existing Q-learning schemes. Furthermore, for large-scale problems, where linear basis function approximations and simulation-based temporal di erence implementations are used, our algorithm resolves e ectively the inherent di culties of existing schemes due to inadequate exploration. ",math
1104,Mining and visualizing recommendation spaces for elliptic PDEs with continuous attributes,ACM Transactions on Mathematical Software," In this paper we extend previous work in mining recommendation spaces based on symbolic problem features to PDE problems with continuous-valued attributes. We identify the research issues in mining such spaces, present a dynamic programming algorithm from the data-mining literature, and describe how a priori domain metaknowledge can be used to control the complexity of induction. A visualization aid for continuous-valued recommendation spaces is also outlined. Two case studies are presented to illustrate our approach and tools: (i) a comparison of an iterative and a direct linear system solver on nearly singular problems, and (ii) a comparison of two iterative solvers on problems posed on nonrectangular domains. Both case studies involve continuously varying problem and method parameters which strongly influence the choice of best algorithm in particular cases. By mining the results from thousands of PDE solves, we can gain valuable insight into the relative performance of these methods on similar problems. ",math
1105,A representation for coordination fault detection in large-scale multi-agent systems,Annals of Mathematics and Artificial Intelligence,"Teamwork requires that team members coordinate their actions. The representation of the coordination is a key requirement since it influences the complexity and flexibility of reasoning team-members. One aspect of this requirement is detecting coordination faults as a result of intermittent failures of sensors, communication failures, etc. Detection of such faults, based on observations of the behavior of agents, is of prime importance. Though different solutions have been presented thus far, none has presented a comprehensive and efficient resolution for large-scale teams. This paper presents a formal approach to representing multi-agent coordination, and multi-agent observations, using matrix structures. This representation facilitates easy representation of coordination requirements, modularity, flexibility and reuse of existing systems. Based on this representation, we present a novel solution for fault-detection that is both generic and efficient for large-scale teams. We demonstrate the modularity of the representation by presenting a reuse of existing systems and by importing other models (e.g. hierarchical systems) into the new representation. Finally, we extend the representation to support dynamical aspects of complex systems. © Springer Science + Business Media B.V. 2009.",math
1106,"Block tridiagonalization of ""effectively"" sparse symmetric matrices",ACM Transactions on Mathematical Software," A block tridiagonalization algorithm is proposed for transforming a sparse (or “effectively” sparse) symmetric matrix into a related block tridiagonal matrix, such that the eigenvalue error remains bounded by some prescribed accuracy tolerance. It is based on a heuristic for imposing a block tridiagonal structure on matrices with a large percentage of zero or “effectively zero” (with respect to the given accuracy tolerance) elements. In the light of a recently developed block tridiagonal divide-andconquer eigensolver [6], for which block tridiagonalization is needed as a preprocessing step, the algorithm also provides an option for attempting to produce at least a few very small diagonal blocks in the block tridiagonal matrix. This leads to low time complexity of the last merging operation in the block divide-andconquer method. Numerical experiments are presented and various potential block tridiagonalization strategies are compared. ",math
1107,Variations on variable-metric methods. (With discussion),Mathematics of Computation," In unconstrained minimization of a function/, the method of Davidon-FleteherPowell (a ""variable-metric"" method) enables the inverse of the Hessian H of / to be approximated step wise, using only values of the gradient of/. It is shown here that, by solving a certain variational problem, formulas for the successive corrections to H can be derived which closely resemble Davidon's. A symmetric correction matrix is sought which minimizes a weighted Euclidean norm, and also satisfies the ""DFP condition."" Numerical tests are described, comparing the performance (on four ""standard"" test functions) of two variationally-derived formulas with Davidon's. A proof by Y. Bard, modelled on Fletcher and Powell's, showing that the new formulas give the exact H after N steps, is included in an appendix. 1. The DFP Method. The class of gradient methods strained minimum of a function fix)* in which the direction step from Xhto Xk+xis computed from a formula such as: is called the class of variable-metric methods. Here Gk is a (preferably) positivedefinite N X N matrix and gk is the gradient Vf evaluated at XkThe reason for this nomenclature is that Sk is the direction in which the directional derivative of / is a minimum, i.e., the direction in which ",math
1108,Learning dynamic algorithm portfolios,Annals of Mathematics and Artificial Intelligence,"Algorithm selection can be performed using a model of runtime distribution, learned during a preliminary training phase. There is a trade-off between the performance of model-based algorithm selection, and the cost of learning the model. In this paper, we treat this trade-off in the context of bandit problems. We propose a fully dynamic and online algorithm selection technique, with no separate training phase: all candidate algorithms are run in parallel, while a model incrementally learns their runtime distributions. A redundant set of time allocators uses the partially trained model to propose machine time shares for the algorithms. A bandit problem solver mixes the model-based shares with a uniform share, gradually increasing the impact of the best time allocators as the model improves. We present experiments with a set of SAT solvers on a mixed SAT-UNSAT benchmark; and with a set of solvers for the Auction Winner Determination problem. © Springer Science+Business Media, Inc. 2007.",math
1109,Stable models and difference logic,Annals of Mathematics and Artificial Intelligence, The paper studies the relationship between logic programs with the stable model semantics and difference logic recently considered in the Satisfiability Modulo Theories framework. Characterizations of stable models in terms of level rankings are developed building on simple linear integer constraints allowed in difference logic. Based on a characterization with level rankings a translation is devised which maps a normal program to a difference logic formula capturing stable models of the program as satisfying valuations of the resulting formula. The translation makes it possible to use a solver for difference logic to compute stable models of logic programs. ,math
1110,Spherical Marcinkiewicz-Zygmund inequalities and positive quadrature,Mathematics of Computation," Geodetic and meteorological data, collected via satellites for example, are genuinely scattered and not con ned to any special set of points. Even so, known quadrature formulas used in numerically computing integrals involving such data have had restrictions either on the sites (points) used or, more signi cantly, on the number of sites required. Here, for the unit sphere embedded in Rq, we obtain quadrature formulas that are exact for spherical harmonics of a xed order, have nonnegative weights, and are based on function values at scattered sites. To be exact, these formulas require only a number of sites comparable to the dimension of the space. As a part of the proof, we derive L1-Marcinkiewicz-Zygmund inequalities for such sites. ",math
1111,Randomized Methods for Linear Constraints: Convergence Rates and Conditioning,Mathematics of Operations Research," We study randomized variants of two classical algorithms: coordinate descent for systems of linear equations and iterated projections for systems of linear inequalities. Expanding on a recent randomized iterated projection algorithm of Strohmer and Vershynin (Strohmer, T., R. Vershynin. 2009. A randomized Kaczmarz algorithm with exponential convergence. J. Fourier Anal. Appl. 15 262-278) for systems of linear equations, we show that, under appropriate probability distributions, the linear rates of convergence (in expectation) can be bounded in terms of natural linear-algebraic condition numbers for the problems. We relate these condition measures to distances to ill-posedness and discuss generalizations to convex systems under metric regularity assumptions. ",math
1112,A Novel Feature-Based Approach to Characterize Algorithm Performance for the Traveling Salesman Problem,Annals of Mathematics and Artificial Intelligence,"Meta-heuristics are frequently used to tackle NP-hard combinatorial optimization problems. With this paper we contribute to the understanding of the success of 2-opt based local search algorithms for solving the traveling salesperson problem (TSP). Although 2-opt is widely used in practice, it is hard to understand its success from a theoretical perspective. We take a statistical approach and examine the features of TSP instances that make the problem either hard or easy to solve. As a measure of problem difficulty for 2-opt we use the approximation ratio that it achieves on a given instance. Our investigations point out important features that make TSP instances hard or easy to be approximated by 2-opt. © 2013 Springer Science+Business Media Dordrecht.",math
1113,Computational functional analysis,Mathematics of Computation," htp:/ow.ly/uDoWT Advanced topics in difference equations , Ravi P. Agarwal, Patricia J. Y. Wong, 1997, Mathematics, 507 pages. This monograph is a collection of the results the authors have obtained on difference equations and inequalities. In the last few years this discipline has gone through such a. Functional analysis and numerical mathematics , Lothar Collatz, 1966, Mathematics, 473 pages. . Mathematical elements of scientific computing , Ramon E. Moore, 1975, Mathematics, 237 pages. . A Guide to Functional Analysis , Steven G. Krantz, Jun 6, 2013, Mathematics, 150 pages. This book is a quick but precise and careful introduction to the subject of functional analysis. It covers the basic topics that can be found in a basic graduate analysis text. Functional analysis an introduction for physicists, Nino Boccara, Sep 28, 1990, Mathematics, 327 pages. Based on a third-year course for French students of physics, this book is a graduate text in functional analysis emphasizing applications to physics. It introduces Lebesgue. Basic Methods of Linear Functional Analysis , John D. Pryce, 2011, Mathematics, 320 pages. "" An introduction to the themes of mathematical analysis, this text is geared toward advanced undergraduate and graduate students. Topics include operators, function spaces. ",math
1114,Invariants for homology classes with application to optimal search and planning problem in robotics,Annals of Mathematics and Artificial Intelligence," We consider planning problems on Euclidean spaces of the form RD Oe, where Oe is viewed as a collection of obstacles. Such spaces are of frequent occurrence as configuration spaces of robots, where Oe represent either physical obstacles that the robots need to avoid (e.g., walls, other robots, etc.) or illegal states (e.g., all legs off-the-ground). As state-planning is translated to pathplanning on a configuration space, we collate equivalent plannings via topologically-equivalent paths. This prompts finding or exploring the different homology classes in such environments and finding representative optimal trajectories in each such class. In this paper we start by considering the general problem of finding a complete set of easily computable homology class invariants for (N 1)-cycles in (RD Oe). We achieve this by finding explicit generators of the (N 1)st de Rham cohomology group of this punctured Euclidean space, and using their integrals to define cocycles. The action of those dual cocycles on (N 1)-cycles gives the desired complete set of invariants. We illustrate the computation through examples. We then show, for the case when N = 2, due to the integral approach in our formulation, this complete set of invariants is well-suited for efficient search-based planning of optimal robot trajectories with topological constraints. In particular, we show how to construct an 'augmented graph', Gb, from an arbitrary graph G in the configuration space. A graph construction and search algorithm can hence be used to find optimal trajectories in different topological classes. Finally, we extend this approach to computation of invariants in spaces derived from (RD Oe) by collapsing a subspace, thereby permitting application to a wider class of non-Euclidean ambient spaces. ",math
1115,Statistical Properties of Dynamical Systems with Some Hyperbolicity,Annals of Mathematics," This paper is about the ergodic theory of attractors and conservative dynamical systems with hyperbolic properties on large parts (though not necessarily all) of their phase spaces. The main results are for discrete time systems. To put this work into context, recall that for Axiom A attractors the picture has been fairly complete since the 1970's (see [S1], [B], [R2]). Since then much progress has been made on two fronts: there is a general nonuniform theory that deals with properties common to all diffeomorphisms with nonzero Lyapunov exponents ([O], [P1], [Ka], [LY]), and there are detailed analyses of specific kinds of dynamical systems including, for example, billiards, 1-dimensional and H´enon-type maps ([S2], [BSC]; [HK], [J]; [BC2], [BY1]). Statistical properties such as exponential decay of correlations are not enjoyed by all diffeomorphisms with nonzero Lyapunov exponents. The goal of this paper is a systematic understanding of these and other properties for a class of dynamical systems larger than Axiom A. This class will not be defined explicitly, but it includes some of the much studied examples. By looking at regular returns to sets with good hyperbolic properties, one could give systems in this class a simple dynamical representation. Conditions for the existence of natural invariant measures, exponential mixing and central limit theorems are given in terms of the return times. These conditions can be checked in concrete situations, giving a unified way of proving a number of results, some new and some old. Among the new results are the exponential decay of correlations for a class of scattering billiards and for a positive measure set of H´enon-type maps. ",math
1116,Aggregated Fuzzy Answer Set Programming,Annals of Mathematics and Artificial Intelligence,"Fuzzy Answer Set Programming (FASP) is an extension of answer set programming (ASP), based on fuzzy logic. It allows to encode continuous optimization problems in the same concise manner as ASP allows to model combinatorial problems. As a result of its inherent continuity, rules in FASP may be satisfied or violated to certain degrees. Rather than insisting that all rules are fully satisfied, we may only require that they are satisfied partially, to the best extent possible. However, most approaches that feature partial rule satisfaction limit themselves to attaching predefined weights to rules, which is not sufficiently flexible for most real-life applications. In this paper, we develop an alternative, based on aggregator functions that specify which (combination of) rules are most important to satisfy. We extend upon previous work by allowing aggregator expressions to define partially ordered preferences, and by the use of a fixpoint semantics. © 2011 Springer Science+Business Media B.V.",math
1117,Computing Stark units for totally real cubic fields,Mathematics of Computation," A method for computing provably accurate values of partial zeta functions is used to numerically con rm the rank one abelian Stark Conjecture for some totally real cubic elds of discriminant less than 50000. The results of these computations are used to provide explicit Hilbert class elds and some ray class elds for the cubic extensions. Let K=k be an abelian extension of number elds with Galois group G = Gal(K=k) and let S be a nite set of places of k that includes the Archimedean places of k and all the places of k rami ed in K=k. For any 2 G the partial zeta function S (s; ) is de ned for Re(s) > 1 by A integral (A;S)=1 A= where the sum extends over all integral ideals A of k whose Frobenius symbol A for the abelian extension K=k is the given element of G. For a character of G de ne LS (s; ) to be the L-series for with the Euler factors at S removed, which for Re(s) > 1 is given by the usual convergent Euler product: ",math
1118,A BDI agent system for the cow herding domain,Annals of Mathematics and Artificial Intelligence," We describe the current state of our multi-agent system for agents playing games in grid-like domains. The framework follows the BDI model of agency and is used as the main project for a seminar course on agent-oriented programming and design. When it comes to design, the Prometheus methodology has been used by relying on the Prometheus design tool (PDT) that supports the methodology. In terms of programming language, we have used the JACK agent platform. We believe the domains developed as part of the Multi-Agent Programming Contest are the right type of settings for evaluating our research, in that it provides enough complexity to explore various aspects of multi-agent development while being sufficiently bounded to make the development effort worthwhile. ",math
1119,Geometry of relative plausibility and relative belief of singletons,Annals of Mathematics and Artificial Intelligence,"The study of the interplay between belief and probability can be posed in a geometric framework, in which belief and plausibility functions are represented as points of simplices in a Cartesian space. Probability approximations of belief functions form two homogeneous groups, which we call ""affine"" and ""epistemic"" families. In this paper we focus on relative plausibility, belief, and uncertainty of probabilities of singletons, the ""epistemic"" family. They form a coherent collection of probability transformations in terms of their behavior with respect to Dempsters rule of combination. We investigate here their geometry in both the space of all pseudo belief functions and the probability simplex, and compare it with that of the affine family. We provide sufficient conditions under which probabilities of both families coincide. © 2010 Springer Science+Business Media B.V.",math
1120,Buy-at-Bulk Network Design with Protection,Mathematics of Operations Research," We consider approximation algorithms for buy-at-bulk network design, with the additional constraint that demand pairs be protected against a single edge or node failure in the network. In practice, the most popular model used in high speed telecommunication networks for protection against failures, is the so-called 1+1 model. In this model, two edge or node-disjoint paths are provisioned for each demand pair. We obtain the rst nontrivial approximation algorithms for buy-at-bulk network design in the 1+1 model for both edge and nodedisjoint protection requirements. Our results are for the single-cable cost model, which is prevalent in optical networks. More speci cally, we present a constant-factor approximation for the single-sink case, and an O log3 n approximation for the multi-commodity case. These results are of interest for practical applications and also suggest several new challenging theoretical problems. ",math
1121,A basis enumeration algorithm for linear systems with geometric applications,Applied Mathematics Letters," We present a new pivot-based algorithm which can be used with minor modification for the enumeration of all bases, or all feasible bases, of a linear system. The algorithm has the following properties: no additional storage is required beyond the input data; the output list produced is free of duplicates; the running time is output sensitive for non-degenerate inputs; the algorithm is easy to efficiently parallelize. It can be used for various geometric enumeration problems including Snding all facets of the convex hull of a set of points and the enumeration of all vertices of a convex polyhedron or hyperplane arrangement, and can be extended to oriented matroids. ",math
1122,The design of MA48: a code for the direct solution of sparse unsymmetric linear systems of equations,ACM Transactions on Mathematical Software," We describe the design of a new code for the direct solution of sparse unsymmetric linear systems of equations. The new code utilizes a novel restructuring of the symbolic and numerical phases, which increases speed and saves storage without sacrifice of numerical stability. Other features include switching to full-matrix processing in all phases of the computation enabling the use of all three levels of BLAS, treatment of rectangular or rank-deficient matrices, partial factorization, and integrated facilities for iterative refinement and error estimation. Categories and Subject Descriptors: G.1.3 [Mathematics of Computing]: Numerical Linear Algebra-linear systems (direct methods); sparse and very large systems Additional Key Words and Phrases: BLAS, block triangular form, error estimation, Gaussian elimination, sparse unsymmetric matrices This article describes the design of MA48, a collection of Fortran 77 subroutines for the direct solution of a sparse unsymmetric set of linear equations ACM Transactions on Mathematical Software, Vol. 22, No. 2, June 1996, Pages 187-226. ",math
1123,An inequality for probabilities,Proceedings of the American Mathematical Society," Corollary 2. If AkE%, k = l, 2, 3, • • • , with Yt=iP(.Ak) = +«< then P( Ut.fiAk) ^ 1/c w&ere _ "" (£'<->)' Received by the editors June 10, 1966. 1 This paper was written in part while the first author was a fellow of the Summer Research Institute of the Canadian Mathematical Congress. ",math
1124,The action of the heat operator on Jacobi forms,Proceedings of the American Mathematical Society,"We investigate the action of the heat operator on Jacobi forms. In particular, we present two explicit characterizations of this action on Jacobi forms of index 1. Furthermore, we study congruences and filtrations of Jacobi forms. As an application, we determine when an analog of Atkins U-operator applied to a Jacobi form is nonzero modulo a prime. © 2008 American Mathematical Society.",math
1125,On sparse and symmetric matrix updating subject to a linear equation,Mathematics of Computation,"A procedure for symmetric matrix updating subject to a linear equation and retaining any sparsity present in the original matrix is derived. The main feature of this procedure is the reduction of the problem to the solution of an n dimensional sparse system of linear equations. The matrix of this system is shown to be symmetric and positive definite. The method depends on the Frobenius matrix norm. Comments are made on the difficulties of extending the technique so that it uses more general norms, the main points being shown by a numerical example. © 1977 American Mathematical Society.",math
1126,Evaluation of Neighbourhood Selection Methods in Decentralized Recommendation Systems,SIAM Journal on Discrete Mathematics," Recommendation systems are important in social networks that allow the injection of user-generated content and let users indicate their preferences towards the content introduced by others. Considering the increase of usage of these collaborative systems, it seems only a matter of time before the current centralized systems will be replaced by decentralized solutions. However, current collaborative filtering systems assume that recommendations can be based on the entire data collection in the network. This work evaluates the performance of user-based collaborative filtering systems when only partial knowledge about the network is available at an end-user's computer. We propose a utility model that combines three important aspects of network users (similarity, confidence and usefulness) in order to create a semantic overlay network optimized for autonomous content recommendations. We compare different similarity functions on the most common dataset in collaborative filtering and we show the influence of the confidence and usefulness parameters on both dense and sparse data. We find that the commonly used similarity function results in sub-optimal performance when used as updating criterion for locally stored rating profiles. We show that taking into account the level of confidence in the computed similarity can greatly improve recommendation accuracy, especially when a small user neighborhood is selected. Also, conventional methods select many users that cannot contribute to the recommendation, because they have rated too few items. The usefulness parameter that we introduce compensates for this problem, so that even a small local cache in very sparse data provides valuable recommendations. ",math
1127,Arrowian characterizations of latticial federation consensus functions,Mathematical Social Sciences,"In this paper we begin the construction of an abstract axiomatic theory of consensus functions in order to account for several classes of similar concrete results in the domain of the axiomatic approach to consensus. Such results, which belong to various fields, are mentioned in our introduction and conclusion. Among the consensus functions defined on a semilattice L, i.e. the functions Ln → L, we study the federation consensus functions associated with cohereditary families of subsets of {1,...,n} called federations (or simple games). We obtain several axiomatic characterizations of all federation consensus functions, or of significant subclasses of such functions, and especially of the meet projection (oligarchic) consensus functions. These characterizations depend on the structural properties of the considered semilattice, which are the key to understanding the concrete results mentioned above and allow us to obtain new results. We end by pointing out some further lines of research. © 1990.",math
1128,A theorem on parallel processing models with a generalized stopping rule,Mathematical Social Sciences," Inequalities on reaction time distribution functions for parallel processing models with an unlimited capacity assumption are presented, extending previous work on ¯rst-terminating and exhaustive stopping rules to kth-terminating processes. This extension thus generates predictions for situations in which the observer's response is determined by the kth-terminating subprocess. Moreover, methods to determine the number k are discussed. ",math
1129,Predicting optimal solution cost with conditional probabilities,Annals of Mathematics and Artificial Intelligence," Heuristic search algorithms are designed to return an optimal path from a start state to a goal state. They find the optimal solution cost as a side effect. However, there are applications in which all one wants to know is an estimate of the optimal solution cost. The actual path from start to goal is not initially needed. For instance, one might be interested in quickly assessing the monetary cost of a project for bidding purposes. In such cases only the cost of executing the project is required. The actual construction plan could be formulated later, after bidding. In this paper we propose an algorithm, named Solution Cost Predictor (SCP), that accurately and efficiently predicts the optimal solution cost of a problem instance without finding the actual solution. While SCP can be viewed as a heuristic function, it differs from a heuristic conceptually in that: 1) SCP is not required to be fast enough to guide search algorithms; 2) SCP is not required to be admissible; 3) our measure of effectiveness is the prediction accuracy, which is in contrast to the solution quality and number of nodes expanded used to measure the effectiveness of heuristic functions. We show empirically that SCP makes accurate predictions on several heuristic search benchmarks. ",math
1130,CONTEST: A Controllable Test Matrix Toolbox for MATLAB,ACM Transactions on Mathematical Software,"Large, sparse networks that describe complex interactions are a common feature across a number of disciplines, giving rise to many challenging matrix computational tasks. Several random graph models have been proposed that capture key properties of real-life networks. These models provide realistic, parametrized matrices for testing linear system and eigenvalue solvers. CONTEST (CONtrollable TEST matrices) is a random network toolbox for MATLAB that implements nine models. The models produce unweighted directed or undirected graphs; that is, symmetric or unsymmetric matrices with elements equal to zero or one. They have one or more parameters that affect features such as sparsity and characteristic pathlength and all can be of arbitrary dimension. Utility functions are supplied for rewiring, adding extra shortcuts and subsampling in order to create further classes of networks. Other utilities convert the adjacency matrices into real-valued coefficient matrices for naturally arising computational tasks that reduce to sparse linear system and eigenvalue problems. © 2009 ACM.",math
1131,Optimal Auction Design,Mathematics of Operations Research," Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at http://www.jstor.org/action/showPublisher?publisherCode=informs. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. JSTOR is a not-for-profit organization founded in 1995 to build trusted digital archives for scholarship. We work with the scholarly community to preserve their work and the materials they rely upon, and to build a common research platform that promotes the discovery and use of these resources. For more information about JSTOR, please contact support@jstor.org. ",math
1132,"A further generalization of the Kakutani fixed theorem, with application to Nash equilibrium points",Proceedings of the American Mathematical Society," Introduction. Kakutani's fixed point theorem [3]1 states that in Euclidean «-space a closed point to (nonvoid) convex set map of a convex compact set into itself has a fixed point. Kakutani showed that this implied the minimax theorem for finite games. The object of this note is to point out that Kakutani's theorem may be extended to convex linear topological spaces, and implies the minimax theorem for continuous games with continuous payoff as well as the existence of Nash equilibrium points. ",math
1133,Reducing reinforcement learning to KWIK online regression,Annals of Mathematics and Artificial Intelligence," One of the key problems in reinforcement learning (RL) is balancing exploration and exploitation. Another is learning and acting in large Markov decision processes (MDPs) where compact function approximation has to be used. This paper introduces REKWIRE, a provably efficient, model-free algorithm for finitehorizon RL problems with value function approximation (VFA) that addresses the exploration-exploitation tradeoff in a principled way. The crucial element of this algorithm is a reduction of RL to online regression in the recently proposed KWIK learning model. We show that, if the KWIK online regression problem can be solved efficiently, then the sample complexity of exploration of REKWIRE is polynomial. Therefore, the reduction suggests a new and sound direction to tackle general RL problems. The efficiency of our algorithm is verified on a set of proof-of-concept experiments where popular, ad hoc exploration approaches fail. ",math
1134,Automated Deduction Techniques for the Management of Personalized Documents,Annals of Mathematics and Artificial Intelligence," This work is about a “real-world” application of automated deduction. The application is the management of documents (such as mathematical textbooks) that are decomposed (“sliced”) into small units. A particular application task is to assemble a new document from such units in a selective way, based on the user's current interest. It is argued that this task can be naturally expressed through logic, and that automated deduction technology can be exploited for solving it. More precisely, we rely on full first-order clausal logic (beyond Horn logic) with some default negation principle, and we propose a model computation theorem prover as a suitable deduction mechanism. On the theoretical side, we start with the hyper tableau calculus and modify it to suit the selected logic. On the practical side, only little modifications of an existing implementation were necessary. Beyond solving the task at hand as such, it is hoped to contribute with this work to the quest for arguments in favor of automated deduction techniques in the “real world”, in particular why they are possibly the best choice. ",math
1135,Algorithm 805: computation and uses of the semidiscrete matrix decomposition,ACM Transactions on Mathematical Software," We present algorithms for computing a semidiscrete approximation to a matrix in a weighted norm, with the Frobenius norm as a special case. The approximation is formed as a weighted sum of outer products of vectors whose elements are 61 or 0, so the storage required by the approximation is quite small. We also present a related algorithm for approximation of a tensor. Applications of the algorithms are presented to data compression, filtering, and information retrieval; software is provided in C and in Matlab. Categories and Subject Descriptors: D.3.2 [Programming Languages]: Language Classifications-C; Matlab; G.1.2 [Numerical Analysis]: Approximation; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval Additional Key Words and Phrases: Singular value decomposition, semidiscrete decomposition, latent semantic indexing, compression, matrix decomposition ",math
1136,Valuated Matroid Intersection I: Optimality Criteria,SIAM Journal on Discrete Mathematics," The independent assignment problem (or the weighted matroid intersection problem) is extended using Dress-Wenzel's matroid valuations, which are attached to the vertex set of the underlying bipartite graph as an additional weighting. Specifically, the problem considered is: Given a bipartite graph G = (V +, V ¡; A) with arc weight w : A → R and matroid valuations ω+ and ω¡ on V + and V ¡ respectively, find a matching M (⊆ A) that maximizes P{w(a) | a ∈ M } + ω+(∂+M ) + ω¡(∂¡M ), where ∂+M and ∂¡M denote the sets of vertices in V + and V ¡ incident to M . As natural extensions of the previous results for the independent assignment problem, two optimality criteria are established; one in terms of potentials and the other in terms of negative cycles in an auxiliary graph. ",math
1137,Counting and Detecting Small Subgraphs via Equations,SIAM Journal on Discrete Mathematics," We present a general technique for detecting and counting small subgraphs. It consists of forming special linear combinations of the numbers of occurrences of different induced subgraphs of fixed size in a graph. These combinations can be efficiently computed by rectangular matrix multiplication. Our two main results utilizing the technique are as follows. Let H be a fixed graph with k vertices and an independent set of size s. 1. Detecting if an n-vertex graph contains a (not necessarily induced) subgraph isomorphic to H can be done in time O(nω( (k−s)/2 ,1, (k−s)/2 )), where ω(p, q, r) is the exponent of fast arithmetic matrix multiplication of an np × nq matrix by an nq × nr matrix. 2. When s = 2, counting the number of (not necessarily induced) subgraphs isomorphic to H can be done in the same time, i.e., in time O(nω( (k−2)/2 ,1, (k−2)/2 )). ",math
1138,Planning in partially-observable switching-mode continuous domains,Annals of Mathematics and Artificial Intelligence," Continuous-state POMDPs provide a natural representation for a variety of tasks, including many in robotics. However, most existing parametric continuousstate POMDP approaches are limited by their reliance on a single linear model to represent the world dynamics. We introduce a new switching-state dynamics model that can represent multi-modal state-dependent dynamics. We present the Switching Mode POMDP (SM-POMDP) planning algorithm for solving continuousstate POMDPs using this dynamics model. We also consider several procedures to approximate the value function as a mixture of a bounded number of Gaussians. Unlike the majority of prior work on approximate continuous-state POMDP planners, we provide a formal analysis of our SM-POMDP algorithm, providing bounds, where possible, on the quality of the resulting solution. We also analyze the computational complexity of SM-POMDP. Empirical results on an unmanned aerial vehicle collisions avoidance simulation, and a robot navigation simulation where the robot ",math
1139,Optimization of heuristic search using recursive algorithm selection and reinforcement learning,Annals of Mathematics and Artificial Intelligence,"The traditional approach to computational problem solving is to use one of the available algorithms to obtain solutions for all given instances of a problem. However, typically not all instances are the same, nor a single algorithm performs best on all instances. Our work investigates a more sophisticated approach to problem solving, called Recursive Algorithm Selection, whereby several algorithms for a problem (including some recursive ones) are available to an agent that makes an informed decision on which algorithm to select for handling each sub-instance of a problem at each recursive call made while solving an instance. Reinforcement learning methods are used for learning decision policies that optimize any given performance criterion (time, memory, or a combination thereof) from actual execution and profiling experience. This paper focuses on the well-known problem of state-space heuristic search and combines the A* and RBFS algorithms to yield a hybrid search algorithm, whose decision policy is learned using the Least-Squares Policy Iteration (LSPI) algorithm. Our benchmark problem domain involves shortest path finding problems in a real-world dataset encoding the entire street network of the District of Columbia (DC), USA. The derived hybrid algorithm exhibits better performance results than the individual algorithms in the majority of cases according to a variety of performance criteria balancing time and memory. It is noted that the proposed methodology is generic, can be applied to a variety of other problems, and requires no prior knowledge about the individual algorithms used or the properties of the underlying problem instances being solved. © 2010 Springer Science+Business Media B.V.",math
1140,Some Estimates for a Weighted L 2 Projection,Mathematics of Computation,"This paper is devoted to the error estimates for some weighted L projections. Nearly optimal estimates are obtained. These estimates can be applied to the analysis of the usual multigrid method, multilevel preconditioner and domain decomposition method for solving elliptic boundary problems whose coefficients have large jump discontinuities. ÂŠ 1991 American Mathematical Society.",math
1141,A new methodology for developing deduction methods,Annals of Mathematics and Artificial Intelligence," This paper explores the use of resolution as a meta-framework for developing various, different deduction calculi. In this work the focus is on developing deduction calculi for modal dynamic logics. Dynamic modal logics are PDL-like extended modal logics which are closely related to description logics. We show how tableau systems, modal resolution systems and Rasiowa-Sikorski systems can be developed and studied by using standard principles and methods of first-order theorem proving. The approach is based on the translation of reasoning problems in modal logic to first-order clausal form and using a suitable refinement of resolution to construct and mimic derivations of the desired proof method. The inference rules of the calculus can then be read off from the clausal form. We show how this approach can be used to generate new proof calculi and prove soundness, completeness and decidability results. This slightly unusual approach allows us to gain new insights and results for familiar and less familiar logics, for different proof methods, and compare them not only theoretically but also empirically in a uniform framework. ",math
1142,A bound on solutions of linear integer equalities and inequalities,Proceedings of the American Mathematical Society," Consider a system of linear equalities and inequalities with integer coefficients. We describe the set of rational solutions by a finite generating set of solution vectors. The entries of these vectors can be bounded by the absolute value of a certain subdeterminant. The smallest integer solution of the system has coefficients not larger than this subdeterminant times the number of indeterminates. Up to the latter factor, the bound is sharp. ",math
1143,Hyperequivalence of logic programs with respect to supported models,Annals of Mathematics and Artificial Intelligence," Recent research in nonmonotonic logic programming has focused on program equivalence relevant for program optimization and modular programming. So far, most results concern the stable-model semantics. However, other semantics for logic programs are also of interest, especially the semantics of supported models which, when properly generalized, is closely related to the autoepistemic logic of Moore. In this paper, we consider a framework of equivalence notions for logic programs under the supported (minimal) modelsemantics and provide characterizations for this framework in model-theoretic terms. We use these characterizations to derive complexity results concerning testing hyperequivalence of logic programs wrt supported (minimal) models. ",math
1144,Integer Programming and Arrovian Social Welfare Functions,Mathematics of Operations Research," We formulate the problem of deciding which preference domains admit a non-dictatorial Arrovian Social Welfare Function as one of verifying the feasibility of an integer linear program. Many of the known results about the presence or absence of Arrovian Social Welfare Functions, impossibility theorems in Social Choice theory, and properties of majority rules etc., can be derived in a simple and unified way from this integer program. We characterize those preference domains that admit a non-dictatorial, neutral Arrovian Soical Welfare function and give a polyhedral characterization of Arrovian Social Welfare Functions on single-peaked domains. ",math
1145,An extended Čencov characterization of the information metric,Proceedings of the American Mathematical Society," Cencov has shown that Riemannian metrics which are derived from the Fisher information matrix are the only metrics which preserve inner products under certain probabilistically important mappings. In Cencov's theorem, the underlying differentiable manifold is the probability simplex E""x, = 1, x¡ > 0. For some purposes of using geometry to obtain insights about probability, it is more convenient to regard the simplex as a hypersurface in the positive cone. In the present paper Cencov's result is extended to the positive cone. The proof uses standard techniques of differential geometry but does not use the language of category theory. ",math
1146,Convergence analysis of sample average approximation methods for a class of stochastic mathematical programs with equality constraints,Mathematics of Operations Research," In this paper we discuss the sample average approximation (SAA) method for a class of stochastic programs with nonsmooth equality constraints. We derive a uniform Strong Law of Large Numbers for random compact set-valued mappings and use it to investigate the convergence of Karush-Kuhn-Tucker points of SAA programs as the sample size increases. We also study the exponential convergence of global minimizers of the SAA problems to their counterparts of the true problem. The convergence analysis is extended to a smoothed SAA program. Finally, we apply the established results to a class of stochastic mathematical programs with complementarity constraints and report some preliminary numerical test results. ",math
1147,A reconstruction formula for band limited functions in $L,Proceedings of the American Mathematical Society," It is shown that a band limited function from L2(Rd) can be reconstructed from irregularly sampled values as a limit of spline functions. The assumption about the sampling sequence is that it should be dense enough. It is known that band limited functions, i.e. functions whose Fourier transform have compact support, are uniquely determined and can be recovered from their values on speci c discrete set of points xn. The classical Whittaker-Shannon Theorem gives the sharp answer in the case of equally spaced points in R. The irregularly spaced sets of sampling points were considered by Paley and Wiener [6]. More recent results and extensive lists of references can be found in surveys of Benedetto [1] and Feichtinger and Gr¨ochening [2]. We prove that a band limited function from L2(Rd) can be recovered from its values on speci c irregularly distributed sampling points as the limit of d-dimensional polyharmonic spline functions. The assumption about the sampling sequence is that it should be dense enough. Our reconstruction formula is very stable due to the fact that fundamental splines have exponential decay. Moreover, we prove that convergence takes place not only in the space Lp(Rd), 2 p 1, but also in the Ck norm for any k > 0. In particular, it shows the way to reconstruct all derivatives of a band limited function using samples of the function. Splines as a tool for reconstruction in the one-dimensional case were used by Schoenberg [9] for equally spaced knots and recently by Lyubarskii and Madych [3] under the assumption that corresponding exponential functions form a Riesz basis in L2([− ; ]). Technically our paper is close to [5]. Our proofs rely explicitly on the observation that certain constants in inequalities from Lemmas 1 and 2 below have a very special form: they depend exponentially of the order of derivatives involved in inequalities. This allows us to control the convergence when the order of derivatives goes to in nity. This technique is very flexible and can be used to solve the reconstruction problem in new situations [7], [8]. A new application of this technique is also given at the end of this paper. Let X ( ) denote a countable set fxγ g 2 Rd such that the rectangles D(x ; ) of diameter each containing exactly one point x form a disjoint cover of Rd. ",math
1148,MPFR: A multiple-precision binary floating-point library with correct rounding,ACM Transactions on Mathematical Software," This paper presents a multiple-precision binary floating-point library, written in the ISO C language, and based on the GNU MP library. Its particularity is to extend to arbitrary-precision ideas from the IEEE 754 standard, by providing correct rounding and exceptions. We demonstrate how these strong semantics are achieved - with no significant slowdown with respect to other arbitrary-precision tools - and discuss a few applications where such a library can be useful. Categories and Subject Descriptors: D.3.0 [Programming Languages]: General-Standards; G.1.0 [Numerical Analysis]: General-computer arithmetic, multiple precision arithmetic; G.1.2 [Numerical Analysis]: Approximation-elementary and special function approximation; G 4 [Mathematics of Computing]: Mathematical Software-algorithm design, efficiency, portability General Terms: algorithms, standardization, performance Additional Key Words and Phrases: multiple-precision arithmetic, IEEE 754 standard, floatingpoint arithmetic, correct rounding, elementary function, portable software Introduction and Motivation The ANSI/IEEE 754-1985 standard for floating-point arithmetic (IEEE 754) has now become a common standard, even if some features like gradual underflow (i.e. subnormals) are still discussed [IEEE 1985]. An important consequence is that programs using the formats and operations specified by IEEE 754 have exactly the same behaviour on every configuration, as long as the processor, operating system and compiler are IEEE 754 compliant1. Another consequence is that researchers are able to design efficient algorithms using those formats and operations, and prove their correctness: interval arithmetic, floating-point expansions [Priest 1991], correctly-rounded elementary functions [Ziv 1991; The Arenaire project 2005; de Dinechin et al. 2005; Sun Microsystems 2004]. Thus, even if the IEEE 754 standard received several criticisms - both from hardware constructors who argued it would be too difficult to implement in a chip, and from software engineers who thought it would significantly slow down their programs - it is now accepted by everybody, and has enabled great progress in terms of correctness and portability of numerical software. However, the IEEE 754 standard specifies fixed formats only, in particular single and double precision, with respectively 24 and 53 bits of mantissa. Several software tools exist for multiple-precision floating-point arithmetic, for example, MP [Brent 1978], GMP [Granlund 2004], CLN [Haible and Kreckel 2005], PARI/GP [Batut 1Assuming no extended precision is used internally, and the compiler does not over-optimize floating-point expressions. ",math
1149,Vertex-Ant-Walk – A robust method for efficient exploration of faulty graphs,Annals of Mathematics and Artificial Intelligence," We consider a problem of decentralized exploration of a faulty network by several simple, memoryless agents. The model we adopt for a network is a directed graph. We design an asynchronous algorithm that can cope with failures of network edges and nodes. The algorithm is self-stabilizing in the sense that it can be started with arbitrary initializations and scalable new agents can be added while other agents are already running. ",math
1150,On the boundedness of an iterative procedure for solving a system of linear inequalities,Proceedings of the American Mathematical Society," In this paper it is proved that the perceptron correction procedure stays bounded, even when no solution system of linear inequalities exists. This supplements papers by B. Efron and by M. Minsky and S. Papert. 1. Introduction. Let F-{<b} be a finite set of vectors in Euclidean «-space En, and consider the problem of finding a vector w in En such that Received by the editors February 10, 1970. AMS 1968 subject classifications. Primary 1550, 6510, 6533; Secondary 6530, 9370,9440. ",math
1151,Some tests of generalized bisection,ACM Transactions on Mathematical Software," This paper addresses the task of reliably finding approximations to all solutions to a system of nonlinear equations within a region defined by bounds on each of the individual coordinates. Various forms of generalized bisection were proposed some time ago for this task. This paper systematically compares such generalized bisection algorithms to themselves, to continuation methods, and to hybrid steepest descent/quasi-Newton methods. A specific algorithm containing novel “expansion” and “exclusion” steps is fully described, and the effectiveness of these steps is evaluated. A test problem consisting of a small, high-degree polynomial system that is appropriate for generalized bisection, but very difticult for continuation methods, is presented. This problem forms part of a set of 17 test problems from published literature on the methods being compared; this test set is fully described here. Additional Key Words and Phrases: Generalized bisection, global constrained optimization, homotopy method, interval arithmetic, MINPACK, quasi-Newton method ACM Transactions on Mathematical Software, Vol. 13, No. 3, September1987,Pages 197-220. ",math
1152,On descent from local minima,Mathematics of Computation," When a local minimum of a function of several variables has been found by use of an algorithm for finding such minima numerically, one often runs the same algorithm many times with different starting values in the hopes of finding a lower minimum. Here, under the assumption that a local minimum is known, a process with analytical criteria is described which sometimes finds smaller local minima in an algorithmic manner. Methods of descent are useful for minimizing functions of several variables. Generally, one can always obtain points (if such exist) for which the gradient vanishes, and moreover, points which are local minima. At saddle points one can continue descent with second derivative information. A point which is a local minimum for a function may or may not be a global minimum. At this juncture one resorts to search techniques to attempt to further decrease the function. The process to be described sometimes finds smaller local minima in an algorithmic manner with analytical criteria. One has no general test, of course, for a global minimum. Consider first the problem of finding the global minimum for a 2«th degree polynomial Pi(x) in one variable. The coefficient of x2n will be positive. Let Xi be a local minimizer of i\. Then one may write if xx is not a global minimum, then for some point £ ^ Xi, P2(xu Q < 0, so that *(9 < *(xi). We seek such a point by trying to find a local minimum of P2(xi, x). Suppose that at x = x2, P2(xi, x) has a local minimum. If P2(x1;x2) < 0, x2 may be used as a new starting value to find a lower minimum of Pi(x). On the other hand, if Received March 30, 1970. AMS 1970 subject classifications. Primary 65D99, 68A10. ",math
1153,Delayed theory combination vs. Nelson-Oppen for satisfiability modulo theories: a comparative analysis,Annals of Mathematics and Artificial Intelligence,"Most state-of-the-art approaches for Satisfiability Modulo Theories (SMT (T)) rely on the integration between a SAT solver and a decision procedure for sets of literals in the background theory T(T -slover). Often T is the combination T1 ∪ T2 of two (or more) simpler theories (SMT(T1 ∪ T2)), s.t. the specific Ti-solvers must be combined. Up to a few years ago, the standard approach to SMT(T1 ∪ T2)was to integrate the SAT solver with one combined T1 ∪ T2-solver, obtained from two distinct Ti-solvers by means of evolutions of Nelson and Oppens (NO) combination procedure, in which the Ti-solvers deduce and exchange interface equalities. Nowadays many state-of-the-art SMT solvers use evolutions of a more recent SMT(T1 ∪ T2) procedure called Delayed Theory Combination (DTC), in which each Ti-solvers interacts directly and only with the SAT solver, in such a way that part or all of the (possibly very expensive) reasoning effort on interface equalities is delegated to the SAT solver itself. In this paper we present a comparative analysis of DTC vs. NO for SMT(T1 ∪ T2). On the one hand, we explain the advantages of DTC in exploiting the power of modern SAT solvers to reduce the search. On the other hand, we show that the extra amount of Boolean search required to the SAT solver can be controlled. In fact, we prove two novel theoretical results, for both convex and non-convex theories and for different deduction capabilities of the Ti-solvers, which relate the amount of extra Boolean search required to the SAT solver by DTC with the number of deductions and case-splits required to the Ti-solvers by NO in order to perform the same tasks: (i) under the same hypotheses of deduction capabilities of the Ti-solvers required by NO, DTC causes no extra Boolean search; (ii) using with limited or no deduction capabilities, the extra Boolean search required can be reduced down to a negligible amount by controlling the quality of the T-conflict sets returned by the Ti-solvers. © Springer Science+Business Media B.V.2009.",math
1154,TestU01: A C library for empirical testing of random number generators,ACM Transactions on Mathematical Software," We introduce TestU01, a software library implemented in the ANSI C language, and offering a collection of utilities for the empirical statistical testing of uniform random number generators (RNGs). It provides general implementations of the classical statistical tests for RNGs, as well as several others tests proposed in the literature, and some original ones. Predefined tests suites for sequences of uniform random numbers over the interval (0, 1) and for bit sequences are available. Tools are also offered to perform systematic studies of the interaction between a specific test and the structure of the point sets produced by a given family of RNGs. That is, for a given kind of test and a given class of RNGs, to determine how large should be the sample size of the test, as a function of the generator's period length, before the generator starts to fail the test systematically. Finally, the library provides various types of generators implemented in generic form, as well as many specific generators proposed in the literature or found in widely used software. The tests can be applied to instances of the generators predefined in the library, or to user-defined generators, or to streams of random numbers produced by any kind of device or stored in files. Besides introducing TestU01, the article provides a survey and a classification of statistical tests for RNGs. It also applies batteries of tests to a long list of widely used RNGs. Categories and Subject Descriptors: G.4 [Mathematical Software]; G.3 [Probability and Statistics]-Random number generation, statistical software ",math
1155,Topological Grammars for Data Approximation,Applied Mathematics Letters," A method of topological grammars is proposed for multidimensional data approximation. For data with complex topology we define a principal cubic complex of low dimension and given complexity that gives the best approximation for the dataset. This complex is a generalization of linear and non-linear principal manifolds and includes them as particular cases. The problem of optimal principal complex construction is transformed into a series of minimization problems for quadratic functionals. These quadratic functionals have a physically transparent interpretation in terms of elastic energy. For the energy computation, the whole complex is represented as a system of nodes and springs. Topologically, the principal complex is a product of onedimensional continuums (represented by graphs), and the grammars describe how these continuums transform during the process of optimal complex construction. This factorization of the whole process onto one-dimensional transformations using minimization of quadratic energy functionals allows us to construct efficient algorithms. c 2006 Elsevier Ltd. All rights reserved. ",math
1156,Efficiency Loss in a Network Resource Allocation Game,Mathematics of Operations Research," We explore the properties of a congestion game in which users of a congested resource anticipate the effect of their actions on the price of the resource. When users are sharing a single resource, we establish that the aggregate utility received by the users is at least 3/4 of the maximum possible aggregate utility. We also consider extensions to a network context, where users submit individual payments for each link in the network they may wish to use. In this network model, we again show that the selfish behavior of the users leads to an aggregate utility that is no worse than 3/4 of the maximum possible aggregate utility. We also show that the same analysis extends to a wide class of resource allocation systems where end users simultaneously require multiple scarce resources. These results form part of a growing literature on the “price of anarchy,” i.e., the extent to which selfish behavior affects system efficiency. ",math
1157,An analysis of the Rayleigh—Ritz method for approximating eigenspaces,Mathematics of Computation,"This paper concerns the Rayleigh-Ritz method for computing an approximation to an eigenspace X of a general matrix A from a subspace W that contains an approximation to X. The method produces a pair (N, X̃) that purports to approximate a pair (L, X), where X is a basis for X and AX = XL. In this paper we consider the convergence of (N, X̃) as the sine ε of the angle between X and W approaches zero. It is shown that under a natural hypothesis - called the uniform separation condition - the Ritz pairs (N, X̃) converge to the eigenpair (L, X). When one is concerned with eigenvalues and eigenvectors, one can compute certain refined Ritz vectors whose convergence is guaranteed, even when the uniform separation condition is not satisfied. An attractive feature of the analysis is that it does not assume that A has distinct eigenvalues or is diagonalizable.",math
1158,Cross-conformal predictors,Annals of Mathematics and Artificial Intelligence,"Inductive conformal predictors have been designed to overcome the computational inefficiency exhibited by conformal predictors for many underlying prediction algorithms. Whereas computationally efficient, inductive conformal predictors sacrifice different parts of the training set at different stages of prediction, which affects their informational efficiency. This paper introduces the method of cross-conformal prediction, which is a hybrid of the methods of inductive conformal prediction and cross-validation, and studies its validity and informational efficiency empirically. The computational efficiency of cross-conformal predictors is comparable to that of inductive conformal predictors, and they produce valid predictions in our empirical studies.",math
1159,Characterizing functional dependencies in formal concept analysis with pattern structures,Annals of Mathematics and Artificial Intelligence,"Computing functional dependencies from a relation is an important database topic, with many applications in database management, reverse engineering and query optimization. Whereas it has been deeply investigated in those fields, strong links exist with the mathematical framework of Formal Concept Analysis. Considering the discovery of functional dependencies, it is indeed known that a relation can be expressed as the binary relation of a formal context, whose implications are equivalent to those dependencies. However, this leads to a new data representation that is quadratic in the number of objects w.r.t. the original data. Here, we present an alternative avoiding such a data representation and show how to characterize functional dependencies using the formalism of pattern structures, an extension of classical FCA to handle complex data. We also show how another class of dependencies can be characterized with that framework, namely, degenerated multivalued dependencies. Finally, we discuss and compare the performances of our new approach in a series of experiments on classical benchmark datasets.",math
1160,Assisted proof document authoring,Mathematical Knowledge Management," Recently, significant advances have been made in formalised mathematical texts for large, demanding proofs. But although such large developments are possible, they still take an inordinate amount of effort and time, and there is a significant gap between the resulting formalised machine-checkable proof scripts and the corresponding human-readable mathematical texts. We present an authoring system for formal proof which addresses these concerns. It is based on a central document format which, in the tradition of literate programming, allows one to extract either a formal proof script or a human-readable document; the two may have differing structure and detail levels, but are developed together in a synchronised way. Additionally, we introduce ways to assist production of the central document, by allowing tools to contribute backflow to update and extend it. Our authoring system builds on the new PG Kit architecture for Proof General, bringing the extra advantage that it works in a uniform interface, generically across various interactive theorem provers. ",math
1161,Experimental evaluation of pheromone models in ACOPlan,Annals of Mathematics and Artificial Intelligence," ACOplan is a planner based on the ant colony optimization framework. Using the ACO framework to solve planning optimization problems, one of the main issues to address is the choice of informative and easy to compute pheromone models. In this paper we present and discuss an experimental evaluation of the several pheromone models implemented in ACOPlan. The experiments have been run solving the problems used in the last planning competition. The results suggest that fuzzy pheromone models represents a suitable tradeoff between performance and cost. ",math
1162,"Products of indecomposable, aperiodic, stochastic matrices",Proceedings of the American Mathematical Society," 1. Introduction. A finite square chastic if pij^Q for all i, j, and matrix P is called indecomposable matrix P ={/»«} is called stoHipa=l for all i. A stochastic and aperiodic (SIA) if exists and all the rows of Q are the same. SIA matrices are defined differently in books on probability theory; see, for example, [l] or [2]. The latter definition is more intuitive, takes longer to state, is easier to verify, and explains why the probabilist is interested in SIA matrices. A theorem in probability theory or matrix theory then says that the customary definition is equivalent to the one we have given. The latter is brief and emphasizes the property which will interest us in this note. We define Ô(P) by 5(P) = max max | pixi - pitj \ . i fi.ti Thus S(P) measures, in a certain sense, how different the rows of P are. If the rows of P are identical, 5(P) =0 and conversely. Let Ai, ■ • • , At be any square matrices of the same order. By a word (in the A's) of length t we mean the product of t A's (repetitions permitted). The object of this note is to prove the following: Presented to the Society, June 6, 1962; received by the editors June 7, 1962. 1 Research under contract with the Office of Scientific Research of the U. S. Air Force. ",math
1163,Analysis of iterative methods for saddle point problems: a unified approach,Mathematics of Computation," In this paper two classes of iterative methods for saddle point problems are considered: inexact Uzawa algorithms and a class of methods with symmetric preconditioners. In both cases the iteration matrix can be transformed to a symmetric matrix by block diagonal matrices, a simple but essential observation which allows one to estimate the convergence rate of both classes by studying associated eigenvalue problems. The obtained estimates apply for a wider range of situations and are partially sharper than the known estimates in literature. A few numerical tests are given which con rm the sharpness of the estimates. ",math
1164,Bounds for the determinant of the sum of hermitian matrices,Proceedings of the American Mathematical Society," Best possible lower and upper bounds for the determinant of the sum of two hermitian matrices in terms of the eigenvalues of both matrices are obtained. It is the main purpose of this note to prove the following Proof of the Lemma. If P = 7, the identity matrix, (3) is immediate. Then it suffices to use this for the second term in det(P+e0 = detPdet(7+€<2P-1). ",math
1165,Solving job shop scheduling with setup times through constraint-based iterative sampling: an experimental analysis,Annals of Mathematics and Artificial Intelligence,"This paper presents a heuristic algorithm for solving a job-shop scheduling problem with sequence dependent setup times and min/max separation constraints among the activities (SDST-JSSP/max). The algorithm relies on a core constraint-based search procedure, which generates consistent orderings of activities that require the same resource by incrementally imposing precedence constraints on a temporally feasible solution. Key to the effectiveness of the search procedure is a conflict sampling method biased toward selection of most critical conflicts and coupled with a non-deterministic choice heuristic to guide the base conflict resolution process. This constraint-based search is then embedded within a larger iterative-sampling search framework to broaden search space coverage and promote solution optimization. The efficacy of the overall heuristic algorithm is demonstrated empirically both on a set of previously studied job-shop scheduling benchmark problems with sequence dependent setup times and by introducing a new benchmark with setups and generalized precedence constraints. © 2011 Springer Science+Business Media B.V.",math
1166,Centers for Random Walks on Trees,SIAM Journal on Discrete Mathematics," We consider two distinct centers which arise in measuring how quickly a random walk on a tree mixes. Lovasz and Winkler [9] point out that stopping rules which \look where they are going"" (rather than simply walking a xed number of steps) can achieve a desired distribution exactly and e ciently. Considering an optimal stopping rule that re ects some aspect of mixing, we can use the expected length of this rule as a mixing measure. On trees, a number of these mixing measures identify particular nodes with central properties. In this context, we study a variety of natural notions of centrality. Each of these criteria identi es the barycenter of the tree as the \average"" center and the newly de ned focus as the \extremal"" center. ",math
1167,THE PRODUCT FORM FOR THE INVERSE IN THE SIMPLEX METHOD,Mathematics of Computation," Now it is easily seen that the error in any approximation Pí/Qí is of the order of (Qi)~2 and therefore, per average cycle, the error should decrease by a factor of 4{K + (4 + K2)*}~2 = 1/9.1 or approximately one decimal place. ",math
1168,Easy constructions in complexity theory: Gap and speed-up theorems,Proceedings of the American Mathematical Society," Perhaps the two most basic phenomena discovered by the recent application of recursion theoretic methods to the developing theories of computational complexity have been Blum's speed-up phenomena, with its extension to operator speed-up by Meyer and Fischer, and the Borodin gap phenomena, with its extension to operator gaps by Constable. In this paper we present a proof of the operator gap theorem which is much simpler than Constable's proof. We also present an improved proof of the Blum speed-up theorem which has a straightforward generalization to obtain operator speed-ups. The proofs of this paper are new; the results are not. The proofs themselves are entirely elementary: we have eliminated all priority mechanisms and all but the most transparent appeals to the recursion theorem. Even these latter appeals can be eliminated in some ""reasonable"" complexity measures. Implicit in the proofs is what we believe to be a new method for viewing the construction of ""complexity sequences."" Unspecified notation follows Rogers [12]. Xi(ptis any standard indexing of the partial recursive functions. N is the set of all nonnegative integers. XiDi is a canonical indexing of all finite subsets of N: from i we can list Di and know when the listing is completed. Similarly, XiFt is a canonical indexing of all finite functions defined (exactly) on some initial segment {0, 1, 2, • • • , n}. X./bi is any Blum measure of computational complexity or resource. Specifically, for all i, domain Oi=domain fa, and the ternary relation <P¿(x)^jy is decidable (recursive). Intuitively, 0¿(x) might be the amount of time or space used by Turing machine /' or the ¿th ALGOL ",math
1169,Optimal matching and empirical measures,Proceedings of the American Mathematical Society," Using results from optimal matching, we find the exact order of convergence for ß(Pn,P) and p(Pn, P) , where ß denotes the dual bounded Lipschitz metric, p the Prokhorov metric and P„ the nth empirical measure associated to P , the uniform measure on the unit square. The results solve a long-open problem in empirical measures. This article describes some interesting connections between certain optimal matching problems and Glivenko-Cantelli convergence of empirical measures. More precisely, it is observed that average edge length matching and minimax grid matching problems have close connections to empirical processes. Classical duality results from linear programming as well as the deep recent results of Shor [Shi] and Leighton and Shor [LS] play an important connecting role. It turns out that tight bounds for these matching problems are essentially equivalent to sharp rates for Glivenko-Cantelli convergence. In this way, some long-open problems in empirical processes are solved in a relatively simple fashion. Also, some new estimates for matching problems are found in the non-i.i.d. case. Notation. Throughout, ¿P(R ) denotes the set of all Borel probability on R , d > 1 . Weak-star convergence in 3°(R ) is metrizable metrics; the following definition recalls two of them. ",math
1170,When Does a Mixture of Products Contain a Product of Mixtures?,SIAM Journal on Discrete Mathematics," We derive relations between theoretical properties of restricted Boltzmann machines (RBMs), popular machine learning models which form the building blocks of deep learning models, and several natural notions from discrete mathematics and convex geometry. We give implications and equivalences relating RBM-representable probability distributions, perfectly reconstructible inputs, Hamming modes, zonotopes and zonosets, point con gurations in hyperplane arrangements, linear threshold codes, and multi-covering numbers of hypercubes. As a motivating application, we prove results on the relative representational power of mixtures of product distributions and products of mixtures of pairs of product distributions (RBMs) that formally justify widely held intuitions about distributed representations. In particular, we show that a mixture of products requiring an exponentially larger number of parameters is needed to represent the probability distributions which can be obtained as products of mixtures. ",math
1171,Computing the minimal covering set,Mathematical Social Sciences,"We present the first polynomial-time algorithm for computing the minimal covering set of a (weak) tournament. The algorithm draws upon a linear programming formulation of a subset of the minimal covering set known as the essential set. On the other hand, we show that no efficient algorithm exists for two variants of the minimal covering set-the minimal upward covering set and the minimal downward covering set-unless P equals NP. Finally, we observe a strong relationship between von Neumann-Morgenstern stable sets and upward covering on the one hand, and the Banks set and downward covering on the other. © 2008 Elsevier B.V. All rights reserved.",math
1172,A cardinal spline approach to wavelets,Proceedings of the American Mathematical Society," While it is well known that the mth order 5-spline Nm(x) with integer knots generates a multiresolution analysis, • • • C V_x c V0 C • ■• , with the with order of approximation, we prove that i//(x) := Ú1mJ¡{2x- 1), where L2m(x) denotes the (2m)th order fundamental cardinal interpolatory spline, generates the orthogonal complementary wavelet spaces Wk. Note that for m = 1 , when the ß-spline Nx(x) is the characteristic function of the unit interval [0, 1), our basic wavelet L2(2x - 1) is simply the well-known Haar wavelet. In proving that Vk+l = Vkffi Wk , we give the exact formulation of Nm(2x - j), j e Z , in terms of integer translates of Nm(x) and y/{x). This allows us to derive a wavelet decomposition algorithm without relying on orthogonality nor construction of a dual basis. ",math
1173,A survey of approximability and inapproximability results for social welfare optimization in multiagent resource allocation,Annals of Mathematics and Artificial Intelligence," We survey recent approximability and inapproximability results on social welfare optimization in multiagent resource allocation, focusing on the two most central representation forms for utility functions of agents, the bundle form and the k-additive form. In addition, we provide some new (in)approximability results on maximizing egalitarian social welfare and social welfare with respect to the Nash product when restricted to certain special cases. ",math
1174,Superconvergent discontinuous Galerkin methods for second-order elliptic problems,Mathematics of Computation,"We identify discontinuous Galerkin methods for second-order elliptic problems in several space dimensions having superconvergence properties similar to those of the Raviart-Thomas and the Brezzi-Douglas-Marini mixed methods. These methods use polynomials of degree k ≥ 0 for both the potential as well as the flux. We show that the approximate flux converges in L2 with the optimal order of k + 1. and that the approximate potential and its numerical trace superconverge, in L2-like norms, to suitably chosen projections of the potential, with order k + 2. We also apply element-by-element postprocessing of the approximate solution to obtain new approximations of the flux and the potential. The new approximate flux is proven to have normal components continuous across inter-element boundaries, to converge in L 2 with order k + 1, and to have a divergence converging in L 2 also with order k + 1. The new approximate potential is proven to converge with order k + 2 in L2. Numerical experiments validating these theoretical results arc presented. ©2008 American Mathematical Society.",math
1175,Applications of propositional logic to workflow analysis,Applied Mathematics Letters,"In this paper our main goal is to describe the structure of workflows. A workflow is an abstraction of a business process that consists of one or more tasks to be executed to reach a final objective. In our approach we describe a workflow as a graph whose vertices represent workflow tasks and the arcs represent workflow transitions. Moreover, every arc (tk, tl) (i.e., a transition) has attributed a Boolean value to specify the execution/non-execution of tasks tk, tl. With this attribution we are able to identify the natural flow in the workflow. Finally, we establish a necessary and sufficient condition for the termination of workflows. In other words, we identify conditions under which a business process will be complete. © 2009 Elsevier Ltd. All rights reserved.",math
1176,On the Existence of Pure Nash Equilibria in Weighted Congestion Games,Mathematics of Operations Research," We study the existence of pure Nash equilibria in weighted congestion games. Let C denote a set of cost functions. We say that C is consistent if every weighted congestion game with cost functions in C possesses a pure Nash equilibrium. Our main contribution is a complete characterization of consistency of continuous cost functions. We prove that a set C of continuous functions is consistent for two-player games if and only if C contains only monotonic functions and for all nonconstant functions c11 c2 2 C, there are constants a1 b 2 such that c14x5 D a c24x5 C b for all x 2 0. For games with at least three players, we prove that C is consistent if and only if exactly one of the following cases holds: (a) C contains only affine functions; (b) C contains only exponential functions such that c4x5 D ac ex C bc for some ac1 bc1  2 , where ac and bc may depend on c, while  must be equal for every c 2 C. The latter characterization is even valid for three-player games. Finally, we derive several characterizations of consistency of cost functions for games with restricted strategy spaces, such as weighted network congestion games or weighted congestion games with singleton strategies. ",math
1177,A New Lower Bound for Tree-Width Using Maximum Cardinality Search,SIAM Journal on Discrete Mathematics," The tree-width of a graph is of great importance in applied problems in graphical models. The complexity of inference problems on Markov random fields is exponential in the treewidth of the graph. However, computing tree-width is NP-hard in general. Easily computable upper bounds exist, but there are few lower bounds. We give a novel technique to compute a lower bound for the tree-width of a graph using maximum cardinality search. This bound is efficiently computable and is guaranteed to do at least as well as finding the largest clique in the graph. ",math
1178,pyMDO: An Object-Oriented Framework for Multidisciplinary Design Optimization,ACM Transactions on Mathematical Software,"We present pyMDO, an object-oriented framework that facilitates the usage and development of algorithms for multidisciplinary optimization (MDO). The resulting implementation of the MDO methods is efficient and portable. The main advantage of the proposed framework is that it is flexible, with a strong emphasis on object-oriented classes and operator overloading, and it is therefore useful for the rapid development and evaluation of new MDO methods. The top layer interface is programmed in Python and it allows for the layers below the interface to be programmed in C, C-temp, Fortran, and other languages. We describe an implementation of pyMDO and demonstrate that we can take advantage of object-oriented programming to obtain intuitive, easy-to-read, and easy-to-develop codes that are at the same time efficient. This allows developers to focus on the new algorithms they are developing and testing, rather than on implementation details. Examples demonstrate the user interface and the corresponding results show that the various MDO methods yield the correct solutions. © 2009 ACM.",math
1179,Note on generalization in experimental algorithmics,ACM Transactions on Mathematical Software," A recurring theme in mathematical software evaluation is the generalization of rankings of algorithms on test problems to build knowledge-based recommender systems for algorithm selection. A key issue is to profile algorithms in terms of the qualitative characteristics of benchmark problems. In this methodological note, we adapt a novel all-pairs algorithm for the profiling task - Given performance rankings for m algorithms on n problem instances, each described with p features, identify a (minimal) subset of p that is useful for assessing the selective superiority of an algorithm over another, for all pairs of m algorithms. We show how techniques presented in the mathematical software literature are inadequate for such profiling purposes. In conclusion, we also address various statistical issues underlying the effective application of this technique. Categories and Subject Descriptors: G.4.2 [Mathematical Software]: Certification and Testing ",math
1180,Simulated Annealing for Convex Optimization,Mathematics of Operations Research," We apply the method known as simulated annealing to the following problem in convex optimization: minimize a linear function over an arbitrary convex set, where the convex set is speci¯ed only by a membership oracle. Using distributions from the Boltzmann-Gibbs family leads to an algorithm that needs only O¤(pn) phases for instances in Rn. This gives an optimization algorithm that makes O¤(n4:5) calls to the membership oracle, in the worst case, compared to the previous best guarantee of O¤(n5). The bene¯ts of using annealing here are surprising due to the fact that such problems have no local minima that are not also global minima. Hence, we conclude that one of the advantages of simulated annealing, in addition to avoiding poor local minima, is that in these problems it converges faster to the minima that it ¯nds. We also give a proof that under certain general conditions, the Boltzmann-Gibbs distributions are optimal for annealing on these convex problems. History: Received: Xxxx xx, xxxx; revised: Yyyyyy yy, yyyy and Zzzzzz zz, zzzz. ",math
1181,The Skyline algorithm for POMDP value function pruning,Annals of Mathematics and Artificial Intelligence," We address the pruning or ltering problem, encountered in exact value iteration in POMDPs and elsewhere, in which a collection of linear functions is reduced to the minimal subset retaining the same maximal surface. We introduce the Skyline algorithm, which traces the graph corresponding to the maximal surface. The algorithm has both a complete and an iterative version, which we present, along with the classical Lark's algorithm, in terms of the basic dictionary-based simplex iteration from linear programming. We discuss computational complexity results, and present comparative experiments on both randomly-generated and well-known POMDP benchmarks. ",math
1182,A Mathematical Approach to Ontology Authoring and Documentation,Mathematical Knowledge Management," The semantic web ontology languages RDFS and OWL are widely used but limited in both their expressivity and their support for modularity and integrated documentation. Expressivity, modularity, and documentation of formal knowledge have always been important issues in the MKM community. Therefore, we try to improve these ontology languages by well-tried MKM techniques. Concretely, we propose embedding the language concepts into OMDoc to make use of its modularity and documentation infrastructure. We show how OMDoc can be made compatible with semantic web ontology languages, focusing on knowledge representation, modular design, documentation, and metadata. We evaluate our technology by re-implementing the Friend-of-afriend (FOAF) ontology and applying it in a novel metadata framework for technical documents (including ontologies). ",math
1183,Recovering a tree from the leaf colourations it generates under a Markov model,Applied Mathematics Letters," we describe a simple transformation that allows for the fast recovery of a tree from the probabilities such a tree induces on the colourations of its leaves under a simple Markov process (with unknown parameters). This generalizes earlier results by not requiring the transition matrices associated with the edges of the tree to be of a particular form, or to be related by some fixed rate matrix, and by not insisting on a particular distribution of colours at the root of the tree. Applications to taxonomy are outlined briefly in three corollaries. ",math
1184,The Economics of Matching: Stability and Incentives,Mathematics of Operations Research," Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at http://www.jstor.org/action/showPublisher?publisherCode=informs. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. JSTOR is a not-for-profit organization founded in 1995 to build trusted digital archives for scholarship. We work with the scholarly community to preserve their work and the materials they rely upon, and to build a common research platform that promotes the discovery and use of these resources. For more information about JSTOR, please contact support@jstor.org. ",math
1185,Uniqueness theorems for periodic functions,Proceedings of the American Mathematical Society," 1. Introduction. In this note we are concerned with theorems to the effect that two periodic functions which agree with each other on a ""sufficiently large"" set agree identically. Our criteria for ""largeness"" are best possible in all cases considered, namely the discrete case, the continuous case with commensurable periods and the continuous case with incommensurable periods. The results are given by the following three theorems. ",math
1186,An Algebraic Representation of Calendars,Annals of Mathematics and Artificial Intelligence,"This paper uses an algebraic approach to define temporal granularities and calendars. All the granularities in a calendar are expressed as algebraic expressions based on a single ""bottom"" granularity. The operations used in the algebra directly reflect the ways with which people construct new granularities from existing ones, and hence yield more natural and compact granularities definitions. Calendar is formalized on the basis of the algebraic operations, and properties of calendars are studied. As a step towards practical applications, the paper also presents algorithms for granule conversions between granularities in a calendar.",math
1187,A polynomial-time maximum common subgraph algorithm for outerplanar graphs and its application to chemoinformatics,Annals of Mathematics and Artificial Intelligence,  ,math
1188,Abstract generalized bisection and a cost bound,Mathematics of Computation," The purpose of this paper is to study desirable properties of binary search algorithms for isolating all solutions to nonlinear systems of equations F( X) = 0 within a given compact domain D £ R"". We devise a general framework such that any algorithm fitting into the general framework will always isolate all solutions ZeD such that F(Z) = 0; this framework contains a new idea for handling the occurrence of roots on boundaries. We then present and prove a bound on the total amount of computation which is valid for any algorithm in the class. Finally, we define a specific prototypical algorithm valid for F satisfying certain natural smoothness properties; we show that it satisfies the hypotheses for the general framework. This algorithm is based on ""bisection"" of generalized rectangles, the Kantorovich theorem, and second-order Taylor type models for F. It is meant to provide further guidelines for the development of effective heuristics, etc., for actual implementations. ",math
1189,"GALAHAD, a library of thread-safe Fortran 90 packages for large-scale nonlinear optimization",ACM Transactions on Mathematical Software,"We describe the design of version 1.0 of GALAHAD, a library of Fortran 90 packages for large-scale nonlinear optimization. The library particularly addresses quadratic programming problems, containing both interior point and active set algorithms, as well as tools for preprocessing problems prior to solution. It also contains an updated version of the venerable nonlinear programming package, LANCELOT.",math
1190,Guest editors’ preface to the special issue on conformal prediction and its applications,Annals of Mathematics and Artificial Intelligence," Quantifying the uncertainty of predictions produced by classification and regression techniques is an important problem in the field of Machine Learning. This special issue is dedicated to Conformal Prediction (CP), which is a recently developed framework for producing provably valid measures of confidence in predictions. It can be used for extending conventional Machine Learning algorithms and thus developing methods, called Conformal Predictors (CPs), whose predictions are guaranteed to satisfy a given level of confidence without requiring anything more than that the data are generated independently by the same probability distribution (i.i.d.). More specifically, CPs produce as their predictions prediction regions, which are sets of labels sufficient to satisfy the required level of confidence. The idea behind Conformal Prediction originated in a series of discussions at Royal Holloway, University of London, in the summer of 1996, between Alexander Gammerman, Vladimir N. Vapnik and Vladimir Vovk. These discussions, which were mainly concerned with Vapnik's work on Support Vector Machines (SVM), led to the realization that the number of support vectors used by an SVM could serve as basis for the production of confidence measures for individual predictions (in fact, similar ideas may be traced back to the joint work of Chervonenkis and Vapnik in the 1960s: see [5], footnote 4). This initial idea was described in [8] and was later improved to its current form in [37] and [33]. The first Conformal Predictor proposed in [37] and [33], which was then called Transductive Confidence ",math
1191,Linearized Bregman iterations for compressed sensing,Mathematics of Computation," Finding a solution of a linear equation Au = f with various minimization properties arises from many applications. One such application is compressed sensing, where an efficient and robust-to-noise algorithm to find a minimal 1 norm solution is needed. This means that the algorithm should be tailored for large scale and completely dense matrices A, while Au and AT u can be computed by fast transforms and the solution we seek is sparse. Recently, a simple and fast algorithm based on linearized Bregman iteration was proposed in [28, 32] for this purpose. This paper is to analyze the convergence of linearized Bregman iterations and the minimization properties of their limit. Based on our analysis here, we derive also a new algorithm that is proven to be convergent with a rate. Furthermore, the new algorithm is simple and fast in approximating a minimal 1 norm solution of Au = f as shown by numerical simulations. Hence, it can be used as another choice of an efficient tool in compressed sensing. ",math
1192,Solving Hierarchical Constraints over Finite Domains with Local Search,Annals of Mathematics and Artificial Intelligence," Many real world problems have requirements and constraints which conflict with each other. One approach for dealing with such over-constrained problems is with constraint hierarchies. In the constraint hierarchy framework, constraints are classified into ranks, and appropriate solutions are selected using a comparator which takes into account the constraints and their ranks. In this paper, we present a local search solution to solving hierarchical constraint problems over finite domains (HCPs). This is an extension of local search for over-constrained integer programs WSAT(OIP) to constraint hierarchies and general finite domain constraints. The motivation for this work arose from solving large airport gate allocation problems. We show how gate allocation problems can be formulated as HCPs using typical gate allocation constraints. Using the gate allocation benchmarks, we investigate how constraint heirarchy selection strategies and the problem formulation using two models: a 0-1 linear constraint hierarchy model and a nonlinear finite domain constraint hierarchy model. ",math
1193,Numerical methods and software,Mathematics of Computation," DOWNLOAD htp:/bit.ly/1WtjWZN htp:/www.goodreads.com/search?utf8=%E2%9C%93&query=Numerical+Methods+and+Software Mathematics of Applied Maple for Engineers and Scientists , Christopher Tocci, Steven G. Adams, Jan 1, 1996, Technology & Engineering, 406 pages. Fast becoming the first choice in computer algebra systems (CAS) among engineers and scientists, Maple is easy-to-use software that performs numerical and symbolic analysis to. An introduction to numerical analysis , Kendall E. Atkinson, 1978, Mathematics, 587 pages. This Second Edition of a standard numerical analysis text retains organization of the original edition, but all sections have been revised, some extensively, and bibliographies. ",math
1194,Robust inference of trees,Annals of Mathematics and Artificial Intelligence," This paper is concerned with the reliable inference of optimal treeapproximations to the dependency structure of an unknown distribution generating data. The traditional approach to the problem measures the dependency strength between random variables by the index called mutual information. In this paper reliability is achieved by Walley's imprecise Dirichlet model, which generalizes Bayesian learning with Dirichlet priors. Adopting the imprecise Dirichlet model results in posterior interval expectation for mutual information, and in a set of plausible trees consistent with the data. Reliable inference about the actual tree is achieved by focusing on the substructure common to all the plausible trees. We develop an exact algorithm that infers the substructure in time O(m4), m being the number of random variables. The new algorithm is applied to a set of data sampled from a known distribution. The method is shown to reliably infer edges of the actual tree even when the data are very scarce, unlike the traditional approach. Finally, we provide lower and upper credibility limits for mutual information under the imprecise Dirichlet model. These enable the previous developments to be extended to a full inferential method for trees. ",math
1195,Correlated Equilibrium in Quitting Games,Mathematics of Operations Research," A quitting game is a sequential game where each player has two actions: to continue or to quit. The game continues as long as all players decide to continue. The moment at least one player decides to quit, the game terminates. The terminal payoff depends on the subset of players who quit at the terminating stage. If the game continues forever, then the payoff for the players is some fixed-payoff vector. We prove that every quitting game admits a correlated uniform -equilibrium-a uniform -equilibrium in an extended game that includes a correlation device that sends one signal to each player before start of play. ",math
1196,On the rapid computation of various polylogarithmic constants,Mathematics of Computation, 1. NASA Ames Research ,math
1197,A faster algorithm for betweenness centrality*,Journal of Mathematical Sociology," The betweenness centrality index is essential in the analysis of social networks, but costly to compute. Currently, the fastest known algorithms require (n3) time and (n2) space, where n is the number of actors in the network. Motivated by the fast-growing need to compute centrality indices on large, yet very sparse, networks, new algorithms for betweenness are introduced in this paper. They require O(n + m) space and run in O(nm) and O(nm + n2 log n) time on unweighted and weighted networks, respectively, where m is the number of links. Experimental evidence is provided that this substantially increases the range of networks for which centrality analysis is feasible. ",math
1198,Linearly Parameterized Bandits,Mathematics of Operations Research," We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an r-dimensional random vector Z ∈ Rr, where r ≥ 2. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order Θ(r√T ), by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phasebased policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form O(r√T log3/2 T ). Since its introduction by Thompson (1933), the multiarmed bandit problem has served as an important model for decision making under uncertainty. Given a set of arms with unknown reward profiles, the decision maker must choose a sequence of arms to maximize the expected total payoff, where the decision in each period may depend on the previously observed rewards. The multiarmed bandit problem elegantly captures the tradeoff between the need to exploit arms with high payoff and the incentive to explore previously untried arms for information gathering purposes. Much of the previous work on the multiarmed bandit problem assumes that the rewards of the arms are statistically independent (see, for example, Lai and Robbins (1985) and Lai (1987)). This assumption enables us to consider each arm separately, but it leads to policies whose regret scales linearly with the number of arms. Most policies that assume independence require each arm to be ",math
1199,SNOBFIT -- Stable Noisy Optimization by Branch and Fit,ACM Transactions on Mathematical Software," The software package Snobfit for bound constrained (and soft constrained) noisy optimization of an expensive objective function is described. It combines global and local search by branching and local fits. The program is made robust and flexible for practical use by allowing for hidden constraints, batch function evaluations, change of search regions, etc. Categories and Subject Descriptors: G.1.6 [Optimization]: Global optimization; Constrained optimization General Terms: Algorithms Additional Key Words and Phrases: branch and bound, derivative-free, surrogate model, parallel evaluation, hidden constraints, expensive function values, noisy function values, soft constraints ACM Transactions on Mathematical Software, Vol. V, No. N, Month 20YY, Pages 1-0??. ",math
1200,Algorithm 733: TOMP–Fortran modules for optimal control calculations,ACM Transactions on Mathematical Software," KRAFT General Terms: Algorithms Additional Key Words and Phrases: Boundary value problems, manipulators, optimal control, robotics, shooting method ",math
1201,A New Complexity Result on Solving the Markov Decision Problem,Mathematics of Operations Research," We present a new complexity result on solving the Markov decision problem (MDP) with n states and a number of actions for each state, a special class of real-number linear programs with the Leontief matrix structure. We prove that, when the discount factor µ is strictly less than 1, the problem can be solved in at most O(n1:5(log 1¡µ +log n)) 1 classical interior-point method iterations and O(n4(log 1¡µ + log n)) arithmetic operations. Our method is a 1 combinatorial interior-point method related to the work of Ye [30] and Vavasis and Ye [26]. To our knowledge, this is the ¯rst strongly polynomial-time algorithm for solving the MDP when the discount factor is a constant less than 1. ",math
1202,Methods of Relevance Ranking and Hit-content Generation in Math Search,Mathematical Knowledge Management," To be e®ective and useful, math search systems must not only maximize precision and recall, but also present the query hits in a form that makes it easy for the user to identify quickly the truly relevant hits. To meet that requirement, the search system must sort the hits according to domain-appropriate relevance criteria, and provide with each hit a query-relevant summary of the hit target. The standard relevance measures in text search, which rely mostly on keyword frequencies and document sizes, turned out to be inadequate in math search. Therefore, alternative relevance measures must be de¯ned, which give more weight to certain types of information than to others and take into account cross-reference statistics. In this paper, new, multi-dimensional relevance metrics are de¯ned for math search, methods for computing and implementing them are discussed, and comparative performance evaluation results are presented. Query-relevant hit-summary generation is another factor that enables users to quickly determine the relevance of the presented hits. Although the hit title accompanied by a few leading sentences from the target document is simple to produce, this often fails to convey to the user the document's relevant excerpts. This shifts the burden onto the user to pursue many of the hits, and read signi¯cant portions of their target documents, to ¯nally locate the wanted documents. Clearly, this task is too time-consuming and should be largely automated. This paper presents query-relevant hit-summary generation methods, outlines implementation strategies, and presents performance evaluation results. ",math
1203,Investment Timing Under Incomplete Information,Mathematics of Operations Research," * We are grateful to Bruno Biais, Nicole El Karouni, Christian Gollier and Damien Lamberton for thoughtful discussions and suggestions. We would also like to thank seminar participants at ESC Toulouse, Institut Henri Poincaré and Séminaire Bachelier for their comments. Financial support from STICERD is gratefully acknowledged by the second author. We remain, of course, solely responsible for the content of this paper. ",math
1204,Generalized Hermite interpolation via matrix-valued conditionally positive definite functions,Mathematics of Computation,"In this paper, we consider a broad class of interpolation problems, for both scalar- and vector-valued multivariate functions subject to linear side conditions, such as being divergence-free, where the data are generated via integration against compactly supported distributions. We show that, by using certain families of matrix-valued conditionally positive definite functions, such interpolation problems are well poised; that is, the interpolation matrices are invertible. As a sample result, we show that a divergence-free vector field can be interpolated by a linear combination of convolutions of the data-generating distributions with a divergence-free, 3x3 matrix-valued conditionally positive definite function. In addition, we obtain norm estimates for inverses of interpolation matrices that arise in a class of multivariate Hermite interpolation problems. © 1994 American Mathematical Society.",math
1205,Similarity measures in formal concept analysis,Annals of Mathematics and Artificial Intelligence,"Formal concept analysis (FCA) has been applied successively in diverse fields such as data mining, conceptual modeling, social networks, software engineering, and the semantic web. One shortcoming of FCA, however, is the large number of concepts that typically arise in dense datasets hindering typical tasks such as rule generation and visualization. To overcome this shortcoming, it is important to develop formalisms and methods to segment, categorize and cluster formal concepts. The first step in achieving these aims is to define suitable similarity and dissimilarity measures of formal concepts. In this paper we propose three similarity measures based on existent set-based measures in addition to developing the completely novel zeros-induced measure. Moreover, we formally prove that all the measures proposed are indeed similarity measures and investigate the computational complexity of computing them. Finally, an extensive empirical evaluation on real-world data is presented in which the utility and character of each similarity measure is tested and evaluated. © 2011 Springer Science+Business Media B.V.",math
1206,"Maximizing Supermodular Functions on Product Lattices, with Application to Maximum Constraint Satisfaction",SIAM Journal on Discrete Mathematics," Recently, a strong link has been discovered between supermodularity on lattices and tractability of optimization problems known as maximum constraint satisfaction problems. The present paper strengthens this link. We study the problem of maximizing a supermodular function which is de¯ned on a product of n copies of a ¯xed ¯nite lattice and given by an oracle. We exhibit a large class of ¯nite lattices for which this problem can be solved in oracle-polynomial time in n. We also obtain new large classes of tractable maximum constraint satisfaction problems. ",math
1207,An Infinite-Dimensional Linear Programming Algorithm for Deterministic Semi-Markov Decision Processes on Borel Spaces,Mathematics of Operations Research," We devise an algorithm for solving the infinite dimensional linear programs that arise from general deterministic semi-Markov decision processes on Borel spaces. The algorithm constructs a sequence of approximate primal/dual solutions that converge to an optimal one. The innovative idea is to approximate the dual solution with continuous piecewise linear ridge functions that naturally represent functions defined on a high dimensional domain as linear combinations of functions defined on only a single dimension. This approximation gives rise to a primal/dual pair of semi-infinite programs, for which we show strong duality. In addition, we prove various properties of the underlying ridge functions. ",math
1208,Facets of the complementarity knapsack polytope,Mathematics of Operations Research," We present a polyhedral study of the complementarity knapsack problem. Traditionally, complementarity constraints are modeled by introducing auxiliary binary variables and additional constraints, and the model is tightened by introducing strong inequalities valid for the resulting MIP. We use an alternative approach, in which we keep in the model only the continuous variables, and we tighten the model by introducing inequalities that define facets of the convex hull of the set of feasible solutions in the space of the continuous variables. To obtain the facet-defining inequalities, we extend the concepts of cover and cover inequality, commonly used in 0-1 programming, for this problem, and we show how to sequentially lift cover inequalities. We obtain tight bounds for the lifting coefficients, and we present two families of facet-defining inequalities that can be derived by lifting cover inequalities. We show that unlike 0-1 knapsack polytopes, in which different facet-defining inequalities can be derived by fixing variables at 0 or 1, and then sequentially lifting cover inequalities valid for the projected polytope, any sequentially lifted cover inequality for the complementarity knapsack polytope can be obtained by fixing variables at 0. ",math
1209,Nonlinear Matroid Optimization and Experimental Design,SIAM Journal on Discrete Mathematics," We study the problem of optimizing nonlinear objective functions over matroids presented by oracles or explicitly. Such functions can be interpreted as the balancing of multicriteria optimization. We provide a combinatorial polynomial time algorithm for arbitrary oracle-presented matroids, that makes repeated use of matroid intersection and an algebraic algorithm for vectorial matroids. Our work is partly motivated by applications to minimum-aberration model-fitting in experimental design in statistics, which we discuss and demonstrate in detail. ",math
1210,t-DeLP: an argumentation-based Temporal Defeasible Logic Programming framework,Annals of Mathematics and Artificial Intelligence,"The aim of this paper is to propose an argumentation-based defeasible logic, called t-DeLP, that focuses on forward temporal reasoning for causal inference. We extend the language of the DeLP logical framework by associating temporal parameters to literals. A temporal logic program is a set of basic temporal facts and (strict or defeasible) durative rules. Facts and rules combine into durative arguments representing temporal processes. As usual, a dialectical procedure determines which arguments are undefeated, and hence which literals are warranted, or defeasibly follow from the program. t-DeLP, though, slightly differs from DeLP in order to accommodate temporal aspects, like the persistence of facts. The output of a t-DeLP program is a set of warranted literals, which is first shown to be non-contradictory and be closed under sub-arguments. This basic framework is then modified to deal with programs whose strict rules encode mutex constraints. The resulting framework is shown to satisfy stronger logical properties like indirect consistency and closure. © 2013 Springer Science+Business Media Dordrecht.",math
1211,The complexity of priced control in elections,Annals of Mathematics and Artificial Intelligence," We study the complexity of priced control in elections. Naturally, if a given control type is NP-hard for a given voting system E then its priced variant is NP-hard for this rule as well. It is, however, interesting what effect introducing prices has on the complexity of those control problems that without prices are tractable. We show that for four prominent voting rules (plurality, approval, Condorcet, and Copeland) introducing prices does not increase the complexity of control by adding/deleting candidates/voters. However, we do show an example of a scoring rule for which such an effect takes place. ",math
1212,Advances in the theory of box integrals,Mathematics of Computation," Box integrals-expectations |r|s or |r − q|s over the unit ncube-have over three decades been occasionally given closed forms for isolated n, s. By employing experimental mathematics together with a new, global analytic strategy, we prove that for each of n = 1, 2, 3, 4 dimensions the box integrals are for any integer s hypergeometrically closed (“hyperclosed”) in an explicit sense we clarify herein. For n = 5 dimensions, such a complete hyperclosure proof is blocked by a single, unresolved integral we call K5; although we do prove that all but a finite set of (n = 5) cases enjoy hyperclosure. We supply a compendium of exemplary closed forms that arise naturally from the theory. ",math
1213,Implementing a random number package with splitting facilities,ACM Transactions on Mathematical Software," Multiple generators are often required in simulation studies, for instance, to facilitate synchronization for variance reduction purposes, and multiple independent streams per generator are helpful to make independent replications. A portable set of software tools is described for uniform random variates generation. It provides for multiple generators running simultaneously, and each generator has its sequence of numbers partitioned into many long (disjoint) sub streams. Simple procedure calls allow the user to make any generator “jump” ahead to the beginning of its next sub stream, back to the beginning of its current substream, or back to the beginning of its first substream. A simple switch permits a change from regular to antithetic variates or vice versa. Implementation issues are discussed An efficient and portable code is also provided for computing (as MOD m) for any positive integer values of a < m, s < m, and m <2 b -1 on a b-bit computer. This code is used to implement the package. A Pascal implementation for 32-bit computers is described. The basic underlying generator for this implementation has been proposed in a previous paper; it combines two multiplicative linear congruential generators and has a period of p = 23 x 1018 Categories tics-random and Subject number ",math
1214,Automatic Construction of Accurate Models of Physical Systems.,Annals of Mathematics and Artificial Intelligence," 12a. DISTRIBUTION/AVAILABILITY STATEMENT System Identification, Artificial Intelligence ",math
1215,Approximating the exponential from a Lie algebra to a Lie group,Mathematics of Computation," Consider a di erential equation y0 = A(t; y)y; y(0) = y0 with y0 2 G and A : R+ G ! g, where g is a Lie algebra of the matricial Lie group G. Every B 2 g can be mapped to G by the matrix exponential map exp (tB) with t 2 R. Most numerical methods for solving ordinary di erential equations (ODEs) on Lie groups are based on the idea of representing the approximation yn of the exact solution y(tn), tn 2 R+, by means of exact exponentials of suitable elements of the Lie algebra, applied to the initial value y0. This ensures that yn 2 G. When the exponential is di cult to compute exactly, as is the case when the dimension is large, an approximation of exp (tB) plays an important role in the numerical solution of ODEs on Lie groups. In some cases rational or polynomial approximants are unsuitable and we consider alternative techniques, whereby exp (tB) is approximated by a product of simpler exponentials. In this paper we present some ideas based on the use of the Strang splitting for the approximation of matrix exponentials. Several cases of g and G are considered, in tandem with general theory. Order conditions are discussed, and a number of numerical experiments conclude the paper. ",math
1216,The Vector Partition Problem for Convex Objective Functions,Mathematics of Operations Research," The partition problem concerns the partitioning of a given set of n vectors in d-space into p parts to maximize an objective function that is convex on the sum of vectors in each part. The problem has broad expressive power and captures NP-hard problems even if either p or d is fixed. In this article we show that when both p d are fixed, the problem is solvable in strongly polynomial time using O nd p−1 −1 arithmetic operations. This improves upon the previously known bound of O ndp2 . Our method is based on the introduction of the signing zonotope of a set of points in space. We study this object, which is of interest in its own right, and show that it is a refinement of the so-called partition polytope of the same set of points. ",math
1217,Two Fast Algorithms for Sparse Matrices: Multiplication and Permuted Transposition,ACM Transactions on Mathematical Software,"Let A and B be two sparse matnces whose orders are p by q and q by r. Their product C =AB requires N nontrivial multiplications where [formula omitted]. The operation count of our algorithm is usually proportional to N; however, its worse case is O(p, r, NA, N) where NA IS the number of elements m A This algorithm can be used to assemble the sparse matrix arismg from a fimte element problem from the basic elements, using [fromula omitted] operations where m is the total number of basic elements and ordert») IS the order of the vth element matnx. The concept of an unordered merge plays a key role m obtammg our fast multiplication algorithm It forces us to accept an unordered sparse row-wise format as output for the product C The permuted transposinon algorithm computes (RA)T m Otp, q, NA) operations where R IS a permutation matrix It also orders an unordered sparse row-wise representation. We can combme these algorithms to produce an O(M) algorithm to solve Ax = b where M is the number of multiplications needed to factor A mto LV. © 1978, ACM. All rights reserved.",math
1218,Math-Literate Computers,Mathematical Knowledge Management,"Math notation is a familiar, everyday tool widely used in society. Computers need math literacy - the ability to read and write math notation - in order to assist people with accessing mathematical documents and carrying out mathematical investigations. In this paper, we discuss issues in making computers math-literate. Software for generating math notation is widely used. Software for recognition of math notation is not as widely used: to avoid the intrusiveness and unpredictability of recognition errors, people often prefer to enter and edit math expressions using a computer-oriented representation, such as LaTeX or a structure-based editor. However, computer recognition of math notation is essential in large-scale recognition of mathematical documents; as well, it offers the ability to create people-centric user interfaces focused on math notation rather than computer-centric user interfaces focused on computer-oriented representations. Issues that arise in computer math literacy include the diversity of math notation, the challenges in designing effective user interfaces, and the difficulty of defining and assessing performance. © 2009 Springer-Verlag Berlin Heidelberg.",math
1219,Algorithm 862: MATLAB tensor classes for fast algorithm prototyping,ACM Transactions on Mathematical Software,"Tensors (also known as multidimensional arrays or N-way arrays) are used in a variety of applications ranging from chemometrics to psychometrics. We describe four MATLAB classes for tensor manipulations that can be used for fast algorithm prototyping. The tensor class extends the functionality of MATLABs multidimensional arrays by supporting additional operations such as tensor multiplication. The tensor_as_matrix class supports the matricization of a tensor, that is, the conversion of a tensor to a matrix (and vice versa), a commonly used operation in many algorithms. Two additional classes represent tensors stored in decomposed formats: cp_tensor and tucker_tensor. We describe all of these classes and then demonstrate their use by showing how to implement several tensor algorithms that have appeared in the literature. ÂŠ 2006 ACM.",math
1220,An Environment for Building Mathematical Knowledge Libraries,Mathematical Knowledge Management," Proving is an activity that makes use of mathematical knowledge. For a human, this knowledge - together with the know-how about proving techniques - is accumulated over years of studying mathematics. For a theorem proving assistant, the mathematical knowledge needed in the proving process has to be provided beforehand. In this paper we describe a user-friendly environment for building up a mathematical knowledge base that can be accessed by an automated proving assistant. ",math
1221,Reconfigurations in Graphs and Grids,SIAM Journal on Discrete Mathematics," Let G be a connected graph, and let V and V ′ two n-element subsets of its vertex set V (G). Imagine that we place a chip at each element of V and we want to move them into the positions of V ′ (V and V ′ may have common elements). A move is defined as shifting a chip from v1 to v2 (v1, v2 ∈ V (G)) on a path formed by edges of G so that no intermediate vertices are occupied. We give upper and lower bounds on the number of moves that are necessary, and analyze the computational complexity of this problem under various assumptions: labeled versus unlabeled chips, arbitrary graphs versus the case when the graph is the rectangular (infinite) planar grid, etc. We prove hardness and inapproximability results for several variants of the problem. We also give a linear-time algorithm which performs an optimal (minimum) number of moves for the unlabeled version in a tree, and a constant-ratio approximation algorithm for the unlabeled version in a graph. The graph algorithm uses the tree algorithm as a subroutine. ",math
1222,"Common knowledge, communication, and convergence of beliefs",Mathematical Social Sciences,"A conceptual framework is established for the analysis of common knowledge and communication when information structures are represented by sigma-algebras instead of partitions. It involves construction of Boolean sigma-algebras of generalized events, events which are identified only up to addition or subtraction of null sets. If all agents posterior expectations of a random variable are common knowledge, then they are almost surely identical. If the posterior expectations are communicated back and forth, then they converge almost surely to a consensus expectation. If the posterior distributions of a random variable are communicated back and forth, then they will almost surely converge weakly to a consensus distribution. ÂŠ 1984.",math
1223,Random sampling with a reservoir,ACM Transactions on Mathematical Software," VITTER We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O(n(1 + log(N/n))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin. CR Categories and Subject Descriptors: G.3 [Mathematics of Computing]: Probability and Statistics-probabilistic algorithms, random number generation, statistical software; G.4 [Mathematics of Computing]: Mathematical Software-algorithm analysis Additional Key Words and Phrases: Analysis of algorithms, optimization, method, reservoir ACM Transactions ",math
1224,Capturing abstract matrices from paper,Mathematical Knowledge Management," Capturing and understanding mathematics from print form is an important task in translating written mathematical knowledge into electronic form. While the problem of syntactically recognising mathematical formulas from scanned images has received attention, very little work has been done on semantic validation and correction of recognised formulas. We present a rst step towards such an integrated system by combining the Infty system with a semantic analyser for matrix expressions. We applied the combined system in experiments on the semantic analysis of matrix images scanned from textbooks. While the rst results are encouraging, they also demonstrate many ambiguities one has to deal with when analysing matrix expressions in di erent contexts. We give a detailed overview of the problems we encountered that motivate further research into semantic validation of mathematical formula recognition. ",math
1225,Variations on Variable-Metric Methods,Mathematics of Computation," In unconstrained minimization of a function/, the method of Davidon-FleteherPowell (a ""variable-metric"" method) enables the inverse of the Hessian H of / to be approximated step wise, using only values of the gradient of/. It is shown here that, by solving a certain variational problem, formulas for the successive corrections to H can be derived which closely resemble Davidon's. A symmetric correction matrix is sought which minimizes a weighted Euclidean norm, and also satisfies the ""DFP condition."" Numerical tests are described, comparing the performance (on four ""standard"" test functions) of two variationally-derived formulas with Davidon's. A proof by Y. Bard, modelled on Fletcher and Powell's, showing that the new formulas give the exact H after N steps, is included in an appendix. 1. The DFP Method. The class of gradient methods strained minimum of a function fix)* in which the direction step from Xhto Xk+xis computed from a formula such as: is called the class of variable-metric methods. Here Gk is a (preferably) positivedefinite N X N matrix and gk is the gradient Vf evaluated at XkThe reason for this nomenclature is that Sk is the direction in which the directional derivative of / is a minimum, i.e., the direction in which ",math
1226,Ontological Engineering and Mathematical Knowledge Management: A Formalization of Projective Geometry,Annals of Mathematics and Artificial Intelligence," The work presented in this paper deals with the formalization of the ontology underlying projective geometry. This formalization is done by using the conceptual graph model which has been defined in the Artificial Intelligence community. Through this experiment, we endeavour to show that applying knowledge representation techniques to mathematical fields is a relevant way to improve the reliability and efficiency of tools dedicated to mathematical knowledge management. Our proposal is based on the construction of knowledge bases (defined according to ontologies) which must be considered as the core of any mathematical knowledge management tool such as mathematical search engines on the web, mathematical intelligent tutoring systems, mathematical theorem provers, etc. This paper also aims at highlighting the contributions provided by ontological engineering when dealing with mathematical knowledge management. Mathematics is a very structured field of knowledge but also an extremely vast one. Gathering information from documents related to this area (for instance, documents available on the Web) can be very difficult as the current search engines are based on the use of keywords. One solution to this problem consists in automating information retrieval by integrating mathematical knowledge in Information Systems. Originating in the field of Artificial Intelligence and more precisely Knowledge Engineering [38], these systems are called Knowledge-Based Systems (KBSs). Heirs to Expert Systems, KBSs are currently used for a variety of purposes such as Enterprise Knowledge Management (capitalization, sharing and appropriation of knowledge and know-how), Education (exploitation of intelligent tutoring systems for managing cooperative problem solving), Electronic Commerce and the Semantic Web1 (indexation and annotation of web resources by making their semantics explicit). ",math
1227,On two theorems of Jessen,Proceedings of the American Mathematical Society," 1. Introduction. Let X he the measure-theoretic product of a sequence of measure spaces Xx, X2, ■ ■ ■ , Xn, ■ • • , each of measure 1, and let / be a summable1 real-valued function on X. Jessen2 has proved the following two theorems: As n-><», and for almost every x = (xx, x2, ■ ■ • ), we have I I • • • I f(x)dxxdx2• • • <2xn->I f(x)dx. ",math
1228,Algorithm 905: SHEPPACK: Modified Shepard Algorithm for Interpolation of Scattered Multivariate Data,ACM Transactions on Mathematical Software,"Scattered data interpolation problems arise in many applications. Shepards method for constructing a global interpolant by blending local interpolants using local-support weight functions usually creates reasonable approximations. SHEPPACK is a Fortran 95 package containing five versions of the modified Shepard algorithm: quadratic (Fortran 95 translations of Algorithms 660, 661, and 798), cubic (Fortran 95 translation of Algorithm 791), and linear variations of the original Shepard algorithm. An option to the linear Shepard code is a statistically robust fit, intended to be used when the data is known to contain outliers. SHEPPACK also includes a hybrid robust piecewise linear estimation algorithm RIPPLE (residual initiated polynomial-time piecewise linear estimation) intended for data from piecewise linear functions in arbitrary dimension m. Themain goal of SHEPPACK is to provide users with a single consistent package containing most existing polynomial variations of Shepards algorithm. The algorithms target data of different dimensions. The linear Shepard algorithm, robust linear Shepard algorithm, and RIPPLE are the only algorithms in the package that are applicable to arbitrary dimensional data. © 2010 ACM.",math
1229,Minimal Belief and Negation as Failure in Multi-Agent Systems,Annals of Mathematics and Artificial Intelligence," We propose an epistemic, nonmonotonic approach to the formalization of knowledge in a multi-agent setting. From the technical viewpoint, a family of nonmonotonic logics, based on Lifschitz's modal logic of minimal belief and negation as failure, is proposed, which allows for formalizing an agent which is able to reason about both its own knowledge and other agents' knowledge and ignorance. We de¯ne a reasoning method for such a logic and characterize the computational complexity of the major reasoning tasks in this formalism. From the practical perspective, we argue that our logical framework is well-suited for representing situations in which an agent cooperates in a team, and each agent is able to communicate his knowledge to other agents in the team. In such a case, in many situations the agent needs nonmonotonic abilities, in order to reason about such a situation based on his own knowledge and the other agents' knowledge and ignorance. Finally, we show the e®ectiveness of our framework in the robotic soccer application domain. ",math
1230,The longest chain among random points in Euclidean space,Proceedings of the American Mathematical Society," Let n random points be chosen independently from the uniform distribution on the unit fc-cube (0, l]k. Order the points coordinate-wise and let Hfc(7i) be the cardinality of the largest chain in the resulting partially ordered set. We show that there are constants C\,C2, ■■■ such that c*, < e, lim¡t_00 c^ = e, and limn_oo H/t(n)/n1/''c = c;t in probability. This generalizes results of Hammersley, Kingman and others on Ulam's ascending subsequence problem, and settles a conjecture of Steele. ",math
1231,Extraction of Logical Structure from Articles in Mathematics,Mathematical Knowledge Management," We propose a mathematical knowledge browser which helps people to read mathematical documents. By the browser printed mathematical documents can be scanned and recognized by OCR (Optical Character Recognition). Then the meta-information (e.g. title, author) and the logical structure (e.g. section, theorem) of the documents are automatically extracted. The purpose of this paper is to show the extraction method of logical structure specialized for mathematical documents. We implemented this method in INFTY which is an integrated OCR system for mathematical documents. In order to show the feasibility of the method we made a correct database from an existing mathematical OCR database, and made an experiment. ",math
1232,On the structure of representations of continuous functions of several variables as finite sums of continuous functions of one variable,Proceedings of the American Mathematical Society," DAVID A.SPRECHER 1. Introduction and summary. Let Cn denote the set of all real and continuous functions of ra variables, defined on the ra-dimensional unit cube, S„, in Euclidean space, (R„: in particular, 6 will mark the set of real and continuous functions defined on the real line, (R. Considered in this paper are the representations of arbitrary functions of C„ (ra2:2) as finite superpositions of functions of 6, using only addition. That is, we are interested in the representations of an arbitrary function, /(xi, • • • , xn)E&n, as Presented to the Society, January 26, 1965 under the title On the representations continuous functions of several variables; received by the editors March 15, 1965. 1 A function, <j>(x),is Holder-continuous (with exponent a) if there are constants a and /3, 0<a<l, for which | <f>(x)-4>(y)\ <0\x- y\a for all points, x and y, in the domain of <j>. ",math
1233,How large should a coalition be to manipulate an election?,Mathematical Social Sciences,"Assuming the IC conjecture, we show that, for any faithful scoring rule, when the number of participating agents n tends to infinity, the probability that a random profile will be manipulable for a coalition of size Cn α, with 0≤ α <1/2 and C constant, is of order O (1/n1/2-α). © 2003 Elsevier B.V. All rights reserved.",math
1234,Borda and the maximum likelihood approach to vote aggregation,Mathematical Social Sciences,"It is shown that, for a suitably chosen probability function, the maximum likelihood rule for vote aggregation coincides with the Borda rule, thus, partially reconciling the Borda and the Condorcet methods. © 2007 Elsevier B.V. All rights reserved.",math
1235,Centered graphs and the structure of ego networks,Mathematical Social Sciences," A way of comparing ego networks through examining patterns among their tics I$ introduced. It is derived from graph-theoretic ideas about. centered graph<. An illustration using data from a computer conference is provided. This essay is about a special kind of social networks called ego networks. Like other networks, they are made up of social units - persons, groups or organizations - and social relations - like marriage or friendship - that Iink pairs of units into some sort of overall patterned structure. Ego networks, however, are special in that they are built around a particular social unit designated a!, ego. To uncover an ego network you begin by choosing some social unit as ego. You then define a symmetric social relation or combination or relations and determine all the other units with whom ego has the relation. Finally, you note all pairs among those other units who are directly connected by the relation. The result is a mini-network or immediate neighborhood surroundtng ego that can, perhaps, reveal something important about the social world from ego's perspective. We might guess, for example, that if the social units are persons, those in the ego network are the ones that have the greatest impact on ego's attitudes, norms, values, goals and perceptions of the world. Moreover, those are the ones to whom ego must turn to seek information, help and support. In some sense, then, an ego network is a map of ego's personal social world. It shows something about hokf that ego is tied in to the larger human society. Bstt (1957) first called attention to the importance of such ego networks. She called them social networks and reasoned that to the extent that ego was involved * This is a report of a computer conference on social neri\orks supported by the National Science Foundation under Grant No. DS177.16578. ",math
1236,Hypergraph regularity and the multidimensional Szemer'edi theorem,Annals of Mathematics," We prove analogues for hypergraphs of Szemer´edi's regularity lemma and the associated counting lemma for graphs. As an application, we give the first combinatorial proof of the multidimensional Szemer´edi theorem of Furstenberg and Katznelson, and the first proof that provides an explicit bound. Similar results with the same consequences have been obtained independently by Nagle, R¨odl, Schacht and Skokan. ",math
1237,Interpolation and approximation,Mathematics of Computation," Course description Orthogonal polynomials. Pointwise and uniform convergence of trigonometric and orthogonal polynomial series. Fourier Transformation. Elements of approximation theory. Stone's theorem, Bohman-Korovkin theorem. Best approximation by polynomials. Jackson's theorems. Interpolation. Spline functions. Approximation by rational functions. The Lebesgue function of Lagrange interpolation. Erdős-Bernstein conjecture on the optimal nodes. Grünwald-Marczinkiewicz theorem. ",math
1238,A framework for multi-robot node coverage in sensor networks,Annals of Mathematics and Artificial Intelligence,"Area coverage is a well-known problem in robotics. Extensive research has been conducted for the single robot coverage problem in the past decades. More recently, the research community has focused its attention on formulations where multiple robots are considered. In this paper, a new formulation of the multi-robot coverage problem is proposed. The novelty of this work is the introduction of a sensor network, which cooperates with the team of robots in order to provide coordination. The sensor network, taking advantage of its distributed nature, is responsible for both the construction of the path and for guiding the robots. The coverage of the environment is achieved by guaranteeing the reachability of the sensor nodes by the robots. Two distributed algorithms for path construction are discussed. The first aims to speed up the construction process exploiting a concurrent approach. The second aims to provide an underlying structure for the paths by building a Hamiltonian path and then partitioning it. A statistical analysis has been performed to show the effectiveness of the proposed algorithms. In particular, three different indexes of quality, namely completeness, fairness, and robustness, have been studied. © 2009 Springer Science+Business Media B.V.",math
1239,A Graph-Based Approach towards Discerning Inherent Structures in a Digital Library of Formal Mathematics,Mathematical Knowledge Management," As the amount of online formal mathematical content grows, for example through active efforts such as the Mathweb [21], MOWGLI [4], Formal Digital Library, or FDL [1], and others, it becomes increasingly valuable to find automated means to manage this data and capture semantics such as relatedness and significance. We apply graph-based approaches, such as HITS, or Hyperlink-Induced Topic Search, [11] used for World Wide Web document search and analysis, to formal mathematical data collections. The nodes of the graphs we analyze are theorems and definitions, and the links are logical dependencies. By exploiting this link structure, we show how one may extract organizational and relatedness information from a collection of digital formal math. We discuss the value of the information we can extract, yielding potential applications in math search tools, theorem proving, and education. ",math
1240,Approximate Linear Programming for Average Cost MDPs,Mathematics of Operations Research,"We consider the linear programming approach to approximate dynamic programming with an average cost objective and a finite state space. Using a Lagrangian form of the linear program (LP), the average cost error is shown to be a multiple of the best fit differential cost error. This result is analogous to previous error bounds for a discounted cost objective. Second, bounds are derived for average cost error and performance of the policy generated from the LP that involve the mixing time of the Markov decision process (MDP) under this policy or the optimal policy. These results improve on a previous performance bound involving mixing times. ÂŠ 2013 INFORMS.",math
1241,Construction of CM Picard curves,Mathematics of Computation," In this article we generalize the CM method for elliptic and hyperelliptic curves to Picard curves. We describe the algorithm in detail and discuss the results of our implementation. deg(f (x)) = 4; where f is a polynomial without multiple roots in . If contains the third roots of unity, the curve is equipped with an automorphism of order 3 de ned over . There exists an algorithm for an e cient addition law on the degree zero divisor class group, Pic0C ( ), of a Picard curve de ned over a nite eld = Fq [7, 29]. Because of the Pohlig-Hellmann attack [27], the curve C de ned over Fq should be chosen such that the order of Pic0C (Fq) contains a large prime factor. To tackle this problem, we need an e cient point counting algorithm for the curve C (or Pic0C (Fq)). For elds of small characteristic p this problem has been solved using p-adic methods [9], but for large prime elds the question is still unanswered. In this paper, we consider an alternative method for constructing Picard curves over large prime elds suitable for cryptography using complex multiplication. Note that the complex multiplication (CM) method is well-known for elliptic curves [2, 3]. Recently, this method has been extended to hyperelliptic curves of genus g 3 [34, 36, 37, 38]. We now describe the CM method from an abstract point of view. Given a CM eld K with nK = [K : Q] 6, set g = nK =2. In general the CM method can be described as follows: Received by the editor February 3, 2003 and, in revised form, July 14, 2003. 2000 Mathematics Subject Classi cation. Primary 14H45, 11G15; Secondary 14G50, 14K22. The rst author was supported by the Alexander von Humboldt Stiftung. The second author was supported by the Maria Sibylla Merian program of the university of Essen. ",math
1242,Automating the implementation of Kalman filter algorithms,ACM Transactions on Mathematical Software," AUTOFILTER is a tool that generates implementations that solve state estimation problems using Kalman filters. From a high-level, mathematics-based description of a state estimation problem, AUTOFILTER automatically generates code that computes a statistically optimal estimate using one or more of a number of well-known variants of the Kalman filter algorithm. The problem description may be given in terms of continuous or discrete, linear or nonlinear process and measurement dynamics. From this description, AUTOFILTER automates many common solution methods (e.g., linearization, discretization) and generates C or Matlab code fully automatically. AUTOFILTER surpasses toolkit-based programming approaches for Kalman filters because it requires no low-level programming skills (e.g., to “glue” together library function calls). AUTOFILTER raises the level of discourse to the mathematics of the problem at hand rather than the details of what algorithms, data structures, optimizations and so on are required to implement it. An overview of AUTOFILTER is given along with an example of its practical application to deep space attitude estimation. Categories and Subject Descriptors: D.2.1 [Software Engineering]: Requirements/Specifications; D.2.3 [Software Engineering]: Coding Tools and Techniques; D.2.13 [Software Engineering]: Reusable Software; G.4 [Mathematical Software]-Algorithm design and analysis ",math
1243,An Adaptive Algorithm for Finding the Optimal Base-Stock Policy in Lost Sales Inventory Systems with Censored Demand,Mathematics of Operations Research," We consider a periodic-review single-location single-product inventory system with lost sales and positive replenishment lead times. It is well known that the optimal policy does not possess a simple structure. Motivated by recent results showing that base-stock policies perform well in these systems, we study the problem of finding the best base-stock policy in such a system. In contrast to the classical inventory literature, we assume that the manager does not know the demand distribution a priori, but must make the replenishment decision in each period based only on the past sales (censored demand) data. We develop a nonparametric adaptive algorithm that generates a sequence of order-up-to levels whose T -period running average of the inventory holding and lost sales penalty cost converges to the cost of the optimal base-stock policy at the rate of O(1/T 1/3). Our analysis is based on recent advances in stochastic online convex optimization and on the uniform ergodicity of Markov chains associated with bases-stock policies. ",math
1244,An extended set of FORTRAN basic linear algebra subprograms,ACM Transactions on Mathematical Software, This paper describes an extension to the set of Basic Linear Algebra Subprograms. The extensions are targeted at matrix-vector operations that should provide for efficient and portable implementations of algorithms for high-performance computers. Categories and Subject Descriptors: F.2.1 [Analysis Numerical Algorithms and Problems-computation General--numerical algorithms; G.1.3 [Numerical systems (direct and iterative methods); G.4 [Mathematics certification and testing; efficiency; portability; reliability of Algorithms and Problem Complexity]: on matrices; G.l.O [Numerical Analysis]: Analysis]: Numerical Linear Algebra--linear of Computing]: Mathematical Softwareand robustness; verification ,math
1245,Asymptotic factorial powers expansions for binomial and negative binomial reciprocals,Proceedings of the American Mathematical Society," By considering the variance formula for a shifted reciprocal of a binomial proportion, the asymptotic expansions of any order for rst negative moments of binomial and negative binomial distributions truncated at zero are obtained. The expansions are given in terms of the factorial powers of the number of trials n. The obtained formulae are more accurate than those of Marciniak and Wesolowski (1999) and simpler, as they do not involve the Eulerian polynomials. ",math
1246,Double Bubbles Minimize,Annals of Mathematics," The classical isoperimetric inequality in R3 states that the surface of smallest area enclosing a given volume is a sphere. We show that the least area surface enclosing two equal volumes is a double bubble, a surface made of two pieces of round spheres separated by a °at disk, meeting along a single circle at an angle of 120±. ",math
1247,An axiomatic characterization of the Hirsch-index,Mathematical Social Sciences,"The Hirsch-index is a well-known index for measuring and comparing the output of scientific researchers. The main contribution of this article is an axiomatic characterization of the Hirsch-index in terms of three natural axioms. Furthermore, two other scientific impact indices (called the w-index and the maximum-index) are defined and characterized in terms of similar axioms. ÂŠ 2008 Elsevier B.V. All rights reserved.",math
1248,Relational concept analysis: mining concept lattices from multi-relational data,Annals of Mathematics and Artificial Intelligence,"The processing of complex data is admittedly among the major concerns of knowledge discovery from data (kdd). Indeed, a major part of the data worth analyzing is stored in relational databases and, since recently, on the Web of Data. This clearly underscores the need for Entity-Relationship and rdf compliant data mining (dm) tools. We are studying an approach to the underlying multi-relational data mining (mrdm) problem, which relies on formal concept analysis (fca) as a framework for clustering and classification. Our relational concept analysis (rca) extends fca to the processing of multi-relational datasets, i. e., with multiple sorts of individuals, each provided with its own set of attributes, and relationships among those. Given such a dataset, rca constructs a set of concept lattices, one per object sort, through an iterative analysis process that is bound towards a fixed-point. In doing that, it abstracts the links between objects into attributes akin to role restrictions from description logics (dls). We address here key aspects of the iterative calculation such as evolution in data description along the iterations and process termination. We describe implementations of rca and list applications to problems from software and knowledge engineering. © 2013 Springer Science+Business Media Dordrecht.",math
1249,Separation properties of convex cones,Proceedings of the American Mathematical Society," 1. Introduction. If A and B are convex subsets of a topological linear space P, we say that A and B can be separated by a hyperplane provided P admits a continuous linear functional /, not identically zero, such that sup^/^infB/. Except in rather special cases, the known separation theorems require that A and B have no common point, and thus fail to cover the interesting case in which A and B are closed convex cones whose intersection and common vertex is the origin <p.This case is discussed in the present note. Our main results, extending theorems of Aronszajn [l ] and Tagamlitzki [6], are proved in §2; §3 contains examples showing that these results cannot be substantially improved. In addition to standard notation and terminology, we employ the following: A <p-cone in a topological linear space is a closed convex cone having vertex <p; for a 0-cone A, A' will denote the linear subspace A(~\- A. Set-theoretic sum and difference are indicated by KJ and \ respectively, + and - being reserved for the linear operations. For points x and y of a linear space, [x, y] = {ta + (l -t)y: Ogtg 1}, ]x, y[={tx+(l-t)y: 0<*<l}, etc. ",math
1250,PRIMES is in P,Annals of Mathematics,  ,math
1251,Queues with Many Servers: The Virtual Waiting-Time Process in the QED Regime,Mathematics of Operations Research," We consider a multi-server queue (G/GI/N) in the Quality- and Efficiency-Driven (QED) regime. In this regime, which was first formalized by Halfin and Whitt, the number of servers N is not small, servers' utilization is 1 − O(1/√N ) (Efficiency-Driven) while waiting time is O(1/√N ) (Quality-Driven). This is equivalent to having the number of servers N being approximately equal to R + β√R, where R is the offered load and β is a positive constant. For the G/GI/N queue in the QED regime, we analyze the virtual waiting time VN (t), as N increases indefinitely. Assuming that the service time distribution has a finite support, it is shown that, in the limit, the scaled virtual waiting time VˆN (t) = √N VN (t)/ES is representable as a supremum over a random weighted tree (S denotes a service time). Informally, it is then argued that, for large N , ∗Research partially supported by the BSF (Binational Science Foundation) grant 2001685/2005175, by the ISF (Israeli Science Foundation) grants 126/02 and 1046/04, and by the Technion funds for the promotion of research and sponsored research. ",math
1252,The primes contain arbitrarily long arithmetic progressions,Annals of Mathematics," We prove that there are arbitrarily long arithmetic progressions of primes. There are three major ingredients. The first is Szemer´edi's theorem, which asserts that any subset of the integers of positive density contains progressions of arbitrary length. The second, which is the main new ingredient of this paper, is a certain transference principle. This allows us to deduce from Szemer´edi's theorem that any subset of a sufficiently pseudorandom set (or measure) of positive relative density contains progressions of arbitrary length. The third ingredient is a recent result of Goldston and Yıldırım, which we reproduce here. Using this, one may place (a large fraction of) the primes inside a pseudorandom set of “almost primes” (or more precisely, a pseudorandom measure concentrated on almost primes) with positive relative density. ",math
1253,Resolution cannot polynomially simulate compressed-BFS,Annals of Mathematics and Artificial Intelligence,"Many algorithms for Boolean satisfiability (SAT) work within the framework of resolution as a proof system, and thus on unsatisfiable instances they can be viewed as attempting to find proofs by resolution. However it has been known since the 1980s that every resolution proof of the pigeonhole principle (PHP n m ), suitably encoded as a CNF instance, includes exponentially many steps [18]. Therefore SAT solvers based upon the DLL procedure [12] or the DP procedure [13] must take exponential time. Polynomial-sized proofs of the pigeonhole principle exist for different proof systems, but general-purpose SAT solvers often remain confined to resolution. This result is in correlation with empirical evidence. Previously, we introduced the Compressed-BFS algorithm to solve the SAT decision problem. In an earlier work [27], an implementation of a Compressed-BFS algorithm empirically solved PHPn n+1̄ instances in Θ(n 4) time. Here, we add to this claim, and show analytically that these instances are solvable in polynomial time by Compressed-BFS. Thus the class of tautologies efficiently provable by Compressed-BFS is different than that of any resolution-based procedure. We hope that the details of our complexity analysis shed some light on the proof system implied by Compressed-BFS. Our proof focuses on structural invariants within the compressed data structure that stores collections of sets of open clauses during the Compressed-BFS algorithm. We bound the size of this data structure, as well as the overall memory, by a polynomial. We then use this to show that the overall runtime is bounded by a polynomial. © Springer 2005.",math
1254,"Decomposable capacities, distorted probabilities and concave capacities",Mathematical Social Sciences,"During,the last few years, capacities have been used extensively to model attitudes towards uncertainty. We describe the links between some classes of capacities, namely between decomposable capacities introduced by Dubois and Prade and other capacities, such as concave or convex capacities, and distorted probabilities that appeared in two new models of non-additive expected utility theory (Schmeidler, Econometrica, 1989, 57, 571-587; Yaari, Econometrica, 1987, 55, 95-115). It is shown that the most well-known decomposable capacities prove to be distorted probabilities, and that any concave distortion of a probability is decomposable. The paper ends by successively characterizing decomposable capacities that are concave distortions of probabilities, and ⊥-decomposable capacities (for triangular conorms ⊥) that are concave, since decomposable capacities prove to be much more related to concavity than convexity.",math
1255,On Constraint Sampling in the Linear Programming Approach to Approximate Dynamic Programming,Mathematics of Operations Research," In the linear programming approach to approximate dynamic programming, one tries to solve a certain linear program-the ALP-that has a relatively small number K of variables but an intractable number M of constraints. In this paper, we study a scheme that samples and imposes a subset of m M constraints. A natural question that arises in this context is: How must m scale with respect to K and M in order to ensure that the resulting approximation is almost as good as one given by exact solution of the ALP? We show that, given an idealized sampling distribution and appropriate constraints on the K variables, m can be chosen independently of M and need grow only as a polynomial in K. We interpret this result in a context involving controlled queueing networks. ",math
1256,Possibilistic uncertainty handling for answer set programming,Annals of Mathematics and Artificial Intelligence,"In this work, we introduce a new framework able to deal with a reasoning that is at the same time non monotonic and uncertain. In order to take into account a certainty level associated to each piece of knowledge, we use possibility theory to extend the non monotonic semantics of stable models for logic programs with default negation. By means of a possibility distribution we define a clear semantics of such programs by introducing what is a possibilistic stable model. We also propose a syntactic process based on a fix-point operator to compute these particular models representing the deductions of the program and their certainty. Then, we show how this introduction of a certainty level on each rule of a program can be used in order to restore its consistency in case of the program has no model at all. Furthermore, we explain how we can compute possibilistic stable models by using available softwares for Answer Set Programming and we describe the main lines of the system that we have developed to achieve this goal. © Springer Science+Business Media, Inc. 2006.",math
1257,Eigenvalues of Laplacians on a closed Riemannian manifold and its nets,Proceedings of the American Mathematical Society,We show that the eigenvalues of the Laplacian of a closed manifold M is approximated in a certain sense by the eigenvalues of the Laplacian of the graph of a 1/n-net in M as n → ∞. Our approximation needs no assumption on M except for dimension. © 1995 American Mathematical Society.,math
1258,The dark side of interval temporal logic: marking the undecidability border,Annals of Mathematics and Artificial Intelligence,"Unlike the Moon, the dark side of interval temporal logics is the one we usually see: their ubiquitous undecidability. Identifying minimal undecidable interval logics is thus a natural and important issue in that research area. In this paper, we identify several new minimal undecidable logics amongst the fragments of Halpern and Shohams logic HS, including the logic of the overlaps relation, over the classes of all finite linear orders and all linear orders, as well as the logic of the meets and subinterval relations, over the classes of all and dense linear orders. Together with previous undecidability results, this work contributes to bringing the identification of the dark side of interval temporal logics very close to the definitive picture. © 2013 Springer Science+Business Media Dordrecht.",math
1259,On estimating the largest eigenvalue with the Lanczos algorithm,Mathematics of Computation,"The Lanczos algorithm applied to a positive definite matrix produces good approximations to the eigenvalues at the extreme ends of the spectrum after a few iterations. In this note we utilize this behavior and develop a simple algorithm which computes the largest eigenvalue. The algorithm is especially economical if the order of the matrix is large and the accuracy requirements are low. The phenomenon of misconvergence is discussed. Some simple extensions of the algorithm are also indicated. Finally, some numerical examples and a comparison with the power method are given. © 1982 American Mathematical Society.",math
1260,Generation of finite difference formulas on arbitrarily spaced grids,Mathematics of Computation," Simple recursions are derived for calculating the weights in compact finite difference formulas for any order of derivative and to any order of accuracy on onedimensional grids with arbitrary spacing. Tables are included for some special cases (of equispaced grids). 1. Introduction. Previously published methods to generate finite difference weights (e.g., references [l]-[5]) have been of considerable complexity and often been limited to derivatives of low order on equidistantly spaced grids. The most ambitious attempt to tabulate weights for many orders of derivatives and to high orders of accuracy appears to be the work by Keller and Pereyra [4]. However, their algorithms (limited to equispaced grids) were very involved, and the resulting tables contain both isolated and systematic errors. In the present study we describe two simple recursion relations which give the weights for any order of derivative (including the Oth derivative, corresponding to interpolation), approximated to any order of accuracy on an arbitrary grid in one dimension. Since, in general, only four arithmetic operations are needed to determine each weight, the main anticipated application of the present method is to dynamically changing grids. However, the method is also well suited to generate tables of weights. Such tables (in the special case of equispaced grids, up to the 4th derivative and up to 9 weights) are included in the cases of one-sided and centered approximations at a grid point and at a 'half-way point' between grid points. ",math
1261,Linearized augmented Lagrangian and alternating direction methods for nuclear norm minimization,Mathematics of Computation," The nuclear norm is widely used to induce low-rank solutions for many optimization problems with matrix variables. Recently, it has been shown that the augmented Lagrangian method (ALM) and the alternating direction method (ADM) are very efficient for many convex programming problems arising from various applications, provided that the resulting subproblems are sufficiently simple to have closed-form solutions. In this paper, we are interested in the application of the ALM and the ADM for some nuclear norm involved minimization problems. When the resulting subproblems do not have closed-form solutions, we propose to linearize these subproblems such that closed-form solutions of these linearized subproblems can be easily derived. Global convergence of these linearized ALM and ADM are established under standard assumptions. Finally, we verify the effectiveness and efficiency of these new methods by some numerical experiments. ",math
1262,Symmetric approximate linear programming for factored MDPs with application to constrained problems,Annals of Mathematics and Artificial Intelligence," A weakness of classical Markov decision processes (MDPs) is that they scale very poorly due to the flat state-space representation. Factored MDPs address this representational problem by exploiting problem structure to specify the transition and reward functions of an MDP in a compact manner. However, in general, solutions to factored MDPs do not retain the structure and compactness of the problem representation, forcing approximate solutions, with approximate linear programming (ALP) emerging as a promising MDP-approximation technique. To date, most ALP work has focused on the primal-LP formulation, while the dual LP, which forms the basis for solving constrained Markov problems, has received much less attention. We show that a straightforward linear approximation of the dual optimization variables is problematic, because some of the required computations cannot be carried out efficiently. Nonetheless, we develop a composite approach that symmetrically approximates the primal and dual optimization variables (effectively approximating both the objective function and the feasible region of the LP), leading to a formulation that is computationally feasible and suitable for solving constrained MDPs. We empirically show that this new ALP formulation also performs well on unconstrained problems. ",math
1263,Dagwood: a system for manipulating polynomials given by straight-line programs,ACM Transactions on Mathematical Software," We discuss the design, implementation, and benchmarking of a system that can manipulate symbolic expressions represented by their straight-line computations. Our system is capable of performing rational arithmetic on, evaluating, differentiating, taking greatest common divisors of, and factoring polynomials in straight-line format. The straight-line results can also be converted to standard sparse format. We show by example that our system can handle problems for which conventional methods lead to excessive intermediate expression swell. ",math
1264,"Entropy waves, the zig-zag graph product, and new constant-degree expanders",Annals of Mathematics," The main contribution of this work is a new type of graph product, which we call the zig-zag product. Taking a product of a large graph with a small graph, the resulting graph inherits (roughly) its size from the large one, its degree from the small one, and its expansion properties from both! Iteration yields simple explicit constructions of constant-degree expanders of arbitrary size, starting from one constantsize expander. Crucial to our intuition (and simple analysis) of the properties of this graph product is the view of expanders as functions which act as “entropy wave” propagators - they transform probability distributions in which entropy is concentrated in one area to distributions where that concentration is dissipated. In these terms, the graph product affords the constructive interference of two such waves. Subsequent work [ALW01, MW01] relates the zig-zag product of graphs to the standard semidirect product of groups, leading to new results and constructions on expanding Cayley graphs. ",math
1265,On the Empirical State-Action Frequencies in Markov Decision Processes Under General Policies,Mathematics of Operations Research," We consider the empirical state-action frequencies and the empirical reward in weakly communicating finite-state Markov decision processes under general policies. We define a certain polytope and establish that every element of this polytope is the limit of the empirical frequency vector, under some policy, in a strong sense. Furthermore, we show that the probability of exceeding a given distance between the empirical frequency vector and the polytope decays exponentially with time under every policy. We provide similar results for vector-valued empirical rewards. ",math
1266,The Geometry of Fractional Stable Matchings and its Applications,Mathematics of Operations Research," CHUNG-PIAW TEO AND JAY SETHURAMAN We study the classical stable marriage and stable roommates problems using a polyhedral approach. We propose a new LP formulation for the stable roommates problem, which has a feasible solution if and only if the underlying roommates problem has a stable matching. Furthermore, for certain special weight functions on the edges, we construct a 2-approximation algorithm for the optimal stable roommates problem. Our technique exploits features of the geometry of fractional solutions of this formulation. For the stable marriage problem, we show that a related geometry allows us to express any fractional solution in the stable marriage polytope as a convex combination of stable marriage solutions. This also leads to a genuinely simple proof of the integrality of the stable marriage polytope. ",math
1267,On the distribution of the power generation,Mathematics of Computation," We present a new method to study the power generator of pseudorandom numbers modulo a Blum integer m. This includes as special cases the RSA generator and the Blum{Blum{Shub generator. We prove the uniform distribution of these, provided that the period t m3=4+ with xed > 0 and, under the same condition, the uniform distribution of a positive proportion of the leftmost and rightmost bits. This sharpens and generalizes previous results which dealt with the RSA generator, provided the period t m23=24+ . We apply our results to deduce that the period of the binary sequence of the rightmost bit has exponential length. ",math
1268,The Dodgson ranking and the Borda Count : A binary comparison,Mathematical Social Sciences,"This paper provides a binary comparison of two preference aggregation rules, the Borda rule and Dodgsons rule. Both of these rules guarantee a transitive ranking of the alternatives for every list of individual preferences and therefore avoid the problem of voting cycles. It will be shown that for certain lists of individual preferences the rankings derived from the Borda rule and Dodgsons rule are antagonistic. © 2004 Elsevier B.V. All rights reserved.",math
1269,Programs to generate Niederreiter's low-discrepancy sequences,ACM Transactions on Mathematical Software,"This note points out programs to implement Niederreiters low-discrepancy sequences. © 1994, ACM. All rights reserved.",math
1270,On the number of nonzeros added when Gaussian elimination is performed on sparse random matrices,Mathematics of Computation," This paper studies the fill-in properties of Gaussian elimination on sparse random matrices. A theoretical study using the concept of random graphs yields formulae from which the fill-in can be evaluated. The predictions are examined for matrices of various orders and densities and are found to be in close agreement with experimental results. A possible consequence of the results of this paper relating to error analysis for sparse systems is given in the concluding section. 1. Introduction. This paper studies the fill-in properties of Gaussian elimination on sparse random matrices. Graph theoretical ideas are introduced and the problem is examined using the concept of random graphs. This concept of using random graphs is not a new one. They are used as a tool in the development of existence theorems in graph theory [Erdös (1967)] and some work has been done on the structure of a graph where the number of lines is a function of the number of points, the analysis holding in the limit as the number of points tends to infinity [Palásti (1966, 1970)].Ogilvie (1968) has shown that these asymptotic results hold where the graph has only a small number of vertices (about 10) but none of this work is directly concerned with matrix decomposition or indeed with the adjacency matrix of the graph. Heap (1966) has extended the notion of random graphs to examine properties of their adjacency matrices. However, his Tesults on matrix reducibility are only valid for comparatively dense matrices (probability density > 0.15). A serious attempt to tackle the problem of this paper has been made by Hsieh and Ghausi (1971) where they have tried to make theoretical predictions on the results of Brayton et al. (1970). Their work has, however, several serious defects. Although they make more allowance for interaction than the discussion in Section 2, their formulae only include correlation between elements in the same column ignoring the correlation which exists between elements in the same row. In addition, some of their formulae break down if the density of the matrix increases above 20% and their correlation factor H is chosen empirically to obtain a fit with Brayton's data rather than determined theoretically. The impetus for writing this paper came from remarks by Robert Brayton in the panel at the end of the IBM symposium [Rose and Willoughby (1972)] when he stated that the above problem, although itself not perhaps of devastating importance, has consumed vast ",math
1271,Equivalence of Convex Problem Geometry and Computational Complexity in the Separation Oracle Model,Mathematics of Operations Research, by OF TECHNOLOGY ,math
1272,Approximation of curves by line segments,Mathematics of Computation," 1. Introduction. Most methods of linear and nonlinear programming developed up to this time were designed to find the maximum or minimum value of a linear or nonlinear function inside a region bounded by hyperplanes. In applications of these methods it is often found that the formulation leads to constraints which are nonlinear. For example, in gasoline blending problems the relationship between lead concentration and octane number of a blend (the lead-susceptibility curve) has been found to be exponential in form. One method of dealing with such problems has been to approximate the given curves by a series of broken line segments [1]. The usual method of finding such an approximation has been, by visual examination of the graph, first to decide on the number of line segments, second, to select the intervals over which each line segment is to apply, and third, to draw in what appears to be good linear approximations to the curves over the selected intervals. Since the problem of obtaining a best fit of broken line segments to a curve does not seem to have been previously investigated, it is the purpose of this paper to formulate the problem, give a closed form solution when the given function is quadratic, show a general numerical method of solution, and apply this numerical procedure to the lead susceptibility curve. In the case when the function is quadratic, an interesting and simple result was obtained. It was found that in fitting N lines over some interval (a, b), that the points at which one line segment was discontinued and the next line segment started were equally spaced over the interval. This result allowed the equations for the lines to be expressed in a very simple form. ",math
1273,Principal values for the Cauchy integral and rectifiability,Proceedings of the American Mathematical Society," We give a geometric characterization of those positive nite measures on C with the upper density lim supr!0 (f :j r zj rg) nite at almost every z 2 C, such that the principal value of the Cauchy integral of , Z lim ""!0 j Notice that the de nition above does not make sense, in general, for z 2 supp( ). This is the reason why one considers the truncated Cauchy integral of , which is de ned as ",math
1274,Bounds on the Travel Cost of a Mars Rover Prototype Search Heuristic,SIAM Journal on Discrete Mathematics,"D* is a greedy heuristic planning method that is widely used in robotics, including several Nomad class robots and the Mars rover prototype, to reach a destination in unknown terrain. We obtain nearly sharp lower and upper bounds of Ω(n log n/log log n) and O(n log n), respectively, on the worst-case total distance traveled by the robot, for the grid graphs on n vertices typically used in robotics applications. For arbitrary graphs we prove an O(n log                          2 n) upper bound. © 2005 Society for Industrial and Applied Mathematics.",math
1275,Evaluating higher derivative tensors by forward propagation of univariate Taylor series,Mathematics of Computation," This article considers the problem of evaluating all pure and mixed partial derivatives of some vector function de ned by an evaluation procedure. The natural approach to evaluating derivative tensors might appear to be their recursive calculation in the usual forward mode of computational di erentiation. However, with the approach presented in this article, much simpler data access patterns and similar or lower computational counts can be achieved through propagating a family of univariate Taylor series of a suitable degree. It is applicable for arbitrary orders of derivatives. Also it is possible to calculate derivatives only in some directions instead of the full derivative tensor. Explicit formulas for all tensor entries as well as estimates for the corresponding computational complexities are given. ",math
1276,Modifying pivot elements in Gaussian elimination,Mathematics of Computation,"The rounding-error analysis of Gaussian elimination shows that the method is stable only when the elements of the matrix do not grow excessively in the course of the reduction. Usually such growth is prevented by interchanging rows and columns of the matrix so that the pivot element is acceptably large. In this paper the alternative of simply altering the pivot element is examined. The alteration, which amounts to a rank one modification of the matrix, is undone at a later stage by means of the well-known formula for the inverse of a modified matrix. The technique should prove useful in applications in which the pivoting strategy has been fixed, say to preserve sparseness in the reduction. ÂŠ 1974, American Mathematical Society.",math
1277,Dealing with explicit preferences and uncertainty in answer set programming,Annals of Mathematics and Artificial Intelligence,"In this paper, we show how the formalism of Logic Programs with Ordered Disjunction (LPODs) and Possibilistic Answer Set Programming (PASP) can be merged into the single framework of Logic Programs with Possibilistic Ordered Disjunction (LPPODs). The LPPODs framework embeds in a unified way several aspects of common-sense reasoning, nonmonotonocity, preferences, and uncertainty, where each part is underpinned by a well established formalism. On one hand, from LPODs it inherits the distinctive feature of expressing context-dependent qualitative preferences among different alternatives (modeled as the atoms of a logic program). On the other hand, PASP allows for qualitative certainty statements about the rules themselves (modeled as necessity values according to possibilistic logic) to be captured. In this way, the LPPODs framework supports a reasoning which is nonmonotonic, preference- and uncertainty-aware. The LPPODs syntax allows for the specification of (1) preferences among the exceptions to default rules, and (2) necessity values about the certainty of program rules. As a result, preferences and uncertainty can be used to select the preferred uncertain default rules of an LPPOD and, consequently, to order its possibilistic answer sets. Furthermore, we describe the implementation of an ASP-based solver able to compute the LPPODs semantics. © 2012 Springer Science+Business Media B.V.",math
1278,Some characterizations of lower probabilities and other monotone capacities through the use of Mobius inversion,Mathematical Social Sciences,Monotone capacities (on finite sets) of finite or infinite order (lower probabilities) are characterized by properties of their Möbius inverses. A necessary property of probabilities dominating a given capacity is demonstrated through the use of Gales theorem for the transshipment problem. This property is shown to be also sufficient if and only if the capacity is monotone of infinite order. A characterization of dominating probabilities specific to capacities of order 2 is also proved. © 1989.,math
1279,Equivalent Representations of Set Functions,Mathematics of Operations Research," This paper introduces four alternative representations of a set function: the M¨obius transformation, the co-M¨obius transformation, and the interactions between elements of any subset of a given set as extensions of Shapley and Banzhaf values. The links between the five equivalent representations of a set function are emphasized in this presentation. ",math
1280,Applying the Thorne-Kishino-Felsenstein model to sequence evolution on a star-shaped tree,Applied Mathematics Letters,"Stochastic models that allow site substitutions, insertions, and deletions provide a useful framework for a statistical approach to DNA sequence evolution. Such a model, and recursions to calculate the probability of evolving two sequences, have been known for almost a decade. In this paper we show how the pairwise recursions can be generalised to a three-sequence tree, and more generally to an r-sequence star-shaped tree. 漏 2001 Elsevier Science Ltd.",math
1281,"Permanents, Pfaffian orientations, and even directed circuits",Annals of Mathematics," Given a 0-1 square matrix A, when can some of the 1's be changed to ¡1's in such a way that the permanent of A equals the determinant of the modi¯ed matrix? When does a real square matrix have the property that every real matrix with the same sign pattern (that is, the corresponding entries either have the same sign or are both zero) is nonsingular? When is a hypergraph with n vertices and n hyperedges minimally nonbipartite? When does a bipartite graph have a \Pfa±an orientation""? Given a digraph, does it have no directed circuit of even length? Given a digraph, does it have a subdivision with no even directed circuit? It is known that all of the above problems are equivalent. We prove a structural characterization of the feasible instances, which implies a polynomial-time algorithm to solve all of the above problems. The structural characterization says, roughly speaking, that a bipartite graph has a Pfa±an orientation if and only if it can be obtained by piecing together (in a speci¯ed way) planar bipartite graphs and one sporadic nonplanar bipartite graph. ",math
1282,Representing Epistemic Uncertainty by Means of Dialectical Argumentation,Annals of Mathematics and Artificial Intelligence," We articulate a dialectical argumentation framework for qualitative representation of epistemic uncertainty in scientific domains. The framework is grounded in specific philosophies of science and theories of rational mutual discourse. We study the formal properties of our framework and provide it with a game theoretic semantics. With this semantics, we examine the relationship between the snaphots of the debate in the framework and the long run position of the debate, and prove a result directly analogous to the standard (Neyman-Pearson) approach to statistical hypothesis testing. We believe this formalism for representating uncertainty has value in domains with only limited knowledge, where experimental evidence is ambiguous or conflicting, or where agreement between different stakeholders on the quantification of uncertainty is difficult to achieve. All three of these conditions are found in assessments of carcinogenic risk for new chemicals. ",math
1283,Technical Aspects of the Digital Library of Mathematical Functions,Annals of Mathematics and Artificial Intelligence," The NIST Digital Library of Mathematical Functions (DLMF) Project, begun in 1997, is preparing a handbook and Web site intended for wide communities of users. The contents are primarily mathematical formulas, graphs, methods of computation, references, and links to software. The task of developing a Web handbook of this nature presents several technical challenges. We describe the goals of the Digital Library of Mathematical Functions Project and the realities that constrain those goals. We propose practical initial solutions, in order to ease the authoring of adaptable content: a LATEX class which encourages a modestly semantic markup style; and a mathematical search engine that adapts a text search engine to the task. ",math
1284,Anatomy of high-performance matrix multiplication,ACM Transactions on Mathematical Software," We present the basic principles which underlie the high-performance implementation of the matrixmatrix multiplication that is part of the widely used GotoBLAS library. Design decisions are justified by successively refining a model of architectures with multilevel memories. A simple but effective algorithm for executing this operation results. Implementations on a broad selection of architectures are shown to achieve excellent performance. Categories and Subject Descriptors: G.4 [Mathematical Software]: -Efficiency Additional Key Words and Phrases: linear algebra, matrix multiplication, basic linear algebra subprogrms Implementing matrix multiplication so that near-optimal performance is attained requires a thorough understanding of how the operation must be layered at the macro level in combination with careful engineering of high-performance kernels at the micro level. This paper primarily addresses the macro issues, while a companion paper will address the micro issues [Goto and Gunnels ]. In [Gunnels et al. 2001] a layered approach to the implementation of matrix multiplication was reported. The approach was shown to optimally amortize the required movement of data between two adjacent memory layers of an architecture with a complex multi-level memory. Like other work in the area [Agarwal et al. 1994; Whaley et al. 2001], that paper ([Gunnels et al. 2001]) casts computation in terms of an “inner-kernel” that computes C := A˜B + C for some mc × kc matrix A˜ that is stored contiguously in some packed format and fits in cache memory. Unfortunately, the model for the memory hierarchy that was used is unrealistic in at least two ways: ACM Transactions on Mathematical Software, Vol. V, No. N, Month 20YY, Pages 1-24. ",math
1285,On Maximal Frequent and Minimal Infrequent Sets in Binary Matrices,Annals of Mathematics and Artificial Intelligence," Given an m × n binary matrix A, a subset C of the columns is called t-frequent if there are at least t rows in A in which all entries belonging to C are nonzero. Let us denote by α the number of maximal t-frequent sets of A, and let β denote the number of those minimal column subsets of A which are not t-frequent (so called t-infrequent sets). We prove that the inequality α ≤ (m − t + 1)β holds for any binary matrix A in which not all column subsets are t-frequent. This inequality is sharp, and allows for an incremental quasi-polynomial algorithm for generating all minimal t-infrequent sets. We also prove that the analogous generation problem for maximal t-frequent sets is NP-hard. Finally, we discuss the complexity of generating closed frequent sets and some other related problems. ",math
1286,On two methods for approximating minimal surfaces in parametric form,Mathematics of Computation," Two methods for approximating minimal surfaces in parametric form are considered. One minimizes the area of the surface, and the other the energy of the surface. The convergence of the algorithm of the first method is proved. The application of the second method to the approximation of conformai maps is examined. Several examples of computations are given. 1. Introduction. There are many papers studying numerical approximations of minimal surfaces but, as far as the author knows, most of them consider minimal surfaces in nonparametric form. In this paper we consider minimal surfaces in parametric form. Let R"" (n > 2) be the «-dimensional Euclidean space, and let R2 d ß be a bounded domain with the Lipschitz boundary 9ß. R"" 3 T = {yx,..., ym) is a system of m Jordan curves, which is homeomorphic to 9ß. Let PCX(U) be a set of piecewise smooth functions in ß, and C(ß) be a set of continuous functions on ß, where ß denotes the closure of the domain ß. We define the functional space X as follows: Received August 6, 1984; revised February 7, 1985. 1980 MathematicsSubjectClassificationP.rimary49D20,49F10,65E05. ",math
1287,Implicates and reduction techniques for temporal logics,Annals of Mathematics and Artificial Intelligence," Reduction strategies are introduced for the future fragment of a temporal propositional logic on linear discrete time, named FNext. These reductions are based in the information collected from the syntactic structure of the formula, which allow the development of e±cient strategies to decrease the size of temporal propositional formulas, viz. new criteria to detect the validity or unsatis¯ability of subformulas, and a strong generalisation of the pure literal rule. These results, used as an inner processing step, allow to improve the performance of any automated theorem prover. ",math
1288,Absorbing boundary conditions for the numerical simulation of waves,Mathematics of Computation," In practical calculations, it is often essential to introduce artificial boundaries to limit the area of computation. Here we develop a systematic method for obtaining a hierarchy of local boundary conditions at these artificial boundaries. These boundary conditions not only guarantee stable difference approximations but also minimize the (unphysical) artificial reflections which occur at the boundaries. Introduction. When calculating solutions to partial differential equations it is often essential to introduce artificial boundaries to limit the area of computation. Important areas of application which use artificial boundaries are local weather prediction (see [8] and [9]), geophysical calculations involving acoustic and elastic waves (see [6] and [7]), and a variety of other problems in fluid dynamics. One needs boundary conditions at these artificial boundaries in order to guarantee a unique and well-posed solution to the differential equation. In turn, this is a necessary condition to guarantee stable difference approximation. Of course, one hopes that these artificial boundaries and boundary conditions affect the solution in a manner such that it closely approximates the free space solution which exists in the absence of these boundaries. In particular, one would like to minimize the amplitudes of waves reflected from these artificial boundaries. In this work we develop perfectly absorbing boundary conditions for general classes of wave equations by applying the recently developed theory for reflection of singularities for solutions of differential equations (see [2], [3], [4] ). Unfortunately, these boundary conditions necessarily have to be nonlocal in both space and time and thus are not useful for practical calculations. Hence, in the main part of the paper, we derive a hierarchy of highly absorbing local boundary conditions which approximate the theoretical nonlocal boundary condition. Of course, it is also very important for applications that these boundary conditions generate well-posed mixed initial boundary value problems. (See [1] for a discussion of well-posedness.) The general approach is applied specifically to produce absorbing artificial boundaries for the acoustic wave equation in Cartesian and polar coordinates and for the linearized shallow water equations in two space dimensions. We illustrate our results by the following hierarchy of absorbing boundary conditions at the wall x = 0 for the scalar wave equation, wtt = wxx+wyy> t,x>0, ",math
1289,Random numbers generated by linear recurrence modulo two,Mathematics of Computation," 1. Introduction. Many situations arise in various fields of interest for which the mathematical model utilizes a random sequence of numbers, events, or both. In many of these applications it is often extremely advantageous to generate, by some deterministic means, a sequence which appears to be random, even if, upon closer and longer observation, certain regularities become evident. For example, electronic computer programs for generating random numbers to be used in Monte Carlo experiments have proved extremely useful. This article describes a random number generator of this type with several outstanding properties. The numbers are generated by modulo 2 linear recurrence techniques long used to generate binary codes for communications. ",math
1290,A geometric Littlewood-Richardson rule,Annals of Mathematics," We describe an explicit geometric Littlewood-Richardson rule, interpreted as deforming the intersection of two Schubert varieties so that they break into Schubert varieties. There are no restrictions on the base field, and all multiplicities arising are 1; this is important for applications. This rule should be seen as a generalization of Pieri's rule to arbitrary Schubert classes, by way of explicit homotopies. It has a straightforward bijection to other Littlewood-Richardson rules, such as tableaux and Knutson and Tao's puzzles. This gives the first geometric proof and interpretation of the Littlewood-Richardson rule. It has a host of geometric consequences, described in [V2]. The rule also has an interpretation in K-theory, suggested by Buch, which gives an extension of puzzles to K-theory. The rule suggests a natural approach to the open question of finding a Littlewood-Richardson rule for the flag variety, leading to a conjecture, shown to be true up to dimension 5. Finally, the rule suggests approaches to similar open problems, such as Littlewood-Richardson rules for the symplectic Grassmannian and two-flag varieties. ",math
1291,"Algorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate",ACM Transactions on Mathematical Software,"CHOLMOD is a set of routines for factorizing sparse symmetric positive definite matrices of the form A or AAT, updating/downdating a sparse Cholesky factorization, solving linear systems, updating/downdating the solution to the triangular system Lx = b, and many other sparse matrix functions for both symmetric and unsymmetric matrices. Its supernodal Cholesky factorization relies on LAPACK and the Level-3 BLAS, and obtains a substantial fraction of the peak performance of the BLAS. Both real and complex matrices are supported. CHOLMOD is written in ANSI/ISO C, with both C and MATLABTM interfaces. It appears in MATLAB 7.2 as x = A\b when A is sparse symmetric positive definite, as well as in several other sparse matrix functions. © 2008 ACM.",math
1292,Bayesian optimization for learning gaits under uncertainty,Annals of Mathematics and Artificial Intelligence," Designing gaits and corresponding control policies is a key challenge in robot locomotion. Even with a viable controller parametrization, finding near-optimal parameters can be daunting. Typically, this kind of parameter optimization requires specific expert knowledge and extensive robot experiments. Automatic black-box gait optimization methods greatly reduce the need for human expertise and time-consuming design processes. Many different approaches for automatic gait optimization have been suggested to date. However, no extensive comparison among them has yet been performed. In this article, we thoroughly discuss multiple automatic optimization methods in the context of gait optimization. We extensively evaluate Bayesian optimization, a model-based approach to black-box optimization under uncertainty, on both simulated problems and real robots. This evaluation demonstrates that Bayesian optimization is particularly suited for robotic applications, where it is crucial to find a good set of gait parameters in a small number of experiments. ",math
1293,The Complexity of Decentralized Control of Markov Decision Processes,Mathematics of Operations Research," We consider decentralized control of Markov decision processes and give complexity bounds on the worst-case running time for algorithms that find optimal solutions. Generalizations of both the fully observable case and the partially observable case that allow for decentralized control are described. For even two agents, the finite-horizon problems corresponding to both of these models are hard for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov decision processes. In contrast to the problems involving centralized control, the problems we consider provably do not admit polynomial-time algorithms. Furthermore, assuming EXP = NEXP, the problems require superexponential time to solve in the worst case. ",math
1294,Perturbing polynomials with all their roots on the unit circle,Mathematics of Computation," Given a monic real polynomial with all its roots on the unit circle, we ask to what extent one can perturb its middle coe cient and still have a polynomial with all its roots on the unit circle. We show that the set of possible perturbations forms a closed interval of length at most 4, with 4 achieved only for polynomials of the form x2n + cxn + 1 with c in [−2; 2]. The problem can also be formulated in terms of perturbing the constant coe cient of a polynomial having all its roots in [−1; 1]. If we restrict to integer coe cients, then the polynomials in question are products of cyclotomics. We show that in this case there are no perturbations of length 3 that do not arise from a perturbation of length 4. We also investigate the connection between slightly perturbed products of cyclotomic polynomials and polynomials with small Mahler measure. We describe an algorithm for searching for polynomials with small Mahler measure by perturbing the middle coe cients of products of cyclotomic polynomials. We show that the complexity of this algorithm is p O(C d), where d is the degree, and we report on the polynomials found by this algorithm through degree 64. ",math
1295,Reasoning on temporal class diagrams: Undecidability results,Annals of Mathematics and Artificial Intelligence,"This paper introduces a temporal class diagram language useful to model temporal varying data. The atemporal portion of the language contains the core constructors available in both EER diagrams and UML class diagrams. The temporal part of the language is able to distinguish between temporal and atemporal constructs, and it has the ability to represent dynamic constraints between classes. The language is characterized by a model-theoretic (temporal) semantics. Reasoning services as logical implication and satisfiability are also defined. We show that reasoning on finite models is different from reasoning on unrestricted ones. Then, we prove that reasoning on temporal class diagrams is an undecidable problem on both unrestricted models and on finite ones. ÂŠ Springer 2006.",math
1296,SSB Utility theory: an economic perspective,Mathematical Social Sciences,"SSB utility theory for decision-making under risk is a generalization of the expected utility theory of von Neumann and Morgenstern that preserves much of their structure for the quantitative representation of preferences, including continuity, convexity, and monotonicity. but drops their transitivity and independence axioms. It represents preferences by a skew-symmetric bilinear functional φ that is unique up to multiplication by a positive constant and has φ(p,q)>0if and only if risky prospect p is preferred to risky prospect q. This generalization has far-reaching consequences. It is compatible with the phenomena of preference cycles, preference reversals, and systematic violations of independence: provides a viable theory of choice based on maximally preferred alternatives; supports a rationale for social choice lotteries that is untroubled by cyclical majorities and is consistent with majority choice and Pareto optimality; offers a much more general theory of risk attitudes; retains intact the central propositions of stochastic dominance; and suggests a new approach to decision making in the face of uncertainty. © 1984.",math
1297,Object-oriented software for quadratic programming,ACM Transactions on Mathematical Software," The object-oriented software package OOQP for solving convex quadratic programming problems (QP) is described. The primal-dual interior point algorithms supplied by OOQP are implemented in a way that is largely independent of the problem structure. Users may exploit problem structure by supplying linear algebra, problem data, and variable classes that are customized to their particular applications. The OOQP distribution contains default implementations that solve several important QP problem types, including general sparse and dense QPs, bound-constrained QPs, and QPs arising from support vector machines and Huber regression. The implementations supplied with the OOQP distribution are based on such well known linear algebra packages as MA27/57, LAPACK, and PETSc. OOQP demonstrates the usefulness of object-oriented design in optimization software development, and establishes standards that can be followed in the design of software packages for other classes of optimization problems. A number of the classes in OOQP may also be reusable directly in other codes. Categories and Subject Descriptors: D.2.2 [Software Engineering]: Design Tools and Techniques; G.1.6 [Numerical Analysis]: Optimization-quadratic programming methods; G.4 [Mathematical Software]: algorithm design and analysis ",math
1298,Concurrency and Commitment: Network Scheduling and Its Consequences for Diffusion 1,Journal of Mathematical Sociology," Network ties are thought to be concurrent-one can ''have'' many friends at once, for instance-but their concrete enactment is largely serial and episodic, guided by priorities that steer a person from one encounter to the next. Further, dyadic encounters require that two people be simultaneously available to interact, creating the need for coordinated scheduling. Here I study the consequences of scheduling for network diffusion, using a computer simulation that interposes a scheduling process between a pre-existing network and instances of contagion. The pace and extent of diffusion are shown to depend upon the interaction of network topology, contagion rule (on first-contact versus at some threshold), and whether actors try to remedy past scheduling imperfections. Scheduling turns central actors into diffusion bottlenecks, but can also trigger early adoption by giving actors false readings on the status of their network alters. The implications of scheduling extend beyond diffusion, to other outcomes such as decision-making, as well as to network evolution. ",math
1299,Characterization of quasi-Banach spaces which coarsely embed into a Hilbert space,Proceedings of the American Mathematical Society," We show that a quasi-Banach space coarsely embeds into a Hilbert space if and only if it is isomorphic to a linear subspace of L0(µ) for some probability space (Ω , B, µ). A (not necessarily continuous) map f between two metric spaces (X, d) and (Y, δ) is called a coarse embedding (see [G, 7.G]) if there exist two nondecreasing functions ϕ1 : [0, ∞) → [0, ∞) and ϕ2 : [0, ∞) → [0, ∞) such that (1) ϕ1(d(x, y)) ≤ δ(f (x), f (y)) ≤ ϕ2(d(x, y)), (2) ϕ1(t) → ∞ as t → ∞. It was proved in [JR] that p does not coarsely embed into a Hilbert space when p > 2. The present article is a strengthening of that result by giving a full characterization of quasi-Banach spaces that coarsely embed into a Hilbert space. This result, as well as its proof, mirrors the theorem in [AMM] which characterizes spaces that uniformly embed into a Hilbert space. The combination of Theorem 1 and the theorem in [AMM] yield that a quasi-Banach space uniformly embeds into a Hilbert space if and only if it coarsely embeds into a Hilbert space. This is counterintuitive in that a uniform embedding gives information only on small distances while a coarse embedding gives information only on large distances. ",math
1300,"A sharp inequality for transport maps in W1,p(R) via approximation",Applied Mathematics Letters,"For f convex and increasing, we prove the inequality ∫f(| U′|)<∫f(nT′), every time that U is a Sobolev function of one variable and T is the non-decreasing map defined on the same interval with the same image measure as U, and the function n(x) takes into account the number of pre-images of U at each point. This may be applied to some variational problems in a mass-transport framework or under volume constraints. © 2011 Elsevier Ltd. All rights reserved.",math
1301,Methods of numerical integration,Mathematics of Computation," Numerical Mathematics and Computing , E. Cheney, David Kincaid, Aug 3, 2007, Mathematics, 784 pages. Authors Ward Cheney and David Kincaid show students of science and engineering the potential computers have for solving numerical problems and give them ample opportunities to. A First Course in Numerical Analysis , Anthony Ralston, Philip Rabinowitz, 2001, Mathematics, 606 pages. Outstanding text, oriented toward computer solutions, stresses errors in methods and computational efficiency. Problems вЂ” some strictly mathematical, others requiring a. Handbook of Computational Methods for Integration , Prem K. Kythe, Michael R. SchГ¤ferkotter, Dec 20, 2004, Mathematics, 624 pages. During the past 20 years, there has been enormous productivity in theoretical as well as computational integration. Some attempts have been made to find an optimal or best. ",math
1302,An algorithm for identifying Morishima and anti-Morishima matrices and balanced digraphs,Mathematical Social Sciences,"We present an algorithm for identifying Morishima and anti-Morishima matrices. Since Morishima matrices have signed diagraphs that are balanced, our algorithm also identifies such diagraphs. We actually work with a signed graph and our algorithm is a depth first traversal with certain markers added to keep track of parsing of nodes and signs of edges. The algorithm is of course, linear in the amount of input. ÂŠ 1983.",math
1303,Toward Leaner Binary-Clause Reasoning in a Satisfiability Solver,Annals of Mathematics and Artificial Intelligence,  ,math
1304,Towards Collaborative Content Management and Version Control for Structured Mathematical Knowledge,Mathematical Knowledge Management," We propose an infrastructure for collaborative content management and version control for structured mathematical knowledge. This will enable multiple users to work jointly on mathematical theories with minimal interference. We describe the API and the functionality needed to realize a cvs-like version control and distribution model. This architecture extends the cvs architecture in two ways, motivated by the speci¯c needs of distributed management of structured mathematical knowledge on the Internet. On the one hand the one-level client/server model of cvs is generalized to a multi-level graph of client/server relations, and on the other hand the underlying change-detection tools take the math-speci¯c structure of the data into account. ",math
1305,Convex Rank Tests and Semigraphoids,SIAM Journal on Discrete Mathematics," Convex rank tests are partitions of the symmetric group which have desirable geometric properties. The statistical tests defined by such partitions involve counting all permutations in the equivalence classes. Each class consists of the linear extensions of a partially ordered set specified by data. Our methods refine existing rank tests of nonparametric statistics, such as the sign test and the runs test, and are useful for exploratory analysis of ordinal data. We establish a bijection between convex rank tests and probabilistic conditional independence structures known as semigraphoids. The subclass of submodular rank tests is derived from faces of the cone of submodular functions or from Minkowski summands of the permutohedron. We enumerate all small instances of such rank tests. Of particular interest are graphical tests, which correspond to both graphical models and to graph associahedra. ",math
1306,Focusing qualitative simulation using temporal logic: theoretical foundations,Annals of Mathematics and Artificial Intelligence," We illustrate TEQSIM, a qualitative simulator for continuous dynamical systems that combines the expressive power of qualitative differential equations with temporal logic to constrain and refine the resulting predicted behaviors. Temporal logic is used to specify constraints that restrict the simulation to a region of the state space and to specify trajectories for input variables. A propositional linear-time temporal logic is adopted, which is extended to a three valued logic that allows a formula to be conditionally entailed when quantitative information specified in the formula can be applied to a behavior to refine it. We present a formalization of the logic with correctness and completeness results for the adopted model checking algorithm. We show an example of the simulation of a non-autonomous dynamical system and illustrate possible application tasks, ranging from simulation to monitoring and control of continuous dynamical systems, where TEQSIM can be applied. ",math
1307,Smooth convex -norms do not exist,Proceedings of the American Mathematical Society, We show that smooth convex t-norms do not exist. ,math
1308,Factorization of probability measures and absolutely measurable sets,Proceedings of the American Mathematical Society," We find necessary and sufficient conditions for a separable metric space Y to possess the property that for any measurable space (X, A) and probability measure P on X X Y, P can be factored. 1. Introduction. We characterize separable metric spaces Y which have the following property: for any measurable space (X, A) and any probability measure P on the product space (XxY,Ax B(Y)), where S(Z) will denote the Borel c-field of a metric space Z, P can be factored: P - QxT, where Q. is a probability measure on (X, A) and T: X x B(Y) -*[0,1] is an ^-measurable transition function such that ■ for every A G A and B G B(Y). The factorability of P is of obvious interest to the Bayesian statistician. For, imagine that (Y, B(Y)) is the parameter space and that (X, A) is the sample space. A prior on (Y, B(Y)) together with a model (that is, a S(F)-measurable transition function T: Yx A -►[0,1]) determine a probability measure P on (XxY, AxB(Y)). The Bayesian bases his inference on the posterior distribution of the parameter given the sample, so he needs to factor P as above. The main result of the paper is as follows. THEOREM. Let Y be a separable metric space. Then the following conditions on Y are equivalent. (a) Y is absolutely measurable, i.e., if Y is a metric completion of Y and A is a probability measure on the Borel o-field ofY, then Y is X-measurable. (b) For any measurable space (X, A) and any probability measure P on (X x Y,Ax B(Y)), P can be factored. (c) For any Polish space X and any probability measure P on (X xY,B (X) x B(Y)), P can be factored. ",math
1309,Maximally equidistributed combined Tausworthe generators,Mathematics of Computation," Tausworthe random number generators based on a primitive trinomial allow an easy and fast implementation when their parameters obey certain restrictions. However, such generators, with those restrictions, have bad statistical properties unless we combine them. A generator is called maximally equidistributed if its vectors of successive values have the best possible equidistribution in all dimensions. This paper shows how to nd maximally equidistributed combinations in an e cient manner, and gives a list of generators with that property. Such generators have a strong theoretical support and lend themselves to very fast software implementations. ",math
1310,Diffuse Interface Methods for Multiclass Segmentation of High-Dimensional Data,Applied Mathematics Letters,"We present two graph-based algorithms for multiclass segmentation of high-dimensional data, motivated by the binary diffuse interface model. One algorithm generalizes Ginzburg-Landau (GL) functional minimization on graphs to the Gibbs simplex. The other algorithm uses a reduction of GL minimization, based on the Merriman-Bence-Osher scheme for motion by mean curvature. These yield accurate and efficient algorithms for semi-supervised learning. Our algorithms outperform existing methods, including supervised learning approaches, on the benchmark datasets that we used. We refer to Garcia-Cardona (2014) for a more detailed illustration of the methods, as well as different experimental examples. © 2014 Elsevier Ltd. All rights reserved.",math
1311,"FORTRAN codes for estimating the one-norm of a real or complex matrix, with applications to condition estimation",ACM Transactions on Mathematical Software," FORTRAN 77 codes SONEST and CONEST are presented for estimating the l-norm (or the mnorm) of a real or complex matrix, respectively. The codes are of wide applicability in condition estimation since explicit access to the matrix, A, is not required; instead, matrix-vector products Ax and A “'n are computed by the calling program via a reverse communication interface. The algorithms are based on a convex optimization method for estimating the l-norm of a real matrix devised by Hager [Condition estimates. SIAM J. Sci. Stat. Comput. 5 (1984), 311-3161. We derive new results concerning the behavior of Hager's method, extend it to complex matrices, and make several algorithmic modifications in order to improve the reliability and efficiency. ",math
1312,Enhancements of ANALYZE: a computer-assisted analysis system for linear programming,ACM Transactions on Mathematical Software," at Denver This describes enhancements to provide more advanced computer-assisted analysis of instances of hnear programming models. Three categories of enhancements are described: views, engines for obtaining information, and rule-based advising. Examples of their uses include redundancy and infeasibility diagnoses. Categories and Subject gramming; ANALYZE; lation and Modeling]: Descriptors: 1.6.2 [Simulation Model Validation G.1.6 [Numerical and Modeling]: and Analysls Additional Key Words and Phrases: Computer-assisted tems, linear programming, modeling, sensitivity analysis ",math
1313,"Riemannian ^{} center of mass: Existence, uniqueness, and convexity",Proceedings of the American Mathematical Society," Let M be a complete Riemannian manifold and ν a probability measure on M . Assume 1 ≤ p ≤ ∞. We derive a new bound (in terms of p, the injectivity radius of M and an upper bound on the sectional curvatures of M ) on the radius of a ball containing the support of ν which ensures existence and uniqueness of the global Riemannian Lp center of mass with respect to ν. A significant consequence of our result is that under the best available existence and uniqueness conditions for the so-called “local” Lp center of mass, the global and local centers coincide. In our derivation we also give an alternative proof for a uniqueness result by W. S. Kendall. As another contribution, we show that for a discrete probability measure on M , under the existence and uniqueness conditions, the (global) Lp center of mass belongs to the closure of the convex hull of the masses. We also give a refined result when M is of constant curvature. ",math
1314,Smooth function extension based on high dimensional unstructured data,Mathematics of Computation," Many applications, including the image search engine, image inpainting, hyperspectral image dimensionality reduction, pattern recognition, and time series prediction, can be facilitated by considering the given discrete data-set as a point-cloud P in some high dimensional Euclidean space Rs. Then the problem is to extend a desirable objective function f from a certain relatively smaller training subset C ⊂ P to some continuous manifold X ⊂ Rs that contains P, at least approximately. More precisely, when the point cloud P of the given data-set is modeled in the abstract by some unknown compact manifold embedded in the ambient Euclidean space Rs, the extension problem can be considered as the interpolation problem of seeking the objective function on the manifold X that agrees with f on C under certain desirable specifications. For instance, by considering groups of cardinality s of data values as points in a point-cloud in Rs, such groups that are far apart in the original spatial data domain in R1 or R2, but have similar geometric properties, can be arranged to be close neighbors on the manifold. The objective of this paper is to incorporate the consideration of data geometry and spatial approximation, with immediate implications to the various directions of application areas. Our main result is a point-cloud interpolation formula that provides a near-optimal degree of approximation to the target objective function on the unknown manifold. ",math
1315,Accuracy Certificates for Computational Problems with Convex Structure,Mathematics of Operations Research," The goal of the current paper is to introduce the notion of certi¯cates which verify the accuracy of solutions of computational problems with convex structure; such problems include minimizing convex functions, variational inequalities with monotone operators, computing saddle points of convex-concave functions and solving convex Nash equilibrium problems. We demonstrate how the implementation of the Ellipsoid method and other cutting plane algorithms can be augmented with the computation of such certi¯cates without essential increase of the computational e®ort. Further, we show that (computable) certi¯cates exist whenever an algorithm is capable to produce solutions of guaranteed accuracy. ",math
1316,Is computational complexity a barrier to manipulation?,Annals of Mathematics and Artificial Intelligence,"When agents are acting together, they may need a simple mechanism to decide on joint actions. One possibility is to have the agents express their preferences in the form of a ballot and use a voting rule to decide the winning action(s). Unfortunately, agents may try to manipulate such an election by mis-reporting their preferences. Fortunately, it has been shown that it is NP-hard to compute how to manipulate a number of different voting rules. However, NP-hardness only bounds the worst-case complexity. In this survey article, we summarize the evidence for and against computational complexity being a barrier to manipulation. We look both at techniques identified to increase complexity (for example, hybridizing together two or more voting rules), as well as other features that may change the computational complexity of computing a manipulation (for example, if votes are restricted to be single peaked then some of the complexity barriers fall away). We discuss recent theoretical results that consider the average case, as well as simple greedy and approximate methods. We also describe how computational ""phase transitions"", which have been fruitful in identifying hard instances of propositional satisfiability and other NP-hard problems, have provided insight into the hardness of manipulating voting rules in practice. Finally, we consider manipulation of other related problems like stable marriage and tournament problems. © 2011 Springer Science+Business Media B.V.",math
1317,Factoring polynomials over large finite fields,Mathematics of Computation,"This paper reviews some of the known algorithms for factoring polynomials over finite fields and presents a new deterministic procedure for reducing the problem of factoring an arbitrary polynomial over the Galois field GF(p""!) to the problem of finding the roots in GF(p) of certain other polynomials over GF(p). The amount of computation and the storage space required by these algorithms are algebraic in both the degree of the polynomial to be factored and the logarithm of the order of the finite field. ÂŠ 1970 American Mathematical Society.",math
1318,Distance semantics for database repair,Annals of Mathematics and Artificial Intelligence," In many scenarios, a database instance violates a given set of integrity constraints. In such cases, it is often required to repair the database, that is, to restore its consistency. A primary motif behind the repairing approaches is the principle of minimal change, which is the aspiration to keep the recovered data as faithful as possible to the original (inconsistent) database. In this paper, we represent this qualitative principle quantitatively, in terms of distance functions and some underlying metrics, and so introduce a general framework for repairing inconsistent databases by distance-based considerations. The uniform way of representing repairs and their semantics clarifies the essence behind several approaches to consistency restoration in database systems, helps to compare the underlying formalisms, and relates them to existing methods of defining belief revision operators, merging data sets, and integrating information systems. ",math
1319,Topological Uniqueness of the Nash Equilibrium for Selfish Routing with Atomic Users,Mathematics of Operations Research," We consider the problem of selfish routing in a congested network shared by several users, where each user wishes to minimize the cost of its own flow. Users are atomic, in the sense that each has a nonnegligible amount of flow demand, and flows may be split over different routes. The total cost for each user is the sum of its link costs, which, in turn, may depend on the user's own flow as well as the total flow on that link. Our main interest here is network topologies that ensure uniqueness of the Nash equilibrium for any set of users and link cost functions that satisfy some mild convexity conditions. We characterize the class of two-terminal network topologies for which this uniqueness property holds, and show that it coincides with the class of nearly parallel networks that was recently shown by Milchtaich [Milchtaich, I. 2005. Topological conditions for uniqueness of equilibrium in networks. Math. Oper. Res. 30 225-244] to ensure uniqueness in nonatomic (or Wardrop) routing games. We further show that uniqueness of the link flows holds under somewhat weaker convexity conditions, which apply to the mixed Nash-Wardrop equilibrium problem. We finally propose a generalized continuum-game formulation of the routing problem that allows for a unified treatment of atomic and nonatomic users. ",math
1320,Learning parallel portfolios of algorithms,Annals of Mathematics and Artificial Intelligence," A wide range of combinatorial optimization algorithms have been developed for complex reasoning tasks. Frequently, no single algorithm outperforms all the others. This has raised interest in leveraging the performance of a collection of algorithms to improve performance. We show how to accomplish this using a Parallel Portfolio of Algorithms (PPA). A PPA is a collection of diverse algorithms for solving a single problem, all running concurrently on a single processor until a solution is produced. The performance of the portfolio may be controlled by assigning different shares of processor time to each algorithm. We present an effective method for finding a PPA in which the share of processor time allocated to each algorithm is fixed. Finding the optimal static schedule is shown to be an NP-complete problem for a general class of utility functions. We present bounds on the performance of the PPA over random instances and evaluate the performance empirically on a collection of 23 state-of-the-art SAT algorithms. The results show significant performance gains over the fastest individual algorithm in the collection. ",math
1321,Fluid Limits for Processor-Sharing Queues with Impatience,Mathematics of Operations Research," We investigate a processor sharing queue with renewal arrivals and generally distributed service times. Impatient jobs may abandon the queue, or renege, before completing service. The random time representing a job's patience has a general distribution and may be dependent on its initial service time requirement. A scaling procedure that gives rise to a fluid model with nontrivial yet tractable steady state behavior is presented. This fluid model model captures many essential features of the underlying stochastic model, and it is used to analyze the impact of impatience in processor sharing queues. ",math
1322,On the distribution of a scaled condition number,Mathematics of Computation,"In this note, we give the exact distribution of a scaled condition number used by Demmel to model the probability that matrix inversion is difficult. Specifically, consider a random matrix A and the scaled condition number KD(A) = AF · A−l. Demmel provided bounds for the condition number distribution when A has real or complex normally distributed elements. Here, we give the exact formula. © 1992 American Mathematical Society.",math
1323,Some estimates for a weighted ² projection,Mathematics of Computation,"This paper is devoted to the error estimates for some weighted L projections. Nearly optimal estimates are obtained. These estimates can be applied to the analysis of the usual multigrid method, multilevel preconditioner and domain decomposition method for solving elliptic boundary problems whose coefficients have large jump discontinuities. ÂŠ 1991 American Mathematical Society.",math
1324,Multiple-Instance Learning of Real-Valued Geometric Patterns,Annals of Mathematics and Artificial Intelligence," Recently, there has been a signi cant amount of research studying the multipleinstance learning model, yet all of this work has only considered this model when there are boolean labels. However, in many of the application areas for which the multiple-instance model ts, real-valued labels are more appropriate than boolean labels. In this paper we de ne and study a real-valued multiple-instance model in which each multiple-instance example is given a real-valued classi cation in [0; 1]. The real-valued classi cation indicates the degree to which the example satis es the target concept. To provide additional structure to the resulting learning problem, we associate a real-valued label with each point in the multiple-instance example. These values are then combined using a real-valued aggregation operator to obtain the classi cation for the example. Motivated by the possible application of learning geometric patterns to problems in pattern recognition and scene classi cation (with applications to content-based image retrieval), we provide on-line agnostic algorithms for learning real-valued multiple-instance geometric concepts de ned by axis-aligned boxes in constant dimensional space. We obtain our learning algorithm by reducing the problem to one in which the exponentiated gradient (or gradient descent) algorithm can be used. We also give a novel application of the virtual weights technique. In typical applications of the virtual weights technique, all of the concepts in a group have the same weight and prediction which allows a single \representative"" concept from each group to be tracked. However, in our application there are an exponential number of di erent weights (and possible predictions). Hence, boxes in each group have di erent weights and predictions making the computation of the contribution of a group signi cantly more involved. However, we are able to both keep the number of groups polynomial in the number of trials and e ciently compute the overall prediction. ",math
1325,Flows and Decompositions of Games: Harmonic and Potential Games,Mathematics of Operations Research," In this paper we introduce a novel ow representation for nite games in strategic form. This representation allows us to develop a canonical direct sum decomposition of an arbitrary game into three components, which we refer to as the potential, harmonic and nonstrategic components. We analyze natural classes of games that are induced by this decomposition, and in particular, focus on games with no harmonic component and games with no potential component. We show that the rst class corresponds to the well-known potential games. We refer to the second class of games as harmonic games, and study the structural and equilibrium properties of this new class of games. Intuitively, the potential component of a game captures interactions that can equivalently be represented as a common interest game, while the harmonic part represents the con icts between the interests of the players. We make this intuition precise, by studying the properties of these two classes, and show that indeed they have quite distinct and remarkable characteristics. For instance, while nite potential games always have pure Nash equilibria, harmonic games generically never do. Moreover, we show that the nonstrategic component does not a ect the equilibria of a game, but plays a fundamental role in their e ciency properties, thus decoupling the location of equilibria and their payo -related properties. Exploiting the properties of the decomposition framework, we obtain explicit expressions for the projections of games onto the subspaces of potential and harmonic games. This enables an extension of the properties of potential and harmonic games to \nearby"" games. We exemplify this point by showing that the set of approximate equilibria of an arbitrary game can be characterized through the equilibria of its projection onto the set of potential games. ",math
1326,Conformance Analysis of ASML’s Test Process,Mathematics of Computation," Process mining allows for the automated discovery of process models from event logs. These models provide insights and enable various types of model-based analysis. However, in many situations already some normative process model is given, and the goal is not to discover a model, but to check its conformance. The process mining framework ProM provides a conformance checker able to investigate and quantify deviations between the real process (as recorded in the event log) and the modeled process. The conformance checker is one of the few tools available today that is able support regulatory compliance, i.e., ensuring that organizations and people take steps to comply with relevant laws, regulations, and procedures. In this paper, we report on a case study where the ProM framework has been applied to the test processes of ASML (the leading manufacturer of wafer scanners in the world). In this case study, we focus on the conformance aspect and compare the test process as it is really executed to the idealized reference model that ASML is using to instruct their test teams. This revealed that the real process is much more complicated than the idealized reference process. Moreover, we were able to suggest concrete improvement actions for the test process at ASML. ",math
1327,Unifying Math Ontologies: A Tale of Two Standards,Mathematical Knowledge Management,"One of the fundamental and seemingly simple aims of mathematical knowledge management (MKM) is to develop and standardize formats that allow to ""represent the meaning of the objects of mathematics"". The open formats OpenMath and MathML address this, but differ subtly in syntax, rigor, and structural viewpoints (notably over calculus). To avoid fragmentation and smooth out interoperability obstacles, effort is under way to align them into a joint format OpenMath/MathML 3. We illustrate the issues that come up in such an alignment by looking at three main areas: bound variables and conditions, calculus (which relates to the previous) and ""lifted"" n-ary operators. © 2009 Springer-Verlag Berlin Heidelberg.",math
1328,Algorithm 864: General and robot-packable variants of the three-dimensional bin packing problem,ACM Transactions on Mathematical Software,We consider the problem of orthogonally packing a given set of rectangular-shaped boxes into the minimum number of three-dimensional rectangular bins. The problem is NP-hard in the strong sense and extremely difficult to solve in practice. We characterize relevant subclasses of packing and present an algorithm which is able to solve moderately large instances to optimality. Extensive computational experiments compare the algorithm for the three-dimensional bin packing when solving general orthogonal packings and when restricted to robot packings. © 2007 ACM.,math
1329,On the conjecture of Birch and Swinnerton-Dyer for an elliptic curve of rank 3,Mathematics of Computation,"The elliptic curve y2 = 4x - 28x + 25 has rank 3 over Q. Assuming the Weil-Taniyama conjecture for this curve, we show that its-series L(s) has a triple zero at * = 1 and compute limy_] L(s)/(s - I)3 to 28 decimal places; its value agrees with the product of the regulator and real period, in accordance with the Birch-Swinnerton-Dyer conjecture if III is trivial. ÂŠ 1985 American Mathematical Society.",math
1330,A remarkable class of continued fractions,Proceedings of the American Mathematical Society," For any irrational number a and integer a > 1, the continued fraction of (a- l)2^il/a'""""' is computed explicitly in terms of the continued fraction of a. 1. Introduction. In [2], the second author proved the remarkable result that 2""=,l/2lrai where a = (1 + vT)/2 has as its continued fraction [0, 1, t2, r3,. .. ] where tn = 2f""~2in > 2) and/„ is the «th Fibonacci number. In this paper we generalize this result. ",math
1331,"Intermediate inequality: concepts, indices, and welfare implications",Mathematical Social Sciences,"A new concept of inequality-equivalence, called intermediate, is presented. By means of a parameter ν, it allows one to vary the value judgement on inequality between the well-known relative and absolute views. It is shown that the intermediate concept - unlike the earlier suggested centrist concept - indeed lies between these two concepts (in a certain sense). Furthermore, an ethical justification for the use of intermediate inequality measures is provided. © 1990.",math
1332,Rigorous sensitivity analysis for systems of linear and nonlinear equations,Mathematics of Computation," Methods are presented for performing a rigorous sensitivity analysis of numerical problems with independent, noncorrelated data for general systems of linear and nonlinear equations. The methods may serve for the following two purposes. First, to bound the dependency of the solution on changes in the input data. In contrast to condition numbers a componentwise sensitivity analysis of the solution vector is performed. Second, to estimate the true solution set for problems the input data of which are a²icted with tolerances. The methods presented are very e®ective with the additional property that, due to an automatic error control mechanism, every computed result is guaranteed to be correct. Examples are given for linear systems demonstrating that the computed bounds are in general very sharp. Interesting comparisons to traditional condition numbers are given. ",math
1333,Algorithm 782: codes for rank-revealing QR factorizations of dense matrices,ACM Transactions on Mathematical Software," This article describes a suite of codes as well as associated testing and timing drivers for computing rank-revealing QR (RRQR) factorizations of dense matrices. The main contribution is an efficient block algorithm for approximating an RRQR factorization, employing a windowed version of the commonly used Golub pivoting strategy and improved versions of the RRQR algorithms for triangular matrices originally suggested by Chandrasekaran and Ipsen and by Pan and Tang, respectively. We highlight usage and features of these codes. Categories and Subject Descriptors: D.3.2 [Programming Languages]: Language Classifications-Fortran 77; G.1.3 [Numerical Analysis]: Numerical Linear Algebra; G.4 [Mathematics of Computing]: Mathematical Software ",math
1334,Some inequalities for elementary mean values,Mathematics of Computation,"Upper and lower bounds for the difference between the arithmetic and harmonic means of n positive numbers are obtained in terms of n and the largest and smallest of the numbers. Also, results of S. H. Tung 2, are used to obtain upper and lower bounds for the elementary mean values Mp of Hardy. Littlewood, and Polya. ÂŠ 1984 American Mathematical Society.",math
1335,Rational extrapolation for the PageRank vector,Mathematics of Computation," An important problem in web search is to determine the importance of each page. From the mathematical point of view, this problem consists in finding the nonnegative left eigenvector of a matrix corresponding to its dominant eigenvalue 1. Since this matrix is neither stochastic nor irreducible, the power method has convergence problems. So, the matrix is replaced by a convex combination, depending on a parameter c, with a rank one matrix. Its left principal eigenvector now depends on c, and it is the PageRank vector we are looking for. However, when c is close to 1, the problem is ill-conditioned, and the power method converges slowly. So, the idea developed in this paper consists in computing the PageRank vector for several values of c, and then to extrapolate them, by a conveniently chosen rational function, at a point near 1. The choice of this extrapolating function is based on the mathematical expression of the PageRank vector as a function of c. Numerical experiments end the paper. ",math
1336,Manipulation of optimal matchings via predonation of endowment,Mathematical Social Sciences," In this paper we answer a question posed by Sertel and O¨ zkal-Sanver (2002) on the manipulability of optimal matching rules in matching problems with endowments. We characterize the classes of consumption rules under which optimal matching rules can be manipulated via predonation of endowment. In a recent paper, Sertel and O¨ zkal-Sanver (2002), hereafter S & O¨ -S, explained that in two-sided matching models “... the consumption possibilities of an agent may depend on the respective endowments of the pair in which the agent ends up under a matching” (S & ∗We thank Bettina Klaus, Jordi Mass´o, and I˙pek O¨zkal-Sanver for valuable comments and conversations. G. Fiestras-Janeiro received financial support from the Spanish Ministerio de Ciencia y Tecnolog´ıa and the FEDER through projects PB98-0613-C02-02 and BEC2002-04102-C02-02, and from the Xunta de Galicia through grant PGIDT00PXI20703PN. The work of F. Klijn is partially supported by Research Grant BEC2002-02130 from the Spanish Ministerio de Ciencia y Tecnolog´ıa and by a Marie Curie Fellowship of the European Community programme “Improving Human Research Potential and the Socioeconomic Knowledge Base” under contract number HPMF-CT-2001-01232. The work of E. S´anchez is supported by project BEC2002-04102-C02-02 from the Spanish Ministerio de Ciencia y Tecnolog´ıa and the FEDER. 1Depart. de Estat´ıstica e Investigaci´on Operativa, Universidade de Vigo, Spain; fiestras@uvigo.es 2Corresponding author. CODE and Departament d'Economia i d'Hist`oria Econ`omica, Universitat Aut`onoma de Barcelona, Edifici B, 08193 Bellaterra, Spain. Tel. (34) 93 581 1720; Fax. (34) 93 581 2012; fklijn@pareto.uab.es 3Depart. de Estat´ıstica e Investigaci´on Operativa, Universidade de Vigo, Spain; esanchez@uvigo.es ",math
1337,Spanning Trees with Many Leaves,SIAM Journal on Discrete Mathematics," We prove, that every connected graph with s vertices of degree 3 and t vertices of degree at least 4 has a spanning tree with at least 52 t + 15 s + leaves, where 58 . Moreover, 2 for all graphs besides three exclusions. All exclusion are regular graphs of degree 4, they are explicitly described in the paper. We present in nite series of graphs, containing only vertices of degrees 3 and 4, for which the maximal number of leaves in a spanning tree is equal for 52 t + 15 s + 2. Therefore we prove that our bound is tight. We consider unoriented graphs without loops and multiple edges. We use standart notations. For a graph G we denote the set of its vertices by V (G) and the set of its edges by E(G). We use notations v(G) and e(G) for the number of vertices and edges of G, respectively. We denote the degree of a vertex x in the graph G by dG(x). For any set of vertices W V (G) we denote by dG;W (x) the number of vertices of the set W , which are adjacent to x in the graph G. As usual, we denote the minimal vertex degree of the graph G by (G). Let NG(x) denote the neighborhood of a vertex w 2 V (G) (i.e. the set of all vertices, adjacent to w). For any edge e 2 E(G) we denote by G e the graph, in which the ends of the edge e = xy are contracted into one vertex, which is incident to all ",math
1338,Theory of Aces: Fame by chance or merit?,Journal of Mathematical Sociology," We study empirically how fame of WWI fighter-pilot aces, measured in numbers of web pages mentioning them, is related to their achievement or merit, measured in numbers of opponent aircraft destroyed. We find that on the average fame grows exponentially with achievement; to be precise, there is a strong correlation (~0.7) between achievement and the logarithm of fame. At the same time, the number of individuals achieving a particular level of merit decreases exponentially with the magnitude of the level, leading to a power-law distribution of fame. A stochastic model that can explain the exponential growth of fame with merit is also proposed. ",math
1339,Explicit formula for scalar non‐linear conservation laws with boundary condition,Mathematical Methods in The Applied Sciences,"We prove an uniqueness and existence theorem for the entropy weak solution of non-linear hyperbolic conservation laws of the form , with initial data and boundary condition. The scalar function u = u(x, t), x > 0, t > 0, is the unknown; the function f = f(u) is assumed to be strictly convex. We also study the weighted Burgers' equation: α ϵ ℝ . We give an explicit formula, which generalizes a result of Lax. In particular, a free boundary problem for the flux f(u(.,.)) at the boundary is solved by introducing a variational inequality. The uniqueness result is obtained by extending a semigroup property due to Keyfitz.",math
1340,Static and dynamic structural symmetry breaking,Annals of Mathematics and Artificial Intelligence," We reconsider the idea of structural symmetry breaking (SSB) for constraint satisfaction problems (CSPs). We show that the dynamic dominance checks used in symmetry breaking by dominance-detection search for CSPs with piecewise variable and value symmetries have a static counterpart: there exists a set of constraints that can be posted at the root node and that breaks all these symmetries. The amount of these symmetry-breaking constraints is linear in the size of the problem, but they possibly remove a super-exponential number of symmetries on both values and variables. Moreover, static and dynamic structural symmetry breaking coincide for static variable and value orderings. ",math
1341,Games of strategy : theory and applications,Mathematics of Computation, from www.rand.org as a public service of Limited Electronic Distribution Rights ,math
1342,Selection of Perturbation Experiments for Model Discrimination,Annals of Mathematics and Artificial Intelligence," It often occurs that a system can be described by several competing models. In order to distinguish among the alternative models, further information about the behavior of the system is required. One way to obtain such information is to perform suitably chosen perturbation experiments. We introduce a method for the selection of optimal perturbation experiments for discrimination among a set of dynamical models. The models are assumed to have the form of semi-quantitative differential equations. The method employs an optimization criterion based on the entropy measure of information. ",math
1343,A survey on the complexity of tournament solutions,Mathematical Social Sciences,"In voting theory, the result of a paired comparison method such as the one suggested by Condorcet can be represented by a tournament, i.e., a complete asymmetric directed graph. When there is no Condorcet winner, i.e., a candidate preferred to any other candidate by a majority of voters, it is not always easy to decide who is the winner of the election. Different methods, called tournament solutions, have been proposed for defining the winners. They differ in their properties and usually lead to different winners. Among these properties, we consider in this survey the algorithmic complexity of the most usual tournament solutions: some are polynomial, some are NP-hard, while the complexity status of others remains unknown. © 2009 Elsevier B.V. All rights reserved.",math
1344,Finite element approximation of the Cahn-Hilliard equation with concentration dependent mobility,Mathematics of Computation," We consider the Cahn-Hilliard equation with a logarithmic free energy and non-degenerate concentration dependent mobility. In particular we prove that there exists a unique solution for su ciently smooth initial data. Further, we prove an error bound for a fully practical piecewise linear nite element approximation in one and two space dimensions. Finally some numerical experiments are presented. ",math
1345,Combinatorial Optimization with Rational Objective Functions,Mathematics of Operations Research," Let A be the problem of minimizing c l x l + . . . + c,x, subject to certain constraints on x = ( x , , . . . , x,), and let B be the problem of minimizing ( a , + a , x l + . . . + o,,x,)/(b, + b , x , + . . . + b,x,) subject to the same constraints, assuming the denominator is always positive. It is shown that if A is solvable within O [ p ( n ) ]comparisons and O [ q ( n ) ]additions, then B is solvable in time O [ p ( n ) ( q ( n )+ p ( n ) ) ] . This applies to most of the ""network"" algorithms. Consequently, minimum ratio cycles, minimum ratio spanning trees, minimum ratio (simple) paths, maximum ratio weighted matchings, etc., can be computed withing -pol-ynomial-time in the number of variables. This improves a result of E. L. Lawler, namely, that a minimum ratio cycle can be computed within a time bound which is polynomial in the number of bits required to specify an instance of the problem. A recent result on minimum ratio spanning trees by R. Chandrasekaran is also improved by the general arguments presented in this paper. Algorithms of time-complexity O(IEJ .IvJ2l.og1 VJ) for a minimum ratio cycle and O(IE1. log21VJ.log IoglVI) for a minimum ratio spanning tree are developed. ",math
1346,Partial equilibrium logic,Annals of Mathematics and Artificial Intelligence,"Partial equilibrium logic (PEL) is a new nonmonotonic reasoning formalism closely aligned with logic programming under well-founded and partial stable model semantics. In particular it provides a logical foundation for these semantics as well as an extension of the basic syntax of logic programs. In this paper we describe PEL, study some of its logical properties and examine its behaviour on disjunctive and nested logic programs. In addition we consider computational features of PEL and study different approaches to its computation. © 2007 Springer Science+Business Media B.V.",math
1347,The Parisi Formula,Annals of Mathematics," Using Guerra's interpolation scheme, we compute the free energy of the Sherrington-Kirkpatrick model for spin glasses at any temperature, confirming a celebrated prediction of G. Parisi. The Hamiltonian of the Sherrington-Kirkpatrick (SK) model for spin glasses [10] is given at inverse temperature β by ",math
1348,A 2-Approximation Algorithm for Stochastic Inventory Control Models with Lost Sales,Mathematics of Operations Research," In this paper, we describe the ¯rst computationally e±cient policies for stochastic inventory models with lost sales and replenishment lead times that admit worst-case performance guarantees. In particular, we introduce dual-balancing policies for lost-sales models that are conceptually similar to dualbalancing policies recently introduced for a broad class of inventory models in which demand is backlogged rather than lost. That is, in each period, we balance two opposing costs: the expected marginal holding costs against the expected marginal lost-sales cost. Speci¯cally, we show that the dual-balancing policies for the lost-sales models provide a worst-case performance guarantee of 2 under relatively general demand structures. In particular, the guarantee holds for independent (not necessarily identically distributed) demands and for models with correlated demands such as the AR(1) model and the multiplicative auto-regressive demand model. The policies and the worst-case guarantee extend to models with capacity constraints on the size of the order and stochastic lead times. Our analysis has several novel elements beyond the balancing ideas for backorder models. OR/MS subject classi¯cation: Primary: inventory/production , approximation/heuristics ; Secondary: production/scheduling , approximation/heuristics ",math
1349,Nash networks with heterogeneous links,Mathematical Social Sciences," A non-cooperative model of network formation is developed. Link formation is one-sided. Information flow is two-way. The model builds on the work of Bala and Goyal who permit links to fail with a certain common probability. In our model the probability of failure can be different for different links. The set of networks which are Nash for suitably chosen model parameters consists of all essential networks. We specifically investigate Nash networks that are connected, superconnected, or stars. Efficiency, Pareto-optimality, and existence issues are discussed through examples. Three alternative model specifications are explored to address potential shortcomings. D 2005 Elsevier B.V. All rights reserved. ",math
1350,Rational Chebyshev approximations for the error function,Mathematics of Computation," This note presents nearly-best rational approximations ftir the functions erf (i) and erfc (x), with maximal relative errors ranging down to between 6 X 10-19 and 3 X 10-20. In [1] Hart, et al., present rational approximations for the function ",math
1351,A Unifying Version-Space Representation,Annals of Mathematics and Artificial Intelligence," In this paper we consider the open problem how to unify version-space representations. We present a first solution to this problem, namely a new version-space representation called adaptable boundary sets (ABSs). We show that a version space can have a space of ABSs representations. We demonstrate that this space includes the boundary-set representation and the instance-based boundary-set representation; i.e., the ABSs unify these two representations. We consider the task of learning ABSs as a task of identifying a proper representation within the space of ABSs depending on the applicability requirements given. This is demonstrated in a series of examples where ABSs are used to overcome the complexity problem of the boundary sets. ",math
1352,Counting Stars and Other Small Subgraphs in Sublinear-Time,SIAM Journal on Discrete Mathematics," Detecting and counting the number of copies of certain subgraphs (also known as network motifs or graphlets), is motivated by applications in a variety of areas ranging from Biology to the study of the World-Wide-Web. Several polynomial-time algorithms have been suggested for counting or detecting the number of occurrences of certain network motifs. However, a need for more e cient algorithms arises when the input graph is very large, as is indeed the case in many applications of motif counting. In this paper we design sublinear-time algorithms for approximating the number of copies of certain constant-size subgraphs in a graph G. That is, our algorithms do not read the whole graph, but rather query parts of the graph. Speci cally, we consider algorithms that may query the degree of any vertex of their choice and may ask for any neighbor of any vertex of their choice. The main focus of this work is on the basic problem of counting the number of length-2 paths and more generally on counting the number of stars of a certain size. Speci cally, we design an algorithm that, given an approximation parameter 0 < < 1 and query access to a graph G, outputs an estimate ^s such that with high constant probability, (1 ) s(G) ^s (1+ ) s(G), where s(G) denotes the number of stars of size s + 1 in the graph. The expected query complexity and running time of the to the study of the World-Wide-Web (see e.g., [28, 24, basic quest to understand simple structural properties of graphs. ",math
1353,On some inequalities for the gamma and psi functions,Mathematics of Computation," We present new inequalities for the gamma and psi functions, and we provide new classes of completely monotonic, star-shaped, and superadditive functions which are related to Γ and . e−ttx−1 dt Received by the editor October 13, 1995 and, in revised form, March 4, 1996. 1991 Mathematics Subject Classi cation. Primary 33B15; Secondary 26D07. ",math
1354,Basic properties of SLE,Annals of Mathematics," SLEκ is a random growth process based on Loewner's equation with driving parameter a one-dimensional Brownian motion running with speed κ. This process is intimately connected with scaling limits of percolation clusters and with the outer boundary of Brownian motion, and is conjectured to correspond to scaling limits of several other discrete processes in two dimensions. The present paper attempts a first systematic study of SLE. It is proved that for all κ = 8 the SLE trace is a path; for κ ∈ [0, 4] it is a simple path; for κ ∈ (4, 8) it is a self-intersecting path; and for κ > 8 it is space-filling. It is also shown that the Hausdorff dimension of the SLEκ trace is almost surely (a.s.) at most 1 + κ/8 and that the expected number of disks of size ε needed to cover it inside a bounded set is at least ε−(1+κ/8)+o(1) for κ ∈ [0, 8) along some sequence ε 0. Similarly, for κ ≥ 4, the Hausdorff dimension of the outer boundary of the SLEκ hull is a.s. at most 1 + 2/κ, and the expected number of disks of radius ε needed to cover it is at least ε−(1+2/κ)+o(1) for a sequence ε 0. ",math
1355,Minimizing a Convex Cost Closure Set,SIAM Journal on Discrete Mathematics," Many applications in the area of production and statistical estimation are problems of convex optimization subject to ranking constraints that represent a given partial order. This problem - which we call the convex cost closure problem, or (CCC) - is a generalization of the known maximum (or minimum) closure problem and the isotonic regression problem. For a (CCC) problem on n variables and m constraints we describe an algorithm that has the complexity of the minimum cut problem plus the complexity of finding the minima of up to n convex functions. Since the convex cost closure problem is a generalization of both minimum cut and of minimization of n convex functions this complexity is fastest possible. For the quadratic problem the complexity of our 2 algorithm is strongly polynomial, O(mn log nm ). For the isotonic regression problem the complexity is O(n log U ), for U the largest range for a variable value. ",math
1356,Structural properties for two classes of combined random number generators,Mathematics of Computation,"We analyze a class of combined random number generators recently proposed by LEcuyer, which combines a set of linear congruential generators (LCG’s) with distinct prime moduli. We show that the geometrical behavior of the vectors of points produced by the combined generator can be approximated by the lattice structure of an associated LCG, whose modulus is the product of the moduli of the individual components. The approximation is good if these individual moduli are near each other and if the dimension of the vectors is large enough. The associated LCG is also exactly equivalent to a slightly different combined generator of the form suggested by Wichmann and Hill. We give illustrations, for which we examine the approximation error and assess the quality of the lattice structure of the associated LCG. © 1991 American Mathematical Society.",math
1357,Period of the power generator and small values of Carmichael's function,Mathematics of Computation,"Consider the pseudorandom number generator un ≡ ue n-1 (mod m), 0 ≤ un ≤ m - 1, n = 1, 2, . . . , where we are given the modulus m, the initial value u0 = θ and the exponent e. One case of particular interest is when the modulus m is of the form pl, where p, l are different primes of the same magnitude. It is known from work of the first and third authors that for moduli m = pl, if the period of the sequence (un) exceeds m3/4+ε, then the sequence is uniformly distributed. We show rigorously that for almost all choices of p, l it is the case that for almost all choices of θ, e, the period of the power generator exceeds (pl)1-ε. And so, in this case, the power generator is uniformly distributed. We also give some other cryptographic applications, namely, to ruling-out the cycling attack on the RSA cryptosystem and to so-called time-release crypto. The principal tool is an estimate related to the Carmichael function λ(m), the size of the largest cyclic subgroup of the multiplicative group of residues modulo m. In particular, we show that for any Δ ≥ (log log N)3, we have λ(m) ≥ N exp(-Δ) for all integers m with 1 ≤ m ≤ N, apart from at most N exp (-0.69 (Δ log Δ)1/3) exceptions.",math
1358,Inefficiency of Nash equilibria,Mathematics of Operations Research," curL'.', FOlJNuATIN: FOR ~ESEARCH IN ECONOMICS !NTTFICIENCY OF NASH EQUILIBRIA: ! ",math
1359,Subspaces of small codimension of finite-dimensional Banach spaces,Proceedings of the American Mathematical Society,"Given a finite-dimensional Banach space E and a Euclidean norm on E, we study relations between the norm and the Euclidean norm on subspaces of E of small codimension. Then for an operator taking values ina Hubert space, we deduce an inequality for entropy numbers of the operator and its dual. © 1986 American Mathematical Society.",math
1360,The Lanczos algorithm with selective orthogonalization,Mathematics of Computation,"‘The simple Lanczos process is very effective for finding a few extreme eigenvalues of a large symmetric matrix along with the associated eigenvectors. Unfortunately, the process computes redundant copies of the outermost eigenvectors and has to be used with some skill. In this paper it is shown how a modification called selective orthogonalization stifles the formation of duplicate eigenvectors without increasing the cost of a Lanczos step significantly. The degree of linear independence among the Lanczos vectors is controlled without the costly process of reorthogonalization. © 1979 American Mathematical Society.",math
1361,Examples of genus two CM curves defined over the rationals,Mathematics of Computation," We present the results of a systematic numerical search for genus two curves de ned over the rationals such that their Jacobians are simple and have endomorphism ring equal to the ring of integers of a quartic CM eld. Including the well-known example y2 = x5 − 1 we nd 19 non-isomorphic such curves. We believe that these are the only such curves. Received by the editor June 13, 1996. 1991 Mathematics Subject Classi cation. Primary 14-04; Secondary 14K22. This work was partially supported by grant LEQSF(1995-97)-RD-A-09 from the Louisiana Educational Quality Support Fund. ",math
1362,Bargaining over multiple issues in finite horizon alternating-offers protocol,Annals of Mathematics and Artificial Intelligence," In this paper we study multi issue alternating-offers bargaining in a perfect information finite horizon setting, we determine the pertinent subgame perfect equilibrium, and we provide an algorithm to compute it. The equilibrium is determined by making a novel use of backward induction together with convex programming techniques in multi issue settings. We show that the agents reach an agreement immediately and that such an agreement is Pareto efficient. Furthermore, we prove that, when the multi issue utility functions are linear, the problem of computing the equilibrium is tractable and the related complexity is polynomial with the number of issues and linear with the deadline of bargaining. ",math
1363,Generic Uniqueness of Equilibrium in Large Crowding Games,Mathematics of Operations Research," A crowding game is a noncooperative game in which the payo¤ of each player depends only on the players action and the size of the set of players choosing that particular action: The larger the set, the smaller the payo¤. Finite, n-player crowding games often have multiple equilibria. However, a large crowding game generically has just one equilibrium, and the equilibrium payo¤s in such a game are always unique. Moreover, the sets of equilibria of the m-replicas of a nite crowding game generically converge to a singleton as m tends to innity. This singleton consists of the unique equilibrium of the limit large crowding game. This equilibrium generically has the following graph-theoretic property: The bipartite graph, in which each player in the original, nite crowding game is joined with all best-response actions for (copies of) that player, does not contain cycles. ",math
1364,An Inexact Hybrid Generalized Proximal Point Algorithm and Some New Results on the Theory of Bregman Functions,Mathematics of Operations Research," We present a new Bregman-function-based algorithm which is a modification of the generalized proximal point method for solving the variational inequality problem with a maximal monotone operator. The principal advantage of the presented algorithm is that it allows a more constructive error tolerance criterion in solving the proximal point subproblems. Furthermore, we eliminate the assumption of pseudomonotonicity which was, until now, standard in proving convergence for paramonotone operators. Thus we obtain a convergence result which is new even for exact generalized proximal point methods. Finally, we present some new results on the theory of Bregman functions. For example, we show that the standard assumption of convergence consistency is a consequence of the other properties of Bregman functions, and is therefore superfluous. ∗ Research of the first author is supported by CNPq Grant 300734/95-6 and by PRONEX-Optimization, research of the second author is supported by CNPq Grant 301200/93-9(RN) and by PRONEX-Optimization. † Instituto de Matem´atica Pura e Aplicada, Estrada Dona Castorina 110, Jardim Botˆanico, Rio de Janeiro, RJ 22460-320, Brazil. Email : solodov@impa.br and benar@impa.br . ",math
1365,On the feasibility of applying capture–recapture experiments for web evolution estimations,Applied Mathematics Letters,"This paper proposes a novel application for estimating the size and evolution of web page populations, based on the capturerecapture methodology, which is mainly used in wildlife biological studies. Firstly, we present the necessary modifications and amendments needed for the web capturerecapture paradigm, and then we discuss the limitations confronted. The paper provides the implementation details of the proposed web capture recapture model along with its initial assessment. The anticipated outcome was to examine whether we can conduct capturerecapture experiments on the web (or a web sub-universe such as an Internet search engine directory), in order to further estimate evolution rates in web page populations. ÂŠ 2011 Elsevier Ltd. All rights reserved.",math
1366,Combinatorics of random processes and sections of convex bodies,Annals of Mathematics," We find a sharp combinatorial bound for the metric entropy of sets in Rn and general classes of functions. This solves two basic combinatorial conjectures on the empirical processes. 1. A class of functions satisfies the uniform Central Limit Theorem if the square root of its combinatorial dimension is integrable. 2. The uniform entropy is equivalent to the combinatorial dimension under minimal regularity. Our method also constructs a nicely bounded coordinate section of a symmetric convex body in Rn. In the operator theory, this essentially proves for all normed spaces the restricted invertibility principle of Bourgain and Tzafriri. ",math
1367,The complexity of embedded axiomatization for a class of closed database views,Annals of Mathematics and Artificial Intelligence,"It is well known that the complexity of testing the correctness of an arbitrary update to a database view can be far greater than the complexity of testing a corresponding update to the main schema. However, views are generally managed according to some protocol which limits the admissible updates to a subset of all possible changes. The question thus arises as to whether there is a more tractable relationship between these two complexities in the presence of such a protocol. In this paper, this question is addressed for closed update strategies, which are based upon the constant-complement approach of Bancilhon and Spyratos. The approach is to address a more general question - that of characterizing the complexity of axiomatization of views, relative to the complexity of axiomatization of the main schema. For schemata constrained by denial or consistency constraints, that is, statements which rule out certain situations, such as the equality-generating dependencies (EGDs) or, more specifically, the functional dependencies (FDs) of the relational model, a broad and comprehensive result is obtained in a very general framework which is not tied to the relational model in any way. It states that every such schema is governed by an equivalent set of constraints which embed into the component views, and which are no more complex than the original set. For schemata constrained by generating dependencies, of which tuple-generating dependencies (TGDs) in general and, more specifically, both join dependencies (JDs) and inclusion dependencies (INDs) are examples within the relational model, a similar result is obtained, but only within a context known as meet-uniform decompositions, which fails to recapture some important situations. To address the all-important case of relational schemata constrained by both FDs and INDs, a hybrid approach is also developed, in which the general theory regarding denial constraints is blended with a focused analysis of a special but very practical subset of the INDs known as fanout-free unary inclusion dependencies (fanout-free UINDs), to obtain results parallel to the above-mentioned cases: every such schema is governed by an equivalent set of constraints which embed into the component views, and which are no more complex than the original set. In all cases, the question of view update complexity is then answered via a corollary to this main result. © Springer 2006.",math
1368,A new class of radial basis functions with compact support,Mathematics of Computation," Radial basis functions are well-known and successful tools for the interpolation of data in many dimensions. Several radial basis functions of compact support that give rise to nonsingular interpolation problems have been proposed, and in this paper we study a new, larger class of smooth radial functions of compact support which contains other compactly supported ones that were proposed earlier in the literature. ",math
1369,A Best Possible Heuristic for the k-Center Problem,Mathematics of Operations Research," Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at . http://www.jstor.org/page/info/about/policies/terms.jsp . JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. http://www.jstor.org ",math
1370,Time optimal boundary controllability of a simple linear viscoelastic liquid,Mathematical Methods in The Applied Sciences,"A viscoelastic liquid confined between two parallel plates is considered under the action of L2 (O, T)-boundary controls. Applying Laplace-Transform-techniques it is shown that L2 (O, l)-states are exactly controllable in finite time, depending on the speed of propagation of singularities. Finally the existence of time-optimal controls respecting a given norm bound is shown.",math
1371,A symmetric space of noncompact type has no equivariant isometric immersions into the Euclidean space,Proceedings of the American Mathematical Society," The purpose of this department is to publish very short papers of an unusually elegant and polished character, for which there is no other outlet. Let M be a Riemannian globally symmetric space of noncompact type. Let G be the identity-connected component of the isometry group of M. Then (1) G acts transitively on M. (2) G is a semisimple Lie group, having noncompact simple factors. For a proof cf. [2, p. 194]. ",math
1372,Decision procedures for extensions of the theory of arrays,Annals of Mathematics and Artificial Intelligence,"The theory of arrays, introduced by McCarthy in his seminal paper ""Towards a mathematical science of computation,"" is central to Computer Science. Unfortunately, the theory alone is not sufficient for many important verification applications such as program analysis. Motivated by this observation, we study extensions of the theory of arrays whose satisfiability problem (i.e., checking the satisfiability of conjunctions of ground literals) is decidable. In particular, we consider extensions where the indexes of arrays have the algebraic structure of Presburger arithmetic and the theory of arrays is augmented with axioms characterizing additional symbols such as dimension, sortedness, or the domain of definition of arrays. We provide methods for integrating available decision procedures for the theory of arrays and Presburger arithmetic with automatic instantiation strategies which allow us to reduce the satisfiability problem for the extension of the theory of arrays to that of the theories decided by the available procedures. Our approach aims to re-use as much as possible existing techniques so as to ease the implementation of the proposed methods. To this end, we show how to use model-theoretic, rewriting-based theorem proving (i.e., superposition), and techniques developed in the Satisfiability Modulo Theories communities to implement the decision procedures for the various extensions. © 2007 Springer Science+Business Media B.V.",math
1373,The norms of powers of matrices with unit spectral radius,Applied Mathematics Letters,"We show that the norm of the powers of a matrix with unit spectral radius which is not of bounded type grows as 0(np-1), where p is the order of the largest nonlinear divisor of the matrix associated with a unit eigenvalue. ÂŠ 1994.",math
1374,Structures induced by collections of subsets: a hypergraph approach,Mathematical Social Sciences,"During recent years, much attention has been paid by anthropologists and sociologists to the analysis of social networks. These networks arise from dyadic relationships such as kinship or friendship and they have been studied using techniques derived from graph theory. Although the study of such networks can cast much light on the social structure of a population, many important aspects of this structure cannot be addressed using dyadic relationships alone. For example, group membership gives rise to natural non-dyadic relationships which will be distorted if they are forced into a dyadic mold. The purpose of this paper is to propose an analytical scheme which will permit the study of structure induced by non-dyadic relationships. The concepts used derive from the theory of hypergraphs, and it is shown that these concepts permit a wide variety of structural questions to be posed. ÂŠ 1981.",math
1375,The impact of voters’ preference diversity on the probability of some electoral outcomes,Mathematical Social Sciences,"Voting rules are known to exhibit various paradoxical or problematic behaviors, typically in the form of their failure to meet the Condorcet criterion or in their vulnerability to strategic voting. Our basic premise is that a decrease in the number of coalitions of voters that exist with similar preference rankings should generally lead to a reduced propensity of voting rules to yield undesired results. Surprisingly enough, conclusions that are reported by Felsenthal etal. (1990) in an early study do not corroborate this intuition. This study reconsiders and extends the Felsenthal et al. analysis by using a modified Impartial Anonymous Culture (IAC) model. It turns out that the results obtained with this probabilistic assumption are much more consistent with the stated intuitive premise. © 2013 Elsevier B.V.",math
1376,On the storage requirement in the out-of-core multifrontal method for sparse factorization,ACM Transactions on Mathematical Software,"Two techniques are introduced to reduce the working storage requirement for the recent multifrontal method of Duff and Reid used in the sparse out-of-core factorization of symmetric matrices. For a given core size, the reduction in working storage allows some large problems to be solved without having to use auxiliary storage for the working arrays. Even if the working arrays exceed the core size, it will reduce the amount of input-output traffic necessary to manipulate the working vectors. Experimental results are provided to demonstrate significant storage reduction on practical problems using the proposed techniques. © 1986, ACM. All rights reserved.",math
1377,A Fortran 90 environment for research and prototyping of enclosure algorithms for nonlinear equations and global optimization,ACM Transactions on Mathematical Software,"An environment for general research into and prototyping of algorithms for reliable constrained and unconstrained global nonlinear optimization and reliable enclosure of all roots of nonlinear systems of equations, with or without inequality constraints, is being developed. This environment should be portable, easy to learn, use, and maintain, and sufficiently fast for some production work. The motivation, design principles, uses, and capabilities for this environment are outlined. The environment includes an interval data type, a symbolic form of automatic differentiation to obtain an internal representation for functions, a special technique to allow conditional branches with operator overloading and interval computations, and generic routines to give interval and noninterval function and derivative information. Some of these generic routines use a special version of the backward mode of automatic differentiation. The package also includes dynamic data structures for exhaustive search algorithms. ÂŠ 1995, ACM. All rights reserved.",math
1378,SBA: A software package for generic sparse bundle adjustment,ACM Transactions on Mathematical Software," Bundle adjustment constitutes a large, nonlinear least-squares problem that is often solved as the last step of feature-based structure and motion estimation computer vision algorithms to obtain optimal estimates. Due to the very large number of parameters involved, a general purpose leastsquares algorithm incurs high computational and memory storage costs when applied to bundle adjustment. Fortunately, the lack of interaction among certain subgroups of parameters results in the corresponding Jacobian being sparse, a fact that can be exploited to achieve considerable computational savings. This article presents sba, a publicly available C/C++ software package for realizing generic bundle adjustment with high efficiency and flexibility regarding parameterization. Categories and Subject Descriptors: I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Motion, shape, stereo, time-varying imagery, tracking; G.4 [Mathematical Software]: Algorithm design and analysis, efficiency; G.1.3 [Numerical Analysis]: Numerical Linear Algebra-Linear systems (direct and iterative methods), sparse, structural, and very large systems (direct and iterative methods) Additional Key Words and Phrases: Unconstrained optimization, nonlinear least squares, Levenberg-Marquardt, sparse Jacobian, bundle adjustment, structure and motion estimation, multiple-view geometry, engineering applications The financial support of the European Union through projects IST-2001-34545 Lifeplus, COOP-CT2005-017405 RECOVER, and FP6-507752 NoE MUSCLE is acknowledged. Authors' addresses: Computational Vision and Robotics Laboratory, Institute of Computer Science, Foundation for Research and Technology-Hellas (FORTH), N. Plastira 100, Vassilika Vouton, 700 13, Heraklion, Crete, Greece; email: {lourakis,argyros}@ics.forth.gr. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. C 2009 ACM 0098-3500/2009/03-ART2 $5.00 DOI 10.1145/1486525.1486527 http://doi.acm.org/ 10.1145/1486525.1486527 ",math
1379,A study of homogeneity in relational databases,Annals of Mathematics and Artificial Intelligence,"We define four different properties of relational databases which are related tothe notion of homogeneity in classical model theory. The main question for their definition is, for any given database to determine the minimum integer k, such that whenever two k-tuples satisfy the same properties which are expressible in first order logic with up to k variables (FOk), then there is an automorphism which maps each of these k-tuples onto each other. We study these four properties as a means to increase the computational power of subclasses of the reflective relational machines (RRMs) of bounded variable complexity. These were introduced by S. Abiteboul, C. Papadimitriou and V. Vianu and are known to be incomplete. For this sake we first give a semantic characterization of the subclasses of total RRM with variable complexity k (RRMk) for every natural number k. This leads to the definition of classes of queries denoted as script Q signCscript Q signk. We believe these classes to be of interest in their own right. For each k > 0, we define the subclass script Q signCscript Q signk as the total queries in the class Cscript Q sign of computable queries which preserve realization of properties expressible in FOk. The nature of these classes is implicit in the work of S. Abiteboul, M. Vardi and V. Vianu. We prove script Q signCscript Q signk = total(RRMk) for every k > 0. We also prove that these classes form a strict hierarchy within a strict subclass of total(Cscript Q sign). This hierarchy is orthogonal to the usual classification of computable queries in time-space-complexity classes. We prove that the computability power of RRMk machines is much greater when working with classes of databases which are homogeneous, for three of the properties which we define. As to the fourth one, we prove that the computability power of RRM with sublinear variable complexity also increases when working on databases which satisfy that property. The strongest notion, pairwise k-homogeneity, allows RRMk machines to achieve completeness.",math
1380,An algorithm for determining whether a given binary matroid is graphic.,Proceedings of the American Mathematical Society," 1. Introduction. In a recent series of papers [l-4] on graphs and matroids I used definitions equivalent to the following. A binary chain-group N on a. finite set M is a class of subsets of M forming a group under mod 2 addition. These subsets are the chains of N. A chain of N is elementary if it is non-null and has no other non-null chain of ATas a subset. A binary matroid is the class of elementary chains of a binary chain-group. As an example of a binary chain-group we may take the class of all cuts of a given finite graph G. A cut of G is determined by a partition of its set of vertices into two disjoint subsets U and V, and is defined as the set of all edges having one end in U and the other in V. I have called the corresponding binary matroid the bond-matroid of G. In the above-mentioned series of papers I obtained necessary and sufficient conditions for a given binary matroid to be graphic, that is representable as the bond-matroid of a graph. On several occasions it has been pointed out to me that these results are of interest to electrical engineers,1 but that a practical method for deciding whether or not a given binary matroid was graphic would be still more interesting. In what follows I present an algorithm which I hope will be of some use in this connection. This algorithm is described in §3 and the theorems needed to justify it are collected in §2. ",math
1381,Uniqueness in the Schauder fixed point theorem,Proceedings of the American Mathematical Society," A condition is given which guarantees the uniqueness of the fixed point in the Brouwer and Schauder fixed point theorems. The result is applied to a nonlinear boundary value problem in physiology. 1. Let A' be a real Banach space with a bounded convex open subset D, and let F : D ->D be a continuous function which is also assumed to be compact if X is infinite dimensional. The Brouwer fixed point theorem (Schauder theorem if X is infinite dimensional) gives a point x G D such that x = Fix). Under the assumption that F is differentiable, we give a simple condition which guarantees that the fixed point x is unique. The proof is an application of degree theory. We phrase the argument for the infinite dimensional case; the reader who is interested only in the finite dimensional case may omit the compactness hypothesis. In the last section, the result is applied to a nonlinear boundary value problem arising in physiology. ",math
1382,An incomplete factorization technique for positive definite linear systems,Mathematics of Computation,"This paper describes a technique for solving the large sparse symmetric linear systems that arise from the application of finite element methods. The technique combines an incomplete factorization method called the shifted incomplete Cholesky factorization with the method of generalized conjugate gradients. The shifted incomplete Cholesky factorization produces a splitting of the matrix A that is dependent upon a parameter a. It is shown that if A is positive definite, then there is some a for which this splitting is possible and that this splitting is at least as good as the Jacobi splitting. The method is shown to be more efficient on a set of test problems than either direct methods or explicit iteration schemes. ÂŠ 1980 American Mathematical Society.",math
1383,The strong perfect graph theorem,Annals of Mathematics," A graph G is perfect if for every induced subgraph H, the chromatic number of H equals the size of the largest complete subgraph of H, and G is Berge if no induced subgraph of G is an odd cycle of length at least five or the complement of one. The “strong perfect graph conjecture” (Berge, 1961) asserts that a graph is perfect if and only if it is Berge. A stronger conjecture was made recently by Conforti, Cornu´ejols and Vuˇskovi´c - that every Berge graph either falls into one of a few basic classes, or admits one of a few kinds of separation (designed so that a minimum counterexample to Berge's conjecture cannot have either of these properties). In this paper we prove both of these conjectures. We begin with definitions of some of our terms which may be nonstandard. All graphs in this paper are finite and simple. The complement G of a graph G has the same vertex set as G, and distinct vertices u, v are adjacent in G just when they are not adjacent in G. A hole of G is an induced subgraph of G which is a cycle of length at least 4. An antihole of G is an induced subgraph of G whose complement is a hole in G. A graph G is Berge if every hole and antihole of G has even length. A clique in G is a subset X of V (G) such that every two members of X are adjacent. A graph G is perfect if for every induced subgraph H of G, *Supported by ONR grant N00014-01-1-0608, NSF grant DMS-0071096, and AIM. ∗∗Supported by ONR grants N00014-97-1-0512 and N00014-01-1-0608, and NSF grant DMS-0070912. ∗∗∗Supported by ONR grant N00014-01-1-0608, NSF grants DMS-9970514 and DMS0200595, and AIM. ",math
1384,The Game of Normal Numbers,Mathematics of Operations Research," We introduce a two-player game where at each period one player, say, Player 2, chooses a distribution and the other player, Player 1, a realization. Player 1 wins the game if the sequence of realized outcomes is normal with respect to the sequence of distributions. We present a pure winning strategy of Player 1 and thereby provide a universal algorithm that generates a normal sequence for any discrete stochastic process. It turns out that to select the nth digit, the algorithm conducts O(n2) calculations. The proof uses approachability in infinite-dimensional spaces (Lehrer 2002). ",math
1385,Complex Probabilistic Modeling with Recursive Relational Bayesian Networks,Annals of Mathematics and Artificial Intelligence," A number of representation systems have been proposed that extend the purely propositional Bayesian network paradigm with representation tools for some types of first-order probabilistic dependencies. Examples of such systems are dynamic Bayesian networks and systems for knowledge based model construction. We can identify the representation of probabilistic relational models as a common well-defined semantic core of such systems. Recursive relational Bayesian networks (RRBNs) are a framework for the representation of probabilistic relational models. A main design goal for RRBNs is to achieve greatest possible expressiveness with as few elementary syntactic constructs as possible. The advantage of such an approach is that a system based on a small number of elementary constructs will be much more amenable to a thorough mathematical investigation of its semantic and algorithmic properties than a system based on a larger number of high-level constructs. In this paper we show that with RRBNs we have achieved our goal, by showing, first, how to solve within that framework a number of non-trivial representation problems. In the second part of the paper we show how to construct from a RRBN and a specific query, a standard Bayesian network in which the answer to the query can be computed with standard inference algorithms. Here the simplicity of the underlying representation framework greatly facilitates the development of simple algorithms and correctness proofs. As a result we obtain a construction algorithm that even for RRBNs that represent models for complex first-order and statistical dependencies generates standard Bayesian networks of size polynomial in the size of the domain given in a specific application instance. ",math
1386,Existence and properties of solutions for neural field equations,Mathematical Methods in The Applied Sciences,"The first goal of this work is to study the solvability of the neural field equation (known as ‘Amari equation’) which is an integro-differential equation in m+ 1 dimensions. In particular, we show the existence of global solutions for smooth activation functions f with values in [0, 1] and L1 kernels w via the Banach fixpoint theorem. We note that this setting is much more general than in most related studies, e.g. Ermentrout and McLeod (Proceedings of the Royal Society of Edinburgh 1993; 123A:461–478). For a Heaviside-type activation function f, we show that the approach above fails. However, with slightly more regularity on the kernel function w (we use Hölder continuity with respect to the argument x) we can employ compactness arguments, integral equation techniques and the results for smooth nonlinearity functions to obtain a global existence result in a weaker space. Finally, general estimates on the speed and durability of waves are derived. We show that compactly supported waves with directed kernels (i.e. w(x, y)⩽0 for x⩽y ) decay exponentially after a finite time and that the field has a well-defined finite speed. Copyright © 2009 John Wiley & Sons, Ltd.",math
1387,A variable order Runge-Kutta method for initial value problems with rapidly varying right-hand sides,ACM Transactions on Mathematical Software," Explicit Runge-Kutta methods (RKMs) are among the most popular classes of formulas for the approximate numerical integration of nonstiff, initial value problems. However, high-order RungeKutta methods require more function evaluations per integration step than, for example, Adams methods used in PECE mode, and so, with RKMs, it is especially important to avoid rejected steps. Steps are often rejected when certain derivatives of the solution are very large for part of the region of integration. This corresponds, for example, to regions where the solution has a sharp front or, in the limit, some derivative of the solution is discontinuous. In these circumstances the assumption that the local truncation error is changing slowly is invalid, and so any step-choosing algorithm is likely to produce an unacceptable step. In this paper we derive a family of explicit Runge-Kutta formulas. Each formula is very efficient for problems with smooth solutions as well as problems having rapidly varying solutions. Each member of this family consists of a fifth-order formula that contains imbedded formulas of all orders 1 through 4; By computing solutions at several different orders, it is possible to detect sharp fronts or discontinuities before all the function evaluations defining the full Runge-Kutta step have been computed. We can then either accept a lower order solution or abort the step, depending on which course of action seems appropriate. The efficiency of the new algorithm is demonstrated on the DETEST test set as well as on some difficult test problems with sharp fronts or discontinuities. ",math
1388,Efficient inversion of the Galerkin matrix of general second-order elliptic operators with nonsmooth coefficients,Mathematics of Computation,"This article deals with the efficient (approximate) inversion of finite element stiffness matrices of general second-order elliptic operators with L∞-coefficients. It will be shown that the inverse stiffness matrix can be approximated by hierarchical matrices (ℋ-matrices). Furthermore, numerical results will demonstrate that it is possible to compute an approximate inverse with almost linear complexity. © 2004 American Mathematical Society.",math
1389,Uniformly convex functions on Banach spaces,Proceedings of the American Mathematical Society,"Given a Banach space (X,∥·∥), we study the connection between uniformly convex functions f : X → ℝ bounded above by ∥·∥p and the existence of norms on X with moduli of convexity of power type. In particular, we show that there exists a uniformly convex function f : X → ℝ bounded above by ∥·∥ 2 if and only if X admits an equivalent norm with modulus of convexity of power type 2. © 2008 American Mathematical Society.",math
1390,KBFS: K-Best-First Search,Annals of Mathematics and Artificial Intelligence,"We introduce a new algorithm, K-best-first search (KBFS), which is a generalization of the well known best-first search (BFS). In KBFS, each iteration simultaneously expands the K best nodes from the open-list (rather than just the best as in BFS). We claim that KBFS outperforms BFS in domains where the heuristic function has large errors in estimation of the real distance to the goal state or does not predict dead-ends in the search tree. We present empirical results that confirm this claim and show that KBFS outperforms BFS by a factor of 15 on random trees with dead-ends, and by a factor of 2 and 7 on the Fifteen and Twenty-Four tile puzzles, respectively. KBFS also finds better solutions than BFS and hill-climbing for the number partitioning problem. KBFS is only appropriate for finding approximate solutions with inadmissible heuristic functions.",math
1391,"TetGen, a Delaunay-Based Quality Tetrahedral Mesh Generator",ACM Transactions on Mathematical Software,"TetGen is a C++ program for generating good quality tetrahedral meshes aimed to support numerical methods and scientific computing. The problem of quality tetrahedralmesh generation is challenged by many theoretical and practical issues. TetGen uses Delaunay-based algorithms which have theoretical guarantee of correctness. It can robustly handle arbitrary complex 3D geometries and is fast in practice. The source code of TetGen is freely available. This article presents the essential algorithms and techniques used to develop TetGen. The intended audience are researchers or developers in mesh generation or other related areas. It describes the key software components of TetGen, including an efficient tetrahedral mesh data structure, a set of enhanced local mesh operations (combination of flips and edge removal), and filtered exact geometric predicates. The essential algorithms include incremental Delaunay algorithms for inserting vertices, constrained Delaunay algorithms for inserting constraints (edges and triangles), a new edge recovery algorithm for recovering constraints, and a new constrained Delaunay refinement algorithm for adaptive quality tetrahedral mesh generation. Experimental examples as well as comparisons with other softwares are presented.",math
1392,Minimizing multimodal functions of continuous variables with the “simulated annealing” algorithm Corrigenda for this article is available here,ACM Transactions on Mathematical Software,"A new global optimization algorithm for functions of continuous variables is presented, derived from the “Simulated Annealing” algorithm recently introduced in combinatorial optimization. The algorithm is essentially an iterative random search procedure with adaptive moves along the coordinate directions. It permits uphill moves under the control of a probabilistic criterion, thus tending to avoid the first local minima encountered. The algorithm has been tested against the Nelder and Mead simplex method and against a version of Adaptive Random Search. The test functions were Rosenbrock valleys and multiminima functions in 2,4, and 10 dimensions. The new method proved to be more reliable than the others, being always able to find the optimum, or at least a point very close to it. It is quite costly in term of function evaluations, but its cost can be predicted in advance, depending only slightly on the starting point. © 1987, ACM. All rights reserved.",math
1393,Dimension and product structure of hyperbolic measures,Annals of Mathematics," We prove that every hyperbolic measure invariant under a C1+® di®eomorphism of a smooth Riemannian manifold possesses asymptotically \almost"" local product structure, i.e., its density can be approximated by the product of the densities on stable and unstable manifolds up to small exponentials. This has not been known even for measures supported on locally maximal hyperbolic sets. Using this property of hyperbolic measures we prove the long-standing Eckmann-Ruelle conjecture in dimension theory of smooth dynamical systems: the pointwise dimension of every hyperbolic measure invariant under a C1+® di®eomorphism exists almost everywhere. This implies the crucial fact that virtually all the characteristics of dimension type of the measure (including the Hausdor® dimension, box dimension, and information dimension) coincide. This provides the rigorous mathematical justi¯cation of the concept of fractal dimension for hyperbolic measures. ",math
1394,A fast sweeping method for Eikonal equations,Mathematics of Computation,In this paper a fast sweeping method for computing the numerical solution of Eikonal equations on a rectangular grid is presented. The method is an iterative method which uses upwind difference for discretization and uses Gauss-Seidel iterations with alternating sweeping ordering to solve the discretized system. The crucial idea is that each sweeping ordering follows a family of characteristics of the corresponding Eikonal equation in a certain direction simultaneously. The method has an optimal complexity of O(N) for N grid points and is extremely simple to implement in any number of dimensions. Monotonicity and stability properties of the fast sweeping algorithm are proven. Convergence and error estimates of the algorithm for computing the distance function is studied in detail. It is shown that 2 n Gauss-Seidel iterations is enough for the distance function in n dimensions. An estimation of the number of iterations for general Eikonal equations is also studied. Numerical examples are used to verify the analysis. © 2004 American Mathematical Society.,math
1395,Exact Size of Binary Space Partitionings and Improved Rectangle Tiling Algorithms,SIAM Journal on Discrete Mathematics," We prove the following upper and lower bounds on the exact size of binary space partition (BSP) trees for a set of n isothetic rectangles in the plane: • An upper bound of 3n − 1 in general, and an upper bound of 2n − 1 if the rectangles tile the underlying space. This improves the upper bounds of 4n in [V. Hai Nguyen and P. Widmayer, Binary Space Partitions for Sets of Hyperrectangles, Lecture Notes in Comput. Sci. 1023, Springer-Verlag, Berlin, 1995; F. d'Amore and P. G. Franciosa, Inform. Process. Lett., 44 (1992), pp. 255-259]. A BSP satisfying the upper bounds can be constructed in O(n log n) time. • A worst-case lower bound of 2n − o(n) in general, and 32n − o(n) if the rectangles form a tiling. The BSP tree is one of the most popular data structures in computational geometry, and hence even “small” factor improvements of 43 or 2 on the previously known upper bounds that we show improve the performances of applications relying on the BSP tree. As an illustration, we present improved approximation algorithms for certain dual rectangle tiling problems using our upper bounds on the size of the BSP trees. ",math
1396,Combining Coq and Gappa for Certifying Floating-Point Programs,Mathematical Knowledge Management," Formal verification of numerical programs is notoriously difficult. On the one hand, there exist automatic tools specialized in floatingpoint arithmetic, such as Gappa, but they target very restrictive logics. On the other hand, there are interactive theorem provers based on the LCF approach, such as Coq, that handle a general-purpose logic but that lack proof automation for floating-point properties. To alleviate these issues, we have implemented a mechanism for calling Gappa from a Coq interactive proof. This paper presents this combination and shows on several examples how this approach offers a significant speedup in the process of verifying floating-point programs. ",math
1397,A semiparametric generative model for efficient structured-output supervised learning,Annals of Mathematics and Artificial Intelligence,"We present a semiparametric generative model for supervised learning with structured outputs. The main algorithmic idea is to replace the parameters of an underlying generative model (such as a stochastic grammars) with input-dependent predictions obtained by (kernel) logistic regression. This method avoids the computational burden associated with the comparison between target and predicted structure during the training phase, but requires as an additional input a vector of sufficient statistics for each training example. The resulting training algorithm is asymptotically more efficient than structured output SVM as the size of the output structure grows. At the same time, by computing parameters of a joint distribution as a function of the full input structure, typical expressiveness limitations of related conditional models (such as maximum entropy Markov models) can be potentially avoided. Empirical results on artificial and real data (in the domains of natural language parsing and RNA secondary structure prediction) show that the method works well in practice and scales up with the size of the output structures. © Springer Science+Business Media B.V. 2009.",math
1398,A proof of the Kepler conjecture,Annals of Mathematics," 3. V -cells 3.1. V -cells 3.2. Orientation 3.3. Interaction of V -cells with the Q-system 4. Decomposition stars 4.1. Indexing sets 4.2. Cells attached to decomposition stars 4.3. Colored spaces 5. Scoring (Ferguson, Hales) 5.1. Definitions 5.2. Negligibility 5.3. Fcc-compatibility 5.4. Scores of standard clusters 6. Local optimality 6.1. Results 6.2. Rogers simplices 6.3. Bounds on simplices 6.4. Breaking clusters into pieces 6.5. Proofs ",math
1399,Distributed Boundary Coverage with a Team of Networked Miniature Robots using a Robust Market-Based Algorithm,Annals of Mathematics and Artificial Intelligence,"We study distributed boundary coverage of known environments using a team of miniature robots. Distributed boundary coverage is an instance of the multi-robot task-allocation problem and has applications in inspection, cleaning, and painting among others. The proposed algorithm is robust to sensor and actuator noise, failure of individual robots, and communication loss. We use a market-based algorithm with known lower bounds on the performance to allocate the environmental objects of interest among the team of robots. The coverage time for systems subject to sensor and actuator noise is significantly shortended by on-line task re-allocation. The complexity and convergence properties of the algorithm are formally analyzed. The system performance is systematically analyzed at two different microscopic modeling levels, using agent-based, discrete-event and module-based, realistic simulators. Finally, results obtained in simulation are validated using a team of Alice miniature robots involved in a distributed inspection case study. © 2009 Springer Science+Business Media B.V.",math
1400,The normal holonomy group,Proceedings of the American Mathematical Society,We prove that the restricted normal holonomy group of a submanifold of a space of constant curvature is compact and that the nontrivial part of its representation on the normal space is the isotropy representation of a semisimple Riemannian symmetric space. © 1990 American Mathematical Society.,math
1401,A generalization of Fermat’s theorem,Proceedings of the American Mathematical Society," 1. Introduction. There are various generalizations of Fermat's theorem that ap_1 = 1 (mod p) for any prime p and any integer a not divisible by p. The generalization that we present arises from looking at Fermat's result this way: given any prime p the congruence xp- x = 0 (mod p) is satisfied by every integer x; moreover, if f(x) is a polynomial with integral coefficients such that/(x)=0 (mod p) is satisfied by every integer x, then/(x) is a multiple of xp -x. The problem that we raise and settle here is this: for any positive integer m characterize the class of polynomials f(x) having the property that f(x) =0 (mod m) is satisfied by every integer. It turns out that in general, unlike the Fermat case where m is a prime, these polynomials are not all multiples of a single polynomial. Nevertheless these polynomials form an ideal with a finite set of generators. We now introduce a preferable notation to put the question in more tractable form. Let 7 denote the class of all integers, and I/m the finite ring of integers modulo m, so that I/m consists of elements 0,1,2, ■•■, m -1 with addition and multiplication defined modulo m. In the ring R of polynomials g(x) with coefficients in I/m, let S(m) denote the subring of polynomials/(x) such that f(a) =0 for every element a in I/m. This subring S(m) is an ideal in R, because (i) the difference of any two polynomials in S(m) is again a polynomial in d(m), and (ii) if f(x) is any polynomial in d(m) and g(x) is any polynomial in R, then f(x)g(x) is in <f(m). Our problem is to determine the structure of the ideal S(m). In case m is a prime p, then as we remarked earlier, S(p) is known to be the class of polynomials which are multiples of xp-x. Thus S(p) is a principal ideal with the single generator xp-x. In case m is not a prime we prove that S(m) is not a principal ideal, and we find a basis for $(m). ",math
1402,On a subspace perturbation problem,Proceedings of the American Mathematical Society,"We discuss the problem of perturbation of spectral subspaces for linear self-adjoint operators on a separable Hilbert space. Let A and V be bounded self-adjoint operators. Assume that the spectrum of A consists of two disjoint parts σ and Σ such that d = dist(σ, Σ) > 0. We show that the norm of the difference of the spectral projections E A(σ) and EA+V({λ | dist(λ,σ) < d/2}) for A and A + V is less than one whenever either (i) ||V|| < 2/2+πd or (ii) ||V|| < 1/2d and certain assumptions on the mutual disposition of the sets σ and Σ are satisfied.",math
1403,Large Deviations of Square Root Insensitive Random Sums,Mathematics of Operations Research," We provide a large deviation result for a random sum nN=x0 Xn, where Nx is a renewal counting process and Xn n≥0 are i.i.d. random variables, independent of Nx, with a common distribution that belongs to a class of square root insensitive distributions. Asymptotically, the tails of these distributions are heavier than e−√x and have zero relative decrease in intervals of length √x, hence square root insensitive. Using this result we derive the asymptotic characterization of the busy period distribution in the stable GI/G/1 queue with square root insensitive service times; this characterization further implies that the tail behavior of the busy period exhibits a functional change for distributions that are lighter than e−√x. ",math
1404,CSP dichotomy for special triads,Proceedings of the American Mathematical Society,"For a fixed digraph G, the Constraint Satisfaction Problem with the template G, orCSP(G) for short, is the problem of deciding whether a given input digraph H admits a homomorphism to G. The dichotomy conjecture of Feder and Vardi states that CSP(G), for any choice of G, is solvable in polynomial time or NP-complete. This paper confirms the conjecture for a class of oriented trees called special triads. As a corollary we get the smallest known example of an oriented tree (with 33 vertices) defining an NP-complete CSP(G). © 2009 American Mathematical Society.",math
1405,Conditioning of quasi-Newton methods for function minimization,Mathematics of Computation," Quasi-Newton methods accelerate the steepest-descent technique for function minimization by using computational history to generate a sequence of approximations to the inverse of the Hessian matrix. This paper presents a class of approximating matrices as a function of a scalar parameter. The problem of optimal conditioning of these matrices under an appropriate norm as a function of the scalar parameter is investigated. A set of computational results verifies the superiority of the new methods arising from conditioning considerations to known methods. Received March 14, 1969, revised January 22, 1970. AMS Subject Classifications. Primary 6530; Secondary 6550, 9058. ",math
1406,Single-peaked orders on a tree,Mathematical Social Sciences,"Inada (1969) and Sen and Pattanaik (1969) have characterized the sets of preference orders which ensure the transitivity of the strict majority rule, no matter how each voter selects his own order in the set. But a problem remains untouched: which domains of orders guarantee the existence of a majority winner without necessarily ensuring the transitivity of the strict majority rule. We provide in this paper domains, called sets of single-peaked linear orders on a tree, which enjoy such a property. They appear as a generalization of the well-known sets of single-peaked linear orders. © 1982.",math
1407,Lifts of Convex Sets and Cone Factorizations,Mathematics of Operations Research," In this paper we address the basic geometric question of when a given convex set is the image under a linear map of an a ne slice of a given closed convex cone. Such a representation or \lift"" of the convex set is especially useful if the cone admits an e cient algorithm for linear optimization over its a ne slices. We show that the existence of a lift of a convex set to a cone is equivalent to the existence of a factorization of an operator associated to the set and its polar via elements in the cone and its dual. This generalizes a theorem of Yannakakis that established a connection between polyhedral lifts of a polytope and nonnegative factorizations of its slack matrix. Symmetric lifts of convex sets can also be characterized similarly. When the cones live in a family, our results lead to the de nition of the rank of a convex set with respect to this family. We present results about this rank in the context of cones of positive semide nite matrices. Our methods provide new tools for understanding cone lifts of convex sets. ",math
1408,Estimating the efficiency of backtrack programs,Mathematics of Computation," One of the chief difficulties associated with the so-called backtracking technique for combinatorial problems has been our inability to predict the efficiency of a given algorithm, or to compare the efficiencies of different approaches, without actually writing and running the programs. This paper presents a simple method which produces reasonable estimates for most applications, requiring only a modest amount of hand calculation. The method should prove to be of considerable utility in connection with D. H. Lehmer's branch-and-bound approach to combinatorial optimization. The majority of all combinatorial computing applications can apparently be handled only by what amounts to an exhaustive search through all possibilities. Such searches can readily be performed by using a well-known ""depth-first"" procedure which R. J. Walker [21] has aptly called backtracking. (See Lehmer [16], Golomb and Baumert [6], and Wells [22] for general discussions of this technique, together with numerous interesting examples.) Sometimes a backtrack program will run to completion in less than a second, while other applications seem to go on forever. The author once waited all night for the output from such a program, only to discover that the answers would not be forthcoming for about 106 centuries. A ""slight increase"" in one of the parameters of a backtrack routine might slow down the total running time by a factor of a thousand; conversely, a ""minor improvement"" to the algorithm might cause a hundredfold improvement in speed; and a sophisticated ""major improvement"" might actually make the program ten times slower. These great discrepancies in execution time are characteristic of backtrack programs, yet it is usually not obvious what will happen until the algorithm has been coded and run on a machine. Faced with these uncertainties, the author worked out a simple estimation procedure in 1962, designed to predict backtrack behavior in any given situation. This procedure was mentioned briefly in a survey article a few years later [8] ; and during subsequent years, extensive computer experimentation has confirmed its utility. Several ",math
1409,Non-Archimedean subjective probabilities in decision theory and games,Mathematical Social Sciences," To allow conditioning on counterfactual events, zero probabilities can be replaced by in¯nitesimal probabilities that range over a non-Archimedean ordered ¯eld. This paper considers a suitable minimal ¯eld that is a complete metric space. Axioms similar to those in Anscombe and Aumann (1963) and in Blume, Brandenburger and Dekel (1991) are used to characterize preferences which: (i) reveal unique nonArchimedean subjective probabilities within the ¯eld; and (ii) can be represented by the non-Archimedean subjective expected value of any real-valued von Neumann{Morgenstern utility function in a unique cardinal equivalence class, using the natural ordering of the ¯eld. ",math
1410,Convergence of the linearized Bregman iteration for ℓ₁-norm minimization,Mathematics of Computation," One of the key steps in compressed sensing is to solve the basis pursuit problem minu∈Rn { u 1 : Au = f }. Bregman iteration was very successfully used to solve this problem in [40]. Also, a simple and fast iterative algorithm based on linearized Bregman iteration was proposed in [40], which is described in detail with numerical simulations in [35]. A convergence analysis of the smoothed version of this algorithm was given in [11]. The purpose of this paper is to prove that the linearized Bregman iteration proposed in [40] for the basis pursuit problem indeed converges. ",math
1411,Prime forms and minimal change in propositional belief bases,Annals of Mathematics and Artificial Intelligence,"This paper proposes to use prime implicants and prime implicates normal forms to represent belief sets. This representation is used, on the one hand, to define syntactical versions of belief change operators that also satisfy the rationality postulates but present better complexity properties than those proposed in the literature and, on the other hand, to propose a new minimal distance that adopts as a minimal belief unit a ""fact"", defined as a prime implicate of the belief set, instead of the usually adopted Hamming distance, i. e., the number of propositional symbols on which the models differ. Some experiments are also presented that show that this new minimal distance allows to define belief change operators that usually preserve more information of the original belief set. © 2010 Springer Science+Business Media B.V.",math
1412,Arithmetic on superelliptic curves,Mathematics of Computation," This paper is concerned with algorithms for computing in the divisor class group of a nonsingular plane curve of the form yn = c(x) which has only one point at in nity. Divisors are represented as ideals, and an ideal reduction algorithm based on lattice reduction is given. We obtain a unique representative for each divisor class and the algorithms for addition and reduction of divisors run in polynomial time. An algorithm is also given for solving the discrete logarithm problem when the curve is de ned over a nite eld. ",math
1413,Linear automaton transformations,Proceedings of the American Mathematical Society," Let R be a nonempty set, let N consist of all non-negative rational integers, and denote by RN the set of all functions on N to R. If R is a ring, a map M: R""->P^ is linear if M(rxfx+r2f2)=rx(Mfx) +r2(Mf2) for rx, r2 in R, fx, f2 in RN. For a finite commutative ring with unit we determine which linear transformations M: RN-+RN can be realized by finite automata. More precisely, let A, B he finite nonempty sets. A map M: AN->BN is an automaton transformation if there exists a finite set Q, maps Mq: A X£>-><M2,b: A XQ-*B, elements h in B, q in Q such that corresponding to each/ in AN there exists an h in QN satisfying (In automaton language, .4 is the input alphabet, B is the output alphabet, Q is the set of states, q is the initial state, I is the initial output, while Ms(a, q) and MQ(a, q) are respectively the output and state resulting from input a and state q. For the case that A and B coincide with the set consisting of 0 and 1, the concept of automaton transformation is simply a variant of the concept of representable event of Kleene [l].) Call a matrix m,,-: NXN-+R eventually doubly-periodic if for some positive integers Px, P2, pi, p2: Uij = U(i+Pl)j for all i > Pi and ally, Presented to the Society January 30, 1958; received by the editors December 26, 1957. 1 National Science Foundation postdoctoral fellow. ",math
1414,An Order-Based Theory of Updates for Closed Database Views,Annals of Mathematics and Artificial Intelligence," The fundamental problem in the design of update strategies for views of database schemata is that of selecting how the view update is to be reflected back to the base schema. This work presents a solution to this problem, based upon the dual philosophies of closed update strategies and order-based database mappings. A closed update strategy is one in which the entire set of updates exhibit natural closure properties, including transitivity and reversibility. The order-based paradigm is a natural one; most database formalisms endow the database states with a natural order structure, under which update by insertion is an increasing operation, and update by deletion is decreasing. Upon augmenting the original constant-complement strategy of Bancilhon and Spyratos - which is an early version of a closed update strategy - with compatible order-based notions, the reflection to the base schema of any update to the view schema which is an insertion, a deletion, or a modification which is realizable as a sequence of insertions and deletions is shown to be unique and independent of the choice of complement. In addition to this uniqueness characterization, the paper also develops a theory which identifies conditions under which a natural, maximal, update strategy exists for a view. This theory is then applied to a ubiquitous example - single-relational schemata constrained by equality-generating dependencies. Within this framework it is shown that for a view defined as a projection of the main relation, the only possibility is that the complement defining the update process is also a projection, and that the reconstruction is based upon functional dependencies. ",math
1415,An introduction to fuzzy answer set programming,Annals of Mathematics and Artificial Intelligence,"In this paper we show how the concepts of answer set programming and fuzzy logic can be successfully combined into the single framework of fuzzy answer set programming (FASP). The framework offers the best of both worlds: from the answer set semantics, it inherits the truly declarative non-monotonic reasoning capabilities while, on the other hand, the notions from fuzzy logic in the framework allow it to step away from the sharp principles used in classical logic, e.g., that something is either completely true or completely false. As fuzzy logic gives the user great flexibility regarding the choice for the interpretation of the notions of negation, conjunction, disjunction and implication, the FASP framework is highly configurable and can, e.g., be tailored to any specific area of application. Finally, the presented framework turns out to be a proper extension of classical answer set programming, as we show, in contrast to other proposals in the literature, that there are only minor restrictions one has to demand on the fuzzy operations used, in order to be able to retrieve the classical semantics using FASP. © 2007 Springer Science+Business Media B.V.",math
1416,A simple and space-efficient fragment-chaining algorithm for alignment of DNA and protein sequences,Applied Mathematics Letters," In the segment-based approach to sequence alignment. nucleic acid, and protein sequence alignments are constructed from fragments, i.e., from pairs of ungapped segments of the input sequences. Given a set F of candidate fragments and a weighting function w : F + FL:, the score of an alignment is defined as the sum of weights of the fragments it consists of. and the optimization problem is to find a consistent collection of painuzse dislomt fragments wzth nmxin~um swn of wezghts. Herein, a sparse dynamic programming algorithm is described that solves the pairwise segment-alignment problem in O(L + Nmax ) space where L is the maximum length of t,he input sequences while N ,nax 5 #F holds. With a recently introduced weighting function 1~. small sets F of candidate fragments are sufficient to obtain alignments of high quality. As a result, the proposed algorithm runs in essentially linear space. @ 2001 Elsevier Science Ltd. All rights reserved ",math
1417,An approximate formula for a partial sum of the divergent p-series,Applied Mathematics Letters,We have applied the upper truncated Pareto distribution to modeling the probability distribution of the terms of the p-series. The lower and upper bounds of their partial sum have been found and used for approximating its exact value. A relative error contributed by the proposed approximation has been evaluated. ÂŠ 2008 Elsevier Ltd. All rights reserved.,math
1418,Algorithm 844: Computing sparse reduced-rank approximations to sparse matrices,ACM Transactions on Mathematical Software," In many applications-latent semantic indexing, for example-it is required to obtain a reduced rank approximation to a sparse matrix A. Unfortunately, the approximations based on traditional decompositions, like the singular value and QR decompositions, are not in general sparse. Stewart [(1999), 313-323] has shown how to use a variant of the classical Gram-Schmidt algorithm, called the quasi-Gram-Schmidt-algorithm, to obtain two kinds of low-rank approximations. The first, the SPQR, approximation, is a pivoted, Q-less QR approximation of the form (XR1−11)(R11 R12), where X consists of columns of A. The second, the SCR approximation, is of the form the form A ∼= XTY T, where X and Y consist of columns and rows A and T , is small. In this article we treat the computational details of these algorithms and describe a MATLAB implementation. Categories and Subject Descriptors: G.1.3 [Numerical Analysis]: Numerical Linear AlgebraSparse, structured, and very large systems (direct and iterative method) Additional Key Words and Phrases: Sparse approximations, Gram-Schmidt algorithm, MATLAB In a number of applications [Berry et al. 1999; Jiang and Berry 2000; Stuart and Berry 2003; Berry and Martin 2004] one is given a large matrix A and wishes ",math
1419,Invertibility of random matrices: norm of the inverse,Annals of Mathematics,"Let A be an n × n matrix, whose entries are independent copies of a centered random variable satisfying the subgaussian tail estimate. We prove that the operator norm of A                         -1 does not exceed Cn                         3/2 with probability close to 1.",math
1420,Extending Answer Sets for Logic Programming Agents,Annals of Mathematics and Artificial Intelligence,"We present systems of logic programming agents (LPAS) to model the interactions between decision-makers while evolving to a conclusion. Such a system consists of a number of agents connected by means of unidirectional communication channels. Agents communicate with each other by passing answer sets obtained by updating the information received from connected agents with their own private information. We introduce a credulous answer set semantics for logic programming agents. As an application, we show how extensive games with perfect information can be conveniently represented as logic programming agent systems, where each agent embodies the reasoning of a game player, such that the equilibria of the game correspond with the semantics agreed upon by the agents in the LPAS.",math
1421,Sparse matrix test problems,ACM Transactions on Mathematical Software,"We describe the Harwell-Boeing sparse matrix collection, a set of standard test matrices for sparse matrix problems. Our test set comprises problems in linear systems, least squares, and eigenvalue calculations from a wide variety of scientific and engineering disciplines. The problems range from small matrices, used as counter-examples to hypotheses in sparse matrix research, to large test cases arising in large-scale computation. We offer the collection to other researchers as a standard benchmark for comparative studies of algorithms. The procedures for obtaining and using the test collection are discussed. We also describe the guidelines for contributing further test problems to the collection. © 1989, ACM. All rights reserved.",math
1422,Exploiting zeros on the diagonal in the direct solution of indefinite sparse symmetric linear systems,ACM Transactions on Mathematical Software," We describe the design of a new code for the solution of sparse indefinite symmetric linear systems of equations. The principal difference between this new code and earlier work lies in the exploitation of the additional sparsity available when the matrix has a significant number of zero diagonal entries. Other new features have been included to enhance the execution speed, particularly on vector and parallel machines. ",math
1423,Learning cluster-based structure to solve constraint satisfaction problems,Annals of Mathematics and Artificial Intelligence," The hybrid search algorithm for constraint satisfaction problems described here first uses local search to detect crucial substructures and then applies that knowledge to solve the problem. This paper shows the difficulties encountered by traditional and state-of-the-art learning heuristics when these substructures are overlooked. It introduces a new algorithm, Foretell, to detect dense and tight substructures called clusters with local search. It also develops two ways to use clusters during global search: one supports variable-ordering heuristics and the other makes inferences adapted to them. Together they improve performance on both benchmark and real-world problems. ",math
1424,Authoring LeActiveMath Calculus Content,Mathematical Knowledge Management," Within the LeActiveMath project, a collection of OMDoc files and supporting material has been realized. This content covers the derivative side of calculus and is being used by students in the LeActiveMath learning environment. LeAM-calculus is the first collection trying to make use of most of the features of the learning environment including advanced usages of OpenMath and OMDoc. It has been written in OQMath, a readable xml-syntax. This paper describes the tools to produce it, how they were used and combined, the resulting content and the experience gained. It argues that the declaration of new OpenMath symbols is a requirement and explains challenges of authoring semantic mathematical content. Finally, it presents the management activities to support the authoring process. ",math
1425,"Capturing the Content of Physics: Systems, Observables, and Experiments",Mathematical Knowledge Management," We present a content markup language for physics realized by extending the OMDoc format by an infrastructure for the principal concepts of physics: observables, physical systems, and experiments. The formalization of the description of physics observables follows the structural essence of the operational theory of physics measurements. The representational infrastructure for systems and experiments allow to capture the distinctive practice of physics: natural laws are supported by evidence from experiments which are described, disseminated and reproduced by others. ",math
1426,Generalized Bernoulli numbers and -regular primes,Mathematics of Computation,"A prime p is defined to be m-regular if p does not divide the class number of a certain abelian number field. Several different characterizations are given for a prime to be m-regular, including a description in terms of the generalized Bernoulli numbers. A summary is given of two computations which determine the m-regularity or m-irregularity of primes p for certain values of m and p. ÂŠ 1984 American Mathematical Society.",math
1427,Parallel integer relation detection: Techniques and applications,Mathematics of Computation,"Let {x1, X2, ⋯, xn} be a vector of real numbers. An integer relation algorithm is a computational scheme to find the n integers ak, if they exist, such that a1x1 + a2x2 + ⋯ + anxn = 0. In the past few years, integer relation algorithms have been utilized to discover new results in mathematics and physics. Existing programs for this purpose require very large amounts of computer time, due in part to the requirement for multiprecision arithmetic, yet are poorly suited for parallel processing. This paper presents a new integer relation algorithm designed for parallel computer systems, but as a bonus it also gives superior results on single processor systems. Single-and multi-level implementations of this algorithm are described, together with performance results on a parallel computer system. Several applications of these programs are discussed, including some new results in mathematical number theory, quantum field theory and chaos theory.",math
1428,Some baby-step giant-step algorithms for the low hamming weight discrete logarithm problem,Mathematics of Computation,"In this paper, we present several baby-step giant-step algorithms for the low hamming weight discrete logarithm problem. In this version of the discrete log problem, we are required to find a discrete logarithm in a finite group of order approximately 2m, given that the unknown logarithm has a specified number of ls, say t, in its binary representation. Heiman and Odlyzko presented the first algorithms for this problem. Unpublished improvements by Coppersmith include a deterministic algorithm with complexity O(m(t/2 m/2)), and a Las Vegas algorithm with complexity O (â t(t/2 m/2)). We perform an average-case analysis of Coppersmiths deterministic algorithm. The average-case complexity achieves only a constant factor speed-up over the worst-case. Therefore, we present a generalized version of Coppersmiths algorithm, utilizing a combinatorial set system that we call a splitting system. Using probabilistic methods, we prove a new existence result for these systems that yields a (nonuniform) deterministic algorithm with complexity O (t3/2(log m) (t/2 m/2). We also present some explicit constructions for splitting systems that make use of perfect hash families.",math
1429,Continuity of Generalized Semi-Markov Processes,Mathematics of Operations Research," It is shown that sequences of generalized semi-Markov processes converge in the sense of weak convergence of random functions if associated sequences of defining elements (initial distributions, transition functions and clock time distributions) converge. This continuity or stability is used to obtain information about invariant probability measures. It is shown that there exists an invariant probability measure for any finite-state generalized semi-Markov process in which each clock time distribution has a continuous c,d,f, and a finite mean. For generalized semi-Markov processes with unique invariant probability measures, sequences of invariant probability measures converge when associated sequences of defining elements converge. Hence, properties of invariant measures can be deduced from convenient approximations. For example, insensitivity properties established for special classes of generalized semi-Markov processes by Schassberger (1977), (1978), Konig and Jansen (1976) and Burman (1981) extend to a larger class of generalized semi-Markov processes, 1. Introduction and summary. Among the most promising stochastic processes for modeling complex phenomena in operations research are the generalized semi-Markov processes introduced by Matthes [19] and investigated further by Konig, Matthes and Nawrotzki [15], [16], Konig and Jansen [17], Schassberger [23]-[25], Burman [6] and Fossett [8]. A GSMP moves from state to state with the destination and duration of each transition depending on which of several possible events associated with the occupied state occurs first. Several different events compete for causing the next jump and imposing their own particular jump distribution for determining the next state. An ordinary SMP (semi-Markov process) is the special case in which there is only one event associated with each state. At each transition of a GSMP, new events may be scheduled. For each of these new events, a clock indicating the time until the event is scheduled to occur is set by an independent chance mechanism. An event which is scheduled but does not initiate a transition is either abandoned or it is associated with the next state and its clock just continues running. We think of a GSMP as a model of discrete-event simulation. A good example of a GSMP is provided by the general multiple-heterogeneous-channel queue studied in Iglehart and Whitt [11]. A state could be the number of customers in the system and an indication of which servers are busy. Possible events associated with such a state would be an arrival in one of the arrival channels or a service completion by one of the occupied servers. With the usual independence assumptions and without any Markov assumptions, as in [11], this representation yields a GSMP which is not a SMP. Furthermore, this GSMP is not regenerative; there does not exist an embedded renewal process. (This statement may be confusing, however, because after appending appropriate supplementary variables to the GSMP we obtain an associated Markov process, and recent results of Athreya, McDonald and Ney [1], [2], and Nummelin [21] show that there will often exist a regenerative structure for this Markov process. At ",math
1430,Domain adaptation–can quantity compensate for quality?,Annals of Mathematics and Artificial Intelligence,"The Domain Adaptation problem in machine learning occurs when the distribution generating the test data differs from the one that generates the training data. A common approach to this issue is to train a standard learner for the learning task with the available training sample (generated by a distribution that is different from the test distribution). One can view such learning as learning from a not-perfectly-representative training sample. The question we focus on is under which circumstances large sizes of such training samples can guarantee that the learned classifier preforms just as well as one learned from target generated samples. In other words, are there circumstances in which quantity can compensate for quality (of the training data)? We give a positive answer, showing that this is possible when using a Nearest Neighbor algorithm. We show this under some assumptions about the relationship between the training and the target data distributions (the assumptions of covariate shift as well as a bound on the ratio of certain probability weights between the source (training) and target (test) distribution). We further show that in a slightly different learning model, when one imposes restrictions on the nature of the learned classifier, these assumptions are not always sufficient to allow such a replacement of the training sample: For proper learning, where the output classifier has to come from a predefined class, we prove that any learner needs access to data generated from the target distribution. © 2013 Springer Science+Business Media Dordrecht.",math
1431,Nonmonotonicity and Compatibility Relations in Belief Structures,Annals of Mathematics and Artificial Intelligence," We concern ourselves with the situation in which we use the Dempster-Shafer belief structure to provide a representation of a random variables in which our knowledge of the probability distribution is imprecise. We discuss the role of compatibility relations as a means of enabling inference about one variable, the secondary variable, based upon knowledge about another variable, the primary. We define monotonicity as a condition in which an increase in information about the primary variable in an inference should not result in a decrease in information about the secondary variable. We show what are the conditions required of a compatibility relation to lead to monotonic and nonmonotonic inferences. We provide some examples of nonmonotonic relations. ",math
1432,Differentiation of matrix functionals using triangular factorization,Mathematics of Computation," In various applications, it is necessary to differentiate a matrix functional w(A(x)), where A(x) is a matrix depending on a parameter vector x. Usually, the functional itself can be readily computed from a triangular factorization of A(x). This paper develops several methods that also use the triangular factorization to efficiently evaluate the first and second derivatives of the functional. Both the full and sparse matrix situations are considered. There are similarities between these methods and algorithmic differentiation. However, the methodology developed here is explicit, leading to new algorithms. It is shown how the methods apply to several applications where the functional is a log determinant, including spline smoothing, covariance selection and restricted maximum likelihood. ",math
1433,Efficient Querying and Animation of Periodic Spatio-Temporal Databases,Annals of Mathematics and Artificial Intelligence," We propose a representation of spatio-temporal objects with continuous and cyclic or acyclic periodic movements. We also describe an extended relational algebra query language for databases with such objects. We show that the new spatiotemporal databases are closed under the extended relational algebra queries, and each fixed relational algebra query can be evaluated in PTIME in the size of the input database. ",math
1434,Well-layered maps—A class of greedily optimizable set functions,Applied Mathematics Letters,"Given a finite set E of cardinality, say, N and a map f{hook}:P(E) →R {colon equals} R∪{-∞}, we define a sequence e1, ..., eN of elements of E to be f{hook}-greedy if ei ∉ {e1, ..., ei-1} and f{hook}({e1, ..., ei}) ≥ f{hook}({e1, ..., ei-1} ∪ {e}) for all i ∈ {1, ..., N} and all e ∈ E{plus 45 degree rule}{e1, ..., ei-1}. In addition, for any map η : E →R we put f{hook}η(x) {colon equals} f{hook}(x) + Σe∈xη(e) for all x ⊆ E, and we define f{hook} to be a well-layered map if for every η : E →R, every f{hook}η-greedy sequence e1, ..., eN ∈ E and every layer Pi(E) {colon equals} {y ⊆ E | #y = i} (i = 0, ..., N) we have f{hook}η({e1, ..., ei}) = max{f{hook}η(y) | y ∈ Pi(E)}. In this note, we characterize well-layered maps by some appropriate quantified version of the greedoid exchange condition or-equivalently-by some appropriate greedoidal version of the quantitative relations defining valuated matroids and valuated Δ-matroids, respectively. © 1995.",math
1435,Smoothing Techniques for Computing Nash Equilibria of Sequential Games,Mathematics of Operations Research," We develop first-order smoothing techniques for saddle-point problems that arise in finding a Nash equilibrium of sequential games. The crux of our work is a construction of suitable prox-functions for a certain class of polytopes that encode the sequential nature of the game. We also introduce heuristics that significantly speed up the algorithm, and decomposed game representations that reduce the memory requirements, enabling the application of the techniques to drastically larger games. An implementation based on our smoothing techniques computes approximate Nash equilibria for games that are more than four orders of magnitude larger than what prior approaches can handle. Finally, we show near-linear further speedups from parallelization. ",math
1436,How to Build a Prototype for a Distributed Digital Mathematics Archive Library,Annals of Mathematics and Artificial Intelligence,"The ""Electronic Mathematics Archiving Network Initiative"" (EMANI) recently announced by Springer Verlag, Tsinghua University Library (China), Göttingen State and University Library and Cornell University aims to insure the preservation and dissemination of mathematical information for future generations. In order to be able to use the resulting distributed digital mathematical archive, researchers have to be equipped with freely available readers which enable them to view digital downloaded articles and follow links to reviews in ""Zentralblatt für Mathematik"" or ""Mathematical Reviews"" of the papers they cite. Using then the permanent links between the review articles and their digital full texts on remote servers the mathematicians can also retrieve the digital full text of the cited articles. Such readers can only work if the scientific documents in the digital libraries have been provided with links between them and the review articles. In this article the different steps to solve these technical problems will be discussed. Methods will be presented which allow one to construct a prototype of a distributed searchable, linked mathematical digital archive library.",math
1437,A semantic approach to concept lattice-based information retrieval,Annals of Mathematics and Artificial Intelligence,"The volume of available information is growing, especially on the web, and in parallel the questions of the users are changing and becoming harder to satisfy. Thus there is a need for organizing the available information in a meaningful way in order to guide and improve document indexing for information retrieval applications taking into account more complex data such as semantic relations. In this paper we show that Formal Concept Analysis (FCA) and concept lattices provide a suitable and powerful support for such a task. Accordingly, we use FCA to compute a concept lattice, which is considered both a semantic index to organize documents and a search space to model terms. We introduce the notions of cousin concepts and classification-based reasoning for navigating the concept lattice and retrieve relevant information based on the content of concepts. Finally, we detail a real-world experiment and show that the present approach has very good capabilities for semantic indexing and document retrieval.",math
1438,M-Convex Function on Generalized Polymatroid,Mathematics of Operations Research," The concept of M-convex function, introduced recently by Murota, is a quantitative generalization of the set of integral points in an integral base polyhedron as well as an extension of valuated matroid of Dress{Wenzel (1990). In this paper, we extend this concept to functions on generalized polymatroids with a view to providing a uni ed framework for e ciently solvable nonlinear discrete optimization problems. The restriction of a function to fx 2 ZV j x(V ) = kg for k 2 Z is called a layer. We prove the M-convexity of each layer, and reveal that the minimizers in consecutive layers are closely related. Exploiting these properties, we can solve the optimization on layers e ciently. A number of equivalent exchange axioms are given for M-convex function on generalized polymatroid. ",math
1439,Linear Programming for Finite State Multi-Armed Bandit Problems,Mathematics of Operations Research," We consider the multi-armed bandit problem. We show that when the state space is finite the computation of the dynamic allocation indices can be handled by linear programming methods. 1. iBtrodactfMu An important sequential control problem with a tractable solution is the multi-armed bandit problem. It can be stated as follows. There are N independent projects, e.g., statistical populations (see Robbins 19S2), gambling machines (or bandits) etc.. The state of the pth of them at time t is denoted by x,it) and it belongs to a set of possible states S, which in this paper is assumed to be finite. Let 5, =: { 1 , . . . , A,}. At each point in time one can work on one project only and if the i>th of them is selected, one receives a reward r(f) = r'it) and its state changes according to a stationary transition rule: py = P ( x , ( / + l ) = y | x ^ ( / ) = /) whfle the states of all other projects remain unchanged: x,(< -f- 1) = x^it) iS K¥= P. Let xit) = ( x , ( f ) , . . . , x^it)) and let wit) denote the project selected at time /. The states of all projects are observable and the problem is to choose ir(/) as a function of xit), so as to maximize the expected total discounted reward, given an initial state x(0): This problem can be handled, in principle, by the methods of Markovian Decision Theory, see Derman (1970). However, a major difficulty in computations is the high dimension of the state ^>ace: KiX • • • xK/^. Gittins and Jones (1974) (cf. Gittins 1979, Whittle 1^0) have shown that an (q)timal policy has the following form. There exist numbers M,ii), k E S,, I < v < N, the dynamic allocation (or Gittins) indices, such that they define an optimal policy w"" as follows: 7r**(x(0) = ^ if and only if Af,(x,(O) = n;iax{Af^(x,(/)), 1 < ic < iV}. Furthermore the following two characterizations for Af,(i) were given. ",math
1440,Separable discrete preferences,Mathematical Social Sciences,"An ordering of multidimensional alternatives is separable on a set of dimensions if fixing values on the complementary dimensions always produces the same induced ordering. Most often, studies of separability assume continuous alternative spaces; as we show, separability has different properties when alternative spaces are discrete. For instance, two well-known theorems of Gorman - that common set operations preserve separability and that separable preferences are additive - fail for binary alternative spaces. Intersection is the only set operation that preserves separability. For binary alternative spaces, separability is equivalent to additivity if and only if there are four or fewer dimensions. © 2004 Elsevier B.V. All rights reserved.",math
1441,"Bounds for Dispersers, Extractors, and Depth-Two Superconcentrators",SIAM Journal on Discrete Mathematics," We show that the size of the smallest depth-two N -superconcentrator is Before this work, optimal bounds were known for all depths except two. For the upper bound, we build superconcentrators by putting together a small number of disperser graphs; these disperser graphs are obtained using a probabilistic argument. For obtaining lower bounds, we present two di erent methods. First, we show that superconcentrators contain several disjoint disperser graphs. When combined with the lower bound for disperser graphs of K}ovari, Sos, and Turan, this gives an almost optimal lower bound of ­(N (log N= log log N )2) on the size of N -superconcentrators. The second method, based on the work of Hansel, gives the optimal lower bound. The method of K}ovari, Sos, and Turan can be extended to give tight lower bounds for extractors, in terms of both the number of truly random bits needed to extract one additional bit and the unavoidable entropy loss in the system. If the input is an n-bit source with min-entropy k and the output is required to be within a distance of from uniform distribution, then to extract even one additional bit, one must invest at least log(n ¡ k) + 2 log(1= ) ¡ O(1) truly random bits; to obtain m output bits one must invest at least m ¡ k + 2 log(1= ) ¡ O(1). Thus, there is a loss of 2 log(1= ) bits during the extraction. Interestingly, in the case of dispersers this loss in entropy is only about log log(1= ). ",math
1442,Semi-Quantitative Comparative Analysis And Its Application,Annals of Mathematics and Artificial Intelligence, SQCA is an implemented technique for the semiquantitative comparative analysis of dynamical systems. It is both able to deal with incompletely specified models and make precise predictions by exploiting semi-quantitative information in the form of numerical bounds on the variables and functions occuring in the models. The technique has a solid mathematical foundation which facilitates proofs of correctness and convergence properties. SQCA represents the core of a method for the automated prediction of experimental results. ,math
1443,Randomized Pursuit-Evasion with Local Visibility,SIAM Journal on Discrete Mathematics," We study the following pursuit-evasion game: One or more hunters are seeking to capture an evading rabbit on a graph. At each round, the rabbit tries to gather information about the location of the hunters but it can see them only if they are located on adjacent nodes. We show that two hunters suffice for catching rabbits with such local visibility with high probability. We distinguish between reactive rabbits who move only when a hunter is visible and general rabbits who can employ more sophisticated strategies. We present polynomial time algorithms that decide whether a graph G is hunter-win, that is, if a single hunter can capture a rabbit of either kind on G. ",math
1444,Decay Of Correlations For Sparse Graph Error Correcting Codes,SIAM Journal on Discrete Mathematics," The subject of this paper is transmission over a general class of binary-input memoryless symmetric channels using error correcting codes based on sparse graphs, namely low-density generator-matrix and low-density parity-check codes. The optimal (or ideal) decoder based on the posterior measure over the code bits, and its relationship to the sub-optimal belief propagation decoder, are investigated. We consider the correlation (or covariance) between two codebits, averaged over the noise realizations, as a function of the graph distance, for the optimal decoder. Our main result is that this correlation decays exponentially fast for xed general low-density generator-matrix codes and high enough noise parameter, and also for xed general lowdensity parity-check codes and low enough noise parameter. This has many consequences. Appropriate performance curves - called GEXIT functions - of the belief propagation and optimal decoders match in high/low noise regimes. This means that in high/low noise regimes the performance curves of the optimal decoder can be computed by density evolution. Another interpretation is that the replica predictions of spin-glass theory are exact. Our methods are rather general and use cluster expansions rst developed in the context of mathematical statistical mechanics. ",math
1445,Efficient and binary consensus functions on transitively valued relations,Mathematical Social Sciences,"We give the general form of consensus functions on valued (or fuzzy) quasi-orders which satisfy two simple Arrow-like conditions of efficiency and binariness. Various previously known consensus results on the aggregation of preferences and mathematical classification may be derived from our result: directly, on quasi-orders (Mirkin), equivalences (Mirkin) or ultrametrics (Barthelemy, Leclerc and Monjardet); with further considerations, on complete quasi-orders or weak orders (Arrow; Mas-Collel and Sonnenschein) and orders (Brown). © 1984.",math
1446,Turing computability with neural nets,Applied Mathematics Letters," This paper shows the existence of a nite neural network, made up of sigmoidal neurons, which simulates a universal Turing machine. It is composed of less than 105 synchronously evolving processors, interconnected linearly. High-order connections are not required. ",math
1447,Relative Frequencies of Generalized Simulated Annealing,Mathematics of Operations Research," We consider a class of non-homogeneous Markov chains arising in simulated annealing and related stochastic search algorithms. Using only elementary first principles, we analyze the convergence and rate of convergence of the relative frequencies of visits to states in the Markov chain. We describe in detail three examples, including the standard simulated annealing algorithm, to show how our framework applies to specific stochastic search algorithms-these examples have not previously been recognized to be sufficiently similar to share common analytical grounds. Our analysis, though elementary, provides the strongest samplepath convergence results to date for simulated annealing type Markov chains. Our results serve to illustrate that by taking a purely sample-path view, surprisingly strong statements can be made using only relatively elementary tools. OR/MS subject classification: Primary: Stochastic methods of optimization, Secondary: Simulation 1. Introduction For at least the last 20 years, there has been an interest in stochastic search algorithms for global optimization based on non-homogeneous Markov chains. The prime example is simulated annealing, first suggested for optimization by Kirkpatrick et al. [12] based on techniques of Metropolis et al. [14]. An early application to image processing was described by Geman and Geman [9]. The basic procedure in simulated annealing is to explore the search space by setting up a graph over the space and jumping from point (vertex) to point in this graph according to a non-homogeneous Markov chain. The non-homogeneity arises from the gradually decreasing probability of jumping from one point to a “worse” point in the course of the search (but such a jump also cannot be precluded, because of the need to “climb out” of “cups” around local minimizers). The speed at which this decrease in the transition probabilities occurs depends on a sequence called the “cooling schedule” (described in more detail in Section 3). ",math
1448,Stable effectivity functions and perfect graphs,Mathematical Social Sciences,"We consider the problem of characterizing the stability of effectivity functions (EFF), via a correspondence between game theoretic and well-known combinatorial concepts. To every EFF we assign a pair of hypergraphs, representing clique covers of two associated graphs, and obtain some necessary and some sufficient conditions for the stability of EFFs in terms of graph-properties. These conditions imply, for example, that to check the stability of an EFF is an NP-complete problem. We also translate some well-known conjectures of graph theory into game theoretic language and vice versa. ÂŠ Elsevier Science B.V.",math
1449,ScaLAPACK's MRRR algorithm,ACM Transactions on Mathematical Software," The sequential algorithm of Multiple Relatively Robust Representations, MRRR, can compute numerically orthogonal eigenvectors of an unreduced symmetric tridiagonal matrix T ∈ Rn×n with O(n2) cost. This paper describes the design of ScaLAPACK's parallel MRRR algorithm. One emphasis is on the critical role of the representation tree in achieving both numerical accuracy and parallel scalability. A second point concerns the favorable properties of this code: subset computation, the use of static memory, and scalability. Unlike ScaLAPACK's Divide & Conquer and QR, MRRR can compute subsets of eigenpairs at reduced cost. And in contrast to inverse iteration which can fail, it is guaranteed to produce a numerically satisfactory answer while maintaining memory scalability. ParEig, the parallel MRRR algorithm for PLAPACK, uses dynamic memory allocation. This is avoided by our code at marginal additional cost. We also use a different representation tree criterion that allows for more accurate computation of the eigenvectors but can make parallelization more difficult. ",math
1450,Comparing Mathematical Provers,Mathematical Knowledge Management," We compare ¯fteen systems for the formalizations of mathematics with the computer. We present several tables that list various properties of these programs. The three main dimensions on which we compare these systems are: the size of their library, the strength of their logic and their level of automation. We realize that many judgments in this paper are rather subjective. We apologize in advance to anyone whose system is misrepresented here. We would like to be noti¯ed by e-mail of any errors in this paper at <freek@cs.kun.nl>. ",math
1451,A Nonparametric Asymptotic Analysis of Inventory Planning with Censored Demand,Mathematics of Operations Research," We study stochastic inventory planning with lost sales and instantaneous replenishment, where contrary to the classical inventory theory, the knowledge of the demand distribution is not available. Furthermore, we observe only the sales quantity in each period, and lost sales are unobservable, that is, demand data are censored. The manager must make an ordering decision in each period based only on historical sales data. Excess inventory is either perishable or carried over to the next period. In this setting, we propose non-parametric adaptive policies that generate ordering decisions over time. We show that the T -period average expected cost of our policy differs from the benchmark newsvendor cost - the minimum expected cost that would have incurred if the manager had known the underlying demand distribution - by at most O(1/√T ). ",math
1452,Reorthogonalization and stable algorithms for updating the Gram-Schmidt factorization,Mathematics of Computation,"Numerically stable algorithms are given for updating the GramSchmidt QR factorization of an m x n matrix A (m > n) when A is modified by a matrix of rank one, or when a row or column is inserted or deleted. The algorithms require 0(mn) operations per update, and are based on the use of elementary two-by-two reflection matrices and the Gram-Schmidt process with reorthogonalization. An error analysis of the reorthogonalization process provides rigorous justification for the corresponding ALGOL procedures. ÂŠ 1976, American Mathematical Society.",math
1453,Families of algorithms related to the inversion of a Symmetric Positive Definite matrix,ACM Transactions on Mathematical Software," We study the high-performance implementation of the inversion of a Symmetric Positive Definite (SPD) matrix on architectures ranging from sequential processors to Symmetric MultiProcessors to distributed memory parallel computers. This inversion is traditionally accomplished in three “sweeps”: a Cholesky factorization of the SPD matrix, the inversion of the resulting triangular matrix, and finally the multiplication of the inverted triangular matrix by its own transpose. We state different algorithms for each of these sweeps as well as algorithms that compute the result in a single sweep. One algorithm outperforms the current ScaLAPACK implementation by 20-30 percent due to improved load-balance on a distributed memory architecture. Categories and Subject Descriptors: G.4 [Mathematical Software]: -Efficiency Additional Key Words and Phrases: linear algebra, libraries, symmetric positive definite, inversion ACM Transactions on Mathematical Software, Vol. V, No. N, Month 20YY, Pages 1-22. ",math
1454,"Centered L 2 -discrepancy of random sampling and Latin hypercube design, and construction of uniform designs",Mathematics of Computation,"In this paper properties and construction of designs under a centered version of the L2-discrepancy are analyzed. The theoretic expectation and variance of this discrepancy are derived for random designs and Latin hypercube designs. The expectation and variance of Latin hypercube designs are significantly lower than that of random designs. While in dimension one the unique uniform design is also a set of equidistant points, low-discrepancy designs in higher dimension have to be generated by explicit optimization. Optimization is performed using the threshold accepting heuristic which produces low discrepancy designs compared to theoretic expectation and variance.",math
1455,Newton’s method in Banach spaces,Proceedings of the American Mathematical Society," ROBERT G. BARTLE Presented to the Society, December 29, 1954; received by the editors October 21, 1954 and, in revised form, December 9, 1954. 1 This paper was written while the author was on Contract nonr 609(04) with the Office of Naval Research. 2 Numbers in brackets refer to the list of references at the end. ",math
1456,Solving the $p$-Laplacian on manifolds.,Proceedings of the American Mathematical Society," We prove in this paper that the equation pu + h = 0 on a p-hyperbolic manifold M has a solution with p-integrable gradient for any bounded measurable function h : M ! R with compact support. The p-Laplacian of a function f on a connected oriented Riemannian manifold without boundary M is de ned by pf = div(jrf jp−2rf ); it is the Euler-Lagrange operator associated with the functional RM jrf jp: A function u 2 Wl1o;cp(M ) is said to be a weak solution to the equation We introduce the p-Dirichlet space L1;p(M ) of functions u 2 Wl1o;cp(M ) admitting a weak gradient such that RM krukp < 1. In [2], the following result has been proved: Theorem 1. Suppose that M is p-parabolic, and let h 2 L1(M ) be a function such that RM h 6= 0. Then (1) has no weak solution u 2 L1;p(M ). ",math
1457,DB-HReduction: A data preprocessing algorithm for data mining applications,Applied Mathematics Letters," |Data preprocessing is an important and critical step in the data mining process and it has a huge impact on the success of a data mining project. In this paper, we present an algorithm DBHReduction, which discretizes or eliminates numeric attributes and generalizes or eliminates symbolic attributes very e±ciently and e®ectively. This algorithm greatly decreases the number of attributes and tuples of the data set and improves the accuracy and decreases the running time of the data mining algorithms in the later stage. °c 2003 Elsevier Science Ltd. All rights reserved. ",math
1458,A More Portable Fortran Random Number Generator,ACM Transactions on Mathematical Software,"A Fortran implementation of a random number generator is described which produces a sequence of random mtegers that is machine mdependent as long as the machme can represent all mtegers m the interval [formula omitted]. ÂŠ 1979, ACM. All rights reserved.",math
1459,Partially ordered topological spaces,Proceedings of the American Mathematical Society," 1. Introduction. In this paper we shall consider topological spaces endowed with a reflexive, transitive, binary relation, which, following Birkhoff [l],2 we shall call a quasi order. It will frequently be assumed that this quasi order is continuous in an appropriate sense (see §2) as well as being a partial order. Various aspects of spaces possessing a quasi order have been investigated by L. Nachbin [6; 7; 8], Birkhoff [l, pp. 38-41, 59-64, 80-82, as well as the papers cited on those pages], and the author [13]. In addition, Wallace [10] has employed an argument using quasi ordered spaces to obtain a fixed point theorem for locally connected continua. This paper is divided into three sections, the first of which is concerned with definitions and preliminary results. A fundamental result due to Wallace, which will have later applications, is proved. §3 is concerned with compactness and connectedness properties of chains contained in quasi ordered spaces, and §4 deals with fixed point theorems for quasi ordered spaces. As an application, a new theorem on fixed sets for locally connected continua is proved. This proof leans on a method first employed by Wallace [10]. The author wishes to acknowledge his debt and gratitude to Professor A. D. Wallace for his patient and encouraging suggestions during the preparation of this paper. ",math
1460,On look-ahead heuristics in disjunctive logic programming,Annals of Mathematics and Artificial Intelligence,"Disjunctive logic programming (DLP), also called answer set programming (ASP), is a convenient programming paradigm which allows for solving problems in a simple and highly declarative way. The language of DLP is very expressive and able to represent even problems of high complexity (every problem in the complexity class Σ2 P = NPNP. During the last decade, efficient systems supporting DLP have become available. Virtually all of these systems internally rely on variants of the Davis-Putnam procedure (for deciding propositional satisfiability [SAT]), combined with a suitable model checker. The heuristic for the selection of the branching literal (i.e., the criterion determining the literal to be assumed true at a given stage of the computation) dramatically affects the performance of a DLP system. While heuristics for SAT have received a fair deal of research, only little work on heuristics for DLP has been done so far. In this paper, we design, implement, optimize, and experiment with a number of heuristics for DLP. We focus on different look-ahead heuristics, also called ""dynamic heuristics"" (the DLP equivalent of unit propagation [UP] heuristics for SAT). These are branching rules where the heuristic value of a literal Q depends on the result of taking Q true and computing its consequences. We motivate and formally define a number of look-ahead heuristics for DLP programs. Furthermore, since look-ahead heuristics are computationally expensive, we design two techniques for optimizing the burden of their computation. We implement all the proposed heuristics and optimization techniques in DLV-the state-of-the-art implementation of disjunctive logic programming, and we carry out experiments, thoroughly comparing the heuristics and optimization techniques on a large number of instances of well-known benchmark problems. The results of these experiments are very interesting, showing that the proposed techniques significantly improve the performance of the DLV system. © 2008 Springer Science+Business Media B.V.",math
1461,A parallel splitting up method and its application to Navier-Stokes equations,Applied Mathematics Letters,"A parallel splitting-up method (or the so called alternating-direction method) is proposed in this paper. The method not only reduces the original linear and nonlinear problems into a series of one dimensional linear problems, but also enables us to compute all these one dimensional linear problems by parallel processors. Applications of the method to linear parabolic problem, steady state and nonsteady state Navier-Stokes problems are given. © 1991.",math
1462,Algorithm portfolio selection as a bandit problem with unbounded losses,Annals of Mathematics and Artificial Intelligence," We propose a method that learns to allocate computation time to a given set of algorithms, of unknown performance, with the aim of solving a given sequence of problem instances in a minimum time. Analogous meta-learning techniques are typically based on models of algorithm performance, learned during a separate of f line training sequence, which can be prohibitively expensive. We adopt instead an online approach, named GambleTA, in which algorithm performance models are iteratively updated, and used to guide allocation on a sequence of problem instances. GambleTA is a general method for selecting among two or more alternative algorithm portfolios. Each portfolio has its own way of allocating computation time to the available algorithms, possibly based on performance models, in which case its performance is expected to improve over time, as more runtime data becomes available. The resulting exploration-exploitation trade-off is represented as a bandit problem. In our previous work, the algorithms corresponded to the arms of the bandit, and allocations evaluated by the different portfolios were mixed, using a solver for the bandit problem with expert advice, but this required the setting of an arbitrary bound on algorithm runtimes, invalidating the optimal regret of the solver. In this paper, we propose a simpler version of GambleTA, in which the allocators correspond to the arms, such that a single portfolio is selected for each instance. The selection is represented as a bandit problem with partial information, and an unknown bound on losses. We devise a solver for this game, proving a bound on its expected regret. We present experiments based on results from several ",math
1463,A generalization of a class of test matrices,Mathematics of Computation," We consider matrices of the following form: Gn(ai, o2, • • •, a„_i, 61, &2,• • •, K) = (ßi.i), 1 á i, j S n, where ai, ■• •, a„-i, 61, • • ■, b„ are constants and ßi.i = bj , j à i; ßij = aj, j < i. We deduce in analytic form the determinant, inverse matrix, characteristic equation, and eigenvectors of G». Knowing these properties enables us to generate valuable test matrices by appropriately selecting the order and elements of (7„. ",math
1464,A Distributional Interpretation of Robust Optimization,Mathematics of Operations Research," Motivated by data-driven decision making and sampling problems, we investigate probabilistic interpretations of Robust Optimization (RO). We establish a connection between RO and Distributionally Robust Stochastic Programming (DRSP), showing that the solution to any RO problem is also a solution to a DRSP problem. Specifically, we consider the case where multiple uncertain parameters belong to the same fixed dimensional space, and find the set of distributions of the equivalent DRSP. The equivalence we derive enables us to construct RO formulations for sampled problems (as in stochastic programming and machine learning) that are statistically consistent, even when the original sampled problem is not. In the process, this provides a systematic approach for tuning the uncertainty set. The equivalence further provides a probabilistic explanation for the common shrinkage heuristic, where the uncertainty set used in a RO problem is a shrunken version of the original uncertainty set. ",math
1465,An analysis of stochastic shortest path problems,Mathematics of Operations Research," We consider a stochastic version of the classical shortest path problem whereby for each node of a graph, we must choose a probability distribution over the set of successor nodes so as to reach a certain destination node with minimum expected cost. The costs of transition between successive nodes can be positive as well as negative. We prove natural generalizations of the standard results for the deterministic shortest path problem, and we extend the corresponding theory for undiscounted finite state Markovian decision problems by removing the usual restriction that costs are either all nonnegative or all nonpositive. ",math
1466,Incomplete types for logic databases,Applied Mathematics Letters,"We propose to extend Horn-clause terms by invisibly associating them with incomplete types, that is, convenient representations of taxonomic information. We formally define typed logic databases and show their uses to reduce the length of the proofs needed to deduce some kinds of facts, to provide intensional replies, to perform quick semantic agreement verifications on natural language queries, and to achieve partial execution of some queries. ÂŠ 1991.",math
1467,A formal analysis of interest-based negotiation,Annals of Mathematics and Artificial Intelligence,"In multi-agent systems (MAS), negotiation provides a powerful metaphor for automating the allocation and reallocation of resources. Methods for automated negotiation in MAS include auction-based protocols and alternating offer bargaining protocols. Recently, argumentation-based negotiation has been accepted as a promising alternative to such approaches. Interest-based negotiation (IBN) is a form of argumentation-based negotiation in which agents exchange (1) information about their underlying goals; and (2) alternative ways to achieve these goals. However, the usefulness of IBN has been mostly established in the literature by appeal to intuition or by use of specific examples. In this paper, we propose a new formal model for reasoning about interest-based negotiation protocols. We demonstrate the usefulness of this framework by defining and analysing two different IBN protocols. In particular, we characterise conditions that guarantee their advantage (in the sense of expanding the set of individual rational deals) over the more classic proposal-based approaches to negotiation. © Springer Science + Business Media B.V. 2009.",math
1468,Abductive logic programming agents with destructive databases,Annals of Mathematics and Artificial Intelligence,"In this paper we present an agent language that combines agent functionality with a state transition theory and model-theoretic semantics. The language is based on abductive logic programming (ALP), but employs a simplified state-free syntax, with an operational semantics that uses destructive updates to manipulate a database, which represents the current state of the environment. The language builds upon the ALP combination of logic programs, to represent an agents beliefs, and integrity constraints, to represent the agents goals. Logic programs are used to define macro-actions, intensional predicates, and plans to reduce goals to sub-goals including actions. Integrity constraints are used to represent reactive rules, which are triggered by the current state of the database and recent agent actions and external events. The execution of actions and the assimilation of observations generate a sequence of database states. In the case of the successful solution of all goals, this sequence, taken as a whole, determines a model that makes the agents goals and beliefs all true. ÂŠ 2011 Springer Science+Business Media B.V.",math
1469,Complexity results for answer set programming with bounded predicate arities and implications,Annals of Mathematics and Artificial Intelligence," Answer set programming is a declarative programming paradigm rooted in logic programming and non-monotonic reasoning. This formalism has become a host for expressing knowledge representation problems, which reinforces the interest in efficient methods for computing answer sets of a logic program. The complexity of various reasoning tasks for general answer set programming has been amply studied and is understood quite well. In this paper, we present a language fragment in which the arities of predicates are bounded by a constant. Subsequently, we analyze the complexity of various reasoning tasks and computational problems for this fragment, comprising answer set existence, brave and cautious reasoning, and strong equivalence. Generally speaking, it turns out that the complexity drops significantly with respect to the full non-ground language, but is still harder than for the respective ground or propositional languages. These results have several implications, most importantly for solver implementations: Virtually all currently available solvers have exponential (in the size of the input) space requirements even for programs with bounded predicate arities, while our results indicate that for those programs polynomial space should be sufficient. This can be seen as a manifestation of the “grounding bottleneck” (meaning that programs are first instantiated and Some results in this paper have been presented in preliminary form at KR 2004 [15]. ",math
1470,Improved long-period generators based on linear recurrences modulo 2,ACM Transactions on Mathematical Software,"Fast uniform random number generators with extremely long periods have been defined and implemented based on linear recurrences modulo 2. The twisted GFSR and the Mersenne twister are famous recent examples. Besides the period length, the statistical quality of these generators is usually assessed via their equidistribution properties. The huge-period generators proposed so far are not quite optimal in this respect. In this article, we propose new generators of that form with better equidistribution and ""bit-mixing"" properties for equivalent period length and speed. The state of our new generators evolves in a more chaotic way than for the Mersenne twister. We illustrate how this can reduce the impact of persistent dependencies among successive output values, which can be observed in certain parts of the period of gigantic generators such as the Mersenne twister. © 2006 ACM.",math
1471,Precise asymptotics for a series of T. L. Lai,Proceedings of the American Mathematical Society,"Let X, X, X1, X2... be i.i.d. random variables with EX = 0, and set Sn = X1 + ... + Xn. We prove that, for 1 < p < 3/2, ε→lim σ√2p-3 √ ε2 - σ2 (2p - 2) ∑ n≥2np-2 P(|Sn| ≥ ε√ n log n) = σ √2/p - 1 under the assumption that EX2 = σ 2 and E[|X|2p (log+ |X|)-P] < ∞. Necessary and sufficient conditions for the convergence of the sum above were established by Lai (1974).",math
1472,On isometries of Euclidean spaces,Proceedings of the American Mathematical Society," 1. Introduction. A transformation of a Euclidean line onto itself which preserves unit distances is not necessarily an isometry. This is shown by the simple example of the transformation which displaces all integral points one unit in the same direction and leaves all other points fixed. On the other hand, we show in this paper that a transformation of Euclidean n-space En (2^»<«i) which preserves a single nonzero length must be a Euclidean (rigid) motion of En onto En. It is an immediate consequence that the result also holds for all finite-dimensional unitary spaces. For, if it were false for a unitary space of dimension n, then it would be false for a Euclidean space of dimension 2m. That this result fails for EK, the (real) Hubert space of infinite sequences of real numbers {X} = {(xi, x2, • • • )} where ]C<"" 1** exists, is shown by the following example. There is in E°° a denumerable and everywhere dense set of points. Let such a set be denoted by { 7<}. Define a transformation, R, of E™into { 7,} in such a way that the distance d(X, RX)<l/2. Also, define a transformation, S, on {7<} so that SYi=Af where Ai = (flu, a»2, • • •) and Oiy= 5iy21/s/2 (8,7is the Kronecker delta). It may be readily seen that T=SR is a transformation of Ex into itself which preserves the unit distance. For, if d(Xi, X2) = l, then RXi¿¿RX2, TXi^TXi, and therefore d(TXu TX2) = l. However, not all distances are preserved. For, if Ai and A2 are any two points in E°°, then d(TXu TX2) is either 0 or 1. ",math
1473,Proving a distribution-free generalization of the Condorcet Jury Theorem,Mathematical Social Sciences,"We provide a proof for a result due to Grofman, Owen and Feld (1982), a distribution-free generalization of the Condorcet Jury Theorem (1785). In proving this result we show exactly what distribution of individual competence maximizes/minimizes the judgmental accuracy of group majority decision processes. © 1989.",math
1474,The numerical integration of ordinary differential equations,Mathematics of Computation," Multistep methods for initial value problems are expressed in a matrix form. The application of such methods to higher-order equations is studied with the result that new techniques for both first- and higher-order equations are found. The direct approach to higher-order equations is believed to offer speed and accuracy advantages; some numerical evidence is presented. The new technique applied to first-order equations is a slight extension of the conventional multistep method and avoids the Dahlquist [2] stability theorem, that is, these new fc-step methods are of order 2fc and yet convergent. The matrix formalism introduced provides an easy mechanism for examining the equivalence of methods as introduced by Descloux [3]. It is pointed out that the new first-order method on k- steps, Adams' method on (2fc - l)-steps and Nordsieck's [7] method with 2fc components are equivalent to each other. In fact, all methods discussed can be placed in equivalence classes so that theorems need only be proved for one member of each class. The choice between the members of a class can be made on the basis of round-off errors and amount of computation only. Arguments are given in favor of the extension of Nordsieck's method for general use because of its speed and applicability to higher order problems directly. The theorems ensuring convergence and giving the asymptotic form of the error are stated. The proofs can be found in a cited report. ",math
1475,The complexity of Markov decision processes,Mathematics of Operations Research," Due to the condition of the original material, there are unavoidable flaws in this reproduction. We have made every effort possible to provide you with the best copy available. If you are dissatisfied with this product and find it unusable, please contact Document Services as soon as possible. ",math
1476,On the adoption of innovations with ‘network’ externalities,Mathematical Social Sciences,"We present a simple dynamic model of adoption of an innovation when there are network externalities, that is, when one agents benefit from adoption increases with the number of other adopters. We assume there is a continuum of differentiated potential adopters who are perfectly informed rational agents. Our main conclusion is that if network externalities are strong, then the equilibrium adoption path is discontinuous, even when there is no coordination between potential adopters. We also argue that a steep S-shaped adoption path can be interpreted as the approximation to a discontinuous point (catastrophe) of the equilibrium adoption path. © 1990.",math
1477,A representation theorem for continuous functions of several variables,Proceedings of the American Mathematical Society," Theorem. For any natural number », »^2, there exist real, monotonic increasing functions, hv(x), l^p^n, dependent on », and having the following properties: (i) The function Received by the editors September 16, 1963. 1 This note is part of the author's Ph.D. thesis (University of Maryland, 1963, directed by A. Douglis); its research was supported by the United States Air Force through the AFOSR under Contract No. AF 49(638)-590. ",math
1478,A polyhedral method for solving sparse polynomial systems,Mathematics of Computation,"A continuation method is presented for computing all isolated roots of a semimixed sparse system of polynomial equations. We introduce mixed subdivisions of Newton polytopes, and we apply them to give a new proof and algorithm for Bernsteins theorem on the expected number of roots. This results in a numerical homotopy with the optimal number of paths to be followed. In this homotopy there is one starting system for each cell of the mixed subdivision, and the roots of these starting systems are obtained by an easy combinatorial construction. © 1995 American Mathematical Society.",math
1479,Algorithms for hyperbolic quadratic eigenvalue problems,Mathematics of Computation,"We consider the quadratic eigenvalue problem (or the QEP) (λ2 + λB + C)x = 0, where A,B, and C are Hermitian with A positive definite. The QEP is called hyperbolic if (x*Bx)2 > 4(x*Ax)(x*Cx) for all nonzero z ∈ ℂn. We show that a relatively efficient test for hyperbolicity can be obtained by computing the eigenvalues of the QEP. A hyperbolic QEP is overdamped if B is positive definite and C is positive semidefinite. We show that a hyperbolic QEP (whose eigenvalues are necessarily real) is overdamped if and only if its largest eigenvalue is nonpositive. For overdamped QEPs, we show that all eigenpairs can be found efficiently by finding two solutions of the corresponding quadratic matrix equation using a method based on cyclic reduction. We also present a new measure for the degree of hyperbolicity of a hyperbolic QEP. © 2005 American Mathematical Society.",math
1480,Iterative Dutch combinatorial auctions,Annals of Mathematics and Artificial Intelligence, The combinatorial auction problem can be modeled as a weighted set packing problem. Similarly the reverse combinatorial auction can be modeled as a weighted set covering problem. We use the set packing and set covering formulations to suggest novel iterative Dutch auction algorithms for combinatorial auction problems. We use generalized Vickrey auctions (GVA) with reserve prices in each iteration. We derive worst case bounds for these algorithms. ,math
1481,Learning directed probabilistic logical models: ordering-search versus structure-search,Annals of Mathematics and Artificial Intelligence,"We discuss how to learn non-recursive directed probabilistic logical models from relational data. This problem has been tackled before by upgrading the structure-search algorithm initially proposed for Bayesian networks. In this paper we show how to upgrade another algorithm for learning Bayesian networks, namely ordering-search. For Bayesian networks, ordering-search was found to work better than structure-search. It is non-obvious that these results carry over to the relational case, however, since there ordering-search needs to be implemented quite differently. Hence, we perform an experimental comparison of these upgraded algorithms on four relational domains. We conclude that also in the relational case ordering-search is competitive with structure-search in terms of quality of the learned models, while ordering-search is significantly faster. © Springer Science+Business Media B.V. 2009.",math
1482,On the discrete logarithm problem in class groups of curves,Mathematics of Computation," We study the discrete logarithm problem in degree 0 class groups of curves over finite fields, with particular emphasis on curves of small genus. We prove that for every fixed g ≥ 2, the discrete logarithm problem in degree 0 2 class groups of curves of genus g can be solved in an expected time of O˜(q2− g ), where Fq is the ground field. This result generalizes a corresponding result for hyperelliptic curves given in imaginary quadratic representation with cyclic degree 0 class group, and just as this previous result, it is obtained via an index calculus algorithm with double large prime variation. Generalizing this result, we prove that for fixed g0 ≥ 2 the discrete logarithm problem in class groups of all curves C/Fq of genus g ≥ g0 can be solved in an expected time of O˜((qg) g20 (1− g10 )) and in an expected time of As a complementary result we prove that for any fixed n ∈ N with n ≥ 2 the discrete logarithm problem in the groups of rational points of elliptic curves over finite fields Fqn , q a prime power, can be solved in an expected time of 2 O˜(q2− n ). Furthermore, we give an algorithm for the efficient construction of a uniformly randomly distributed effective divisor of a specific degree, given the curve and its L-polynomial. ",math
1483,Polynomials with small Mahler measure,Mathematics of Computation," We describe several searches for polynomials with integer coefcients and small Mahler measure. We describe the algorithm used to test Mahler measures. We determine all polynomials with degree at most 24 and Mahler measure less than 1:3, test all reciprocal and antireciprocal polynomials with height 1 and degree at most 40, and check certain sparse polynomials with height 1 and degree as large as 181. We nd a new limit point of Mahler measures near 1:309, four new Salem numbers less than 1:3, and many new polynomials with small Mahler measure. None has measure smaller than that of Lehmer's degree 10 polynomial. ",math
1484,Embedding complex decision procedures inside an interactive theorem prover,Annals of Mathematics and Artificial Intelligence,"As is well known, it is important to enrich the basic deductive machinery of an interactive theorem prover with complex decision procedures. Previous work pointed out that one of the most difficult problems is the integration of these decision procedures with the rest of the system. In particular, they should be flexible enough to be effectively usable when building new proof strategies. This paper describes a hierarchical and modular structure of procedures which can be either invoked individually or jointly with the others. To each combination of procedures, there corresponds a proof strategy particularly effective for a given class of formulae. Moreover, the functionalities provided by the procedures can be exploited in an effective way by user-defined proof strategies, whose design and mechanization are therefore greatly simplified. The implementation of the procedures is described and the problems faced in embedding them inside the GETFOL system are discussed. © 1993 J.C. Baltzer AG, Science Publishers.",math
1485,Rate of convergence of Lawson’s algorithm,Mathematics of Computation,The algorithm of Charles L. Lawson determines uniform approximations offunctions as limits of weighted L2 approximations. Lawson noticed from experimental evidence that the algorithm seemed to converge linearly and convergence was related to a factor which was the ratio of the largest nonmaximum error of the best uniform approximation to the maximum error. This paper proves the linear convergence and explores the relation of the rate of convergence to this ratio. ÂŠ 1972 American Mathematical Society.,math
1486,On metric Ramsey-type phenomena,Annals of Mathematics," The main question studied in this article may be viewed as a nonlinear analogue of Dvoretzky's theorem in Banach space theory or as part of Ramsey theory in combinatorics. Given a finite metric space on n points, we seek its subspace of largest cardinality which can be embedded with a given distortion in Hilbert space. We provide nearly tight upper and lower bounds on the cardinality of this subspace in terms of n and the desired distortion. Our main theorem states that for any > 0, every n point metric space contains a subset log(1/ ) of size at least n1− which is embeddable in Hilbert space with O distortion. The bound on the distortion is tight up to the log(1/ ) factor. We further include a comprehensive study of various other aspects of this problem. ",math
1487,Algorithm 856: APPSPACK 4.0: asynchronous parallel pattern search for derivative-free optimization,ACM Transactions on Mathematical Software," APPSPACK is software for solving unconstrained and bound-constrained optimization problems. It implements an asynchronous parallel pattern search method that has been specifically designed for problems characterized by expensive function evaluations. Using APPSPACK to solve optimization problems has several advantages: No derivative information is needed; the procedure for evaluating the objective function can be executed via a separate program or script; the code can be run serially or in parallel, regardless of whether the function evaluation itself is parallel; and the software is freely available. We describe the underlying algorithm, data structures, and features of APPSPACK version 4.0, as well as how to use and customize the software. Categories and Subject Descriptors: J.2 [Computer Applications]: Physical Sciences and Engineering-Mathematics and statistics Additional Key Words and Phrases: Parallel derivative-free optimization, pattern search ",math
1488,A generalized Banach contraction principle that characterizes metric completeness,Proceedings of the American Mathematical Society,We prove a fixed point theorem that is a very simple generalization of the Banach contraction principle and characterizes the metric completeness of the underlying space. We also discuss the Meir-Keeler fixed point theorem. © 2007 American Mathematical Society.,math
1489,Locally Determined Logic Programs and Recursive Stable Models,Annals of Mathematics and Artificial Intelligence," In general, the set of stable models of a recursive logic program can be quite complex. For example, it follows from results of Marek, Nerode, and Remme [17] that there exists nite predicate logic programs and recursive propositional logic programs which have stable models but no hyperarithmetic stable models. In this paper, we shall de ne several conditions which ensure that a recursive propositional logic program P has a stable model which is of low complexity, that is, a recursive stable model, a polynomial time stable model, or a stable model which lies in a low level of the polynomial time hierarchy. ",math
1490,Some New Aspects of the Coupon Collector's Problem,SIAM Journal on Discrete Mathematics," We extend the classical coupon collector's problem to one in which two collectors are simultaneously and independently seeking collections of d coupons. We find, in finite terms, the probability that the two collectors finish at the same trial, and we find, using the methods of GesselViennot, the probability that the game has the following “ballot-like” character: the two collectors are tied with each other for some initial number of steps, and after that the player who first gains the lead remains ahead throughout the game. As a by-product we obtain the evaluation in finite terms of certain infinite series whose coefficients are powers and products of Stirling numbers of the second kind. We study the variant of the original coupon collector's problem in which a single collector wants to obtain at least h copies of each coupon. Here we give a simpler derivation of results of Newman and Shepp and extend those results. Finally we obtain the distribution of the number of coupons that have been obtained exactly once (“singletons”) at the conclusion of a successful coupon collecting sequence. ",math
1491,Shapley-Shubik and Banzhaf Indices Revisited,Mathematics of Operations Research," IVIE working papers o®er in advance the results of economic research under way in order to encourage a discussion process before sending them to scienti¯c journals for their ¯nal publication. * We want to thank M. Maschler and J. M. Zarzuelo for their insightful comments. This research has been supported by the Training and Mobility of Researchers programme initiated by the European Commission, by the Instituto Valenciano de Investigaciones Econ¶omicas (IVIE), and by the DGES of the Spanish Ministerio de Educaci¶on y Cultura, under project PB96-0247. ** A. Laruelle: Universidad de Alicante; F. Valenciano: Universidad del Pa¶is Vasco. ",math
1492,Similarity in soft set theory,Applied Mathematics Letters,"We introduce and study the concept of similarity between soft sets, which is an extension of the equality for soft set theory. We also introduce the concepts of conjunction parameter (α∧β) and disjunction parameter (α∨β) of ordered pair parameter 〈α,β〉 for soft set theory, and we investigate modified operations of soft set theory in terms of ordered parameters. © 2011 Elsevier Ltd. All rights reserved.",math
1493,Anasazi software for the numerical solution of large-scale eigenvalue problems,ACM Transactions on Mathematical Software," Anasazi is a package within the Trilinos software project that provides a framework for the iterative, numerical solution of large-scale eigenvalue problems. Anasazi is written in ANSI C++ and exploits modern software paradigms to enable the research and development of eigensolver algorithms. Furthermore, Anasazi provides implementations for some of the most recent eigensolver methods. The purpose of our paper is to describe the design and development of the Anasazi framework. A performance comparison of Anasazi and the popular FORTRAN 77 code ARPACK is given. Categories and Subject Descriptors: G.1.3 [Numerical Analysis]: Numerical Linear Algebra; G.4 [Mathematical Software]: ; D.2.13 [Software Engineering]: Reusable Software Anasazi is a package within the Trilinos Project [Heroux et al. 2005] that uses ANSI C++ and modern software paradigms to implement algorithms for the numerical solution of large-scale eigenvalue problems. We define a large-scale eigenvalue problem to be one where a small number (relative to the dimension of the problem) of eigenvalues and the associated eigenspace are computed and only knowledge of the underlying matrix via application on a vector (or group of vectors) is assumed. Anasazi has been employed in a number of large-scale scientific codes, for example, performing modal analysis in the Salinas structural dynamics code [Bhardwaj et al. 2002] and stability analysis in LOCA [Salinger et al. 2005]. The purpose of this paper is to document and introduce the Anasazi eigensolver framework to prospective users. These users include practitioners and researchers in need of efficient, large-scale eigensolvers in a modern programming environment. This also includes experts who could exploit the framework provided by Anasazi ",math
1494,Practical Issues on the Projection of Polyhedral Sets,Annals of Mathematics and Artificial Intelligence," Projection of polyhedral sets is a fundamental operation in both geometry and symbolic computation. In most cases, however, it is not practically feasible to generate projections as the size of the output can be exponential in the size of the input. Even when the size of the output is manageable, we still face two serious problems: overwhelming redundancy and degeneracy. Here, we address these problems from a practical point of view. We discuss three algorithms based on algebraic and geometric techniques and we compare their performance in order to assess the feasibility of these approaches. ",math
1495,Evaluation of multivariate polynomials and their derivatives,Mathematics of Computation,An extension of Homer’s algorithm to the evaluation of m-variate polynomials and their derivatives is obtained. The schemes of computation are represented by trees because this type of graph describes exactly in which order the computations must be done. Some examples of algorithms for one and two variables are given. © 1990 American Mathematical Society.,math
1496,"Integers represented as the sum of one prime, two squares of primes and powers of 2",Proceedings of the American Mathematical Society,"In this short paper we prove that every sufficiently large odd integer can be written as a sum of one prime, two squares of primes and 83 powers of 2. © 2008 American Mathematical Society.",math
1497,Entry-deterring policy differentiation by electoral candidates,Mathematical Social Sciences," This paper studies the equilibria of a one-dimensional spatial model in which three candidates seek to maximize their probabilities of winning, are uncertain about the voters' preferences, and may move whenever they wish. In the presence of enough uncertainty there is an equilibrium in which two candidates enter simultaneously at distinct positions in the first period and either the third candidate does not enter or enters between the first two in the second period. 2000 Elsevier Science B.V. All rights reserved. 0165-4896 / 00 / $ - see front matter P I I : S 0 1 6 5 - 4 8 9 6 ( 9 9 ) 0 0 0 4 0 - 2 ",math
1498,Upwind difference schemes for hyperbolic systems of conservation laws,Mathematics of Computation,We derive a new upwind finite difference approximation to systems of nonlinear hyperbolic conservation laws. The scheme has desirable properties for shock calculations. Under fairly general hypotheses we prove that limit solutions satisfy the entropy condition and that discrete steady shocks exist which are unique and sharp. Numerical examples involving the Euler and Lagrange equations of compressible gas dynamics in one and two space dimensions are given. © 1982 American Mathematical Society.,math
1499,The Mathematical Semantic Web,Mathematical Knowledge Management," How can Mathematics and the Semantic Web effectively join? In this paper we provide an account of the key standard technologies that can foster the integration of Mathematical representation into the Semantic Web. Modern approaches to Mathematics on the web, nowadays, have in common with XML the well-known dualism between semantics and presentation: the media should be detached from the meaning ([1]). With the advent of the Semantic Web, however, the potential of this dualism seems to raise even higher, as that “semantics” part can possibly be more than itself, be part of a whole, be a single brick in a big building, the World Wide Web. For Math, this means that not only the Semantic Structure and Presentation Structure play their obvious classical role, but new possibilities come from the integration of various mathematical sources, allowing more powerful global search capabilities, but also associated metadata (context), possibility of computable math (algebraic manipulation / calculation) and so on. In order to achieve this, however, there is the need for integration: integration of the mathematical knowledge expertize/techniques, with the mainstream technologies that constitute the Semantic Web. Here, we focus on those Web technologies that could be potentially low-hanging fruits: RDF, OWL, XML-Schema, XML-Query and Functions and Operators. ",math
1500,The random walk construction of uniform spanning trees and uniform labelled trees,SIAM Journal on Discrete Mathematics," A random walk on a finite graph can be used to construct a uniform random spanning tree. We show how random walk techniques can be applied to the study of several properties of the uniform random spanning tree: the proportion of leaves, the distribution of degrees, and the diameter. ",math
1501,An overview of the Trilinos project,ACM Transactions on Mathematical Software," The Trilinos Project is an effort to facilitate the design, development, integration and ongoing support of mathematical software libraries within an object-oriented framework for the solution of large-scale, complex multi-physics engineering and scientific problems. Trilinos addresses two fundamental issues of developing software for these problems: (i) Providing a streamlined process and set of tools for development of new algorithmic implementations and (ii) promoting interoperability of independently developed software. Trilinos uses a two-level software structure designed around collections of packages. A Trilinos package is an integral unit usually developed by a small team of experts in a particular algorithms area such as algebraic preconditioners, nonlinear solvers, etc. Packages exist underneath the Trilinos top level, which provides a common look-and-feel, including configuration, documentation, licensing, and bug-tracking. Here we present the overall Trilinos design, describing our use of abstract interfaces and default concrete implementations. We discuss the services that Trilinos provides to a prospective package and how these services are used by various packages. We also illustrate how packages can be combined to rapidly develop new algorithms. Finally, we discuss how Trilinos facilitates highquality software engineering practices that are increasingly required from simulation software. Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed-Martin Company, for the United States Department of Energy under Contract DE-AC04-94AL85000. Permission to make digital/hard copy of all or part of this material without fee for personal or classroom use provided that the copies are not made or distributed for profit or commercial advantage, the ACM copyright/server notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists requires prior specific permission and/or a fee. c 2004 ACM 0098-3500/2004/1200-0001 $5.00 ",math
1502,OPT++: An object-oriented toolkit for nonlinear optimization,ACM Transactions on Mathematical Software,"Object-oriented programming is a relatively new tool in the development of optimization software. The code extensibility and the rapid algorithm prototyping capability enabled by this programming paradigm promise to enhance the reliability, utility, and ease of use of optimization software. While the use of object-oriented programming is growing, there are still few examples of general purpose codes written in this manner, and a common approach is far from obvious. This paper describes OPT++, a C++ class library for nonlinear optimization. The design is predicated on the concept of distinguishing between an algorithm-independent class hierarchy for nonlinear optimization problems and a class hierarchy for nonlinear optimization methods that is based on common algorithmic traits. The interface is designed for ease of use while being general enough so that new optimization algorithms can be added easily to the existing framework. A number of nonlinear optimization algorithms have been implemented in OPT++ and are accessible through this interface. Furthermore, example applications demonstrate the simplicity of the interface as well as the advantages of a common interface in comparing multiple algorithms. © 2007 ACM.",math
1503,Variation of the discrete eigenvalues of normal operators,Proceedings of the American Mathematical Society," The Hoifman-Wielandt inequality, which gives a bound for the distance between the spectra of two normal matrices, is generalized to normal operators A, B on a separable Hubert space, such that A - B is HilbertSchmidt. Let A, B be two «-by-n normal matrices. The famous Hoffman-Wielandt inequality states that the eigenvalues X,•, i = I, ... , n , of A and p¡, i = I, ... , n , of B can be ordered in such a way that Received by the editors January 10, 1994. 1991MathematicsSubjectClassification.Primary 47A55,47B10,47B15. ",math
1504,A sharp bound for solutions of linear Diophantine equations,Proceedings of the American Mathematical Society," Let Ax = b be an mxn system of linear equations with rank m and integer coefficients. Denote by Y the maximum of the absolute values of the m x m minors of the augmented matrix (A, b). It is proved that if the system has an integral solution, then it has an integral solution x = (x,) with max |jc,| < Y . The bound is sharp. The existence of small integral solutions to systems of linear equations with integral coefficients has been discussed previously in [1, 2,3,4,5,6,7,8, 11]. Two types of problems have been considered. In the first type the system is assumed to have a nonzero integer solution and the existence of a small solution is proved. A typical result of this type is the classical Siegel's Lemma [7] for homogeneous systems which has been used extensively in the theory of transcendental numbers. This result was generalized in [ 1] where the existence of a small integral basis for systems of linear homogeneous equations is proved. In the second type of problems the system is assumed to have a nontrivial nonnegative integral solution and the existence of a small solution with these properties is proved. More work has been devoted recently to this type because of its implications for the complexity of integer programming [11]. In [3] the conjecture was made that for the second type of problems a nonnegative integral solutions exists with components bounded by the pxp minors of the augmented matrix, where p is the rank of the matrix. This conjecture was proved in several special cases and weaker results were proved in the general case in [4, 5]; however, it is still open in the general case. In [6] the corresponding conjecture for the first type problem is discussed and proved under various additional conditions. In particular it is proved for ",math
1505,Design for the periphery,Mathematics of Computation," In everyday life, we are able to perform various activities simultaneously without consciously paying attention to them. In line with Weiser and Brown s [25] vision of calm technology, we see major opportunities to leverage these skills in interaction with technology by designing interactions that can take place in the periphery of our attention. In order to design such interactions however, a detailed understanding of human attention skills is important. This paper therefore provides an extensive theoretical background on attention theory and links this to the design of interactive systems. The aim is to lay a basis for design-research on interaction design for the periphery. ",math
1506,Law of Large Number Limits of Limited Processor-Sharing Queues,Mathematics of Operations Research," Motivated by applications in computer and communication systems, we consider a processor-sharing queue where the number of jobs served is not larger than K. We propose a measure-valued fluid model for this limited processor-sharing queue and show that there exists a unique associated fluid model solution. In addition, we show that this fluid model arises as the limit of a sequence of appropriately scaled processor-sharing queues. ",math
1507,Locally complete path independent choice functions and their lattices,Mathematical Social Sciences," The concept of path independence (PI) was first introduced by Arrow (1963) as a defense of his requirement that collective choices be rationalized by a weak ordering. Plott (1973) highlighted the dynamic aspects of PI implicit in Arrow's initial discussion. Throughout these investigations two questions, both initially raised by Plott, remained unanswered. What are the precise mathematical foundations for path independence? How can PI choice functions be constructed? We give complete answers to both these questions for finite domains and provide necessary conditions for infinite domains. We introduce a lattice associated with each PI function. For finite domains these lattices coincide with locally lower distributive or meet-distributive lattices and uniquely characterize PI functions. We also present an algorithm, effective and exhaustive for finite domains, for the construction of PI choice functions and hence for all finite locally lower distributive lattices. For finite domains, a PI function is rationalizable if and only if the lattice is distributive. The lattices associated with PI functions that satisfy the stronger condition of the weak axiom of revealed preference are chains of Boolean algebras and conversely. Those that satisfy the strong axiom of revealed preference are chains and conversely. ",math
1508,The Kronecker product of graphs,Proceedings of the American Mathematical Society," Introduction. This note considers a graph product derived from the Kronecker product of matrices. Some indication of the geometrical nature of this product is given and a theorem stating necessary and sufficient conditions for a graph product to be connected is proved. The matrix analogue of the above result is also stated. I. A convenient representation for a finite undirected graph [l] G is an adjacency matrix. If the vertex set of G is {pi], i= 1, • • • , », then an adjacency matrix of G is an »X« matrix (ay) with an equal to the number of lines (paths of length one) joining p{ to p¡. A given graph is then represented by an equivalence class of matrices A = {PítIPí""1! for all permutation matrices P¿ of order equal to the order of A}. Each element of A corresponds to a different ordering of the vertices of G. It is clear that for each such class of adjacency matrices there corresponds a unique class of isomorphic graphs. From this point on ""graph"" will mean a finite undirected graph with no loops. Such a graph has an adjacency matrix (an) whose entries are non-negative integers such that an = an and ati = 0. We also use the following notation: o(G) is the number of vertices of G and is called the order of G, pi-^>pk is a chain in G from vertex pi to vertex pk, and n(pi->pk) is the number of lines (not necessarily distinct) in pi-^pk. If A and B are two adjacency matrices and A o B is some matrix product which is also an adjacency matrix then this matrix operation ",math
1509,Are all linear paired comparison models empirically equivalent?,Mathematical Social Sciences,"Previous authors (Jackson and Fleckenstein, 1957; Mosteller, 1958; Noether, 1960) have found that different models of paired comparisons data lead to similar fits. This phenomenon is examined by means of a set of paired comparison models, based on gamma random variables, that includes the frequently applied Bradley-Terry and Thurstone-Mosteller models. A theoretical result provides a natural ordering of the models in the gamma family on the basis of their composition rules. Analysis of several sports data sets indicates that all of the paired comparison models in the family provide adequate, and almost identical fits to the data. Simulations are used to further explore this result. Although not all approaches to paired comparisons experiments are covered by this discussion, the evidence is strong that for samples of the size usually encountered in practice all linear paired comparison models are virtually equivalent. © 1992.",math
1510,Convergence rates of iterative treatments of partial differential equations,Mathematics of Computation," program orders. Unless, therefore, there are some orders in the store at the beginning of the computation, nothing can be taken in through the input, and the machine cannot start. For this reason, there is a sequence of orders, known as initial orders, permanently wired onto a set of uniselectors (rotary telephone switches). These orders can be transferred to the store by pressing a button. There is considerable latitude in the choice of the initial orders, although once they have been wired onto the uniselectors, it is not easy to change them. The initial orders used in the EDSAC at present enable orders punched in the following form to be taken in from the tape. First a letter indicating the function is punched, then the numerical part of the order in decimal form, and finally the letter F or D indicating, respectively, that the order refers to a long or a short number. If the order has no numerical part, it is punched simply as a letter followed by F. Under the control of the initial orders the machine converts the numerical part of the order to binary form and assembles the order with the function digits and the numerical digits in their correct relative positions. ",math
1511,A formal analysis of the role of multi-point crossover in genetic algorithms,Annals of Mathematics and Artificial Intelligence," On the basis of early theoretical and empirical studies. genetic algorithms have typically used I and Z-point crossover operators as the standard mechanisms for implementing recombination. However. there have been a number of recent studies. primarily empirical in nature, which have shown the benefits or crossover operators involving a higher number of crossover points. From a traditional theoretical point of view. the most surprising of these new results relate to uniform crossover. which involves on the average L /2 crossover points for strings of length L. In this paper we extend the existing theoretical results in an attempt to provide a broader explanatory and predictive theory of the role of multi-point crossover in genetic algorithms. In particular. we extend the traditional disruption analysis to include two general forms of multi-point crossover: n-point crossover and uniform crossover. We also analyze two other aspects or multi-point crossover operators, namely. their recombination potential and exploratory power. The results of this analysis provide a much clearer view .of the role of multi-point crossover in genetic algorithms. The implications of these results on implementation issues and performance arc discussed, and several directions for further research are suggested. ' ",math
1512,An overview of the sparse basic linear algebra subprograms: The new standard from the BLAS technical forum,ACM Transactions on Mathematical Software," MICHAEL A. HEROUX We discuss the interface design for the Sparse Basic Linear Algebra Subprograms (BLAS), the kernels in the recent standard from the BLAS Technical Forum that are concerned with unstructured sparse matrices. The motivation for such a standard is to encourage portable programming while allowing for library-specific optimizations. In particular, we show how this interface can shield one from concern over the specific storage scheme for the sparse matrix. This design makes it easy to add further functionality to the sparse BLAS in the future. We illustrate the use of the Sparse BLAS with examples in the three supported programming languages, Fortran 95, Fortran 77, and C. Categories and Subject Descriptors: G.1.3 [Numerical Analysis]: Numerical Linear Algebrasprase, structured, and very large systems (direct and iterative methods); G.4 [Mathematical Software]: user interfaces General Terms: Design, Standardization Additional Key Words and Phrases: Algorithms, computational kernels, software, sparse BLAS, sparse iterative methods, sparse matrices In this article, we consider the interface design of the Basic Linear Algebra Subprograms for sparse matrices: the Sparse BLAS. This work is part of the ",math
1513,On the ² convergence of an algorithm for solving finite element equations,Mathematics of Computation,An iterative method of multiple grid type is proposed for solving general finite element systems. It is proved that the method can produce a solution to the equations in 0(N) arithmetical operations where N is the number of unknowns. © 1977 American Mathematical Society.,math
1514,An introduction to numerical analysis,Mathematics of Computation," • Chapter 6: Numerical methods for ODE's • Chapter 8: Numerical solution of linear systems • Chapter 9: Numerical solution of eigenvalue problems Grading Policy: There will be two two-hour exams, one at the middle of the semester and one at the end of the semester. Written homework and computational problems will also be assigned. Late homework will be accepted only by special permission of the instructor. Each exam will count 20% of the course grade. The homework will count 30% and lab assignments will count 30% of the course grade. If you have a disability for which you are or may be requesting an accommodation, you are encouraged to contact both your instructor and Disability Resources and Services, 216 William Pitt Union, (412) 648-7890/(412) 383-7355 (TTY), as early as possible in the term. DRS will verify your disability and determine reasonable accommodations for this course. ",math
1515,A convexity condition in Banach spaces and the strong law of large numbers,Proceedings of the American Mathematical Society," Introduction. The strong law of large numbers can be shown under certain hypotheses for random variables which take values in Banach spaces. The general statement reads as follows: Theorem. Let H be a Banach space and let {Xi} be a sequence of independent random ^.-variables (see definition below) with £(X,)=0, all i > 0. Under appropriate conditions on ï and on {X{}, we can then assert that (1/n) T^Li Xj converges to 0 in the strong topology of ï almost surely.2 In a recent paper [l], this author showed this theorem under the hypotheses that ï is uniformly convex and that the variances of Xi are uniformly bounded (Var(Xi) =E(||Xi||2)). At the same time, an example was given of a space in which the theorem fails. It is now possible, using the methods of [l], to show a necessary and sufficient condition on the Banach space ï to yield this particular strong law of large numbers. A Banach space 3Éis said to have property (A) if, for every sequence {Xi} oí independent random X-variables with E(Xi)=0, all i, and Var(Xi) <M, all i, we have A Banach space ï is said to have property (B) if there exists an integer k>0 and an e>0 such that any choice ai, a2, • • ■ , a^ of elements from 36with \\ai\\ ^ 1 gives us ||+ffli + a2 + • ■ • ± a*|| < k(l - e) for some combination of the + and - signs. We shall show that these two conditions are equivalent. 1. Definitions. Let ï be a separable Banach space Presented to the Society, January 23, 1961; received by the editors February 1961. 1 This research was supported by Cornell University under contract with the Office of Naval Research and by the University of Wisconsin under contract number AF 49(638)-868 with the Air Force Officeof Scientific Research. 1 Here and hereafter in this paper unless otherwise specified, all limits are taken as n->oo. ",math
1516,Random Order Congestion Games,Mathematics of Operations Research,"We introduce a new class of games called random order congestion games (ROCGs). In an ROCG, each player has a task that can be carried out by any element of a set of resources, and each resource executes its assigned tasks in a random order. The aim of each player is to minimize his expected cost, which is the sum of the fixed costs over the set of his utilized resources and the expected cost of his task execution. The cost of a players task execution is determined by the earliest time his task is completed, and thus it might be beneficial for him to assign his task to several resources. We prove the existence of pure strategy Nash equilibria in ROCGs. Moreover, we present a polynomial time algorithm for finding such an equilibrium in a given ROCG. ÂŠ2009 INFORMS.",math
1517,An algorithm for the second immanant,Mathematics of Computation,"Let x be an irreducible character of the symmetric group S. For A = (aj) an n-by-n matrix, define the immanant of A corresponding to x by The article contains an algorithm for computing d(A) when x corresponds to the partition. ÂŠ 1984 American Mathematical Society.",math
1518,A starting point strategy for nonlinear interior methods,Applied Mathematics Letters,"This paper presents a strategy for choosing the initial point, slacks, and multipliers in interior methods for nonlinear programming. It consists of first computing a Newton-like step to estimate the magnitude of these three variables and then shifting the slacks and multipliers so that they are sufficiently positive. The new strategy has the option of respecting the initial estimate of the solution given by the user, and attempts to avoid the introduction of artificial nonconvexities. Numerical experiments on a large test set illustrate the performance of the strategy. © 2004 Elsevier Ltd. All rights reserved.",math
1519,Folder Complexes and Multiflow Combinatorial Dualities,SIAM Journal on Discrete Mathematics," In multi ow maximization problems, there are several combinatorial duality relations, such as Ford-Fulkerson's max- ow min-cut theorem for single commodity ows, Hu's max-bi ow min-cut theorem for two-commodity ows, Lovasz-Cherkassky duality theorem for free multi ows, and so on. In this paper, we provide a uni ed framework for such multi ow combinatorial dualities by using the notion of a folder complex, which is a certain 2-dimensional polyhedral complex introduced by Chepoi. We show that for a nonnegative weight on terminal set, -weighted maximum multi ow problem admits a combinatorial duality relation if and only if is represented by distances between certain subsets in a folder complex, and show that the corresponding combinatorial dual problem is a discrete location problem on the graph of the folder complex. This extends a result of Karzanov in the case of metric weights. ",math
1520,An isoperimetric theorem on the cube and the Kintchine-Kahane inequalities,Proceedings of the American Mathematical Society,"For vectors x1, …, xn i;…anach space, we bound the deviation of ‖Σi≤n εixi‖ from its median. © 1988 American Mathematical Society.",math
1521,On numerical integration of ordinary differential equations,Mathematics of Computation," A reliable efficient general-purpose method for automatic digital computer integration of systems of ordinary differential equations is described. The method operates with the current values of the higher derivatives of a polynomial approximating the solution. It is thoroughly stable under all circumstances, incorporates automatic starting and automatic choice and revision of elementary interval size, approximately minimizes the amount of computation for a specified accuracy of solution, and applies to any system of differential equations with derivatives continuous or piece wise continuous with finite jumps. ILLIAC library subroutine #F7, University of Illinois Digital Computer Laboratory, is a digital computer program applying this method. 1. Introduction. A typical common scientific application of automatic digital computers is the integration of systems of ordinary differential equations. The author has developed a general-purpose method for doing this and explains the method here. While it is primarily designed to optimize the efficiency of large-scale calculations on automatic computers, its essential procedures also lend themselves well to hand computation. The method has the following characteristics, all of which are requisite to a satisfactory general-purpose method: a. Thorough stability with a large margin of safety under all circumstances. (Instabilities in the subject differential equations themselves are, of course, reflected in the solution, but no further instabilities are introduced by the numerical procedures.) b. Any integration is started with only the essential initial conditions, i.e. there is a built-in automatic starting procedure. c. An optimum elementary interval size is automatically chosen, and the choice is automatically revised either upward or downward in the course of an integration, to provide the specified accuracy of solution in the minimum number of elementary steps. d. The derivatives need be computed just twice per elementary step, which is the minimum consistent with controlling accuracy. e. Any system of equations Received May 6, 1961. The research presented in this paper was supported U. S. Army, Navy and Air Force. ",math
1522,Sequential minimax search for a maximum,Proceedings of the American Mathematical Society," The problem formulated below was motivated by that of determining an interval containing the point at which a unimodal function on the unit interval possesses a maximum, without postulating regularity conditions involving continuity, derivatives, etc. Our solution is to give, for every e > 0 and every specified number N of values of the argument at which the function may be observed, a procedure which is €-minimax (see (1) below) among the class of all sequential nonrandomized procedures which terminate by giving an interval containing the required point, where the payoff of the computer to nature is the length of this final interval. (The same result holds if, e.g., we consider all nonrandomized procedures and let the payoff be length of interval plus c or 0 according to whether the interval fails to cover or covers the desired point, where c^l/Uif+i, the latter being defined below.) The analogous problem where errors are present in the observations was considered in [l ], but no optimum results are yet known for that more difficult case. Search for a maximum is a ""second-order"" search in the sense that information is given by pairs of observations. Thus, if Xi<x2 and f(xi)^f(x2), and / is a member of the class J described below, the point xU) defined below (the point where / attains its maximum if the latter exists) must lie to the left of x2. If we postulate only that fEJy this is essentially the only information we have about jetf): i.e., that future observations should be taken to the left of x2. Similarly, the problem of finding the point xvl at which a strictly increasing function / on the unit interval attains a value a (weaker restrictions on / can be made) is essentially first-order in the sense that every single observation gives information about where to take the next. (See [2; 3] for the corresponding statistical problem.) The minimax procedure in that case is successively to split the interval in which xV] is known to lie into equal parts at the point of next observation, in an obvious manner. It may be fruitful to consider higherorder search problems, as will be done in a future paper. Let the class J consist of every function / from the closed unit interval I into the reals (R) and for which there is an x^EI such that/ is either strictly increasing for x^xU) and strictly decreasing for x>x<J), or else strictly increasing for x <x(/) and strictly decreasing Received by the editors September 17, 1952. 1 Research under a contract with the ONR. ",math
1523,The median procedure in cluster analysis and social choice theory,Mathematical Social Sciences,"Classical approachs for fitting and aggregation problems, specially in cluster analysis, social choice theory and paired comparisons methods, consist in the minimization of a remoteness function between relational data and a relational model. The notion of median, with its algebraic, metric, geometrical and statistical aspects, allow a unified treatment of many of base problems. Properties of median procedures are organized according to four directions: stabilities and axiomatic characterizations; Arrow-like properties; combinatorial properties; effective computational possibilities. Finally, interesting mathematical problems, related to the notion of median are surveyed. © 1981.",math
1524,Using Formal Concept Analysis in Mathematical Discovery,Mathematical Knowledge Management," Formal concept analysis (FCA) comprises a set of powerful algorithms which can be used for data analysis and manipulation, and a set of visualisation tools which enable the discovery of meaningful relationships between attributes of the data. We explore the potential of combining FCA and mathematical discovery tools in order to better facilitate discovery tasks. In particular, we propose a novel lookup method for the Encyclopedia of Integer Sequences, and we show how conjectures from the Graffiti discovery program can be better understood using FCA visualisation tools. We argue that, not only can FCA tools greatly enhance the management and visualisation of mathematical knowledge, but they can also be used to drive exploratory processes. ",math
1525,Proving BDI Properties of Agent-Oriented Programming Languages,Annals of Mathematics and Artificial Intelligence,"In this paper, we consider each of the nine BDI principles defined by Rao and Georgeff based on Bratmans asymmetry thesis, and we verify which ones are satisfied by Raos AgentSpeak(L), a logic programming language inspired by the BDI architecture for cognitive agents. In order to set the grounds for the proofs, we first introduce a rigorous way in which to define the informational, motivational, and deliberative modalities of BDI logics for AgentSpeak(L) agents, according to its structural operational semantics that we introduced in a recent paper. This computationally grounded semantics for the BDI modalities forms the basis of a framework that can be used to further investigate BDI properties of AgentSpeak(L) agents, and contributes towards establishing firm theoretical grounds for a BDI approach to agent-oriented programming.",math
1526,Propagation failure in the discrete Nagumo equation,Proceedings of the American Mathematical Society," We address the classical problem of propagation failure for monotonic fronts of the discrete Nagumo equation. For a special class of nonlinearities that support unpinned \translationally invariant"" stationary monotonic fronts, we prove that propagation failure cannot occur. Properties of travelling fronts in the discrete Nagumo equation with such special nonlinear functions appear to be similar to those in the continuous Nagumo equation. ",math
1527,Analysis of numerical methods,Mathematics of Computation, NEW YORK UNIVERSITY ,math
1528,A formal mathematical framework for modeling probabilistic hybrid systems,Annals of Mathematics and Artificial Intelligence,"The development of autonomous agents, such as mobile robots and software agents, has generated considerable research in recent years. Robotic systems, which are usually built from a mixture of continuous (analog) and discrete (digital) components, are often referred to as hybrid dynamical systems. Traditional approaches to real-time hybrid systems usually define behaviors purely in terms of determinism or sometimes non-determinism. However, this is insufficient as real-time dynamical systems very often exhibit uncertain behavior. To address this issue, we develop a semantic model, Probabilistic Constraint Nets (PCN), for probabilistic hybrid systems. PCN captures the most general structure of dynamic systems, allowing systems with discrete and continuous time/variables, synchronous as well as asynchronous event structures and uncertain dynamics to be modeled in a unitary framework. Based on a formal mathematical paradigm exploiting abstract algebra, topology and measure theory, PCN provides a rigorous formal programming semantics for the design of hybrid real-time embedded systems exhibiting uncertainty. © Springer Science+Business Media, Inc. 2007.",math
1529,Modular multiplication without trial division,Mathematics of Computation,"Let N1. We present a method for multiplying two integers (called N-residues) modulo N while avoiding division by N. N-residues are represented in a nonstandard way, so this method is useful only if several computations are done modulo one N. The addition and subtraction algorithms are unchanged. ÂŠ 1985 American Mathematical Society.",math
1530,Extensions of Forsythe’s method for random sampling from the normal distribution,Mathematics of Computation," This article is an expansion of G. E. Forsythe's paper ""Von Neumann's comparison method for random sampling from the normal and other distributions"" [5]. It is shown that Forsythe's method for the normal distribution can be adjusted so that the average number Ñ of uniform deviates required drops to 2.53947 in spite of a shorter program. In a further series of algorithms, Ñ is reduced to values close to 1 at the expense of larger tables. Extensive computational experience is reported which indicates that the new methods compare extremely well with known sampling algorithms for the normal distribution. 1. Introduction. This article was originally intended to be a joint paper of G. E. Forsythe and the authors. It grew out of discussions following a presentation of [2] and [4] at Stanford in October 1971. The latter paper stimulated Forsythe to modify J. von Neumann's classical method for the generation of exponential variables (in [9]). In the resulting Stanford report, a surprisingly general sampling algorithm was presented whose specialization to the normal distribution was then programmed by the authors and subsequently modified. Professor Forsythe was enthusiastic about the alternative versions and proposed to write a comprehensive joint paper. His untimely death has rendered this project impossible. Instead, the Stanford report has been submitted separately (as [5]), and the present paper is dedicated to Forsythe's memory. Its main purpose is to remove all objections to the practical applicability of the method in [5] which in its original form required too many samples from the uniform distribution. The paper starts with a restatement of Forsythe's generalization of von Neumann's comparison method. A neater proof is given for the validity of the approach. The calculation of the expected number Ñ of uniform deviates required is also done in a shorter way. Subsequently, this quantity Ñ is considered in more detail for the special case of the normal distribution. It is shown that A? may be decreased by means of suitable subdivisions of the range [0, <»). In the central part (Sections 4 and 5), Forsythe's special algorithm for the normal distribution (called FS) is embedded in a series of sampling procedures which range from the table-free center-tail method (CT) through an improvement of FS (called FT) to algorithms that require longer tables (FL). For the transition from FT to FL, a Received July 7, 1972. AMS (MOS) subject classifications(1970). Primary 65C10; Secondary 68A55. ",math
1531,Exploiting multivalued knowledge in variable selection heuristics for SAT solvers,Annals of Mathematics and Artificial Intelligence,"We show that we can design and implement extremely efficient variable selection heuristics for SAT solvers by identifying, in Boolean clause databases, sets of Boolean variables that model the same multivalued variable and then exploiting that structural information. In particular, we define novel variable selection heuristics for two of the most competitive existing SAT solvers: Chaff, a solver based on look-back techniques, and Satz, a solver based on look-ahead techniques. Our heuristics give priority to Boolean variables that belong to sets of variables that model multivalued variables with minimum domain size in a given state of the search process. The empirical investigation conducted to evaluate the new heuristics provides experimental evidence that identifying multivalued knowledge in Boolean clause databases and using variable selection heuristics that exploit that knowledge leads to large performance improvements. © 2007 Springer Science+Business Media B.V.",math
1532,On Boundedness of Q-Learning Iterates for Stochastic Shortest Path Problems,Mathematics of Operations Research," We consider a totally asynchronous stochastic approximation algorithm, Q-learning, for solving nite space stochastic shortest path (SSP) problems, which are total cost Markov decision processes with an absorbing and cost-free state. For the most commonly used SSP models, existing convergence proofs assume that the sequence of Q-learning iterates is bounded with probability one, or some other condition that guarantees boundedness. We prove that the sequence of iterates is naturally bounded with probability one, thus furnishing the boundedness condition in the convergence proof by Tsitsiklis [Tsi94] and establishing completely the convergence of Q-learning for these SSP models. ",math
1533,Introduction to numerical analysis,Mathematics of Computation," 3 s.h. (Prerequisites: 1701.210 Linear Algebra, 1701.231 Ordinary Differential Equations (or concurrently) and prior computer programming experience) This course includes: elements of error analysis, real roots of an equation, polynomial approximation by finite difference and least square methods, interpolation, quadrature, numerical solution of ordinary differential equations, and numerical solutions of systems of linear equations. The student should expect to program a computer in addition to using a graphing calculator. The purpose of numerical analysis is two-fold: (1) to find acceptable approximate solutions when exact solutions are either impossible or so arduous and time-consuming as to be impractical, and (2) to devise alternate methods of solution better suited to the capabilities of computers. While this course will involve the student in considerable computation in order to apply techniques and obtain acceptable answers, the main emphasis will be on the underlying theory. It will be necessary to draw upon a good bit of calculus, linear algebra, computer science and other branches of mathematics during the course. ",math
1534,A stability analysis of incomplete LU factorizations,Mathematics of Computation,"The combination of iterative methods with preconditionings based on incomplete LU factorizations constitutes an effective class of methods for solving the sparse linear systems arising from the discretization of elliptic partial differential equations. In this paper, we show that there are some settings in which the incomplete LU preconditioners are not effective, and we demonstrate that their poor performance is due to numerical instability. Our analysis consists of an analytic and numerical study of a sample two-dimensional non-selfadjoint elliptic problem discretized by several finite-difference schemes. © 1986 American Mathematical Society.",math
1535,Voting games and acyclic collective choice rules,Mathematical Social Sciences,The subject of this paper is the representation of collective choice rules by voting games and the acyclicity of these rules. A collective choice rule is a function that associates a collective preference with every profile of individual preferences. Such a rule is acyclic if it always yields an acyclic collective preference. A voting game is either a simple game or a non-neutral version of such a game called a binary game in constitutional form. Both are special forms of cooperative games that simply specify the structure of power in a society or an organization. The power structure conferred by certain collective choice rules takes the form of a voting game. The paper classifies collective choice rules that can or cannot be represented by a voting game. Conditions for the acyclicity of the collective choice rules that can be represented by a voting game are then obtained from the structure of their corresponding voting games. These results are applied to a large class of voting rules defined by quotas. © 1995.,math
1536,Comparing Top k Lists,SIAM Journal on Discrete Mathematics," Motivated by several applications, we introduce various distance measures between “top k lists.” Some of these distance measures are metrics, while others are not. For each of these latter distance measures, we show that they are “almost” a metric in the following two seemingly unrelated aspects: (i) they satisfy a relaxed version of the polygonal (hence, triangle) inequality, and (ii) there is a metric with positive constant multiples that bound our measure above and below. This is not a coincidence-we show that these two notions of almost being a metric are same. Based on the second notion, we define two distance measures to be equivalent if they are bounded above and below by constant multiples of each other. We thereby identify a large and robust equivalence class of distance measures. Besides the applications to the task of identifying good notions of (dis-)similarity between two top k lists, our results imply polynomial-time constant-factor approximation algorithms for the rank aggregation problem with respect to a large class of distance measures. ",math
1537,On the observed rate of convergence of an iterative method applied to a model elliptic difference equation,Mathematics of Computation,A proof is given of the fact that the rate of convergence of a multiple grid type of algorithm is O(h1/2) in the case of a model elliptic difference equation. © 1978 American Mathematical Society.,math
1538,Role colouring a graph,Mathematical Social Sciences, The role colouring of a graph is an assignment of colours to the vertices which obeys the rule that two vertices are coloured the same only if their neighbourhoods have the same colour set. We investigate the set of role colourings for a graph proving that it forms a lattice. We also show that this lattice can be trivial and this can only occur if the graph has a trivial automorphism group. group. ,math
1539,Every complete doubling metric space carries a doubling measure,Proceedings of the American Mathematical Society, We prove that a complete metric space X carries a doubling measure if and only if X is doubling and that more precisely the in ma of the homogeneity exponents of the doubling measures on X and of the homogeneity exponents of X are equal. We also show that a closed subset X of Rn carries a measure of homogeneity exponent n. These results are based on the case of compact X due to Vol0berg and Konyagin. ,math
1540,An algorithm for a minimum cover of a graph,Proceedings of the American Mathematical Society," Presented to the Society October 26, 1957; received by the editors January 1958. 1 Supported in part by an Office of Naval Research Logistics Project at Princeton University. 2 Now at the Hebrew University in Jerusalem. ",math
1541,FLAME: Formal Linear Algebra Methods Environment,ACM Transactions on Mathematical Software,"Since the advent of high-performance distributed-memory parallel computing, the need for intelligible code has become ever greater. The development and maintenance of libraries for these architectures is simply too complex to be amenable to conventional approaches to implementation. Attempts to employ traditional methodology have led, in our opinion, to the production of an abundance of anfractuous code that is difficult to maintain and almost impossible to upgrade. Having struggled with these issues for more than a decade, we have concluded that a solution is to apply a technique from theoretical computer science, formal derivation, to the development of high-performance linear algebra libraries. We think the resulting approach results in aesthetically pleasing, coherent code that greatly facilitates intelligent modularity and high performance while enhancing confidence in its correctness. Since the technique is language-independent, it lends itself equally well to a wide spectrum of programming languages (and paradigms) ranging from C and Fortran to C++ and Java. In this paper, we illustrate our observations by looking at the Formal Linear Algebra Methods Environment (FLAME), a framework that facilitates the derivation and implementation of linear algebra algorithms on sequential architectures. This environment demonstrates that lessons learned in the distributed-memory world can guide us toward better approaches even in the sequential world. We present performance experiments on the Intel (R) Pentium (R) III processor that demonstrate that high performance can be attained by coding at a high level of abstraction.",math
1542,Iterative-deepening search with on-line tree size prediction,Annals of Mathematics and Artificial Intelligence,"The memory requirements of best-first graph search algorithms such as A* often prevent them from solving large problems. The best-known approach for coping with this issue is iterative deepening, which performs a series of bounded depth-first searches. Unfortunately, iterative deepening only performs well when successive cost bounds visit a geometrically increasing number of nodes. While it happens to work acceptably for the classic sliding tile puzzle, IDA* fails for many other domains. In this paper, we present an algorithm that adaptively chooses appropriate cost bounds on-line during search. During each iteration, it learns a model of the search tree that helps it to predict the bound to use next. Our search tree model has three main benefits over previous approaches: (1) it will work in domains with real-valued heuristic estimates, (2) it can be trained on-line, and (3) it is able to make more accurate predictions with only a small number of training examples. We demonstrate the power of our improved model by using it to control an iterative-deepening A* search on-line. While our technique has more overhead than previous methods for controlling iterative-deepening A*, it can give more robust performance by using its experience to accurately double the amount of search effort between iterations. © 2013 Springer Science+Business Media Dordrecht.",math
1543,A Critical Comparison of Some Methods for Interpolation of Scattered Data,Mathematics of Computation, REPRODUCED FROM BEST AVAILABLE Copy Richard Franke Final Report for Period January - March 1979 Approved for public release; distribution unlimited ,math
1544,Stochastic Depletion Problems: Effective Myopic Policies for a Class of Dynamic Optimization Problems,Mathematics of Operations Research," This paper presents a general class of dynamic stochastic optimization problems we refer to as Stochastic Depletion Problems. A number of challenging dynamic optimization problems of practical interest are stochastic depletion problems. Optimal solutions for such problems are di cult to obtain, both from a pragmatic computational perspective as also from a theoretical perspective. As such, simple heuristics are desirable. We isolate two simple properties that, if satis ed by a problem within this class, guarantee that a myopic policy incurs a performance loss of at most 50 % relative to the optimal adaptive control policy for that problem. We are able to verify that these two properties are satis ed for several interesting families of stochastic depletion problems and as a consequence identify computationally e cient approximations to optimal control policies for a number of interesting dynamic stochastic optimization problems. ",math
1545,Should social network structure be taken into account in elections?,Mathematical Social Sciences,"If the social network structure among the voters in an election is known, how should this be taken into account by the voting rule? In this brief article, I argue, via the maximum likelihood approach to voting, that it is optimal to ignore the social network structure altogether-one person, one vote. © 2011 Elsevier B.V.",math
1546,Autocorrelation measures for the quadratic assignment problem,Applied Mathematics Letters," journal homepage: www.elsevier.com/locate/aml k avg{f (y)} = f (x) + d (f − f (x)) y∈N(x) A landscape for a combinatorial optimization problem is a triple (X , N , f ), where f : X → R is the objective function to be minimized (or maximized) and the neighborhood function N maps a solution x ∈ X to the set of neighboring solutions. If y ∈ N (x) then y is a neighbor of x. There is a especial kind of landscape, called an elementary landscape, which is of particular interest in present research due to its properties. They are characterized by the Grover's wave equation [1]: where d is the size of the neighborhood, |N (x)|, which we assume the same for all the solutions in the search space (regular neighborhood), f is the average solution evaluation over the entire search space, and k is a characteristic (problemdependent) constant. A general landscape (X , N , f ) cannot always be said to be elementary, but even in this case it is possible to characterize the function f as a sum of elementary landscapes [2], called the elementary components of the landscape. The Quadratic Assignment Problem (QAP) is a well-known NP-hard combinatorial optimization problem that is at the core of many real-world optimization problems [3]. A lot of research has been devoted to analyze and solve the QAP itself, and in fact some other problems can be formulated as special cases of the QAP, e.g., the Traveling Salesman Problem (TSP). Let P be a set of n facilities and L a set of n locations. For each pair of locations i and j, an arbitrary distance is specified rij and for each pair of facilities p and q, a flow is specified wpq. The QAP consists of assigning to each location in L one facility in P in such a way that the total cost of the assignment is minimized. Each location can only contain one facility and all the facilities must be assigned to one location. For each pair of locations the cost is computed as the product of the distance between ",math
1547,Representing von Neumann–Morgenstern Games in the Situation Calculus,Annals of Mathematics and Artificial Intelligence," Sequential von Neumann-Morgernstern (VM) games are a very general formalism for representing multi-agent interactions and planning problems in a variety of types of environments. We show that sequential VM games with countably many actions and continuous utility functions have a sound and complete axiomatization in the situation calculus. We discuss the application of various concepts from VM game theory to the theory of planning and multi-agent interactions, such as representing concurrent actions and using the Baire topology to define continuous payoff functions. Keywords: decision and game theory, multiagent systems, knowledge representation, reasoning about actions and change ",math
1548,Boolean lexicographic optimization: algorithms & applications,Annals of Mathematics and Artificial Intelligence," Multi-Objective Combinatorial Optimization (MOCO) problems find a wide range of practical application problems, some of which involving Boolean variables and constraints. This paper develops and evaluates algorithms for solving MOCO problems, defined on Boolean domains, and where the optimality criterion is lexicographic. The proposed algorithms build on existing algorithms for either Maximum Satisfiability (MaxSAT), Pseudo-Boolean Optimization (PBO), or Integer Linear Programming (ILP). Experimental results, obtained on problem instances from haplotyping with pedigrees and software package dependencies, show that the proposed algorithms can provide significant performance gains over state of the art MaxSAT, PBO and ILP algorithms. Finally, the paper also shows that lexicographic optimization conditions are observed in the majority of the problem instances from the MaxSAT evaluations, motivating the development of dedicated algorithms that can exploit lexicographic optimization conditions in general MaxSAT problem instances. ",math
1549,Löwner's Operator and Spectral Functions in Euclidean Jordan Algebras,Mathematics of Operations Research," We study analyticity, differentiability, and semismoothness of L¨owner's operator and spectral functions under the framework of Euclidean Jordan algebras. In particular, we show that many optimization-related classical results in the symmetric matrix space can be generalized within this framework. For example, the metric projection operator over any symmetric cone defined in a Euclidean Jordan algebra is shown to be strongly semismooth. The research also raises several open questions, whose solution would be of general interest for optimization. ",math
1550,On the Rank of Extreme Matrices in Semidefinite Programs and the Multiplicity of Optimal Eigenvalues,Mathematics of Operations Research," We derive some basic results on the geometry of semidefinite programming ( SDP ) and eigenvalue-optimization , i.e., the minimization of the sum of the k largest eigenvalues of a smooth matrix-valued function. We provide upper bounds on the rank of extreme matrices in SDPs, and the first theoretically solid explanation of a phenomenon of intrinsic interest in eigenvalue-optimization. In the spectrum of an optimal matrix, the k th and ( k / 1 ) st largest eigenvalues tend to be equal and frequently have multiplicity greater than two. This clustering is intuitively plausible and has been observed as early as 1975. When the matrix-valued function is affine, we prove that clustering must occur at extreme points of the set of optimal solutions, if the number of variables is sufficiently large. We also give a lower bound on the multiplicity of the critical eigenvalue. These results generalize to the case of a general matrix-valued function under appropriate conditions. ",math
1551,"Comparing the notions of optimality in CP-nets, strategic games and soft constraints",Annals of Mathematics and Artificial Intelligence," The notion of optimality naturally arises in many areas of applied mathematics and computer science concerned with decision making. Here we consider this notion in the context of three formalisms used for different purposes in reasoning about multi-agent systems: strategic games, CP-nets, and soft constraints. To relate the notions of optimality in these formalisms we introduce a natural qualitative modification of the notion of a strategic game. We show then that the optimal outcomes of a CP-net are exactly the Nash equilibria of such games. This allows us to use the techniques of game theory to search for optimal outcomes of CP-nets and vice-versa, to use techniques developed for CP-nets to search for Nash equilibria of the considered games. Then, we relate the notion of optimality used in the area of soft constraints to that used in a generalization of strategic games, called graphical games. In particular we prove that for a natural class of soft constraints that includes weighted constraints every optimal solution is both a Nash equilibrium and Pareto efficient joint strategy. For a natural mapping in the other direction we show that Pareto efficient joint strategies coincide with the optimal solutions of soft constraints. ",math
1552,A Unified Theorem on SDP Rank Reduction,Mathematics of Operations Research," We consider the problem of ¯nding a low{rank approximate solution to a system of linear equations in symmetric, positive semide¯nite matrices. Speci¯cally, let A1; : : : ; Am 2 Rn£n symmetric, positive semide¯nite matrices, and let b1; : : : ; bm ¸ 0. We show that if there exists a symmetric, positive semide¯nite matrix X to the following system of equations: then for any ¯xed d = 1; : : : ; O(log m), there exists an X0 º 0 of rank at most d such that: ",math
1553,Making a productive use of failure to generate witnesses for coinduction from divergent proof attempts,Annals of Mathematics and Artificial Intelligence, Informatics Research Report EDI-INF-RR-0004 ,math
1554,Elliptic curve cryptosystems,Mathematics of Computation,"We discuss analogs based on elliptic curves over finite fields of public key cryptosystems which use the multiplicative group of a finite field. These elliptic curve cryptosystems may be more secure, because the analog of the discrete logarithm problem on elliptic curves is likely to be harder than the classical discrete logarithm problem, especially over GF(2""). We discuss the question of primitive points on an elliptic curve modulo p, and give a theorem on nonsmoothness of the order of the cyclic subgroup generated by a global point. ÂŠ 1987 AMS.",math
1555,Nonorthogonal decomposition of binary matrices for bounded-error data compression and analysis,ACM Transactions on Mathematical Software," This article presents the design and implementation of a software tool, PROXIMUS, for error-bounded approximation of high-dimensional binary attributed datasets based on nonorthogonal decomposition of binary matrices. This tool can be used for analyzing data arising in a variety of domains ranging from commercial to scientific applications. Using a combination of innovative algorithms, novel data structures, and efficient implementation, PROXIMUS demonstrates excellent accuracy, performance, and scalability to large datasets. We experimentally demonstrate these on diverse applications in association rule mining and DNA microarray analysis. In limited beta release, PROXIMUS currently has over 300 installations in over 10 countries. Categories and Subject Descriptors: G.4 [Mathematics of Computing]: Mathematical Software; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Clustering Additional Key Words and Phrases: Compressing binary-valued vectors, nonorthogonal matrix decompositions, semidiscrete decomposition ",math
1556,The minimal error conjugate gradient method is a regularization method,Proceedings of the American Mathematical Society,"The regularizing properties of the conjugate gradient iteration, applied to the normal equation of a linear ill-posed problem, were established by Nemirovskii in 1986. A seemingly more attractive variant of this algorithm is the minimal error method suggested by King. The present paper analyzes the regularizing properties of the minimal error method. It is shown that the discrepancy principle is no regularizing stopping rule for the minimal error method. Instead, a different stopping rule is suggested which leads to order-optimal convergence rates. © 1995 American Mathematical Society.",math
1557,Tradeoffs in the empirical evaluation of competing algorithm designs,Annals of Mathematics and Artificial Intelligence,"We propose an empirical analysis approach for characterizing tradeoffs between different methods for comparing a set of competing algorithm designs. Our approach can provide insight into performance variation both across candidate algorithms and across instances. It can also identify the best tradeoff between evaluating a larger number of candidate algorithm designs, performing these evaluations on a larger number of problem instances, and allocating more time to each algorithm run. We applied our approach to a study of the rich algorithm design spaces offered by three highly-parameterized, state-of-the-art algorithms for satisfiability and mixed integer programming, considering six different distributions of problem instances. We demonstrate that the resulting algorithm design scenarios differ in many ways, with important consequences for both automatic and manual algorithm design. We expect that both our methods and our findings will lead to tangible improvements in algorithm design methods. © 2010 Springer Science+Business Media B.V.",math
1558,External sources of knowledge and value invention in logic programming,Annals of Mathematics and Artificial Intelligence,"The issue of value invention in logic programming embraces many scenarios, such as logic programming with function symbols, object oriented logic languages, inter-operability with external sources of knowledge, or set unification. This work introduces a framework embedding value invention in a general context. The class of programs having a suitable (but, in general, not decidable) finite grounding property is identified, and the class of value invention restricted programs is introduced. Value invention restricted programs have the finite grounding property and can be decided in polynomial time. They are a very large polynomially decidable class having this property, when no assumption can be made about the nature of invented values (while this latter is the case in the specific literature about logic programming with function symbols). Relationships with existing formalisms are eventually discussed, and the implementation of a system supporting the class of such programs is described. © 2007 Springer Science+Business Media B.V.",math
1559,Fast algorithms for implication bases and attribute exploration using proper premises,Annals of Mathematics and Artificial Intelligence,"A central task in formal concept analysis is the enumeration of a small base for the implications that hold in a formal context. The usual stem base algorithms have been proven to be costly in terms of runtime. Proper premises are an alternative to the stem base. We present a new algorithm for the fast computation of proper premises. It is based on a known link between proper premises and minimal hypergraph transversals. Two further improvements are made, which reduce the number of proper premises that are obtained multiple times and redundancies within the set of proper premises. We have evaluated our algorithms within an application related to refactoring of model variants. In this application an implicational base needs to be computed, and runtime is more crucial than minimal cardinality. In addition to the empirical tests, we provide heuristic evidence that an approach based on proper premises will also be beneficial for other applications. Finally, we show how our algorithms can be extended to an exploration algorithm that is based on proper premises. © 2013 Springer Science+Business Media Dordrecht.",math
1560,On the sequential quadratically constrained quadratic programming methods,Mathematics of Operations Research," An iteration of the sequential quadratically constrained quadratic programming method (SQCQP) consists of minimizing a quadratic approximation of the objective function subject to quadratic approximation of the constraints, followed by a line search in the obtained direction. Methods of this class are receiving attention due to the development of efficient interior point techniques for solving subproblems with this structure, via formulating them as second-order cone programs. Recently, Fukushima et al. (2003) proposed a SQCQP method for convex minimization with twice continuously differentiable data. Their method possesses global and locally quadratic convergence, and it is free of the Maratos effect. The feasibility of subproblems in their method is enforced by switching between the linear and quadratic approximations of the constraints. This strategy requires computing a strictly feasible point, as well as choosing some further parameters. We propose a SQCQP method where feasibility of subproblems is ensured by introducing a slack variable and, hence, is automatic. In addition, we do not assume convexity of the objective function or twice differentiability of the problem data. While our method has all the desirable convergence properties, it is easier to implement. Among other things, it does not require computing a strictly feasible point, which is a nontrivial task. In addition, its global convergence requires weaker assumptions. ",math
1561,Programming matrix algorithms-by-blocks for thread-level parallelism,ACM Transactions on Mathematical Software,"With the emergence of thread-level parallelism as the primary means for continued performance improvement, the programmability issue has reemerged as an obstacle to the use of architectural advances. We argue that evolving legacy libraries for dense and banded linear algebra is not a viable solution due to constraints imposed by early design decisions. We propose a philosophy of abstraction and separation of concerns that provides a promising solution in this problem domain. The first abstraction, FLASH, allows algorithms to express computation with matrices consisting of contiguous blocks, facilitating algorithms-by-blocks. Operand descriptions are registered for a particular operation a priori by the library implementor. A runtime system, SuperMatrix, uses this information to identify data dependencies between suboperations, allowing them to be scheduled to threads out-of-order and executed in parallel. But not all classical algorithms in linear algebra lend themselves to conversion to algorithms-by-blocks. We show how our recently proposed LU factorization with incremental pivoting and a closely related algorithm-by-blocks for the QR factorization, both originally designed for out-of-core computation, overcome this difficulty. Anecdotal evidence regarding the development of routines with a core functionality demonstrates how the methodology supports high productivity while experimental results suggest that high performance is abundantly achievable. © 2009 ACM.",math
1562,Computational methods for database repair by signed formulae*,Annals of Mathematics and Artificial Intelligence,"We introduce a simple and practical method for repairing inconsistent databases. Given a possibly inconsistent database, the idea is to properly represent the underlying problem, i.e., to describe the possible ways of restoring its consistency. We do so by what we call signed formulae, and show how the signed theory that is obtained can be used by a variety of off-the-shelf computational models in order to compute the corresponding solutions, i.e., consistent repairs of the database. © Springer 2006.",math
1563,The Volume of the Giant Component of a Random Graph with Given Expected Degrees,SIAM Journal on Discrete Mathematics," We consider the random graph model G(w) for a given expected degree sequence w = (w1, w2, . . . , wn). If the expected average degree is strictly greater than 1, then almost surely the giant component in G of G(w) has volume (i.e., sum of weights of vertices in the giant component) equal to λ0Vol(G) + O(√n log3.5 n), where λ0 is the unique nonzero root of the equation n n and where Vol(G) = i wi. ",math
1564,Self-organization and emergence in social systems. Modeling the coevolution of social environments and cooperative behavior,Journal of Mathematical Sociology, FSI eWnjia FSI ivews NISTUE ,math
1565,On the construction of hierarchic models,Annals of Mathematics and Artificial Intelligence," One of the main problems in the field of model-based diagnosis of technical systems today is finding the most useful model or models of the system being diagnosed. Often, a model showing the physical components and the connections between them is all that is available. As systems grow larger and larger, the run-time performance of diagnostic algorithms decreases considerably when using these detailed models. A solution to this problem is using a hierarchic model. This allows us to first diagnose the system using an abstract model, and then use this solution to guide the diagnostic process using a more detailed model. The main problem with this approach is acquiring the hierarchic model. We give a generic hierarchic diagnostic algorithm and show how the use of certain classes of hierarchic models can increase the performance of this algorithm. We then present linear time algorithms for the automatic construction of these hierarchic models, using the detailed model and extra information about cost of probing points and invertibility of components. ",math
1566,A foundation for competence set analysis,Mathematical Social Sciences,"For each decision problem, there is a competence set consisting of knowledge, information and skills for its effective solution. How the decision-maker acquires and expands his/her competence set plays a key role in the process and quality of decision-making. This paper provides a mathematical foundation for studying competence sets, their expansion processes and stability. Time and cost functions for expansion, reachable domains, effective expansion using minimal spanning trees, random set decomposition of competence sets, marginal analysis, connectivity, metrization and stability of competence sets are some key concepts introduced. © 1990.",math
1567,A method for solving algebraic equations using an automatic computer,Mathematics of Computation," 1. Iterate the function 2y2 - 1, starting required. 2. Record the signs of the iterates in order. 3. Accumulate the signs; that is, record the ""partial products"" in order. 4. Write descending powers of 2 between the signs accumulated. 5. Multiply the series obtained by ir/2. S. Comparison with Other Methods. The usefulness of the method described above as compared with other methods depends, of course, on the function to be evaluated and on the features of the machine to be used. The fact that each iteration yields exactly one binary bit may be an advantage or a disadvantage ; a method where error decreases faster than 2~"" will converge with fewer iterations than this one. On the other hand, one iteration of this method may consist of fewer commands than an iteration of another method. The logarithm program for the CRC-102A described above has 4 commands per iteration as compared with 14 commands per iteration in another program for logarithm on the same machine. The program for arcsin by the method described above has 9 commands per iteration as compared to 22 for another arcsin program. The fact that each iteration yields exactly one binary bit also simplifies error analysis, for the number of digits of accuracy is exactly one less than the number of digits computed. ",math
1568,On stability of a formal concept,Annals of Mathematics and Artificial Intelligence,"In this paper we define and analyze stability of a formal concept. A stability index is based on the idea of a dependency in a dataset that can be reconstructed from different parts of the dataset. This idea, underlying various methods of estimating scientific hypotheses, is used here for estimating concept-based hypotheses. Combinatorial properties of stability indices, algorithmic comlpexity of their computation, as well as their dynamics with arrival of new examples are studied. ÂŠ 2007 Springer Science+Business Media B.V.",math
1569,Graphs with 1-Factors,Proceedings of the American Mathematical Society," In this paper it is shown that if G is a connected graph of order 2n (n> 1) not containing a 1-factor, then for each k, Kk^n, there exists an induced, connected subgraph of order 2k which also fails to possess a 1-factor. Several other sufficient conditions for a graph to contain a 1-factor are presented. In particular, it is seen that the connected even order line graphs and total graphs always contain a 1-factor. ",math
1570,On stopping criteria in verified nonlinear systems or optimization algorithms,ACM Transactions on Mathematical Software," Traditionally, iterative methods for nonlinear systems use heuristic domain and range stopping criteria to determine when accuracy tolerances have been met. However, such heuristics can cause stopping at points far from actual solutions, and can be unreliable due to the effects of roundoff error or inaccuracies in data. In verified computations, rigorous determination of when a set of bounds has met a tolerance can be done analogously to the traditional approximate setting. Nonetheless, the range tolerance possibly cannot be met. If the criteria are used to determine when to stop subdivision of n-dimensional bounds into subregions, then failure of a range tolerance results in excessive, unnecessary subdivision, and could make the algorithm impractical. On the other hand, interval techniques can detect when inaccuracies or roundoff will not permit residual bounds to be narrowed. These techniques can be incorporated into range thickness stopping criteria that complement the range stopping criteria. In this note, the issue is first introduced and illustrated with a simple example. The thickness stopping criterion is then formally introduced and analyzed. Third, inclusion of the criterion within a general verified global optimization algorithm is studied. An industrial example is presented. Finally, consequences and implications are discussed. Categories and Subject Descriptors: G.1.5 [Numerical Analysis]: Roots of Nonlinear Equations-error analysis, iterative methods, systems of equations; G.1.6 [Numerical Analysis]: Optimization-unconstrained optimization, constrained optimization, global optimization ",math
1571,Bayesian Sequential Change Diagnosis,Mathematics of Operations Research," Sequential change diagnosis is the joint problem of detection and identification of a sudden and unobservable change in the distribution of a random sequence. In this problem, the common probability law of a sequence of i.i.d. random variables suddenly changes at some disorder time to one of finitely many alternatives. This disorder time marks the start of a new regime, whose fingerprint is the new law of observations. Both the disorder time and the identity of the new regime are unknown and unobservable. The objective is to detect the regime-change as soon as possible, and, at the same time, to determine its identity as accurately as possible. Prompt and correct diagnosis is crucial for quick execution of the most appropriate measures in response to the new regime, as in fault detection and isolation in industrial processes, and target detection and identification in national defense. The problem is formulated in a Bayesian framework. An optimal sequential decision strategy is found, and an accurate numerical scheme is described for its implementation. Geometrical properties of the optimal strategy are illustrated via numerical examples. The traditional problems of Bayesian change-detection and Bayesian sequential multi-hypothesis testing are solved as special cases. In addition, a solution is obtained for the problem of detection and identification of component failure(s) in a system with suspended animation. ",math
1572,Relational concept discovery in structured datasets,Annals of Mathematics and Artificial Intelligence,"Relational datasets, i.e., datasets in which individuals are described both by their own features and by their relations to other individuals, arise from various sources such as databases, both relational and object-oriented, knowledge bases, or software models, e.g., UML class diagrams. When processing such complex datasets, it is of prime importance for an analysis tool to hold as much as possible to the initial format so that the semantics is preserved and the interpretation of the final results eased. Therefore, several attempts have been made to introduce relations into the formal concept analysis field which otherwise generated a large number of knowledge discovery methods and tools. However, the proposed approaches invariably look at relations as an intra-concept construct, typically relating two parts of the concept description, and therefore can only lead to the discovery of coarse-grained patterns. As an approach towards the discovery of finer-grain relational concepts, we propose to enhance the classical (object × attribute) data representations with a new dimension that is made out of inter-object links (e.g., spouse, friend, manager-of, etc.). Consequently, the discovered concepts are linked by relations which, like associations in conceptual data models such as the entity-relation diagrams, abstract from existing links between concept instances. The borders for the application of the relational mining task are provided by what we call a relational context family, a set of binary data tables representing individuals of various sorts (e.g., human beings, companies, vehicles, etc.) related by additional binary relations. As we impose no restrictions on the relations in the dataset, a major challenge is the processing of relational loops among data items. We present a method for constructing concepts on top of circular descriptions which is based on an iterative approximation of the final solution. The underlying construction methods are illustrated through their application to the restructuring of class hierarchies in object-oriented software engineering, which are described in UML. © 2007 Springer Science+Business Media B.V.",math
1573,On the lattice structure of certain linear congruential sequences related to AWC/SWB generators,Mathematics of Computation," We analyze the lattice structure of certain types of linear congruential generators (LCGs), which include close approximations to the add-withcarry and subtract-with-borrow (AWC/SWB) random number generators introduced by Marsaglia and Zaman, and also to combinations of the latter with ordinary LCGs. It follows from our results that all these generators have an unfavorable lattice structure in large dimensions. ",math
1574,On the Computational Complexity of Weighted Voting Games,Annals of Mathematics and Artificial Intelligence,"Coalitional games provide a useful tool for modeling cooperation in multiagent systems. An important special class of coalitional games is weighted voting games, in which each player has a weight (intuitively corresponding to its contribution), and a coalition is successful if the sum of its members weights meets or exceeds a given threshold. A key question in coalitional games is finding coalitions and payoff division schemes that are stable, i.e., no group of players has any rational incentive to leave. In this paper, we investigate the computational complexity of stability-related questions for weighted voting games. We study problems involving the core, the least core, and the nucleolus, distinguishing those that are polynomial-time computable from those that are NP-hard or coNP-hard, and providing pseudopolynomial and approximation algorithms for some of the computationally hard problems. © Springer Science + Business Media B.V. 2009.",math
1575,Convergence analysis for a multiplicatively relaxed EM algorithm,Mathematical Methods in The Applied Sciences,"The expectation maximization (EM) algorithm is an iterative procedure used to determine maximum likelihood estimators in situations of incomplete data. In the case of independent Poisson variables it converges to a solution of a problem of the form min ∑[〈ai,x〉 − bi log 〈ai, x〉] such that x ⩾0. Thus, it can be used to solve systems of the form Ax = b, x⩾0 (with A stochastic and b positive.) It converges to a feasible solution if it exists and to an approximate one otherwise (the one that minimizes d (b, Ax), where d is the Kullback–Leibler information divergence). We study the convergence of the multiplicatively relaxed version proposed by Tanaka for use in positron emission tomography. We prove global convergence in the underrelaxed and unrelaxed cases. In the overrelaxed case we present a local convergence theorem together with two partial global results: the sequence generated by the algorithm is bounded and, if it converges, its limit point is a solution of the aforementioned problem.",math
1576,A numerical evaluation of preprocessing and ILU-type preconditioners for the solution of unsymmetric sparse linear systems using iterative methods,ACM Transactions on Mathematical Software,"Recent advances in multilevel LU factorizations and novel preprocessing techniques have led to an extremely large number of possibilities for preconditioning sparse, unsymmetric linear systems for solving with iterative methods. However, not all combinations work well for all systems, so making the right choices is essential for obtaining an efficient solver. The numerical results for 256 matrices presented in this article give an indication of which approaches are suitable for which matrices (based on different criteria, such as total computation time or fill-in) and of the differences between the methods.",math
1577,Metric ternary distributive semi-lattices,Proceedings of the American Mathematical Society," In this paper we show that the ternary operation of a metric ternary distributive semi-lattice, a generalization of the ternary Boolean algebra of Grau [2], uniquely minimizes ternary distance. This generalizes a result of Birkhoff and Kiss [l, Corollary 1, p. 749]. We show, conversely, that in a metric space unique minimizing of ternary distance determines a ternary operation with respect to which the space is a ternary distributive semi-lattice. Particularly, a lattice whose graph satisfies the unique minimal ternary distance condition and certain finiteness conditions must be distributive. This answers a question proposed by Birkhoff and Kiss [l, p. 750]. ",math
1578,Updating quasi-Newton matrices with limited storage,Mathematics of Computation," We study how to use the BFGS quasi-Newton matrices to precondition minimization methods for problems where the storage is critical. We give an update formula which generates matrices using information from the last m iterations, where m is any number supplied by the user. The quasi-Newton matrix is updated at every iteration by dropping the oldest information and replacing it by the newest information. It is shown that the matrices generated have some desirable properties. The resulting algorithms are tested numerically and compared with several wellknown methods. 1. Introduction. For the problem of minimizing an unconstrained function / of n variables, quasi-Newton methods are widely employed [4]. They construct a sequence of matrices which in some way approximate the hessian of /(or its inverse). These matrices are symmetric; therefore, it is necessary to have n(n + l)/2 storage locations for each one. For large dimensional problems it will not be possible to retain the matrices in the high speed storage of a computer, and one has to resort to other kinds of algorithms. For example, one could use the methods (Toint [15], Shanno [12]) which preserve the sparsity structure of the hessian, or conjugate gradient methods (CG) which only have to store 3 or 4 vectors. Recently, some CG algorithms have been developed which use a variable amount of storage and which do not require knowledge about the sparsity structure of the problem [2], [7], [8]. A disadvantage of these methods is that after a certain number of iterations the quasi-Newton matrix is discarded, and the algorithm is restarted using an initial matrix (usually a diagonal matrix). We describe an algorithm which uses a limited amount of storage and where the quasi-Newton matrix is updated continuously. At every step the oldest information contained in the matrix is discarded and replaced by new one. In this way we hope to have a more up to date model of our function. We will concentrate on the BFGS method since it is considered to be the most efficient. We believe that similar algorithms cannot be developed for the other members of the Broyden 0-class [1]. Let / be the function to be nnnimized, g its gradient and h its hessian. We define sk=xk+i~xk ",math
1579,Time Series Generation by Recurrent Neural Networks,Annals of Mathematics and Artificial Intelligence," The properties of time series, generated by continuous valued feed-forward networks in which the next input vector is determined from past output values, are studied. Asymptotic solutions developed suggest that the typical stable behavior is (quasi) periodic with attractor dimension that is limited by the number of hidden units, independent of the details of the weights. The results are robust under additive noise, except for expected noise-induced effects - attractor broadening and loss of phase coherence at large times. These effects, however, are moderated by the size of the network N. ",math
1580,Functional and multivalued dependencies in nested databases generated by record and list constructor,Annals of Mathematics and Artificial Intelligence," The impact of the list constructor on two important classes of relational dependencies is investigated. Lists represent an inevitable data structure whenever order matters and data is allowed to occur repeatedly. The list constructor is therefore supported by many advanced data models such as genomic sequence, deductive and object-oriented data models including XML. The article proposes finite axiomatisations of functional, multivalued and both functional and multivalued dependencies in nested databases supporting record and list constructor. In order to capture different data models at a time, an abstract algebraic approach based on nested attributes is taken. The presence of the list constructor calls for a new inference rule which allows to infer non-trivial functional dependencies from multivalued dependencies. Further differences to the relational theory become apparent when the independence of the inference rules is investigated. The extension of the relational theory to nested databases allows to specify more real-world constraints and increases therefore the number of application domains. ",math
1581,A Two-Sided Discrete-Concave Market with Possibly Bounded Side Payments: An Approach by Discrete Convex Analysis,Mathematics of Operations Research," The marriage model due to Gale and Shapley and the assignment model due to Shapley and Shubik are standard in the theory of two-sided matching markets. We give a common generalization of these models by utilizing discrete concave functions and considering possibly bounded side payments. We show the existence of a pairwise stable outcome in our model. Our present model is a further natural extension of the model examined in our previous paper (Fujishige and Tamura [12]), and the proof of the existence of a pairwise stable outcome is even simpler than the previous one. ",math
1582,Rapid Prototyping of Large Multi-Agent Systems Through Logic Programming,Annals of Mathematics and Artificial Intelligence," Prototyping is a valuable technique to help software engineers explore the design space while gaining insight on the dynamics of the system. In this paper, we describe a method for rapidly building prototypes of large multi-agent systems using logic programming. Our method advocates the use of a description of all permitted interactions among the components of the system, that is, the protocol, as the starting speci cation. The protocol is represented in a way that allows us to automatically check for desirable properties of the system to be built. We then employ the same speci cation to synthesise agents that will correctly follow the protocol. These synthesised agents are simple logic programs that engineers can further customise into more sophisticated software. Our choice of agents as logic programs allows us to provide semi-automatic support for the customisation activity. In our method, a prototype is a protocol with a set of synthesised and customised agents. Executing the prototype amounts to having these agents enact the protocol. We have implemented and describe a distributed platform to simulate prototypes. ",math
1583,Analysis and evaluation of the top-k most influential location selection query,Knowledge and Information Systems,"In this paper, we propose a new type of queries to retrieve the top-k most influential locations from a candidate set (C) given sets of customers (M) and existing facilities (F). The influence models the popularity of a facility. Such queries have wide applications in decision support systems. A naive solution sequentially scans (SS) all data sets, which is expensive, and hence, we investigate two branch-and-bound algorithms for the query, namely Estimate Expanding Pruning (EEP) and Bounding Influence Pruning (BIP). Both algorithms follow the best first traverse. On determining the traversal order, while EEP leverages distance metrics between nodes, BIP relies on half plane pruning which avoids the repetitive estimations in EEP. As our experiments shown, BIP is much faster than SS which outperforms EEP, while the worst-case complexity of EEP and BIP is worse than that of SS. To improve the efficiency, we further propose a Nearest Facility Circle Join (NFCJ) algorithm. NFCJ builds an influence R-tree on the influence relationship between customers and existing facilities and joins the candidate R-tree with the influence R-tree to obtain the results. We compare all algorithms and conclude that NFCJ is the best solution, which outperforms SS, EEP, and BIP by orders of magnitude.",information system
1584,Explaining prediction models and individual predictions with feature contributions,Knowledge and Information Systems,"We present a sensitivity analysis-based method for explaining prediction models that can be applied to any type of classification or regression model. Its advantage over existing general methods is that all subsets of input features are perturbed, so interactions and redundancies between features are taken into account. Furthermore, when explaining an additive model, the method is equivalent to commonly used additive model-specific methods. We illustrate the method’s usefulness with examples from artificial and real-world data sets and an empirical analysis of running times. Results from a controlled experiment with 122 participants suggest that the method’s explanations improved the participants’ understanding of the model.",information system
1585,Symbolic Music Genre Classification Based on Note Pitch and Duration,Advances in Databases and Information Systems,"This paper presents a music genre classification system that relies on note pitch and duration features, derived from their respective histograms. Feature histograms provide a simple but yet effective classifier for the purposes of genre classification in intra-classical genres such as sonatas, fugues, mazurkas, etc. Detailed experimental results illustrate the significant performance gains due to the proposed features, compared to existing baseline features.",information system
1586,Weak Dependencies in Business Process Models,Business Information Systems,"Business process management systems (BPMS) have proven to be effective in high-volume, repetitive production processes. However, the rigid structure of process models used in BPMS hinders their use in less repetitive processes performed by information workers, where a high degree of flexibility is required. In this paper, an extension to the traditional process models is presented, where ad hoc, runtime changes to process instances are enabled. The extension, weak dependencies, is motivated by business cases presented in the paper. Additionally, formal description of weak dependencies as well as a proof-of-concept prototype are presented.",information system
1587,"Space, time, matter and things",Formal Ontology in Information Systems," - I present a logical language for describing spatial, temporal and material properties of the physical world. The formalism is ontologically well-founded in the sense that it is interpreted with respect to model structures which have a specific physical interpretation in terms of the distribution of matter in space and time. Categories & Descriptors - I 2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods- representation langauges, temporal logic, modal logic. ",information system
1588,Mining exceptional relationships with grammar-guided genetic programming,Knowledge and Information Systems,"Given a database of records, it might be possible to identify small subsets of data which distribution is exceptionally different from the distribution in the complete set of data records. Finding such interesting relationships, which we call exceptional relationships, in an automated way would allow discovering unusual or exceptional hidden behaviour. In this paper, we formulate the problem of mining exceptional relationships as a special case of exceptional model mining and propose a grammar-guided genetic programming algorithm (MERG3P) that enables the discovery of any exceptional relationships. In particular, MERG3P can work directly not only with categorical, but also with numerical data. In the experimental evaluation, we conduct a case study on mining exceptional relations between well-known and widely used quality measures of association rules, which exceptional behaviour would be of interest to pattern mining experts. For this purpose, we constructed a data set comprising a wide range of values for each considered association rule quality measure, such that possible exceptional relations between measures could be discovered. Thus, besides the actual validation of MERG3P, we found that the Support and Leverage measures in fact are negatively correlated under certain conditions, while in general experts in the field expect these measures to be positively correlated.",information system
1589,A survey on algorithms for mining frequent itemsets over data streams,Knowledge and Information Systems,  ,information system
1590,The Low-Level Support and Logging for Flexible Transactions,Advances in Databases and Information Systems, In this paper the ways of various types of transaction support in the buffer system of the storage system are considered. Support of active transaction tree as the mechanism for nested transaction execution is described. It is shown that inclusion of some additional information into ATT nodes makes possible to support the wide range of flexible transaction concepts. The implementing of ARIES method for both flat and nested transaction rollback and recovery is described. ,information system
1591,Basic Problems of Mereotopology,Formal Ontology in Information Systems," Mereotopology is today regarded as a major tool for ontological analysis, and for many good reasons. There are, however, a number of open questions that call for an answer. Some of them are philosophical, others have direct import for applications, but all are crucial for a proper assessment of the strengths and limits of mereotopology. This paper is an attempt to put some order into this still untamed area of research. I will not attempt any answers. But I shall try to give an idea of the problems, and of their relevance for the systematic development of formal ontological theories. ",information system
1592,Formal foundations for RDF/S KB evolution,Knowledge and Information Systems," There are ongoing efforts to provide declarative formalisms of integrity constraints over RDF/S data. In this context, addressing the evolution of RDF/S knowledge bases while respecting associated constraints is a challenging issue, yet to receive a formal treatment. We provide a theoretical framework for dealing with both schema and data change requests. We define the notion of a rational change operator as one that satisfies the belief revision principles of Success, Validity and Minimal Change. The semantics of such an operator are subject to customization, by tuning the properties that a rational change should adhere to. We prove some interesting theoretical results and propose a general-purpose algorithm for implementing rational change operators in knowledge bases with integrity constraints, which allows us to handle uniformly any possible change request in a provably rational and consistent manner. Then, we apply our framework to a well-studied RDF/S variant, for which we suggest a specific notion of minimality. For efficiency purposes, we also describe specialized versions of the general evolution algorithm for the RDF/S case, which provably have the same semantics as the general-purpose one for a limited set of (useful in practice) types of change requests. ",information system
1593,A multimodal virtual reality interface for 3D interaction with VTK,Knowledge and Information Systems,"The object-oriented visualization Toolkit (VTK) is widely used for scientific visualization. VTK is a visualization library that provides a large number of functions for presenting three-dimensional data. Interaction with the visualized data is controlled with two-dimensional input devices, such as mouse and keyboard. Support for real three-dimensional and multimodal input is non-existent. This paper describes VR-VTK: a multimodal interface to VTK on a virtual environment. Six degree of freedom input devices are used for spatial 3D interaction. They control the 3D widgets that are used to interact with the visualized data. Head tracking is used for camera control. Pedals are used for clutching. Speech input is used for application commands and system control. To address several problems specific for spatial 3D interaction, a number of additional features, such as more complex interaction methods and enhanced depth perception, are discussed. Furthermore, the need for multimodal input to support interaction with the visualization is shown. Two existing VTK applications are ported using VR-VTK to run in a desktop virtual reality system. Informal user experiences are presented.",information system
1594,Evaluation of Common Counting Method for Concurrent Data Mining Queries,Advances in Databases and Information Systems,Data mining queries are often submitted concurrently to the data mining system. The data mining system should take advantage of overlapping of the mined datasets. In this paper we focus on frequent itemset mining and we discuss and experimentally evaluate the implementation of the Common Counting method on top of the Apriori algorithm. The general idea of Common Counting is to reduce the number of times the common parts of the source datasets are scanned during the processing of the set of frequent pattern queries.,information system
1595,Learning to extract and summarize hot item features from multiple auction web sites,Knowledge and Information Systems,"It is difficult to digest the poorly organized and vast amount of information contained in auction Web sites which are fast changing and highly dynamic. We develop a unified framework which can automatically extract product features and summarize hot item features from multiple auction sites. To deal with the irregularity in the layout format of Web pages and harness the uncertainty involved, we formulate the tasks of product feature extraction and hot item feature summarization as a single graph labeling problem using conditional random fields. One characteristic of this graphical model is that it can model the inter-dependence between neighbouring tokens in a Web page, tokens in different Web pages, as well as various information such as hot item features across different auction sites. We have conducted extensive experiments on several real-world auction Web sites to demonstrate the effectiveness of our framework.",information system
1596,Facilitating business interoperability from the semantic web,Business Information Systems,"Most approaches to B2B interoperability are based on language syntax standardisation, usually by XML Schemas. However, due to XML expressivity limitations, they are difficult to put into practice because language semantics are not available for computerised means. Therefore, there are many attempts to use formal semantics for B2B based on ontologies. However, this is a difficult jump as there is already a huge XML-based B2B framework and ontology-based approaches lack momentum. Our approach to solve this impasse is based on a direct and transparent transfer of existing XML Schemas and XML data to the semantic world. This process is based on a XML Schema to web ontology mapping combined with an XML data to semantic web data one. Once in the semantic space, it is easier to integrate different business standards using ontology alignment tools and to develop business information systems thanks to semantics-aware tools.",information system
1597,Putting Money Where the Mouths Are: The Relation Between Venture Financing and Electronic Word-of-Mouth,Information Systems Research," External financing is critical to ventures that do not have a revenue source, but need to recruit employees, develop products, pay suppliers and market their products/services. There is an increasing belief amongst entrepreneurs that electronic word-of-mouth (eWOM), specifically blog coverage, can aid in achieving venture capital financing. Conflicting findings reported by past studies examining eWOM makes it unclear what to make of such beliefs of entrepreneurs. Even if there were generally agreed upon results, a stream of literature indicates that because of the differences in traits between the prior investigated contexts and venture capital financing, the findings from the prior studies cannot be generalized to venture capital financing. Extant studies also fall short in examining the role of time and the status of entities generating eWOM in determining the influence of eWOM on decision making. To address this dearth of literature in a context that attracts billions of dollars every year, we investigate the effect of eWOM on venture capital financing. This study entails the challenging task of gathering data from hundreds of ventures, besides other sources including VentureXpert, surveys, Google Blogsearch, LexisNexis, and Archive.org. ",information system
1598,Artefacts and Roles: Modelling Strategies in a Multiplicative Ontology,Formal Ontology in Information Systems," The purpose of this paper is to examine different modelling strategies available in a multiplicative formal ontology, and the principles that drive their choice. This study is based on the results of recent work aiming at extending the foundational ontology DOLCE to grasp two quite different notions, that of artefact and that of role. These results, summarized in the paper, show that two multiplicative modelling strategies, entity stacking and property reification, are essential in both cases. ",information system
1599,The effects of task interruption and information presentation on individual decision making,International Conference on Information Systems," Interrupted work environments are commonplace in today's organizations. In addition, organizational work is increasingly performed using some form of computer support. Consequently, there is a need to examine how the design and delivery of information systems can help to mitigate the potentially deleterious effects of interruptions on decision-maker performance. This paper reports the results of two experiments that investigate the influence of interruptions on different types of tasks and the ability of information presentation formats to alleviate them. Interruptions were found to facilitate performance on simple tasks, while inhibiting performance on more complex tasks. Furthermore, there was some evidence that the frequency of interruptions and similarity of the content of the primary and interruption tasks also negatively influenced performance. Finally, interruptions moderated the relationship between information presentation format and specific types of tasks. ",information system
1600,Efficient Monitoring of Patterns in Data Mining Environments,Advances in Databases and Information Systems,"In this article, we introduce a general framework for monitoring patterns and detecting interesting changes without continuously mining the data. Using our approach, the effort spent on data mining can be drastically reduced while the knowledge extracted from the data is kept up to date. Our methodology is based on a temporal representation for patterns, in which both the content and the statistics of a pattern are modeled. We divide the KDD process into two phases. In the first phase, data from the first period is mined and interesting rules and patterns are identified. In the second phase, using the data from subsequent periods, statistics of these rules are extracted in order to decide whether or not they still hold. We applied this technique in a case study on mining mail log data. Our results show that a minimal set of patterns reflecting the invariant properties of the dataset can be identified, and that interesting changes to the population can be recognized indirectly by monitoring a subset of the patterns found in the first phase.",information system
1601,Defining the notion of ‘Information Content’ and reasoning about it in a database,Knowledge and Information Systems,"The problem of ‘information content’ of an information system appears elusive. In the field of databases, the information content of a database has been taken as the instance of a database. We argue that this view misses two fundamental points. One is a convincing conception of the phenomenon concerning information in databases, especially a properly defined notion of ‘information content’. The other is a framework for reasoning about information content. In this paper, we suggest a modification of the well known definition of ‘information content’ given by Dretske(Knowledge and the flow of information,1981). We then define what we call the ‘information content inclusion’ relation (IIR for short) between two random events. We present a set of inference rules for reasoning about information content, which we call the IIR Rules. Then we explore how these ideas and the rules may be used in a database setting to look at databases and to derive otherwise hidden information by deriving new relations from a given set of IIR. A prototype is presented, which shows how the idea of IIR-Reasoning might be exploited in a database setting including the relationship between real world events and database values.",information system
1602,Hybrid Rules with Well-Founded Semantics,Knowledge and Information Systems,A general framework is proposed for integration of rules and external first-order theories. It is based on the well-founded semantics of normal logic programs and inspired by ideas of Constraint Logic Programming (CLP) and constructive negation for logic programs. Hybrid rules are normal clauses extended with constraints in the bodies; constraints are certain formulae in the language of the external theory. A hybrid program consists of a set of hybrid rules and an external theory. Instances of the framework are obtained by specifying the class of external theories and the class of constraints. An example instance is integration of (non-disjunctive) Datalog with ontologies formalized in description logics. The paper defines a declarative semantics of hybrid programs and a goal-driven formal operational semantics. The latter can be seen as a generalization of SLS-resolution. It provides a basis for hybrid implementations combining Prolog with constraint solvers (such as ontology reasoners). Soundness of the operational semantics is proven. Sufficient conditions for decidability of the declarative semantics and for completeness of the operational semantics are given.,information system
1603,Sentiment Lexicon Creation from Lexical Resources,Business Information Systems,"Today’s business information systems face the challenge of analyzing sentiment in massive data sets for supporting, e.g., reputation management. Many approaches rely on lexical resources containing words and their associated sentiment. We perform a corpus-based evaluation of several automated methods for creating such lexicons, exploiting vast lexical resources. We consider propagating the sentiment of a seed set of words through semantic relations or through PageRank-based similarities. We also consider a machine learning approach using an ensemble of classifiers. The latter approach turns out to outperform the others. However, PageRank-based propagation appears to yield a more robust sentiment classifier.",information system
1604,A domain-specific decision support system for knowledge discovery using association and text mining,Knowledge and Information Systems,"We propose a novel association and text mining system for knowledge discovery (ASTEK) from the warranty and service data in the automotive domain. The complex architecture of modern vehicles makes fault diagnosis and isolation a non-trivial task. The association mining isolates anomaly cases from the millions of service and claims records. ASTEK has shown 86% accuracy in correctly identifying the anomaly cases. The text mining subscribes to the diagnosis and prognosis (D&P) ontology, which provides the necessary domain-specific knowledge. The root causes associated with the anomaly cases are identified by discovering frequent symptoms associated with the part failures along with the repair actions used to fix the part failures. The best-practice knowledge is disseminated to the dealers involved in the anomaly cases. ASTEK has been implemented as a prototype in the service and quality department of GM and its performance has been validated in the real life set up. On an average, the analysis time is reduced from few weeks to few minutes, which in real life industry are significant improvements.",information system
1605,Pronouncibility index (Pi) : a distance-based and confusion-based speech quality measure for dysarthric speakers,Knowledge and Information Systems,"Recently, there have been many modern speech technologies, including those of speech synthesis and recognition, developed to help people with disabilities. While most of such technologies have successfully been applied to process speech of normal speakers, they may not be effective for speakers with speech disorder, depending on their severity. This paper proposes an automated method to preliminarily assess the ability of a speaker in pronouncing a word. Based on signal features, an indicator called pronouncibility index (Π) is introduced to express speech quality with two complementary measures, called distance-based and confusion-based factors. In the distance-based factor, the 1-norm, 2-norm and 3-norm distance are investigated while boundary-based and Gaussian-based approaches are introduced for confusion-based factors. The Π is used to estimate performance of speech recognition when it is applied to recognize speech of a dysarthric speaker. Three measures are applied to evaluate the effectiveness of Π, rank-order inconsistency, correlation coefficient, and root-mean-square of difference. The evaluations had been done by comparing its predicted recognition rates with ones predicted by the standard methods called the articulatory and intelligibility tests based on the two recognition systems (HMM and ANN). For the phoneme-test set (the training set), Π outperforms the articulatory and intelligibility tests in all three evaluations. The performance of Π decreases for the device-control set (the test set), and the intelligibility test becomes the best method followed by Π and the articulatory test. In general, Π is a promising indicator for predicting recognition rate with comparison to the standard assessments.",information system
1606,Efficient algorithms for influence maximization in social networks,Knowledge and Information Systems," In recent years, due to the surge in popularity of social-networking web sites, considerable interest has arisen regarding influence maximization in social networks. Given a social network structure, the problem of influence maximization is to determine a minimum set of nodes that could maximize the spread of influences. With a large-scale social network, the efficiency and practicability of such algorithms are critical. Although many recent studies have focused on the problem of influence maximization, these works in general are time-consuming when a social network is large-scale. In this paper, we propose two novel algorithms, CDH-Kcut and Community and Degree Heuristic on Kcut/SHRINK, to solve the influence maximization problem based on a realistic model. The algorithms utilize the community structure, which significantly decreases the number of candidates of influential nodes, to avoid information overlap. The experimental results on both synthetic and real datasets indicate that our algorithms not only significantly outperform the state-of-the-art algorithms in efficiency but also possess graceful scalability. ",information system
1607,A Computational Analysis of Linear Price Iterative Combinatorial Auction Formats,Information Systems Research," Iterative combinatorial auctions (ICAs) are IT-based economic mechanisms where bidders submit bundle bids in a sequence and an auctioneer computes allocations and ask prices in each auction round. The literature in this eld provides equilibrium analysis for ICAs with non-linear personalized prices under strong assumptions on bidders' strategies. Linear pricing has performed very well in the lab and in the eld. In this paper, we compare three selected linear price ICA formats based on allocative e ciency and revenue distribution using di erent bidding strategies and bidder valuations. The goal of this research is to benchmark di erent ICA formats, and design and analyze new auction rules for auctions with pseudo-dual linear prices. The multiitem and discrete nature of linear-price iterative combinatorial auctions and the complex price calculation schemes defy much of the traditional game theoretical analysis in this eld. Computational methods can be of great help in exploring potential auction designs and analyzing the virtues of various design options. In our simulations we found that ICA designs with linear prices performed very well for di erent valuation models even in cases of high synergies among the valuations. There were, however, signi cant di erences in e ciency and in the revenue distributions of the three ICA formats. Heuristic bidding strategies using only a few of the best bundles also led to high levels of e ciency. We have also identi ed a number of auction rules for ask price calculation and auction termination that have shown to perform very well in the simulations. ",information system
1608,Data Preparation for Mining World Wide Web Browsing Patterns,Knowledge and Information Systems," The World Wide Web (WWW) continues to grow at an astounding rate in both the sheer volume of tra c and the size and complexity of Web sites. The complexity of tasks such as Web site design, Web server design, and of simply navigating through a Web site have increased along with this growth. An important input to these design tasks is the analysis of how a Web site is being used. Usage analysis includes straightforward statistics, such as page access frequency, as well as more sophisticated forms of analysis, such as nding the common traversal paths through a Web site. Web Usage Mining is the application of data mining techniques to usage logs of large Web data repositories in order to produce results that can be used in the design tasks mentioned above. However, there are several preprocessing tasks that must be performed prior to applying data mining algorithms to the data collected from server logs. This paper presents several data preparation techniques in order to identify unique users and user sessions. Also, a method to divide user sessions into semantically meaningful transactions is de ned and successfully tested against two other methods. Transactions identied by the proposed methods are used to discover association rules from real world data using the WEBMINER system [15]. ",information system
1609,Variable Granularity Space Filling Curve for Indexing Multidimensional Data,Advances in Databases and Information Systems,"Efficiently accessing multidimensional data is a challenge for building modern database applications that involve many folds of data such as temporal, spatial, data warehousing, bio-informatics, etc. This problem stems from the fact that multidimensional data have no given order that preserves proximity. The majority of the existing solutions to this problem cannot be easily integrated into the current relational database systems since they require modifications to the kernel. A prominent class of methods that can use existing access structures are ‘space filling curves’. In this study, we describe a method that is also based on the space filling curve approach, but in contrast to earlier methods, it connects regions of various sizes rather than points in multidimensional space. Our approach allows an efficient transformation of interval queries into regions of data that results in significant improvements when accessing the data. A detailed empirical study demonstrates that the proposed method outperforms the best available off-the-shelf methods for accessing multidimensional data.",information system
1610,Aspects of the taxonomic relation in the biomedical domain,Formal Ontology in Information Systems," - Taxonomies are commonly used for organizing knowledge, particularly in biomedicine where the taxonomy of living organisms and the classification of diseases are central to the domain. The principles used to produce taxonomies are either intrinsic (properties of the partial ordering relation) or added to make knowledge more manageable (opposition of siblings and economy). The applicability of these principles in the biomedical domain is presented using the Unified Medical Language System (UMLS) and issues raised by the application of these principles are illustrated. While intrinsic principles are not challenged, we argue that the opposition of siblings brings to bear excessive constraints on a domain ontology and that the adverse effects of economy may outweigh its benefits. The two-level structure used in the UMLS is discussed. Categories & Descriptors - I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods - Relation Systems. J.3 [Computer Applications]: Life and Medical Sciences. Taxonomies are useful artifacts for organizing many aspects of knowledge, much of which can be expressed mathematically with partial orders. Taxonomies are used for representing information at appropriate levels of generality and automatically making it available to more specific concepts by means of a mechanism of inheritance [18]. As components of ontologies, taxonomies can provide an organizational model of a domain (domain ontologies), or a model suitable for specific tasks (application ontologies). The principles used to produce taxonomies are either intrinsic (properties of the partial ordering relation) or added to make knowledge more manageable (opposition of siblings and economy). In biomedicine, taxonomies such as the taxonomy of living organisms and the classification of diseases are central to the domain. However, the applicability of these principles in the biomedical domain needs to be assessed. This study is a contribution to the Medical Ontology Research project currently developed at the U.S. National Library of Medicine [2]. The major objective of this project is to develop methods ",information system
1611,Supervised tensor learning,Knowledge and Information Systems,"Tensor representation is helpful to reduce the small sample size problem in discriminative subspace selection. As pointed by this paper, this is mainly because the structure information of objects in computer vision research is a reasonable constraint to reduce the number of unknown parameters used to represent a learning model. Therefore, we apply this information to the vector-based learning and generalize the vector-based learning to the tensor-based learning as the supervised tensor learning (STL) framework, which accepts tensors as input. To obtain the solution of STL, the alternating projection optimization procedure is developed. The STL framework is a combination of the convex optimization and the operations in multilinear algebra. The tensor representation helps reduce the overfitting problem in vector-based learning. Based on STL and its alternating projection optimization procedure, we generalize support vector machines, minimax probability machine, Fisher discriminant analysis, and distance metric learning, to support tensor machines, tensor minimax probability machine, tensor Fisher discriminant analysis, and the multiple distance metrics learning, respectively. We also study the iterative procedure for feature extraction within STL. To examine the effectiveness of STL, we implement the tensor minimax probability machine for image classification. By comparing with minimax probability machine, the tensor version reduces the overfitting problem.",information system
1612,"Diffusion Models for Peer-to-Peer (P2P) Media Distribution: On the Impact of Decentralized, Constrained Supply",Information Systems Research,"In Peer-to-Peer (P2P) media distribution, users obtain content from other users who already have it. This form of decentralized product distribution demonstrates several unique features. Only a small fraction of users in the network are queried when a potential adopter seeks a file and many of these users may even free-ride i.e. not distribute the content to others. As a result, generated demand may not always be fulfilled immediately. We present mixing models for product diffusion in P2P networks that capture decentralized product distribution by current adopters, incomplete demand fulfillment and other unique aspects of P2P product diffusion. The models serve to demonstrate the important role that P2P search process and distribution referrals - payments made to users that distribute files - play in efficient P2P media distribution. We demonstrate the ability of our diffusion models to derive normative insights for P2P media distributors by studying the effectiveness of distribution referrals in speeding product diffusion and determining optimal referral policies for fully decentralized and hierarchical P2P networks.",information system
1613,Evaluation of the Mine-Merge Method for Data Mining Query Processing,Advances in Databases and Information Systems," In this paper we consider concurrent execution of multiple data mining queries in the context of discovery of frequent itemsets. If such data mining queries operate on similar parts of the database, then their overall I/O cost can be reduced by transforming the set of data mining queries into another set of non-overlapping queries, whose results can be used to efficiently answer the original queries. We discuss the problem of multiple data mining query optimization and experimentally evaluate the Mine Merge algorithm to efficiently execute sets of data mining queries. ",information system
1614,Two-layered Blogger identification model integrating profile and instance-based methods,Knowledge and Information Systems,"This paper introduces a two-layered framework that improves the result of authorship identification within larger sample numbers of bloggers as compared with earlier work. Previous studies are mainly divided into two categories: profile-based and instance-based methods. Each of these approaches has its advantages and limitations. The two-layered framework presented here integrates the two previous approaches and presents a new solution to a key problem in authorship identification, namely the drop in accuracy experienced as the number of authors increases. The paper begins by illustrating the regular instance-based core model and the investigated features. It then introduces a new psycholinguistic profile representation of authors, presents similarity grouping extraction over profiles, and applies blogger identification utilizing the two-layered approach. The results confirm the improvement introduced by the proposed two-layered approach against our regular classifier, as well as a selected baseline, for an extended number of users.",information system
1615,"Parent Plan Support System – Context, Functions and Knowledge Base",Business Information Systems,"This paper presents a knowledge base (and foundational issues of its operation) for a support system designed for divorcing parents and intending to decide on issues of exercise of parental authority and contacts with children. The legal and functional context of this system is also presented. However, the main focus is on ideas concerning representation of a very complex legal concept, i.e. well-being of the child.",information system
1616,Opening the neural network black box: an algorithm for extracting rules from function approximating artificial neural networks,International Conference on Information Systems," Artificial neural networks have been successfully applied to solve a variety of business applications involving classification and function approximation. In many such applications, it is desirable to extract knowledge from trained neural networks so that the users can gain a better understanding of the solution. Existing research works have focused primarily on extracting symbolic rules for classification problems with few methods devised for function approximation problems. In order to fill this gap, we propose an approach to extract rules from neural networks that have been trained to solve function approximation problems. The extracted rules divide the data samples into groups. For all samples within a group, a linear function of the relevant input attributes of the data approximates the network output. Experimental results show that the proposed approach generates rules that are more accurate than the existing methods based on decision trees and regression. ",information system
1617,An XML algebra for XQuery,Advances in Databases and Information Systems,"An XML algebra supporting the XQuery query language is presented. The usage of expression constructing operators instead of high-order operations using functions as parameters has permitted us to remain in the limits of first-order structures whose instance is a many-sorted algebra. The set of operators of the presented algebra substantially differs from the set of operators of relation algebra. It is caused by the complex nature of the XML data model comparing with relational one. Actually, only predicative selection is more or less same in both algebra. Yet, the XML algebra in addittion permits selection by node test. The relational projection operator is replaced by the path expression and navigating functions; the join operator is replaced by unnesting join expressions. In addition, a number of node constructing expressions permitting update of the algebra state are defined.",information system
1618,Semantic Enrichment of Ontology Mappings: A Linguistic-Based Approach,Advances in Databases and Information Systems,"There are numerous approaches to match or align ontologies resulting into mappings specifying semantically corresponding ontology concepts. Most approaches focus on finding equality correspondences between concepts, although many concepts may not have a strict equality match in other ontologies. We present a new approach to determine more expressive ontology mappings supporting different kinds of correspondences such as equality, is-a and part-of relationships between ontologies. In contrast to previous approaches, we follow a so-called enrichment strategy that semantically refines the mappings determined with a state-of-the art match tool. The enrichment strategy employs several linguistic approaches to identify the additional kinds of correspondences. An initial evaluation shows promising results and confirms the viability of the proposed enrichment strategy.",information system
1619,Discovering mappings in hierarchical data from multiple sources using the inherent structure,Knowledge and Information Systems," Unprecedented amounts of media data are publicly accessible. However, it is increasingly difficult to integrate relevant media from multiple and diverse sources for effective applications. The functioning of a multimodal integration system requires metadata, such as ontologies, that describe media resources and media components. Such metadata are generally applicationdependent and this can cause difficulties when media needs to be shared across application domains. There is a need for a mechanism that can relate the common and uncommon terms and media components. In this paper, we develop an algorithm to mine and automatically discover mappings in hierarchical media data, metadata, and ontologies, using the structural information inherent in these types of data. We evaluate the performance of this algorithm for various parameters using both synthetic and real-world data collections and show that the structure-based mining of relationships provides high degrees of precision. ",information system
1620,On ontology-driven document clustering using core semantic features,Knowledge and Information Systems,"Incorporating semantic knowledge from an ontology into document clustering is an important but challenging problem. While numerous methods have been developed, the value of using such an ontology is still not clear. We show in this paper that an ontology can be used to greatly reduce the number of features needed to do document clustering. Our hypothesis is that polysemous and synonymous nouns are both relatively prevalent and fundamentally important for document cluster formation. We show that nouns can be efficiently identified in documents and that this alone provides improved clustering. We next show the importance of the polysemous and synonymous nouns in clustering and develop a unique approach that allows us to measure the information gain in disambiguating these nouns in an unsupervised learning setting. In so doing, we can identify a core subset of semantic features that represent a text corpus. Empirical results show that by using core semantic features for clustering, one can reduce the number of features by 90% or more and still produce clusters that capture the main themes in a text corpus.",information system
1621,Context-aware multiagent system: Planning home care tasks,Knowledge and Information Systems,"Context-aware systems are able to capture information from the context in which they are executed, assign a meaning to the gathered information, and change their behavior accordingly. As a result, the systems can offer services to users according to their individual situation within the context. This article analyzes the important aspects of context-aware computing such as capturing information for context attributes and determining the manner of interacting with users in the environment. Used in conjunction with mobile devices, context-aware systems are specifically used to improve the usability of applications and services. This article proposes the home care context-aware computing (HoCCAC) multiagent system that identifies and maintains a permanent fix on the location of patients in their home, and manages the infrastructure of services within their environment securely and reliably by processing and reasoning the data received. Based on the multiagent system, a prototype was developed to monitor patients in their home. The HoCCAC multiagent system uses a critical path method-based planning model that, in the present study, prepares the most optimal task-planning schedule for the patients in their home, is capable of reacting automatically when faced with dangerous or emergency situations, replanning any plans in progress and sending alert messages to the system. The results obtained with this prototype are presented in this article.",information system
1622,Extended Conceptual Retrieval,Knowledge and Information Systems," . In the ESPRIT project CLIME (25.414) we are building a “Legal Information Server” (LIS), an advanced legal information retrieval system. Part of the LIS is a module for what we call “extended conceptual retrieval”. In CLIME we elaborate on the notion of CR in several ways. We describe the principles of extended CR, provide data on the quite extensive model we have built so far, and present first evaluation results. ",information system
1623,An ontology-based procedure for generating object model from text description,Knowledge and Information Systems,"The main objective of the procedure proposed in this paper is to use ontologies to convert a problem domain text description into an object model. The object model of a system consists of objects, identified from the text description and structural linkages corresponding to existing or established relationships. The ontologies provide metadata schemas, offering a controlled vocabulary of concepts. At the center of both object models and ontologies are objects within a given problem domain. The difference is that while the object model should contain explicitly shown structural dependencies between objects in a system, including their properties, relationships, events, and processes, the ontologies are based on related terms only. On the other hand, the object model refers to the collections of concepts used to describe the generic characteristics of objects in object-oriented languages. Because ontology is accepted as a formal, explicit specification of a shared conceptualization, we can naturally link ontologies with object models, which represent a system-oriented map of related objects, described as Data Types (ADTs). This paper addresses ontologies as a basis of a complete methodology for object modeling, including available tools, particularly CORPORUM OntoExtract and VisualText, which can help the conversion process. This paper describes how the developers can implement this methodology on the basis of an illustrative example.",information system
1624,A hybrid multi-group approach for privacy-preserving data mining,Knowledge and Information Systems,"In this paper, we propose a hybrid multi-group approach for privacy preserving data mining. We make two contributions in this paper. First, we propose a hybrid approach. Previous work has used either the randomization approach or the secure multi-party computation (SMC) approach. However, these two approaches have complementary features: the randomization approach is much more efficient but less accurate, while the SMC approach is less efficient but more accurate. We propose a novel hybrid approach, which takes advantage of the strength of both approaches to balance the accuracy and efficiency constraints. Compared to the two existing approaches, our proposed approach can achieve much better accuracy than randomization approach and much reduced computation cost than SMC approach. We also propose a multi-group scheme that makes it flexible for the data miner to control the balance between data mining accuracy and privacy. This scheme is motivated by the fact that existing randomization schemes that randomize data at individual attribute level can produce insufficient accuracy when the number of dimensions is high. We partition attributes into groups, and develop a scheme to conduct group-based randomization to achieve better data mining accuracy. To demonstrate the effectiveness of the proposed general schemes, we have implemented them for the ID3 decision tree algorithm and association rule mining problem and we also present experimental results.",information system
1625,Reconciling Attribute Values from Multiple Data Sources,International Conference on Information Systems," Because of the heterogeneous nature of multiple data sources, data integration is often one of the most challenging tasks of today's information systems. While the existing literature has focused on problems such as schema integration and entity identification, our current study attempts to answer a basic question: When an attribute value for a real-world entity is recorded differently in two databases, how should the “best” value be chosen from the set of possible values? We first show how probabilities for attribute values can be derived, and then propose a framework for deciding the cost-minimizing value based on the total cost of type I, type II, and misrepresentation errors. ",information system
1626,Security issues for the use of semantic web in e-commerce,Business Information Systems,"As the ontologies are the pivotal element of the Semantic Web in E-Commerce, it is necessary to protect the ontology’s integrity and availability. In addition, both suppliers and buyers will use an ontology to store confidential knowledge pertaining to their preferences or possible substitutions for certain products. Thus, parts of an ontology will need to be kept confidential. We propose to use well established standards of XML access control. E-commerce processes require the confidentiality of customer information, the integrity of product offers and the availability of the vendors’ servers. Our main contribution-the introduction of a Security Ontology-helps to structure and simulate IT security risks of e-commerce players that depend on their IT infrastructure.",information system
1627,Five roles of an information system: a social constructionist approach to analyzing the use of ERP systems,International Conference on Information Systems," This paper presents a novel way of thinking about how information systems are used in organisations. Traditionally, computerised information systems are viewed as objects. In contrast, by viewing the information system as an actor, the understanding of the structuration process increases. The user, being influenced by the ERP (Enterprise Resource Planning) system and giving it an actor role, thereby also confers agency on the ERP system; through its very use it influences actions and thus also the structure. Based on a case study of ERP use in an ABB company over a decade, five different roles played by the ERP systems were identified. The ERP systems acted as Bureaucrat, Manipulator, Administrative assistant, Consultant or were dismissed (Dismissed) in the sense that intended users chose to avoid using them. These terms are defined in the full text. The purpose of this approach here is not to “animate” the information systems, to give them life or a mind of their own, but rather to make explicit the socially constructed roles conferred on them by users and others who are affected by them. On this basis, it is possible to suggest how the roles can help us open up new areas of exploration concerning the fruitful use of IT. This paper presents and discusses the influe nce that information systems have on the organising process in an ABB company over a decade. It focuses on the interaction between the use of information systems and the organising of the company. (The ABB Group, employing about 160,000 people in more than 100 countries, serves customers in power transmission and distribution; automation; oil, gas, and petrochemicals; building technologies; and in financial services. The subsidiary company studied in this article produces large components for the power transmission and distribution sector.) ",information system
1628,Geometric data perturbation for privacy preserving outsourced data mining,Knowledge and Information Systems,"Data perturbation is a popular technique in privacy-preserving data mining. A major challenge in data perturbation is to balance privacy protection and data utility, which are normally considered as a pair of conflicting factors. We argue that selectively preserving the task/model specific information in perturbation will help achieve better privacy guarantee and better data utility. One type of such information is the multidimensional geometric information, which is implicitly utilized by many data-mining models. To preserve this information in data perturbation, we propose the Geometric Data Perturbation (GDP) method. In this paper, we describe several aspects of the GDP method. First, we show that several types of well-known data-mining models will deliver a comparable level of model quality over the geometrically perturbed data set as over the original data set. Second, we discuss the intuition behind the GDP method and compare it with other multidimensional perturbation methods such as random projection perturbation. Third, we propose a multi-column privacy evaluation framework for evaluating the effectiveness of geometric data perturbation with respect to different level of attacks. Finally, we use this evaluation framework to study a few attacks to geometrically perturbed data sets. Our experimental study also shows that geometric data perturbation can not only provide satisfactory privacy guarantee but also preserve modeling accuracy well.",information system
1629,Finding associations and computing similarity via biased pair sampling,Knowledge and Information Systems,"Sampling-based methods have previously been proposed for the problem of finding interesting associations in data, even for low-support items. While these methods do not guarantee precise results, they can be vastly more efficient than approaches that rely on exact counting. However, for many similarity measures no such methods have been known. In this paper we show how a wide variety of measures can be supported by a simple biased sampling method. The method also extends to find high-confidence association rules. We demonstrate theoretically that our method is superior to exact methods when the threshold for ""interesting similarity/confidence"" is above the average pairwise similarity/confidence, and the average support is not too low. Our method is particularly good when transactions contain many items. We confirm in experiments on standard association mining benchmarks that this gives a significant speedup on real data sets (sometimes much larger than the theoretical guarantees). Reductions in computation time of over an order of magnitude, and significant savings in space, are observed.",information system
1630,Using Communication Norms for Coordination: Evidence from a Distributed Team,International Conference on Information Systems," In our empirical study of a small geographically-dispersed software development team, we examine the role and importance of communication norms in facilitating effective distributed coordination. Our longitudinal investigation of the ongoing communication engaged in by team members within multiple media highlights the creation and emergence of a number of key norms that were critical to helping the team get its distributed work done. ",information system
1631,Contrary to Duty Obligations A Study in Legal Ontology,Knowledge and Information Systems," In this paper the problems in deontic logic around contrary to duty obligations are used to conduct a study in basic normative ontology. Three causes of the problems around contrary to duty obligations are identified, that is 1) the attempt to analyze obligations in terms of what is ideally the case, 2) the application of deontic inheritance to the presuppositions of obligations, and 3) the failure to distinguish between what will be called 'inclusive' and 'exclusive ought-to-do'. These three causes are all attributed to insufficient distinctions on the ontological level. ",information system
1632,Decision-Centric Active Learning of Binary-Outcome Models,Information Systems Research," It can be expensive to acquire the data required for businesses to employ data-driven predictive modeling, for example to model consumer preferences to optimize targeting. Prior research has introduced “active learning” policies for identifying data that are particularly useful for model induction, with the goal of decreasing the statistical error for a given acquisition cost (error-centric approaches). However, predictive models are used as part of a decision-making process, and costly improvements in model accuracy do not always result in better decisions. This paper introduces a new approach for active data acquisition that targets decision-making specifically. The new decision-centric approach departs from traditional active learning by placing emphasis on acquisitions that are more likely to affect decision-making. We describe two different types of decision-centric techniques. Next, using direct-marketing data, we compare various data-acquisition techniques. We demonstrate that strategies for reducing statistical error can be wasteful in a decision-making context, and show that one decision-centric technique in particular can improve targeting decisions significantly. We also show that this method is robust in the face of decreasing quality of utility estimations, eventually converging to uniform random sampling, and that it can be extended to situations where different data acquisitions have different costs. The results suggest that businesses should consider modifying their strategies for acquiring information through normal business transactions. For example, a firm such as Amazon.com that models consumer preferences for customized marketing may accelerate learning by proactively offering recommendations-not merely to induce immediate sales, but for improving recommendations in the future. With advances in computing power, network reach, availability of data, and the maturity of induction algorithms, businesses are taking advantage of automated predictive modeling to influence repetitive decisions, often as tools for extracting customer, competitor and market intelligence (Berry and Linoff, 2004). Consider an example: telecommunications companies face severe customer retention problems, as customers switch back and forth between carriers (the problem of ""churn""). For each customer, at each point in time, the company faces a decision between doing nothing and intervening in an attempt to retain the customer. ",information system
1633,Social Network Effects on Productivity and Job Security: Evidence from the Adoption of a Social Networking Tool,Information Systems Research,"By studying the change in employees’ network positions before and after the introduction of a social networking tool, I find that information-rich networks (low in cohesion and rich in structural holes), enabled by social media, have a positive effect on various work outcomes. Contrary to the notion that network positions are difficult to alter, I show that social media can induce a change in network structure, one from which individuals can derive economic benefits. In addition, I consider two intermediate mechanisms by which an information-rich network is theorized to improve work performance — information diversity and social communication — and quantify their effects on productivity and job security. Analysis shows that productivity, as measured by billable revenue, is more associated with information diversity than with social communication. However, the opposite is true for job security. Social communication is more correlated with reduced layoff risks than is information diversity. This, in turn, suggests that information-rich networks enabled through the use of social media can drive both work performance and job security, but that there is a tradeoff between engaging in social communication and gathering diverse information.",information system
1634,Banded structure in binary matrices,Knowledge and Information Systems,"A binary matrix has a banded structure if both rows and columns can be permuted so that the non-zero entries exhibit a staircase pattern of overlapping rows. The concept of banded matrices has its origins in numerical analysis, where entries can be viewed as descriptions between the problem variables; the bandedness corresponds to variables that are coupled over short distances. Banded data occurs also in other applications, for example in the physical mapping problem of the human genome, in paleontological data, in network data and in the discovery of overlapping communities without cycles. We study the banded structure of binary matrices, give a formal definition of the concept and discuss its theoretical properties. We consider the algorithmic problems of computing how far a matrix is from being banded, and of finding a good submatrix of the original data that exhibits approximate bandedness. Finally, we show by experiments on real data from ecology and other applications the usefulness of the concept. Our results reveal that bands exist in real datasets and that the final obtained orderings of rows and columns have natural interpretations.",information system
1635,Top 10 algorithms in data mining,Knowledge and Information Systems, g ,information system
1636,Research Note---To Continue Sharing or Not to Continue Sharing? An Empirical Analysis of User Decision in Peer-to-Peer Sharing Networks,Information Systems Research,"Peer-to-peer sharing networks have seen explosive growth recently. In these networks, sharing files is completely voluntary, and there is no financial reward for users to contribute. However, many users continue to share despite the massive free-riding by others. Using a large-scale data set of individual activities in a peer-to-peer music-sharing network, we seek to understand users continued-sharing behavior as a private contribution to a public good. We find that the more benefit users ""get from"" the network, in the form of downloads, browses, and searches, the more likely they are to continue sharing. Also, the more value users ""give to"" the network, in the form of downloads by other users and recognition by the network, the more likely they are to continue sharing. Moreover, our findings suggest that, overall, ""getting from"" is a stronger force for the continued-sharing decision than ""giving to"". ÂŠ 2012 INFORMS.",information system
1637,CanTree: a canonical-order tree for incremental frequent-pattern mining,Knowledge and Information Systems," Since its introduction, frequent-pattern mining has been the subject of numerous studies, including incremental updating. Many existing incremental mining algorithms are Apriori-based, which are not easily adoptable to FP-treebased frequent-pattern mining. In this paper, we propose a novel tree structure, called CanTree (canonical-order tree), that captures the content of the transaction database and orders tree nodes according to some canonical order. By exploiting its nice properties, the CanTree can be easily maintained when database transactions are inserted, deleted, and/or modified. For example, the CanTree does not require adjustment, merging, and/or splitting of tree nodes during maintenance. No rescan of the entire updated database or reconstruction of a new tree is needed for incremental updating. Experimental results show the effectiveness of our CanTree in the incremental mining of frequent patterns. Moreover, the applicability of CanTrees is not confined to incremental mining; CanTrees can also be applicable to other frequent-pattern mining tasks including constrained mining and interactive mining. ",information system
1638,Towards A Realism-Based Metric for Quality Assurance in Ontology Matching,Formal Ontology in Information Systems," Ontology matching is commonly defined as a matter of dealing with semantic correspondences between terms in ontologies and thus refers to more specific activities such as mapping or aligning, possibly with ontology merging in mind. However, it has been pointed out that there still prevails no common understanding of what such 'semantic correspondences' are supposed to be, and that in consequence “human experts do not agree on how ontologies should be merged, and we do not yet have a good enough metric for comparing ontologies.” In what follows we define such a metric, which is designed to allow assessment of the degree to which the integration of two ontologies yields improvements over either of the input ontologies. We start out from the thesis that if two or more ontologies are to be considered for matching, then, however much they may reflect distinct views of reality on the part of their authors, the portions of reality to which they refer must be such as to overlap. Our approach takes account of the fact that both authors and users of ontologies may make mistakes (the former in their interpretation of reality and in the formulation of their views, the latter in misinterpreting the former's intentions).To do justice to such factors, we need to draw a distinction between three levels of: (1) reality; (2) cognitive representations; and (3) publicly accessible concretizations of these representations. We can then define 'semantic correspondence' not, as is usual, in terms of (horizontal) relations of 'association' or 'synonymy' between the terms within the ontologies to be matched, but rather in terms of the (vertical) relation of reference: terms correspond semantically if they refer to the same entities in reality. One conclusion of our argument is that, when ontology matching has been used as the first step towards ontology merging, then the merged ontology can contain inconsistencies only if there are already inconsistencies in at least one of the source ontologies. ",information system
1639,Tuple MapReduce and Pangool: an associated implementation,Knowledge and Information Systems,"This paper presents Tuple MapReduce, a new foundational model extending MapReduce with the notion of tuples. Tuple MapReduce allows to bridge the gap between the low-level constructs provided by MapReduce and higher-level needs required by programmers, such as compound records, sorting, or joins. This paper shows as well Pangool, an open-source framework implementing Tuple MapReduce. Pangool eases the design and implementation of applications based on MapReduce and increases their flexibility, still maintaining Hadoop’s performance. Additionally, this paper shows: pseudo-codes for relational joins, rollup, and the PageRank algorithm; a Pangool’s code example; benchmark results comparing Pangool with existing approaches; reports from users of Pangool in industry; and the description of a distributed database exploiting Pangool. These results show that Tuple MapReduce can be used as a direct, better-suited replacement of the MapReduce model in current implementations without the need of modifying key system fundamentals.",information system
1640,Quete: Ontology-Based Query System for Distributed Sources,Advances in Databases and Information Systems,"The exponential growth of the web and the extended use of database management systems in widely distributed information systems has brought to the fore the need for seamless interconnection of diverse and large numbers of information sources. Our contribution is a system that provides a flexible approach for integrating and transparently querying multiple data sources, using a reference ontology. Global semantic queries are automatically mapped to queries local to the participating sources. The query system is capable of handling complex join constructs and of choosing the appropriate attributes, relations, and join conditions to preserve user query semantics. Moreover, the query engine exploits information on horizontal, vertical, and hybrid fragmentation of database tables, distributed over the various data sources. This optimization improves system’s recall and boosts its effectiveness and performance.",information system
1641,Agent-Based Systems for Intelligent Manufacturing: A State-of-the-Art Survey,Knowledge and Information Systems,"Agent technology has been considered as an important approach for developing distributed intelligent manufacturing systems. A number of researchers have attempted to apply agent technology to manufacturing enterprise integration, supply chain management, manufacturing planning, scheduling and control, materials handling, and holonic manufacturing systems. This paper gives a brief survey of some related projects in this area, and discusses some key issues in developing agent-based manufacturing systems such as agent technology for enterprise integration and supply chain management, agent encapsulation, system architectures, dynamic system reconfiguration, learning, design and manufacturability assessments, distributed dynamic scheduling, integration of planning and scheduling, concurrent scheduling and execution, factory control structures, potential tools and standards for developing agent-based manufacturing systems. An extensive annotated bibliography is provided.",information system
1642,Canonical forms for labelled trees and their applications in frequent subtree mining,Knowledge and Information Systems," Tree structures are used extensively in domains such as computational biology, pattern recognition, XML databases, computer networks, and so on. In this paper, we first present two canonical forms for labelled rooted unordered trees-the breadth-first canonical form (BFCF) and the depth-first canonical form (DFCF). Then the canonical forms are applied to the frequent subtree mining problem. Based on the BFCF, we develop a vertical mining algorithm, RootedTreeMiner, to discover all frequently occurring subtrees in a database of labelled rooted unordered trees. The RootedTreeMiner algorithm uses an enumeration tree to enumerate all (frequent) labelled rooted unordered subtrees. Next, we extend the definition of the DFCF to labelled free trees and present an Apriori-like algorithm, FreeTreeMiner, to discover all frequently occurring subtrees in a database of labelled free trees. Finally, we study the performance and the scalability of our algorithms through extensive experiments based on both synthetic data and datasets from real applications. ",information system
1643,An Intelligent Information System for Organizing Online Text Documents,Knowledge and Information Systems,"Abstract. This paper describes an intelligent information system for effectively managing huge amounts of online text documents (such as Web documents) in a hierarchical manner. The organizational capabilities of this system are able to evolve semi-automatically with minimal human input. The system starts with an initial taxonomy in which documents are automatically categorized, and then evolves so as to provide a good indexing service as the document collection grows or its usage changes. To this end, we propose a series of algorithms that utilize text-mining technologies such as document clustering, document categorization, and hierarchy reorganization. In particular, clustering and categorization algorithms have been intensively studied in order to provide evolving facilities for hierarchical structures and categorization criteria. Through experiments using the Reuters-21578 document collection, we evaluate the performance of the proposed clustering and categorization methods by comparing them to those of well-known conventional methods.",information system
1644,Word-Based Compression Methods and Indexing for Text Retrieval Systems,Advances in Databases and Information Systems,"Abstarct In this article we present a new compression method, called WLZW, which is a word-based modication of classic LZW. The modication is similar to the approach used in the HuffWord compression algorithm. The algorithm is two-phase, the compression ratio achieved is fairly good, on average 22%-20% (see [2],[3]). Moreover, the table of words, which is side product of compression, can be used to create full-text index, especially for dynamic text databases. Overhead of the index is good.",information system
1645,Approximating the number of frequent sets in dense data,Knowledge and Information Systems,"We investigate the problem of counting the number of frequent (item)sets—a problem known to be intractable in terms of an exact polynomial time computation. In this paper, we show that it is in general also hard to approximate. Subsequently, a randomized counting algorithm is developed using the Markov chain Monte Carlo method. While for general inputs an exponential running time is needed in order to guarantee a certain approximation bound, we show that the algorithm still has the desired accuracy on several real-world datasets when its running time is capped polynomially.",information system
1646,Schema-based query optimization for XQuery queries,Advances in Databases and Information Systems," XQuery is widely used for querying XML documents. Within this paper, we examine optimization rules for XQuery queries that exploit type information of the input XML document given in XML Schema. These optimization rules are applicable for all XQuery expressions and are very useful e.g. in the scenario of XQuery queries on XQuery views. The basic idea is to transform the XML Schema definition into a graph, which is extended to a graph representing the XQuery expression. The latter graph is used to delete subexpressions of the XQuery expression that are not used to retrieve the final result of the given XQuery expression. We further include experimental results that demonstrate the improvement of our optimization. ",information system
1647,Searching for similar trajectories on road networks using spatio-temporal similarity,Advances in Databases and Information Systems,"In order to search similar moving object trajectories, the previously used methods focused on Euclidean distance and considered only spatial similarity. Euclidean distance is not appropriate for road network space, where the distance is limited to the space adjacent to the roads. In this paper, we consider the properties of moving objects in road network space and define temporal similarity as well as spatio-temporal similarity between trajectories based on POI (Points of Interest) and TOI (Times of Interest) on road networks. Based on these definitions, we propose methods for searching for similar trajectories in road network space. Experimental results show the accuracy of our methods and the average search time in query processing.",information system
1648,Extended feature combination model for recommendations in location-based mobile services,Knowledge and Information Systems,"With the increasing availability of location-based services, location-based social networks and smart phones, standard rating schema of recommender systems that involve user and item dimensions is extended to three-dimensional (3-D) schema involving context information. Although there are models proposed for dealing with data in this form, the problem of combining it with additional features and constructing a general model suitable for different forms of recommendation system techniques has not been fully explored. This work proposes a technique to reduce 3-D rating data into 2-D for two reasons: employing already developed efficient methods for 2-D on a 3-D data and expanding it with additional features, which are usually 2-D also, if it is necessary. Our experiments show that this reduction is effective. The proposed 2-D model supports content-based, collaborative filtering and hybrid recommendation approaches effectively, whereas we have achieved the best accuracy results for pure collaborative filtering recommendation model. Since our method was built on efficient singular value decomposition-based dimension reduction idea, it also works very efficiently, and in our experiments, we have obtained better run-time results than standard methods developed for 3-D data using higher-order singular value decomposition.",information system
1649,Conceptual analysis of lexical taxonomies: the case of WordNet top-level,Formal Ontology in Information Systems," In this paper we propose an analysis and an upgrade of WordNet's top-level synset taxonomy. We briefly review WordNet and identify its main semantic limitations. Some principles from a forthcoming OntoClean methodology are applied to the ontological analysis of WordNet. A revised top-level taxonomy is proposed, which is meant to be more conceptually rigorous, cognitively transparent, and efficiently exploitable in several applications. Categories & Descriptors H3 .1 [Inform atio n S to rag e an d R etrieval]: C on ten t An aly sis and Indexing -Indexing methods, Linguistic processing, Thesauruses. ",information system
1650,Zipf's law for Web surfers,Knowledge and Information Systems," One of the main activities of web users, known as “surfing”, is to follow links. Lengthy navigation often leads to disorientation when users lose track of the context in which they are navigating and are unsure how to proceed in terms of the goal of their original query. Studying navigation patterns of web users is thus important, since it can lead us to a better understanding of the problems users face when they are surfing. We derive Zipf's rank frequency law (i.e. an inverse power law) from an absorbing Markov chain model of surfers' behaviour assuming that less probable navigation trails are, on average, longer than more probable ones. In our model the probability of a trail is interpreted as the relevance (or “value”) of the trail. We apply our model to two scenarios: in the first the probability of a user terminating the navigation session is independent of the number of links he has followed so far, and in the second the probability of a user terminating the navigation session increases by a constant each time the user follows a link. We analyse these scenarios using two sets of experimental data sets showing that, although the first scenario is only a rough approximation of surfers' behaviour, the data is consistent with the second scenario and can thus provide an explanation of surfers' behaviour. ",information system
1651,On Deploying and Executing Data-Intensive Code on SMart Autonomous Storage (SmAS) Disks,Advances in Databases and Information Systems,"There is an increasing demand for storage capacity and storage throughput, driven largely by new data types such as video data and satellite images as well as by the growing use of the Internet and the web that generate and transmit rapidly evolving datasets. Thus, there is a need for storage architectures that scale the processing power with the growing size of the datasets. In this paper, we present the SMAS system that employs network attached disks with processing capabilities. In the SMAS system, users can deploy and execute code at the disk. Application code is written in a stream-based language that enforces code security and bounds the codeś memory requirements. The SMAS operating system at the disk provides basic support for process scheduling and memory management. We present an initial implementation of the system and report performance results that validate our approach for data-intensive applications.",information system
1652,Mining gene–sample–time microarray data: a coherent gene cluster discovery approach,Knowledge and Information Systems,"Extensive studies have shown that mining microarray data sets is important in bioinformatics research and biomedical applications. In this paper, we explore a novel type of gene–sample–time microarray data sets that records the expression levels of various genes under a set of samples during a series of time points. In particular, we propose the mining of coherent gene clusters from such data sets. Each cluster contains a subset of genes and a subset of samples such that the genes are coherent on the samples along the time series. The coherent gene clusters may identify the samples corresponding to some phenotypes (e.g., diseases), and suggest the candidate genes correlated to the phenotypes. We present two efficient algorithms, namely the Sample-Gene Search and the Gene–Sample Search, to mine the complete set of coherent gene clusters. We empirically evaluate the performance of our approaches on both a real microarray data set and synthetic data sets. The test results have shown that our approaches are both efficient and effective to find meaningful coherent gene clusters.",information system
1653,Opting-in or Opting-out on the Internet: Does it Really Matter?,International Conference on Information Systems," Personal privacy has become one of the pressure points that comprises utmost primacy in the scientific community. An often debated privacy issue concerns the means of soliciting consent on the use of consumer information: should consumers be asked to object to the use of personal data (opt-out), or should they be asked to consent to the use of such data (opt-in)? These questions have been the center of controversy in Internet privacy for the past few years; various industry and consumer associations hold contradictory opinions on these questions. This paper integrates various theoretical perspectives that could potentially explain the difference in consumer participation between opt-in and opt-out configurations. Specifically, an experiment was conducted to observe the responses of a group of subjects under both opt-in and opt-out scenarios. In addition, we measured the privacy concerns of the subjects and examined whether these concerns could influence the effectiveness of the two registration mechanisms. Our results show that the use of opt-in and optout could induce different participation levels, and the disparity in participation was more substantial among the less privacy-concerned population. These findings provide valuable insights to regulatory bodies in formulating privacy policies and help Internet Web sites design proper data collection practices. ",information system
1654,Query-sensitive similarity measures for information retrieval,Knowledge and Information Systems," The application of document clustering to information retrieval has been motivated by the potential effectiveness gains postulated by the cluster hypothesis. The hypothesis states that relevant documents tend to be highly similar to each other, and therefore tend to appear in the same clusters. In this paper we propose an axiomatic view of the hypothesis, by suggesting that documents relevant to the same query (co-relevant documents) display an inherent similarity to each other which is dictated by the query itself. Because of this inherent similarity, the cluster hypothesis should be valid for any document collection. Our research describes an attempt to devise means by which this similarity can be detected. We propose the use of query-sensitive similarity measures that bias interdocument relationships towards pairs of documents that jointly possess attributes that are expressed in a query. We experimentally tested three query-sensitive measures against conventional ones that do not take the context of the query into account, and we also examined the comparative effectiveness of the three query-sensitive measures. We calculated interdocument relationships for varying numbers of top-ranked documents for six document collections. Our results show a consistent and significant increase in the number of relevant documents that become nearest neighbours of any given relevant document when query-sensitive measures are used. These results suggest that the effectiveness of a cluster-based IR system has the potential to increase through the use of query-sensitive similarity measures. ",information system
1655,Trust in e-commerce vendors: a two-stage model,International Conference on Information Systems," This study investigates the development of trust in a Web-based vendor during two stages of a consumer's Web experience: exploration and commitment. Through an experimental design, the study tests the effects of third party endorsements, reputation, and individual differences on trust in the vendor during these two stages. ",information system
1656,A new semantic relatedness measurement using WordNet features,Knowledge and Information Systems,"Computing semantic similarity/relatedness between concepts and words is an important issue of many research fields. Information theoretic approaches exploit the notion of Information Content (IC) that provides for a concept a better understanding of its semantics. In this paper, we present a complete IC metrics survey with a critical study. Then, we propose a new intrinsic IC computing method using taxonomical features extracted from an ontology for a particular concept. This approach quantifies the subgraph formed by the concept subsumers using the depth and the descendents count as taxonomical parameters. In a second part, we integrate this IC metric in a new parameterized multistrategy approach for measuring word semantic relatedness. This measure exploits the WordNet features such as the noun “is a” taxonomy, the nominalization relation allowing the use of verb “is a” taxonomy and the shared words (overlaps) in glosses. Our work has been evaluated and compared with related works using a wide set of benchmarks conceived for word semantic similarity/relatedness tasks. Obtained results show that our IC method and the new relatedness measure correlated better with human judgments than related works.",information system
1657,Efficient mining of maximal frequent itemsets from databases on a cluster of workstations,Knowledge and Information Systems,"In this paper, we propose two parallel algorithms for mining maximal frequent itemsets from databases. A frequent itemset is maximal if none of its supersets is frequent. One parallel algorithm is named distributed max-miner (DMM), and it requires very low communication and synchronization overhead in distributed computing systems. DMM has the local mining phase and the global mining phase. During the local mining phase, each node mines the local database to discover the local maximal frequent itemsets, then they form a set of maximal candidate itemsets for the top-down search in the subsequent global mining phase. A new prefix tree data structure is developed to facilitate the storage and counting of the global candidate itemsets of different sizes. This global mining phase using the prefix tree can work with any local mining algorithm. Another parallel algorithm, named parallel max-miner (PMM), is a parallel version of the sequential max-miner algorithm (Proc of ACM SIGMOD Int Conf on Management of Data, 1998, pp 85–93). Most of existing mining algorithms discover the frequent k-itemsets on the kth pass over the databases, and then generate the candidate (k + 1)-itemsets for the next pass. Compared to those level-wise algorithms, PMM looks ahead at each pass and prunes more candidate itemsets by checking the frequencies of their supersets. Both DMM and PMM were implemented on a cluster of workstations, and their performance was evaluated for various cases. They demonstrate very good performance and scalability even when there are large maximal frequent itemsets (i.e., long patterns) in databases.",information system
1658,DFSP: a Depth-First SPelling algorithm for sequential pattern mining of biological sequences,Knowledge and Information Systems,"Scientific progress in recent years has led to the generation of huge amounts of biological data, most of which remains unanalyzed. Mining the data may provide insights into various realms of biology, such as finding co-occurring biosequences, which are essential for biological data mining and analysis. Data mining techniques like sequential pattern mining may reveal implicitly meaningful patterns among the DNA or protein sequences. If biologists hope to unlock the potential of sequential pattern mining in their field, it is necessary to move away from traditional sequential pattern mining algorithms, because they have difficulty handling a small number of items and long sequences in biological data, such as gene and protein sequences. To address the problem, we propose an approach called Depth-First SPelling (DFSP) algorithm for mining sequential patterns in biological sequences. The algorithm’s processing speed is faster than that of PrefixSpan, its leading competitor, and it is superior to other sequential pattern mining algorithms for biological sequences.",information system
1659,Organisational Ontology Framework for Semantic Business Process Management,Business Information Systems,"The field of Semantic Business Process Management (SBPM) has refuelled interest in using ontologies for the representation of the static and dynamic aspects of an enterprise and value chains. Putting the SBPM vision into practice, however, requires a consistent and operational network of ontologies reflecting the various spheres of enterprise structures and operations. Consistent means that the ontologies are based on compatible paradigms, have a compatible degree of detail, and include at least partial sets of alignment relations which allow data interoperability. Operational means that the ontology specifications are available in a single, current ontology formalism for which scalable repositories, reasoning support, APIs, and tools are available. In this paper, we describe a set of ontologies for SBPM that follows the mentioned requirements, and compare our work with the related efforts.",information system
1660,Interconnecting Workflows Using Services: An Approach for “Case Transfer” with Centralized Control,International Conference on Information Systems,"In this paper, we are interested in structured cooperation based on workflow (WF). The current work proposes an approach based on services for WF interconnection particularly obeying to the ""case transfer"" architecture. This late defines a form of cooperation in Inter-Organizational WF involving a range of partners with common business goals, exercising the same business. All partners share the same WF model implemented at each location and a transfer policy to manage transfer for process instances from one partner to another. By the use of services, our goal is to obtain IOWF models flexible enough so they remain easily adaptable to support process changes. The proposed approach is based on centralized control for transfers. © 2012 Springer-Verlag.",information system
1661,Self-supervised relation extraction from the Web,Knowledge and Information Systems,"Web extraction systems attempt to use the immense amount of unlabeled text in the Web in order to create large lists of entities and relations. Unlike traditional Information Extraction methods, the Web extraction systems do not label every mention of the target entity or relation, instead focusing on extracting as many different instances as possible while keeping the precision of the resulting list reasonably high. SRES is a self-supervised Web relation extraction system that learns powerful extraction patterns from unlabeled text, using short descriptions of the target relations and their attributes. SRES automatically generates the training data needed for its pattern-learning component. The performance of SRES is further enhanced by classifying its output instances using the properties of the instances and the patterns. The features we use for classification and the trained classification model are independent from the target relation, which we demonstrate in a series of experiments. We also compare the performance of SRES to the performance of the state-of-the-art KnowItAll system, and to the performance of its pattern learning component, which learns simpler pattern language than SRES.",information system
1662,Highly discriminative statistical features for email classification,Knowledge and Information Systems,"This paper reports on email classification and filtering, more specifically on spam versus ham and phishing versus spam classification, based on content features. We test the validity of several novel statistical feature extraction methods. The methods rely on dimensionality reduction in order to retain the most informative and discriminative features. We successfully test our methods under two schemas. The first one is a classic classification scenario using a 10-fold cross-validation technique for several corpora, including four ground truth standard corpora: Ling-Spam, SpamAssassin, PU1, and a subset of the TREC 2007 spam corpus, and one proprietary corpus. In the second schema, we test the anticipatory properties of our extracted features and classification models with two proprietary datasets, formed by phishing and spam emails sorted by date, and with the public TREC 2007 spam corpus. The contributions of our work are an exhaustive comparison of several feature selection and extraction methods in the frame of email classification on different benchmarking corpora, and the evidence that especially the technique of biased discriminant analysis offers better discriminative features for the classification, gives stable classification results notwithstanding the amount of features chosen, and robustly retains their discriminative value over time and data setups. These findings are especially useful in a commercial setting, where short profile rules are built based on a limited number of features for filtering emails.",information system
1663,Object Query Optimization in the Stack-Based Approach,Advances in Databases and Information Systems,The problem of query optimization in object bases is addressed. A formalized OQL-like query language SBQL is described. SBQL follows the stack-based approach to object query languages. A general method of query optimization based on rewriting is presented. It consists in detecting and factoring out so-called independent subqueries. The approach is based on static analysis of scoping and binding properties for names occurring in a query. The presented method is very powerful and simple to analyze and implement.,information system
1664,On Top-k Search with No Random Access Using Small Memory,Advances in Databases and Information Systems,Methods of top-k search with no random access can be used to find k best objects using sorted lists of attributes that can be read only by sorted access. Such methods usually need to work with a large number of candidates during the computation. In this paper we propose new methods of no random access top-k search that can be used to compute k best objects using small memory. We present results of experiments showing improvement in speed depending on ratio of memory size and data size. Our system outperforms other also when the total number of attributes is much bigger than number of query attributes (varying with user).,information system
1665,Decision-making on pipe stress analysis enabled by knowledge-based systems,Knowledge and Information Systems,"This paper presents engineering decision-making on pipe stress analysis through the application of knowledge-based systems (KBS). Stress analysis, as part of the design and analysis of process pipe networks, serves to identify whether a given pipe arrangement can cope with weight, thermal, and pressure stress at safe operation levels. An iterative process of design and analysis cycle is done routinely by engineers while analyzing the existing networks or while designing the process pipe networks. In our proposal, the KBS establishes a bidirectional communication with the current engineering software for pipe stress analysis, so that the user benefits from this integration. The stress analysis knowledge base is constructed by registering the senior engineers’ know-how. The engineers’ overall strategy to follow up during the pipe stress analysis, to some extent contained by the KBS, is presented. Advantages in saving engineering man-hours and usefulness in guiding experts in pipe stress analysis are the major services for the process industry.",information system
1666,Tracking clusters in evolving data streams over sliding windows,Knowledge and Information Systems,"Mining data streams poses great challenges due to the limited memory availability and real-time query response requirement. Clustering an evolving data stream is especially interesting because it captures not only the changing distribution of clusters but also the evolving behaviors of individual clusters. In this paper, we present a novel method for tracking the evolution of clusters over sliding windows. In our SWClustering algorithm, we combine the exponential histogram with the temporal cluster features, propose a novel data structure, the Exponential Histogram of Cluster Features (EHCF). The exponential histogram is used to handle the in-cluster evolution, and the temporal cluster features represent the change of the cluster distribution. Our approach has several advantages over existing methods: (1) the quality of the clusters is improved because the EHCF captures the distribution of recent records precisely; (2) compared with previous methods, the mechanism employed to adaptively maintain the in-cluster synopsis can track the cluster evolution better, while consuming much less memory; (3) the EHCF provides a flexible framework for analyzing the cluster evolution and tracking a specific cluster efficiently without interfering with other clusters, thus reducing the consumption of computing resources for data stream clustering. Both the theoretical analysis and extensive experiments show the effectiveness and efficiency of the proposed method.",information system
1667,Looking Without Seeing: Understanding Unsophisticated Consumers' Success and Failure to Detect Internet Deception,International Conference on Information Systems," Do unsophisticated consumers fall prey to Internet consumer frauds? Why? To answer these questions this paper integrates two streams of empirical research: the process-oriented theory of deception, and the broader deception, trust, and risk (DTR) model of Internet consumer behavior. A laboratory experiment tests several alternative hypotheses about the determinants of failure at detecting Internet deceptions. The findings suggest that Internet consumers process the clues that a site may be deceptive, but are unable to effectively evaluate and combine these clues, i.e., to draw correct conclusions from them. This is good news in the ongoing struggle against Internet fraud because it suggests that consumers lack the knowledge, not the capacity, to detect deceptions and that consumer education programs might be effective in helping consumers to protect themselves. ",information system
1668,Object-Oriented Design of a Flexible Workflow Management System,Advances in Databases and Information Systems,"Workflow management systems aim at controlling the execution of complex application processes in distributed environments. Workflow management currently moves from modeling and executing mostly static structured workflows to supporting flexible workflows, which are typically executed in distributed and heterogeneous environments. This paper discusses the use of distributed object technology to built a flexible workflow management system. In particular, based on a detailed object-oriented object model, we discuss the dynamic behavior of workflow instances, and we show how flexibility requirements have influenced our design.",information system
1669,OntoMap: an ontology-based architecture to perform the semantic mapping between an interlingua and software components,Knowledge and Information Systems,"This paper is about the use of natural language to communicate with computers. Most researches that have pursued this goal consider only requests expressed in English. A way to facilitate the use of several languages in natural language systems is by using an interlingua. An interlingua is an intermediary representation for natural language information that can be processed by machines. We propose to convert natural language requests into an interlingua [universal networking language (UNL)] and to execute these requests using software components. In order to achieve this goal, we propose OntoMap, an ontology-based architecture to perform the semantic mapping between UNL sentences and software components. OntoMap also performs component search and retrieval based on semantic information formalized in ontologies and rules.",information system
1670,Supporting use-case reviews,Business Information Systems,"Use cases are a popular way of specifying functional requirements of computer-based systems. Each use case contains a sequence of steps which are described with a natural language. Use cases, as any other description of functional requirements, must go through a review process to check their quality. The problem is that such reviews are time consuming. Moreover, effectiveness of a review depends on quality of the submitted document - if a document contains many easy-to-detect defects, then reviewers tend to find those simple defects and they feel exempted from working hard to detect difficult defects. To solve the problem it is proposed to augment a requirements management tool with a detector that would find easy-to-detect defects automatically.",information system
1671,A Rule-Oriented Architecture to Incorporate Dissemination-Based Information Delivery into Information Integration Environments,Advances in Databases and Information Systems,"Integration of heterogeneous information sources has been one of important research issues in recent advanced application environments. Today, various types of information sources are available. Dissemination-based information delivery services that autonomously deliver information from the server sites to users are among the useful and promising information sources. In this paper, we present incorporation of dissemination-based information delivery into information integration environments. The integration here has two goals: (1) Users can utilize dissemination-based information services as other information sources such as databases and the Web. Namely, they can be sources of information inte gration. (2) Users can obtain integrated information through dissemination-based delivery. We explain this requirement can be met by a combination of an information integration engine and event-driven rule processing scheme. We also explain prototype system development.",information system
1672,Spectral evolution in dynamic networks,Knowledge and Information Systems,"We introduce and study the spectral evolution model, which characterizes the growth of large networks in terms of the eigenvalue decomposition of their adjacency matrices: In large networks, changes over time result in a change of a graph’s spectrum, leaving the eigenvectors unchanged. We validate this hypothesis for several large social, collaboration, rating, citation, and communication networks. Following these observations, we introduce two link prediction algorithms based on the learning of the changes to a network’s spectrum. These new link prediction methods generalize several common graph kernels that can be expressed as spectral transformations. The first method is based on reducing the link prediction problem to a one-dimensional curve-fitting problem which can be solved efficiently. The second algorithm extrapolates a network’s spectrum to predict links. Both algorithms are evaluated on fifteen network datasets for which edge creation times are known.",information system
1673,Optimistic Concurrency Control Algorithm with Dynamic Serialization Adjustment for Firm Deadline Real-Time Database Systems,Advances in Databases and Information Systems,"A new optimistic concurrency control algorithm for firm deadline real-time database systems is presented. The algorithm dynamically adjusts a serialization order among conflicting transactions and, thus, tries to reduce the number of unnecessary restarts of transactions. Instead of aborting a lower priority transaction being in conflict with already committed higher priority transaction the algorithm is looking for a new serialization order, i.e. it tries to serialize the transaction before the conflicting one. Through simulation experiments, we evaluate the performance of the algorithm, and compare the algorithm with two well-known optimistic concurrency control algorithms: OCC and OPT-BC. Experimental results have shown that the performance of the algorithm depends on a system workload. The probability of successful reordering of conflicting transactions decreases with the increasing number of conflicts between the transactions.",information system
1674,Lying on the Web: Implications for Expert Systems Redesign,Information Systems Research," Wapplications in which data provided as input may be distorted by the system user, such as an applicant for e consider a new variety of sequential information gathering problems that are applicable for Web-based a credit card. We propose two methods to compensate for input distortion. The first method, termed knowledge base modification, considers redesigning the knowledge base of an expert system to best account for distortion in the input provided by the user. The second method, termed input modification, modifies the input directly to account for distortion and uses the modified input in the existing (unmodified) knowledge base of the system. These methods are compared with an approach where input noise is ignored. Experimental results indicate that both types of modification substantially improve the accuracy of recommendations, with knowledge base modification outperforming input modification in most cases. Knowledge base modification is, however, more computationally intensive than input modification. Therefore, when computational resources are adequate, the knowledge base modification approach is preferred; when such resources are very limited, input modification may be the only viable alternative. ",information system
1675,Graph OLAP: a multi-dimensional framework for graph data analysis,Knowledge and Information Systems," Databases and data warehouse systems have been evolving from handling normalized spreadsheets stored in relational databases, to managing and analyzing diverse application-oriented data with complex interconnecting structures. Responding to this emerging trend, graphs have been growing rapidly and showing their critical importance in many applications, such as the analysis of XML, social networks, Web, biological data, multimedia data and spatiotemporal data. Can we extend useful functions of databases and data warehouse systems to handle graph structured data? In particular, OLAP (On-Line Analytical Processing) has been a popular tool for fast and user-friendly multi-dimensional analysis of data warehouses. Can we OLAP graphs? Unfortunately, to our best knowledge, there are no OLAP tools available that can interactively view and analyze graph data from different perspectives and with multiple granularities. In this paper, we argue that it is critically important to OLAP graph structured data and propose a novel Graph OLAP framework. According to this framework, given a graph dataset with its nodes and edges associated with respective attributes, a multi-dimensional model can be built to enable efficient online analytical processing so that any portions of the graphs can be generalized/specialized dynamically, offering multiple, versatile views of the data. The contributions of this work are three-fold. First, starting from basic definitions, i.e., what are dimensions and measures in the Graph OLAP scenario, we develop a conceptual framework for data cubes on graphs. We also look into different semantics of OLAP operations, and classify the framework into two major subcases: informational OLAP and topological OLAP. Second, we show how a graph cube can be materialized by calculating a special kind of measure called aggregated graph and how to implement it efficiently. This includes both full materialization and partial ",information system
1676,Advanced social features in a recommendation system for process modeling,Business Information Systems,"Social software is known to stimulate the exchange and sharing of information among peers. This paper describes how an existing system that supports process builders in completing a business process can be enhanced with various social features. In that way, it is easier for process modeler to become aware of new related content. They can use that content to create, update or extend process models that they are building themselves. The proposed way of achieving this is to allow users to generate and modify personalized views on the social networks they are part of. Furthermore, this paper describes mechanisms for propagating relevant changes between peers in such social networks. The presented work is particularly relevant in the context of enterprises that have already built large repositories of process models.",information system
1677,Schema-based Web wrapping,Knowledge and Information Systems,"An effective solution to automate information extraction from Web pages is represented by wrappers. A wrapper associates a Web page with an XML document that represents part of the information in that page in a machine-readable format. Most existing wrapping approaches have traditionally focused on how to generate extraction rules, while they have ignored potential benefits deriving from the use of the schema of the information being extracted in the wrapper evaluation. In this paper, we investigate how the schema of extracted information can be effectively used in both the design and evaluation of a Web wrapper. We define a clean declarative semantics for schema-based wrappers by introducing the notion of (preferred) extraction model, which is essential to compute a valid XML document containing the information extracted from a Web page. We developed the SCRAP (SChema-based wRAPper for web data) system for the proposed schema-based wrapping approach, which also provides visual support tools to the wrapper designer. Moreover, we present a wrapper generalization framework to profitably speed up the design of schema-based wrappers. Experimental evaluation has shown that SCRAP wrappers are not only able to successfully extract the required data, but also they are robust to changes that may occur in the source Web pages.",information system
1678,Ontological reasoning for improving the treatment of emotions in text,Knowledge and Information Systems,"With the advent of affective computing, the task of adequately identifying, representing and processing the emotional connotations of text has acquired importance. Two problems facing this task are addressed in this paper: the composition of sentence emotion from word emotion, and a representation of emotion that allows easy conversion between existing computational representations. The emotion of a sentence of text should be derived by composition of the emotions of the words in the sentence, but no method has been proposed so far to model this compositionality. Of the various existing approaches for representing emotions, some are better suited for some problems and some for others, but there is no easy way of converting from one to another. This paper presents a system that addresses these two problems by reasoning with two ontologies implemented with Semantic Web technologies: one designed to represent word dependency relations within a sentence, and one designed to represent emotions. The ontology of word dependency relies on roles to represent the way emotional contributions project over word dependencies. By applying automated classification of mark-up results in terms of the emotion ontology the system can interpret unrestricted input in terms of a restricted set of concepts for which particular rules are provided. The rules applied at the end of the process provide configuration parameters for a system for emotional voice synthesis.",information system
1679,Issues and Opinions—Information Technologies in Business: A Blueprint for Education and Research,Information Systems Research," Hset of core principles we can apply to thinking about the enabling potential of information technologies ow are business schools thinking about developing leaders for the emerging digital economy? Is there a and their consequences for business and society? We present a business-centric framework and a technologycentric framework that together form a blueprint for answering these questions. The business-centric framework articulates three compelling reasons why information technology (IT) matters in business: (1) IT continually transform industry and society, (2) executive decisions about IT investments, governance, and strategy are critical to organizational success, and (3) deriving value from increasingly available data trails defines effective decision making in the digital economy. However, our conversations with the leadership of 45 business schools and our subsequent data indicate that business schools are challenged by effectively training future executives to think about these reasons and act on them as part of a forward-looking program of business education that is grounded in stable concepts. In response, the technology-centric framework provides a set of grounding concepts and stable principles about IT that have emerged over the last four decades, and leads to a natural set of consequences that can inform thinking about IT in business. We illustrate how these complementary frameworks-business and technology-can be combined to frame an educational program by outlining a set of key questions, by placing these questions in the context suggested by our frameworks, and by providing guidelines toward answering them. These questions also define a natural path for future research about IT in business and society that will lead to stronger intellectual foundations for the field and define future education that is better grounded in concepts and theories that emerge from academic research. ",information system
1680,Learning to detect spyware using end user license agreements,Knowledge and Information Systems,"The amount of software that hosts spyware has increased dramatically. To avoid legal repercussions, the vendors need to inform users about inclusion of spyware via end user license agreements (EULAs) during the installation of an application. However, this information is intentionally written in a way that is hard for users to comprehend. We investigate how to automatically discriminate between legitimate software and spyware associated software by mining EULAs. For this purpose, we compile a data set consisting of 996 EULAs out of which 9.6% are associated to spyware. We compare the performance of 17 learning algorithms with that of a baseline algorithm on two data sets based on a bag-of-words and a meta data model. The majority of learning algorithms significantly outperform the baseline regardless of which data representation is used. However, a non-parametric test indicates that bag-of-words is more suitable than the meta model. Our conclusion is that automatic EULA classification can be applied to assist users in making informed decisions about whether to install an application without having read the EULA. We therefore outline the design of a spyware prevention tool and suggest how to select suitable learning algorithms for the tool by using a multi-criteria evaluation approach.",information system
1681,A hybrid decision tree training method using data streams,Knowledge and Information Systems,"Classical classification methods usually assume that pattern recognition models do not depend on the timing of the data. However, this assumption is not valid in cases where new data frequently become available. Such situations are common in practice, for example, spam filtering or fraud detection, where dependencies between feature values and class numbers are continually changing. Unfortunately, most classical machine learning methods (such as decision trees) do not take into consideration the possibility of the model changing, as a result of so-called concept drift and they cannot adapt to a new classification model. This paper focuses on the problem of concept drift, which is a very important issue, especially in data mining methods that use complex structures (such as decision trees) for making decisions. We propose an algorithm that is able to co-train decision trees using a modified NGE (Nested Generalized Exemplar) algorithm. The potential for adaptation of the proposed algorithm and the quality thereof are evaluated through computer experiments, carried out on benchmark datasets from the UCI Machine Learning Repository.",information system
1682,Dominance relationship analysis with budget constraints,Knowledge and Information Systems,"Creating a new product that dominates all its competitors is one of the main objectives in marketing. Nevertheless, this might not be feasible since in practice the development process is confined by some constraints, e.g., limited funding or low target selling price. We model these constraints by a constraint function, which determines the feasible characteristics of a new product. Given such a budget, our task is to decide the best possible features of the new product that maximize its profitability. In general, a product is marketable if it dominates a large set of existing products, while it is not dominated by many. Based on this, we define dominance relationship analysis and use it to measure the profitability of the new product. The decision problem is then modeled as a budget constrained optimization query (BOQ). Computing BOQ is challenging due to the exponential increase in the search space with dimensionality. We propose a divide-and-conquer based framework, which outperforms a baseline approach in terms of not only execution time but also space complexity. Based on the proposed framework, we further study an approximation solution, which provides a good trade-off between computation cost and quality of result.",information system
1683,Mitigation of Random Query String DoS via Gossip,International Conference on Information Systems," -This paper presents a mitigation scheme to cope with 2 the random query string Denial of Service (DoS) attack, which is 1 based on a vulnerability of current Content Delivery Networks 0 (CDNs). The attack exploits the fact that edge servers composing 2 a CDN, receiving an HTTP request for a resource with an appended random query string never saw before, ask the origin b server for a (novel) copy of the resource. Such characteristics e can be employed to take an attack against the origin server F by exploiting edge servers. Our strategy adopts a simple gossip 3 protocol executed by edge servers to detect the attack. Based 2 on such a detection, countermeasures can be taken to protect the origin server and the CDN against the attack. We provide ] simulation results that show the viability of our approach. ",information system
1684,Supporting virtual enterprise systems using agent coordination,Knowledge and Information Systems,"Open environments like the Internet or corporate intranets enable a large number of interested enterprises to access, filter, process and present information on an as-needed basis. These environments support modern applications, such as virtual enterprises and interorganisational workflow management systems, which involve a number of heterogeneous resources, services and processes. However, any execution of a virtual enterprise system would yield to disjoining and error-prone behaviour without appropriate techniques to coordinate the various business processes. This paper reports on the design and implementation of a flexible agent-based framework for supporting the coordination of virtual enterprises and workflow management systems. The paper also shows how an agent coordination infrastructure, which is explained by social constraints, can impact on the engineering of highly dynamic virtual enterprises and workflow management systems by presenting a simple case study.",information system
1685,A decentralized search engine for dynamic Web communities,Knowledge and Information Systems,"Currently, most Web search engines perform search on corpus comprising nearly entire content of the Web. The same centralized search service can be performed on a single site as well. Nonetheless, there is little research on community-wide search. This paper presents a peer-to-peer search engine ComSearch. ComSearch is designed to provide small- and middle-scale online communities—the ability to perform text search within the community. Communities are formed in a self-organizing style. P2P IR system may suffer unnecessary internal traffic in answering a multi-term query. In this paper, we propose several techniques to optimize the multi-term query process. The simulation results show that our proposed algorithms have good scalability. Compared with baseline approach, our improved algorithm can reduce the communication cost by about two orders of magnitude in the best case. We also deploy the system in a small-scale network and conduct a series of experiments to estimate the actual query response time as well as to investigate the data movement effect caused by node joining. Experimental results show that multiple data movements are quite common during network expansion. However, the percentage of multiple data movements decreases when a network is getting stable after the initial frequent joining activities. This provides possibilities for improvement on P2P data movement management.",information system
1686,A Description Length-Based Decision Criterion for Default Knowledge in the Ripple Down Rules Method,Knowledge and Information Systems," The “Ripple Down Rules (RDR)” Method is a promising approach to directly acquiring and encoding knowledge from human experts. It requires data to be supplied incrementally to the knowledge base being constructed, each new piece of knowledge being added as an exception to the existing knowledge base. Because of this patching principle, the knowledge acquired depends strongly on what is given as the default knowledge, used as an implicit outcome when inference fails. ",information system
1687,Product selection for promotion planning,Knowledge and Information Systems,"This paper addresses a very important question—how to select the right products to promote in order to maximize promotional benefit. We set up a framework to incorporate promotion decisions into the data-mining process, formulate the profit maximization problem as an optimization problem, and propose a heuristic search solution to discover the right products to promote. Moreover, we are able to get access to real supermarket data and apply our solution to help achieve higher profits. Our experimental results on both synthetic data and real supermarket data demonstrate that our framework and method are highly effective and can potentially bring huge profit gains to a marketing campaign.",information system
1688,Statistical semantics for enhancing document clustering,Knowledge and Information Systems,Document clustering algorithms usually use vector space model (VSM) as their underlying model for document representation. VSM assumes that terms are independent and accordingly ignores any semantic relations between them. This results in mapping documents to a space where the proximity between document vectors does not reflect their true semantic similarity. This paper proposes new models for document representation that capture semantic similarity between documents based on measures of correlations between their terms. The paper uses the proposed models to enhance the effectiveness of different algorithms for document clustering. The proposed representation models define a corpus-specific semantic similarity by estimating measures of term–term correlations from the documents to be clustered. The corpus of documents accordingly defines a context in which semantic similarity is calculated. Experiments have been conducted on thirteen benchmark data sets to empirically evaluate the effectiveness of the proposed models and compare them to VSM and other well-known models for capturing semantic similarity.,information system
1689,Developing actionable trading agents,Knowledge and Information Systems," Trading agents are useful for developing and back-testing quality trading strategies to support smart trading actions in the market. However, most of the existing trading agent research oversimplifies trading strategies, and focuses on simulated ones. As a result, there exists a big gap between the deliverables and business needs when the developed strategies are deployed into the real life. Therefore, the actionable capability of developed trading agents is often very limited. This paper for the first time introduces effective approaches for optimizing and integrating multiple classes of strategies through trading agent collaboration. An integration and optimization approach is proposed to identify optimal trading strategy in each category, and further integrate optimal strategies crossing classes. Positions associated with these optimal strategies are recommended for trading agents to take actions in the market. Extensive experiments on a large quantity of real-life market data show that trading agents following the recommended strategies have great potential to obtain high benefits while low costs. This verifies that it is promising to develop trading agents toward workable and satisfying business needs. This work is sponsored in part by Australian Research Council Grants (DP0773412, LP0775041, DP0667060). ",information system
1690,ELECTRONIC INTEGRATION AND STRATEGIC ADVANTAGE: A QUASI-EXPERIMENTAL STUDY IN THE INSURANCE INDUSTRY,Information Systems Research," Strategic advantage through information technology is a popular and an important theme, but the extent of research support is minimal, anecdotal, and sporadic. This paper reports the results ofa quasi-experimental study on the impact of dedicated electronic integration [between a focal insurance carrier and its independent agents in the property and casualty (P&C) market] for the focal carrier. The results indicate that the agents that are electronically interfaced with the carrier report improvements in a set of four performance factors in the expected direction (six months after system installation), but statistically different from a matched set of non-interfaced agents (based on size, state, and location category) only in terms of increases in new business policies, but not in terms of effectiveness-namely, neither increases in premiums and commissions nor operating efficiency. Some explanations, extensions and research implications are outlined. ",information system
1691,Multiplicative distance: a method to alleviate distance instability for high-dimensional data,Knowledge and Information Systems,"Recently, it has been shown that under a broad set of conditions, the commonly used distance functions will become unstable in high-dimensional data space; i.e., the distance to the farthest data point approaches the distance to the nearest data point of a given query point with increasing dimensionality. It has been shown that if dimensions are independently distributed, and normalized to have zero mean and unit variance, instability happens. In this paper, it is shown that the normalization condition is not necessary, but all appropriate moments must be finite. Furthermore, a new distance function, namely multiplicative distance, is introduced. It is theoretically proved that this function is stable for data with independent dimensions (with identical or nonidentical distribution). In contrast to usual distance functions which are based on the summation of distances over all dimensions (distance components), the multiplicative distance is based on the multiplication of distance components. Experimental results show the stability of the multiplicative distance for data with independent and correlated dimensions in the high-dimensional space and the superiority of the multiplicative distance over the norm distances for the high-dimensional data.",information system
1692,Workflow management in the internet age,Advances in Databases and Information Systems,"For the last many years, workflow management (WFM) has been the focus of intense activity in terms of products, standards and research work worldwide. WFM integrates concepts from many areas of computer science. Numerous workflow companies, and industrial and academic research groups are in existence now. Several conferences and workshops relating to WFM are being held regularly. The popularity of the worldwide web, extranets and electronic commerce is further increasing the desirability of standards in the workflow arena to enable interoperability. The recent emphasis on supply chain reengineering as a means of reducing costs and improving responsiveness is also a factor in this regard. In this extended abstract, which is a condensed version of a paper which appears in the proceedings of the NATO Advanced Study Institute in Workflow Management and Interoperability held in Istanbul in August 1997, I briefly summarize the recent trends in WFM products, standards and research. I address technical as well as business trends.",information system
1693,Method-ISM in practice: investigating the relationship between method and understanding in Web page design,International Conference on Information Systems," Whitley, Edgar A. (1998). Method-ism in practice : investigating the relationship between method and understanding in web page design http://icisnet.aisnet.org/ ",information system
1694,Learning decision tree for ranking,Knowledge and Information Systems,"Decision tree is one of the most effective and widely used methods for classification. However, many real-world applications require instances to be ranked by the probability of class membership. The area under the receiver operating characteristics curve, simply AUC, has been recently used as a measure for ranking performance of learning algorithms. In this paper, we present two novel class probability estimation algorithms to improve the ranking performance of decision tree. Instead of estimating the probability of class membership using simple voting at the leaf where the test instance falls into, our algorithms use similarity-weighted voting and naive Bayes. We design empirical experiments to verify that our new algorithms significantly outperform the recent decision tree ranking algorithm C4.4 in terms of AUC.",information system
1695,A Literature Survey on Information Logistics,Business Information Systems," The notion of information logistics (IL) has been introduced as a new information management paradigm. Goal is to enable the effective and e cient delivery of needed information in the right format, granularity and quality, at the right place, at the right point in time to the right actors. IL has received much attention in recent years, both from researchers and practitioners. In order to better understand the state-ofthe-art and current research trends in the research eld of IL, this paper presents a comprehensive IL literature survey. In total, we identi ed 53 scienti c articles discussing IL concepts and approaches. These articles were systematically analyzed and nally classi ed in ten research clusters. Based on these clusters, a more comprehensive understanding of past, current, and future IL developments becomes possible. ",information system
1696,Ontology Integration: Experiences with Medical Terminologies,Formal Ontology in Information Systems," ONIONS is a methodology for ontology analysis and integration. It has been applied to some relevant and very large medical terminologies (e.g. the 332,000 medical concepts singled out by the National Library of Medicine in the UMLS project). Current results include the alignment of the top-level ontologies of the terminologies considered. The paper reviews the formal and conceptual tools employed in this task, presents the most significant results obtained, and discusses two case studies. ",information system
1697,Design and Effects of Information Feedback in Continuous Combinatorial Auctions,International Conference on Information Systems," Advancements in information technologies offer opportunities for designing and deploying innovative market mechanisms. For example, combinatorial auctions, in which bidders can bid on combinations of goods, can increase the economic efficiency of a trade when goods have complementarities. However, lack of real-time bidder support tools has been a major obstacle preventing this mechanism from reaching its full potential. This study uses novel feedback mechanisms to aid bidders in formulating bids in real-time to facilitate participation in continuous combinatorial auctions. Laboratory experiments examine the effectiveness of our feedback mechanisms; the study is the first to examine how bidders behave in such information-rich environments. Our results indicate that feedback results in higher efficiency and higher seller's revenue compared to the baseline case where bidders are not provided feedback. Furthermore, contrary to conventional wisdom, even in complex economic environments, individuals effectively integrate rich information in their decision making. ",information system
1698,Agility from First Principles: Reconstructing the Concept of Agility in Information Systems Development,Information Systems Research," http://search.ebscohost.com/login.aspx?direct=true&db=buh& AN=44517400&site=ehost-live Some rights reserved. For more information, please see the item record link above. ",information system
1699,Hubness-aware shared neighbor distances for high-dimensional k-nearest neighbor classification,Knowledge and Information Systems,"Learning from high-dimensional data is usually quite challenging, as captured by the well-known phrase curse of dimensionality. Data analysis often involves measuring the similarity between different examples. This sometimes becomes a problem, as many widely used metrics tend to concentrate in high-dimensional feature spaces. The reduced contrast makes it more difficult to distinguish between close and distant points, which renders many traditional distance-based learning methods ineffective. Secondary distances based on shared neighbor similarities have recently been proposed as one possible solution to this problem. However, these initial metrics failed to take hubness into account. Hubness is a recently described aspect of the dimensionality curse, and it affects all sorts of (k)-nearest neighbor learning methods in severely negative ways. This paper is the first to discuss the impact of hubs on forming the shared neighbor similarity scores. We propose a novel, hubness-aware secondary similarity measure (simhub_s) and an extensive experimental evaluation shows it to be much more appropriate for high-dimensional data classification than the standard (simcos_s) measure. The proposed similarity changes the underlying (k)NN graph in such a way that it reduces the overall frequency of label mismatches in (k)-neighbor sets and increases the purity of occurrence profiles, which improves classifier performance. It is a hybrid measure, which takes into account both the supervised and the unsupervised hubness information. The analysis shows that both components are useful in their own ways and that the measure is therefore properly defined. This new similarity does not increase the overall computational cost, and the improvement is essentially ‘free’.",information system
1700,An Empirical Analysis of Network Externalities in Peer-to-Peer Music-Sharing Networks,Information Systems Research,"Peer-to-peer file sharing networks are becoming an important medium for the distribution of information goods. However, there is little academic research into the optimal design of these networks under real-world conditions. Our research represents an initial effort to analyze the impact of positive and negative network externalities on the optimal size of these P2P networks. Our analysis uses a unique dataset collected from the six most popular OpenNap peer-to-peer networks between December 19, 2000 and April 22, 2001. We find that users contribute value to the network in terms of additional content and additional replicas of content at a diminishing rate, while they impose costs on the network in terms of congestion on shared resources at an increasing rate. Together these results suggest that the optimal size of peer-to-peer networks is bounded  at some point the costs a marginal user imposes on the network will exceed the value they provide.",information system
1701,Supporting Interoperability of Autonomous Hospital Databases: A Case Study,Advances in Databases and Information Systems, ISBN: 3-540-76227-2 ,information system
1702,Database technologies for L-system simulations in virtual plant applications on bioinformatics,Knowledge and Information Systems,"Abstract. One of the most important advantages of database systems is that the underlying mathematics is rich enough to specify very complex operations with a small number of statements in the database language. This research covers an aspect of biological informatics that is the marriage of information technology and biology, involving the study of real-world phenomena using virtual plants derived from L-systems simulation. L-systems were introduced by Aristid Lindenmayer as a mathematical model of multicellular organisms. Not much consideration has been given to the problem of persistent storage for these simulations. Current procedures for querying data generated by L-systems for scientific experiments, simulations and measurements are also inadequate. To address these problems the research in this paper presents a generic process for data-modeling tools (L-DBM) between L-systems and database systems. This paper shows how L-system productions can be generically and automatically represented in database schemas and how a database can be populated from the L-system strings. This paper further describes the idea of pre-computing recursive structures in the data into derived attributes using compiler generation. A method to allow a correspondence between biologists' terms and compiler-generated terms in a biologist computing environment is supplied. Once the L-DBM gets any specific L-systems productions and its declarations, it can generate the specific schema for both simple correspondence terminology and also complex recursive structure data attributes and relationships.",information system
1703,Knowledge Integration as a Key Problem in an ERP Implementation,International Conference on Information Systems," While previous studies have focused mainly on the potential benefits and critical success factors associated with ERP implementation, very few have explored the important issues of impediments encountered, especially from a knowledge integration perspective. We have adopted a knowledge integration view that focuses not on the distribution and adoption of particular technological artifacts (ERP systems), but on the knowledge integration processes involved in implementation. The focus of this case study is to understand the nature, structure and process of knowledge integration that occurs during ERP implementation. The paper has identified the integration of knowledge as a key problem in ERP implementation. We discovered four reasons: (1) knowledge is embedded in complex organizational processes; (2) knowledge is embedded in legacy systems; (3) knowledge is embedded in externally based processes; and (4) knowledge is embedded in the ERP system. Based on our analysis, we further suggest that to overcome these impediments to knowledge integration requires the development of interpersonal relations (one-to-one based) and community relations (group-based). ",information system
1704,A Standard for Representing Multidimensional Properties: The Common Warehouse Metamodel (CWM),Advances in Databases and Information Systems,"Data warehouses, multidimensional databases, and OLAP tools are based on the multidimensional (MD) modeling. Lately, several approaches have been proposed to easily capture main MD properties at the conceptual level.These conceptual MD models, together with a precise management of metadata, are the core of any related tool imple- mentation. However, the broad diversity of MD models and management of metadata justifies the necessity of a universally understood standard definition for metadata, thereby allowing different tools to share infor- mation in an easy form. In this paper, we make use of the Common Warehouse Metamodel (CWM) to represent the main MD properties at the conceptual level in terms of CWM metadata. Then, CWM-compliant tools could interoperate by exchanging their CWM-based metadata in a commonly understood format and benefit of the expressiveness of the MD model at the conceptual level.",information system
1705,A Benchmarking Technique for DBMS's with Advanced Data Models,Advances in Databases and Information Systems," The majority of database benchmarks currently in use in the industry were designed for relational databases. A different class of benchmarks became required for object oriented databases once they appeared on the market. None of the currently existing benchmarks were designed to adequately exploit the distinctive features native to the semantic databases. A new semantic benchmark is proposed which allows evaluation of the performance of the features characteristic of semantic database applications. An application used in the benchmark represents a class of problems requiring databases with sparse data, complex inheritances and many-to-many relations. Such databases can be naturally accommodated by semantic databases. A predefined implementation is not enforced allowing a designer to choose the most efficient structures available in the DBMS tested. The second part of this paper compares the performance of Sem-ODB binary semantic database vs. one of the leading relational databases. The results of the benchmark are analyzed. ",information system
1706,Beyond one billion time series: indexing and mining very large time series collections with iSAX2+,Knowledge and Information Systems,"There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to index and mine very large collections of time series. Examples of such applications come from astronomy, biology, the web, and other domains. It is not unusual for these applications to involve numbers of time series in the order of hundreds of millions to billions. However, all relevant techniques that have been proposed in the literature so far have not considered any data collections much larger than one-million time series. In this paper, we describe (i)SAX 2.0 and its improvements, (i)SAX 2.0 Clustered and (i)SAX2+, three methods designed for indexing and mining truly massive collections of time series. We show that the main bottleneck in mining such massive datasets is the time taken to build the index, and we thus introduce a novel bulk loading mechanism, the first of this kind specifically tailored to a time series index. We show how our methods allows mining on datasets that would otherwise be completely untenable, including the first published experiments to index one billion time series, and experiments in mining massive data from domains as diverse as entomology, DNA and web-scale image collections.",information system
1707,Is there a cost to privacy breaches? An event study,International Conference on Information Systems," While the infosec economics literature has begun to investigate the stock market impact of security breaches and vulnerability announcements, little more than anecdotal evidence exists on effects of privacy breaches. In this paper we present the first comprehensive analysis of the impact of a company's privacy incidents on its market value. We compile a broad data set of instances of exposure of personal information from a failure of some security mechanism (hacking, stolen or lost equipment, poor process, and others) and we present the results of various empirical analyses, including event study analysis. We show that there exists a negative and statistically significant impact of data breaches on a company's market value on the announcement day for the breach. The cumulative effect increases in magnitudes over day following the breach announcement, but then decreases and loses statistical significance. We also present regression analyses that aim at disentangling the effects of a number of factors on abnormal stock returns due to reported breaches. Finally, we comment on the differences between the impact of the security breaches already described in the literature, and the privacy breaches described here. ",information system
1708,Let the Pirates Patch? An Economic Analysis of Software Security Patch Restrictions,Information Systems Research," Wa software product to apply security patches. We present a joint model of network software security and e study the question of whether a software vendor should allow users of unlicensed (pirated) copies of software piracy and contrast two policies that a software vendor can enforce: (i) restriction of security patches only to legitimate users or (ii) provision of access to security patches to all users whether their copies are licensed or not. We find that when the software security risk is high and the piracy enforcement level is low, or when tendency for piracy in the consumer population is high, it is optimal for the vendor to restrict unlicensed users from applying security patches. When piracy tendency in the consumer population is low, applying software security patch restrictions is optimal for the vendor only when the piracy enforcement level is high. If patching costs are sufficiently low, however, an unrestricted patch release policy maximizes vendor profits. We also show that the vendor can use security patch restrictions as a substitute to investment in software security, and this effect can significantly reduce welfare. Furthermore, in certain cases, increased piracy enforcement levels can actually hurt vendor profits. We also show that governments can increase social surplus and intellectual property protection simultaneously by increasing piracy enforcement and utilizing the strategic interaction of piracy patch restrictions and network security. Finally, we demonstrate that, although unrestricted patching can maximize welfare when the piracy enforcement level is low, contrary to what one might expect, when the piracy enforcement level is high, restricting security patches only to licensed users can be socially optimal. ",information system
1709,Optimal Software Free Trial Strategy: The Impact of Network Externalities and Consumer Uncertainty,Information Systems Research,"Many software firms offer a fully functional version of their products free of charge, for a limited trial time, to ease consumers’ uncertainty about the functionalities of their products and to help the diffusion of their new software. This paper examines the tradeoff between the effects of reduced uncertainty and demand cannibalization, uncovers the condition under which software firms should introduce the time-locked free trial software, and finds the optimal free trial time. As the software firm has the option of providing free trial software with limited trial time with full functionalities or limited functionalities for unlimited time, we develop a unified framework to provide useful guidelines for deciding which free trial strategy is preferred in the presence of network externalities and consumer uncertainty.ow that the time-locked free trial is favorable when the intensity of network effect is modest.",information system
1710,Class separation through variance: a new application of outlier detection,Knowledge and Information Systems,"This paper introduces a new outlier detection approach and discusses and extends a new concept, class separation through variance. We show that even for balanced and concentric classes differing only in variance, accumulating information about the outlierness of points in multiple subspaces leads to a ranking in which the classes naturally tend to separate. Exploiting this leads to a highly effective and efficient unsupervised class separation approach. Unlike typical outlier detection algorithms, this method can be applied beyond the ‘rare classes’ case with great success. The new algorithm FASTOUT introduces a number of novel features. It employs sampling of subspaces points and is highly efficient. It handles arbitrarily sized subspaces and converges to an optimal subspace size through the use of an objective function. In addition, two approaches are presented for automatically deriving the class of the data points from the ranking. Experiments show that FASTOUT typically outperforms other state-of-the-art outlier detection methods on high-dimensional data such as Feature Bagging, SOE1, LOF, ORCA and Robust Mahalanobis Distance, and competes even with the leading supervised classification methods for separating classes.",information system
1711,Efficient Set Similarity Joins Using Min-prefixes,Advances in Databases and Information Systems,"Identification of all objects in a dataset whose similarity is not less than a specified threshold is of major importance for management, search, and analysis of data. Set similarity joins are commonly used to implement this operation; they scale to large datasets and are versatile to represent a variety of similarity notions. Most set similarity join methods proposed so far present two main phases at a high level of abstraction: candidate generation producing a set of candidate pairs and verification applying the actual similarity measure to the candidates and returning the correct answer. Previous work has primarily focused on the reduction of candidates, where candidate generation presented the major effort to obtain better pruning results. Here, we propose an opposite approach. We drastically decrease the computational cost of candidate generation by dynamically reducing the number of indexed objects at the expense of increasing the workload of the verification phase. Our experimental findings show that this trade-off is advantageous: we consistently achieve substantial speed-ups as compared to previous algorithms.",information system
1712,Symbolic data analysis tools for recommendation systems,Knowledge and Information Systems,"Recommender systems have become an important tool to cope with the information overload problem by acquiring data about user behavior. After tracing the user’s behavior, through actions or rates, computational recommender systems use information- filtering techniques to recommend items. In order to recommend new items, one of the three major approaches is generally adopted: content-based filtering, collaborative filtering, or hybrid filtering. This paper presents three information-filtering methods, each of them based on one of these approaches. In our methods, the user profile is built up through symbolic data structures and the user and item correlations are computed through dissimilarity functions adapted from the symbolic data analysis (SDA) domain. The use of SDA tools has improved the performance of recommender systems, particularly concerning the find good items task measured by the half-life utility metric, when there is not much information about the user.",information system
1713,Ipmicra: Toward a Distributed and Adaptable Location Aware Web Crawler,Advances in Databases and Information Systems," Distributed crawling has shown that it can overcome important limitations of the centralized crawling paradigm. However, the distributed nature of current distributed crawlers is currently not fully utilized. The optimal benefits of this approach are usually limited to the sites hosting the crawler. In this work we propose IPMicra, a distributed location aware web crawler that utilizes an IP address hierarchy and allows crawling of links in a near optimal location aware manner. ",information system
1714,Privacy preservation for data cubes,Knowledge and Information Systems," A range query finds the aggregated values over all selected cells of an online analytical processing (OLAP) data cube where the selection is specified by the ranges of contiguous values for each dimension. An important issue in reality is how to preserve the confidential information in individual data cells while still providing an accurate estimation of the original aggregated values for range queries. In this paper, we propose an effective solution, called the zero-sum method, to this problem. We derive theoretical formulas to analyse the performance of our method. Empirical experiments are also carried out by using analytical processing benchmark (APB) dataset from the OLAP Council. Various parameters, such as the privacy factor and the accuracy factor, have been considered and tested in the experiments. Finally, our experimental results show that there is a trade-off between privacy preservation and range query accuracy, and the zero-sum method has fulfilled three design goals: security, accuracy, and accessibility. ",information system
1715,Graph-based local concept coordinate factorization,Knowledge and Information Systems,"Ubiquitous data are increasingly expanding in large volumes due to human activities, and grouping them into appropriate clusters is an important and yet challenging problem. Existing matrix factorization techniques have shown their significant power in solving this problem, e.g., nonnegative matrix factorization, concept factorization. Recently, one state-of-the-art method called locality-constrained concept factorization is put forward, but its locality constraint does not well reveal the intrinsic data structure since it only requires the concept to be as close to the original data points as possible. To address this issue, we present a graph-based local concept coordinate factorization (GLCF) method, which respects the intrinsic structure of the data through manifold kernel learning in the warped Reproducing Kernel Hilbert Space. Besides, a generalized update algorithm is developed to handle data matrices containing both positive and negative entries. Since GLCF is essentially based on the local coordinate coding and concept factorization, it inherits many advantageous properties, such as the locality and sparsity of the data representation. Moreover, it can better encode the locally geometrical structure via graph Laplacian in the manifold adaptive kernel. Therefore, a more compact and better structured representation can be obtained in the low-dimensional data space. Extensive experiments on several image and gene expression databases suggest the superiority of the proposed method in comparison with some alternatives.",information system
1716,Predicting Web Page Status,Information Systems Research,"The World Wide Web has become a key intermediary between producers and consumers of information. Web's linkage structure has been exploited by contemporary search engines to decrease the search cost for consumers while usually also rewarding the producers of higher status Web pages. In addition to influencing visibility and accessibility, in-links, as marks of recognition, accord status to a Web page. In this paper we show how Web page status may be predicted at least in part by page location and topic specificity. Moreover, we observe that the ""philanthropic"" contributions of a Web page - specifically, contributions of information brokerage function - are also good predictors of in-links. The observations are made in the presence of domain and topic-specific effects. Interestingly, all of these features that may predict status are ""local"" to a given Web page and within the control of the owner/author of the page. This is in contrast to the ""global"" nature of Web linkage-based metrics such as in-link count that are derived as a result of downloading and indexing billions of pages. Because the linkage structure of the Web affects browsing, crawling, and retrieval, our results have implications for vertical and general search, business intelligence, and content management.",information system
1717,Knowledge-aware identity services,Knowledge and Information Systems,"The identification problem is concerned with the question whether two objects in an application refer to the same real-world entity. In this paper, the identification problem is investigated from a knowledge modelling point of view. We develop a framework of establishing knowledge-aware identity services by abstracting identity knowledge into an additional identity layer. The knowledge model in the identity service layer provides a capability for combining declarative formulae with concrete data and thus allows us to capture domain-specific identity knowledge at flexible levels of abstraction. By adding validation constraints to the identity service, we are also able to reason about inconsistency of identity knowledge. In doing so, the accuracy of identity knowledge can be improved over time, especially when utilising identity services provided by different communities in a service-oriented architecture. Our experimental study shows the effectiveness of the proposed knowledge modelling approach and the effects of domain-specific identity knowledge on data quality control.",information system
1718,Evaluating interface esthetics,Knowledge and Information Systems,"Abstract. Gestalt psychologists promulgated the principles of visual organization in the early twentieth century. These principles have been discussed and re-emphasized, and their importance and relevance to user interface design are understood. However, a limited number of systems represent and make adequate use of this knowledge in the form of a design tool that supports certain aspects of the user interface design process. The graphic design rules that these systems use are extremely rudimentary and often vastly oversimplified. Most of them have no concept of design basics such as visual balance or rhythm. In this paper, we attempt to synthesize the guidelines and empirical data related to the formatting of screen layouts into a well-defined model. Fourteen esthetic characteristics have been selected for the purpose. The results of our exercise suggest that these characteristics are important to prospective viewers.",information system
1719,CFDC: A Flash-Aware Buffer Management Algorithm for Database Systems,Advances in Databases and Information Systems,"Classical buffer replacement policies, e.g., LRU, are suboptimal for database systems having flash disks for persistence, because they are not aware of the distinguished characteristics of those storage devices. We present CFDC (Clean-First Dirty-Clustered), a flash-aware buffer management algorithm, which emphasizes that clean buffer pages are first considered for replacement and that modified buffer pages are clustered for better spatial locality of page flushes. Our algorithm is complementary to and can be integrated with conventional replacement policies. Our DBMS-based performance studies using both synthetic and real-life OLTP traces reveal that CFDC significantly outperforms previous proposals with a performance gain up to 53%.",information system
1720,The User as Navigator,Advances in Databases and Information Systems," The old idea of navigation in a database is revisited with two essential changes: we address Web users rather than programmers and the navigation is accomplished as a visual metaphor. The Structural Knowledge Graph Navigator (SKGN) is a prototype implementation of this metaphor. It supports end users of Web applications by simple means for ad hoc querying and browsing in an object-oriented database. The interface has been implemented within the European project ICONS. SKGN utilizes three core concepts: intentional navigation (in a schema graph), extensional navigation (in an object graph) and annotated user baskets for storing intermediate and final results of querying. The paper describes the motivation and main ideas of SKGN. ",information system
1721,Finding Generalized Path Patterns for Web Log Data Mining,Advances in Databases and Information Systems," Conducting data mining on logs of web servers involves the determination of frequently occurring access sequences. We examine the problem of nding traversal patterns from web logs by considering the fact that irrelevant accesses to web documents may be interleaved within access patterns due to navigational purposes. We de ne a general type of pattern that takes into account this fact and also, we present a level-wise algorithm for the determination of these patterns, which is based on the underlying structure of the web site. The performance of the algorithm and its sensitivity to several parameters is examined experimentally with synthetic data. ",information system
1722,A semantical framework for hybrid knowledge bases,Knowledge and Information Systems," In the ongoing discussion about combining rules and ontologies on the Semantic Web a recurring issue is how to combine first-order classical logic with nonmonotonic rule languages. Whereas several modular approaches to define a combined semantics for such hybrid knowledge bases focus mainly on decidability issues, we tackle the matter from a more general point of view. In this paper, we show how Quantified Equilibrium Logic (QEL) can function as a unified framework which embraces classical logic as well as disjunctive logic programs under the (open) answer set semantics. In the proposed variant of QEL, we relax the unique names assumption, which was present in earlier versions of QEL. Moreover, we show that this framework elegantly captures the existing modular approaches for hybrid knowledge bases in a unified way. ",information system
1723,Mining indirect antagonistic communities from social interactions,Knowledge and Information Systems,"Antagonistic communities refer to groups of people with opposite tastes, opinions, and factions within a community. Given a set of interactions among people in a community, we develop a novel pattern mining approach to mine a set of antagonistic communities. In particular, based on a set of user-specified thresholds, we extract a set of pairs of communities that behave in opposite ways with one another. We focus on extracting a compact lossless representation based on the concept of closed patterns to prevent exploding the number of mined antagonistic communities. We also present a variation of the algorithm using a divide and conquer strategy to handle large datasets when main memory is inadequate. The scalability of our approach is tested on synthetic datasets of various sizes mined using various parameters. Case studies on Amazon, Epinions, and Slashdot datasets further show the efficiency and the utility of our approach in extracting antagonistic communities from social interactions.",information system
1724,Determining Error Bounds for Spectral Filtering Based Reconstruction Methods in Privacy Preserving Data Mining,Knowledge and Information Systems,"Additive randomization has been a primary tool for hiding sensitive private information. Previous work empirically showed that individual data values can be approximately reconstructed from the perturbed values, using spectral filtering techniques. This poses a serious threat of privacy breaches. In this paper we conduct a theoretical study on how the reconstruction error varies, for different types of additive noise. In particular, we first derive an upper bound for the reconstruction error using matrix perturbation theory. Attackers who use spectral filtering techniques to estimate the true data values may leverage this bound to determine how close their estimates are to the original data. We then derive a lower bound for the reconstruction error, which can help data owners decide how much noise should be added to satisfy a given threshold of the tolerated privacy breach.",information system
1725,Mining the meaningful term conjunctions from materialised faceted taxonomies: algorithms and complexity,Knowledge and Information Systems,"A materialised faceted taxonomy is an information source where the objects of interest are indexed according to a faceted taxonomy. This paper shows how from a materialised faceted taxonomy, we can mine an expression of the Compound Term Composition Algebra that specifies exactly those compound terms (conjunctions of terms) that have non-empty interpretation. The mined expressions can be used for encoding in a very compact form (and subsequently reusing), the domain knowledge that is stored in existing materialised faceted taxonomies. A distinctive characteristic of this mining task is that the focus is given on minimising the storage space requirements of the mined set of compound terms. This paper formulates the problem of expression mining, gives several algorithms for expression mining, analyses their computational complexity, provides techniques for optimisation, and discusses several novel applications that now become possible.",information system
1726,PADS: a simple yet effective pattern-aware dynamic search method for fast maximal frequent pattern mining,Knowledge and Information Systems,"While frequent pattern mining is fundamental for many data mining tasks, mining maximal frequent patterns efficiently is important in both theory and applications of frequent pattern mining. The fundamental challenge is how to search a large space of item combinations. Most of the existing methods search an enumeration tree of item combinations in a depth-first manner. In this paper, we develop a new technique for more efficient max-pattern mining. Our method is pattern-aware: it uses the patterns already found to schedule its future search so that many search subspaces can be pruned. We present efficient techniques to implement the new approach. As indicated by a systematic empirical study using the benchmark data sets, our new approach outperforms the currently fastest max-pattern mining algorithms FPMax* and LCM2 clearly. The source code and the executable code (on both Windows and Linux platforms) are publicly available at http://www.cs.sfu.ca/~jpei/Software/PADS.zip.",information system
1727,Topic-aware social influence propagation models,Knowledge and Information Systems,"We study social influence from a topic modeling perspective. We introduce novel topic-aware influence-driven propagation models that experimentally result to be more accurate in describing real-world cascades than the standard propagation models studied in the literature. In particular, we first propose simple topic-aware extensions of the well-known Independent Cascade and Linear Threshold models. Next, we propose a different approach explicitly modeling authoritativeness, influence and relevance under a topic-aware perspective. We devise methods to learn the parameters of the models from a dataset of past propagations. Our experimentation confirms the high accuracy of the proposed models and learning schemes.",information system
1728,On business rules automation: the BR-centric IS development framework,Advances in Databases and Information Systems,"The business rules (BR) approach in information systems (IS) engineering responds to the need of business practitioners to maintain their ISs efficiently in the volatile business environment. The important requirement is to reduce effects to adapt IS to the changes in business environment. This problem can be solved by the explicit use of enterprise knowledge in the form of BR stored outside of the application logic. A number of BR-based systems, methods, frameworks, and languages were proposed, but only few address automatic BR implementation. In this paper we present the framework which outlines the main components for BR-based IS development using BR automation. In our approach we differentiate three abstraction layers where the understanding, representation, and use of BR differ accordingly. We give the definitions of the components, outline their role in the framework, and present the results of a short case study as an example of the framework instantiation.",information system
1729,Correcting Missing Data Anomalies with Clausal Defeasible Logic,Advances in Databases and Information Systems,"Databases are used globally to store essential information required for various business applications such as automated data capturing. Unfortunately, due to missing record anomalies present within the repository, the overall integrity of stored information is compromised. Currently, filtration and rule-based techniques have been proposed to correct the problem, but due to a lack of high-level reasoning, ambiguous scenarios lead to anomalies persisting within the database. In this paper, we propose an enhanced Non-Monotonic Reasoning cleaning architecture that utilises intelligent analysis coupled with Clausal Defeasible Logic to rectify the missing data by generating and restoring imputed data. From our experimental evaluation, we have found that our proposed technique surpasses other leading intelligence classifiers such as Bayesian and Neural Networks.",information system
1730,Dynamic Changes in Workflow Participant Assignment,Advances in Databases and Information Systems," Workflow management systems (WFMSs) need to adapt dynamic process modifications. In current WFMSs the scope of dynamic modifications is mainly focused on control flow while other dynamic aspects, such as workflow participant assignment (WPA), are neglected. In this paper an approach to adapt dynamic modification in WPA is presented. The approach extends the meaning of WPA that is proposed by the Workflow Management Coalition. The extension covers dynamic aspects and express complex relationships between control, audit and relevant data. On basis of the new definition a WPA Language (WPAL) is proposed. WPAL is a programming interface, which makes it possible to assign dynamically workflow participants. WPAL is implemented in OfficeObjects® WorkFlow and deployed in several major customers of the Rodan Systems. The paper presents implementation results. ",information system
1731,Multi-view constrained clustering with an incomplete mapping between views,Knowledge and Information Systems,"Multi-view learning algorithms typically assume a complete bipartite mapping between the different views in order to exchange information during the learning process. However, many applications provide only a partial mapping between the views, creating a challenge for current methods. To address this problem, we propose a multi-view algorithm based on constrained clustering that can operate with an incomplete mapping. Given a set of pairwise constraints in each view, our approach propagates these constraints using a local similarity measure to those instances that can be mapped to the other views, allowing the propagated constraints to be transferred across views via the partial mapping. It uses co-EM to iteratively estimate the propagation within each view based on the current clustering model, transfer the constraints across views, and then update the clustering model. By alternating the learning process between views, this approach produces a unified clustering model that is consistent with all views. We show that this approach significantly improves clustering performance over several other methods for transferring constraints and allows multi-view clustering to be reliably applied when given a limited mapping between the views. Our evaluation reveals that the propagated constraints have high precision with respect to the true clusters in the data, explaining their benefit to clustering performance in both single- and multi-view learning scenarios.",information system
1732,Action rule discovery from incomplete data,Knowledge and Information Systems,"Action rule is an implication rule that shows the expected change in a decision value of an object as a result of changes made to some of its conditional values. An example of an action rule is ‘credit card holders of young age are expected to keep their cards for an extended period of time if they receive a movie ticket once a year’. In this case, the decision value is the account status, and the condition value is whether the movie ticket is sent to the customer. The type of action that can be taken by the company is to send out movie tickets to young customers. The conventional action rule discovery algorithms build action rules from existing classification rules. This paper discusses an agglomerative strategy that generates the shortest action rules directly from a decision system. In particular, the algorithm can be used to discover rules from an incomplete decision system where attribute values are partially incomplete. As one of the testing domains for our research we take HEPAR system that was built through a collaboration between the Institute of Biocybernetics and Biomedical Engineering of the Polish Academy of Sciences and physicians at the Medical Center of Postgraduate Education in Warsaw, Poland. HEPAR was designed for gathering and processing clinical data on patients with liver disorders. Action rules will be used to construct the decision-support module for HEPAR.",information system
1733,Classification of multivariate time series via temporal abstraction and time intervals mining,Knowledge and Information Systems,"Classification of multivariate time series data, often including both time points and intervals at variable frequencies, is a challenging task. We introduce the KarmaLegoSification (KLS) framework for classification of multivariate time series analysis, which implements three phases: (1) application of a temporal abstraction process that transforms a series of raw time-stamped data points into a series of symbolic time intervals; (2) mining these symbolic time intervals to discover frequent time-interval-related patterns (TIRPs), using Allen’s temporal relations; and (3) using the TIRPs as features to induce a classifier. To efficiently detect multiple TIRPs (features) in a single entity to be classified, we introduce a new algorithm, SingleKarmaLego, which can be shown to be superior for that purpose over a Sequential TIRPs Detection algorithm. We evaluated the KLS framework on datasets in the domains of diabetes, intensive care, and infectious hepatitis, assessing the effects of the various settings of the KLS framework. Discretization using Symbolic Aggregate approXimation (SAX) led to better performance than using the equal-width discretization (EWD); knowledge-based cut-off definitions when available were superior to both. Using three abstract temporal relations was superior to using the seven core temporal relations. Using an epsilon value larger than zero tended to result in a slightly better accuracy when using the SAX discretization method, but resulted in a reduced accuracy when using EWD, and overall, does not seem beneficial. No feature selection method we tried proved useful. Regarding feature (TIRP) representation, mean duration performed better than horizontal support, which in turn performed better than the default Binary (existence) representation method.",information system
1734,Sliding windows over uncertain data streams,Knowledge and Information Systems,"Uncertain data streams can have tuples with both value and existential uncertainty. A tuple has value uncertainty when it can assume multiple possible values. A tuple is existentially uncertain when the sum of the probabilities of its possible values is (<)1. A situation where existential uncertainty can arise is when applying relational operators to streams with value uncertainty. Several prior works have focused on querying and mining data streams with both value and existential uncertainty. However, none of them have studied, in depth, the implications of existential uncertainty on sliding window processing, even though it naturally arises when processing uncertain data. In this work, we study the challenges arising from existential uncertainty, more specifically the management of count-based sliding windows, which are a basic building block of stream processing applications. We extend the semantics of sliding window to define the novel concept of uncertain sliding windows and provide both exact and approximate algorithms for managing windows under existential uncertainty. We also show how current state-of-the-art techniques for answering similarity join queries can be easily adapted to be used with uncertain sliding windows. We evaluate our proposed techniques under a variety of configurations using real data. The results show that the algorithms used to maintain uncertain sliding windows can efficiently operate while providing a high-quality approximation in query answering. In addition, we show that sort-based similarity join algorithms can perform better than index-based techniques (on 17 real datasets) when the number of possible values per tuple is low, as in many real-world applications.",information system
1735,Preventing Orphan Requests by Integrating Replication and Transactions,Advances in Databases and Information Systems,"Replication is crucial to achieve high availability distributed systems. However, non-determinism introduces consistency problems between replicas. Transactions are very well suited to maintain consistency, and by integrating them with replication, support for non-deterministic execution in replicated environments can be achieved. This paper presents an approach where a passively replicated transaction manager is allowed to break replication transparency to abort orphan requests, thus handling non-determinism. A prototype implemented using existing open-source software, Jgroup/ARM and Jini, has been developed, and performance and failover tests have been executed. The results show that while this approach is possible, components specifically tuned for performance must be used to meet real-time requirements.",information system
1736,Using cocitation information to estimate political orientation in web documents,Knowledge and Information Systems,"This paper introduces a simple method for estimating cultural orientation, the affiliation of online entities in a polarized field of discourse. In particular, cocitation information is used to estimate the political orientation of hypertext documents. A type of cultural orientation, the political orientation of a document is the degree to which it participates in traditionally left- or right-wing beliefs. Estimating documents' political orientation is of interest for personalized information retrieval and recommender systems. In its application to politics, the method uses a simple probabilistic model to estimate the strength of association between a document and left- and right-wing communities. The model estimates the likelihood of cocitation between a document of interest and a small number of documents of known orientation. The model is tested on three sets of data, 695 partisan web documents, 162 political weblogs, and 198 nonpartisan documents. Accuracy above 90% is obtained from the cocitation model, outperforming lexically based classifiers at statistically significant levels.",information system
1737,MDL-based time series clustering,Knowledge and Information Systems,"Time series data are pervasive across all human endeavors, and clustering is arguably the most fundamental data mining application. Given this, it is somewhat surprising that the problem of time series clustering from a single stream remains largely unsolved. Most work on time series clustering considers the clustering of individual time series that have been carefully extracted from their original context, for example, gene expression profiles, individual heartbeats, or individual gait cycles. The few attempts at clustering time series streams have been shown to be objectively incorrect in some cases, and in other cases shown to work only on the most contrived synthetic datasets by carefully adjusting a large set of parameters. In this work, we make two fundamental contributions that allow for the first time, the meaningful clustering of subsequences from a time series stream. First, we show that the problem definition for time series clustering from streams currently used is inherently flawed, and a new definition is necessary. Second, we show that the minimum description length framework offers an efficient, effective, and essentially parameter-free method for time series clustering. We show that our method produces objectively correct results on a wide variety of datasets from medicine, speech recognition, zoology, gesture recognition, and industrial process analyses.",information system
1738,Detecting Termination of Active Database Rules Using Symbolic Model Checking,Advances in Databases and Information Systems,"Many algorithms have been proposed that detect non-termination of active database rules. However, most of these provide a conservative estimate–they detect all potential cases of non-termination. The onus is on the database programmers to further analyze these cases and give more definite results about non-termination. In this paper we show how the database programmer can automatically detect non-termination using an existing symbolic model checker. Our approach does not require much expertise on the part of the database programmer, and can be used to detect termination cases which the conservative approaches reject as non-terminating ones.",information system
1739,Data Mining in a Multidimensional Environment,Advances in Databases and Information Systems,"Data Mining and Data Warehousing are two hot topics in the database research area. Until recently, conventional data mining algorithms were primarily developed for a relational environment. But a data warehouse database is based on a multidimensional model. In our paper we apply this basis for a seamless integration of data mining in the multidimensional model for the example of discovering association rules. Furthermore, we propose this method as a userguided technique because of the clear structure both of model and data. We present both the theoretical basis and efficient algorithms for data mining in the multidimensional data model. Our approach uses directly the requirements of dimensions, classifications and sparsity of the cube. Additionally we give heuristics for optimizing the search for rules.",information system
1740,D2S: Document-to-sentence framework for novelty detection,Knowledge and Information Systems,"Novelty detection aims at identifying novel information from an incoming stream of documents. In this paper, we propose a new framework for document-level novelty detection using document-to-sentence (D2S) annotations and discuss the applicability of this method. D2S first segments a document into sentences, determines the novelty of each sentence, then computes the document-level novelty score based on a fixed threshold. Experimental results on APWSJ data show that D2S outperforms standard document-level novelty detection in terms of redundancy-precision (RP) and redundancy-recall (RR). We applied D2S on the document-level data from the TREC 2004 and TREC 2003 Novelty Track and find that D2S is useful in detecting novel information in data with a high percentage of novel documents. However, D2S shows a strong capability to detect redundant information regardless of the percentage of novel documents. D2S has been successfully integrated in a real-world novelty detection system.",information system
1741,The Expressivity of Constraint Query Languages with Boolean Algebra Linear Cardinality Constraints,Advances in Databases and Information Systems,"Constraint query languages with Boolean algebra linear cardinality constraints were introduced recently and shown to be evaluable using a quantifier elimination method in [22]. However, the expressive power of constraint query languages with linear cardinality constraints is still poorly understood in comparison with other cases of constraint query languages. This paper makes several contributions to the analysis of their expressive power. Several problems that were previously provably impossible to express even in FO+POLY are shown to be expressible using first-order query languages with linear cardinality constraints FO+BALC. We also show that all monadic Datalog queries are expressible in FO+BALC. Finally, we also show a new results for FO+LINEAR by expressing in it the problem of finding the time when two linearly moving point objects are closest to each other.",information system
1742,Multirepresentation in Ontologies,Advances in Databases and Information Systems,"The objective of this paper is to define an ontology language to support multiple representations of ontologies. In our research, we focus on the logic-based ontology languages. As a matter of fact, we will consider only languages that are based on description logics (DLs). At first, we propose a sub-language of DL as an ontology language. Furthermore we achieve multiple representations of ontological concepts by extending such sub-language through the use of stamping mechanism proposed in the context of multiple representation of spatial databases. The proposed language should offer a modest solution to the problem of multirepresentation ontologies.",information system
1743,From Business Rules to Application Rules in Rich Internet Applications,Business Information Systems,"The increase of digital bandwidth and computing power of personal computers as well as the rise of the Web 2.0 came along with a new Web programming paradigm: Rich Internet Applications. On the other hand, powerful server-side business rules engines appeared over the last years and let enterprises describe their business policies declaratively as business rules. This paper addresses the problem of how to combine the business rules approach with the new programming paradigm of Rich Internet Applications. We present a novel approach that reuses business rules for deriving declarative presentation and visualization logic. In this paper we discuss complex event processing as an essential requirement for rule-enabled Rich Internet Applications, and introduce a rule-based architecture capable of executing rules directly on the client. We propose to use declarative rules as platform independent model describing the application and presentation logic. By means of AJAX we exemplarily show how to automatically generate client-side executable rules with the aid of Rich Internet Application design patterns.",information system
1744,On processing continuous frequent K-N-match queries for dynamic data over networked data sources,Knowledge and Information Systems,"Similarity search is one of the critical issues in many applications. When using all attributes of objects to determine their similarity, most prior similarity search algorithms are easily influenced by a few attributes with high dissimilarity. The frequent k-n-match query is proposed to overcome the above problem. However, the prior algorithm to process frequent k-n-match queries is designed for static data, whose attributes are fixed, and is not suitable for dynamic data. Thus, we propose in this paper two schemes to process continuous frequent k-n-match queries over dynamic data. First, the concept of safe region is proposed and four formulae are devised to compute safe regions. Then, scheme CFKNMatchAD-C is developed to speed up the process of continuous frequent k-n-match queries by utilizing safe regions to avoid unnecessary query re-evaluations. To reduce the amount of data transmitted by networked data sources, scheme CFKNMatchAD-C also uses safe regions to eliminate transmissions of unnecessary data updates which will not affect the results of queries. Moreover, for large-scale environments, we further propose scheme CFKNMatchAD-D by extending scheme CFKMatchAD-C to employ multiple servers to process continuous frequent k-n-match queries. Experimental results show that scheme CFKNMatchAD-C and scheme CFKNMatchAD-D outperform the prior algorithm in terms of average response time and the amount of produced network traffic.",information system
1745,Partial Repairs That Tolerate Inconsistency,Advances in Databases and Information Systems,"The consistency of databases can be supported by enforcing integrity constraints on the stored data. Constraints that are violated should be repaired by eliminating the causes of the violations. Traditionally, repairs are conceived to be total. However, it may be unfeasible to eliminate all violations. We show that it is possible to get by with partial repairs that tolerate extant inconsistencies. They may not eliminate all causes of integrity violations but preserve the consistent parts of the database. Remaining violations can be controlled by measuring inconsistency, and further reduced by inconsistency-tolerant integrity checking.",information system
1746,Social Media and Firm Equity Value,Information Systems Research,"Companies have increasingly advocated social media technologies to transform businesses and improve organizational performance. This study scrutinizes the predictive relationships between social media and firm equity value, the relative effects of social media metrics compared with conventional online behavioral metrics, and the dynamics of these relationships. The results derived from vector autoregressive models suggest that social media-based metrics (Web blogs and consumer ratings) are significant leading indicators of firm equity value. Interestingly, conventional online behavioral metrics (Google searches and Web traffic) are found to have a significant yet substantially weaker predictive relationship with firm equity value than social media metrics. We also find that social media has a faster predictive value, i.e., shorter ""wear-in"" time, than conventional online media. These findings are robust to a consistent set of volume-based measures (total blog posts, rating volume, total page views, and search intensity). Collectively, this study proffers new insights for senior executives with respect to firm equity valuations and the transformative power of social media. ÂŠ 2013 INFORMS.",information system
1747,Cognitive Context and Arguments from Ontologies for Learning,Formal Ontology in Information Systems," The deployment of learning resources on the web by different experts has resulted in the accessibility of multiple viewpoints about the same topics. In this work we assume that learning resources are underpinned by ontologies. Different formalizations of domains may result from different contexts, different use of terminology, incomplete knowledge or conflicting knowledge. We define the notion of cognitive learning context which describes the cognitive context of an agent who refers to multiple and possibly inconsistent ontologies to determine the truth of a proposition. In particular we describe the cognitive states of ambiguity and inconsistency resulting from incomplete and conflicting ontologies respectively. Conflicts between ontologies can be identified through the derivation of conflicting arguments about a particular point of view. Arguments can be used to detect inconsistencies between ontologies. They can also be used in a dialogue between a human learner and a software tutor in order to enable the learner to justify her views and detect inconsistencies between her beliefs and the tutor's own. Two types of arguments are discussed, namely: arguments inferred directly from taxonomic relations between concepts, and arguments about the necessary and jointly sufficient features that define concepts. ",information system
1748,Subspace and projected clustering: experimental evaluation and analysis,Knowledge and Information Systems,"Subspace and projected clustering have emerged as a possible solution to the challenges associated with clustering in high-dimensional data. Numerous subspace and projected clustering techniques have been proposed in the literature. A comprehensive evaluation of their advantages and disadvantages is urgently needed. In this paper, we evaluate systematically state-of-the-art subspace and projected clustering techniques under a wide range of experimental settings. We discuss the observed performance of the compared techniques, and we make recommendations regarding what type of techniques are suitable for what kind of problems.",information system
1749,Underlying Consumer Heterogeneity in Markets for Subscription-Based IT Services with Network Effects,Information Systems Research," Iinformation technology services that exhibit network effects. Insights into consumer heterogeneity with respect n this paper we explore the underlying consumer heterogeneity in competitive markets for subscription-based to a given service are paramount in forecasting future subscriptions, understanding the impact of price and information dissemination on market penetration growth, and predicting the adoption path for complementary products that target the same customers as the original service. Employing a continuous-time utility model, we capture the behavior of a continuum of consumers who are differentiated by their intrinsic valuations from using the service. We study service subscription patterns under both perfect and imperfect information dissemination. In each case, we first specify the conditions under which consumer rational behavior supported by the utility model can explain a general observed adoption path, and if so, we explicitly derive the analytical closed-form expression for the consumer valuation distribution. We further explore the impact of awareness and distribution skewness on adoption. In particular, we highlight the practical forecasting importance of understanding the information dissemination process in the market as observed past adoption may be explained by several distinct awareness and heterogeneity scenarios that may lead to divergent adoption paths in the future. Moreover, we show that in the later part of the service lifecycle the subscription decision for new customers can be driven predominantly by information dissemination instead of further price markdowns. We also extend our results to time-varying consumer valuation scenarios. Furthermore, based on our framework, we advance a set of heuristic methods to be applied to discrete-time real industry data for estimation and forecasting purposes. In an empirical exercise, we apply our methodology to the Japanese mobile voice services market and provide relevant managerial insights from the analysis. ",information system
1750,Effective and efficient classification on a search-engine model,Knowledge and Information Systems,"Traditional document classification frameworks, which apply the learned classifier to each document in a corpus one by one, are infeasible for extremely large document corpora, like the Web or large corporate intranets. We consider the classification problem on a corpus that has been processed primarily for the purpose of searching, and thus our access to documents is solely through the inverted index of a large scale search engine. Our main goal is to build the “best” short query that characterizes a document class using operators normally available within search engines. We show that surprisingly good classification accuracy can be achieved on average over multiple classes by queries with as few as 10 terms. As part of our study, we enhance some of the feature-selection techniques that are found in the literature by forcing the inclusion of terms that are negatively correlated with the target class and by making use of term correlations; we show that both of those techniques can offer significant advantages. Moreover, we show that optimizing the efficiency of query execution by careful selection of terms can further reduce the query costs. More precisely, we show that on our set-up the best 10-term query can achieve 93% of the accuracy of the best SVM classifier (14,000 terms), and if we are willing to tolerate a reduction to 89% of the best SVM, we can build a 10-term query that can be executed more than twice as fast as the best 10-term query.",information system
1751,Modeling collective blogging dynamics of popular incidental topics,Knowledge and Information Systems,"An extended susceptible-infective (SI) epidemic model is presented in this paper to describe the collective blogging behavior on popular incidental topics. Our model has two major extensions over the classic SI model: in the new model, different blog writers get interested in a specific topic with different probabilities, while in a classic SI model, the infection probability of a disease between any two individuals is identical; the new model takes into consideration the impact of external mainstream media on blog writers, while in a classical SI model, spreading of diseases is merely based on personal contacts between individuals. The new model is capable of explaining the widely observed early burst and heavy tail of topic propagation velocity. The proposed model has a closed-form solution when the individual interest is of uniform distribution with the external influence assumed constant. We validate the proposed model using ten topics from two different data sets: Sina Blog and LiveJournal Blogspace, the results indicating that our model fits the topic propagation velocity and predicts the propagation trend very well.",information system
1752,Ascending Combinatorial Auctions with Allocation Constraints: On Game Theoretical and Computational Properties of Generic Pricing Rules,Information Systems Research," Combinatorial auctions are used in a variety of application domains such as transportation or industrial procurement using a variety of bidding languages and di erent allocation constraints. This exibility in the bidding languages and the allocation constraints is essential in these domains, but has not been considered in the theoretical literature so far. In this paper, we analyze di erent pricing rules for ascending combinatorial auctions which allow for such exibility: winning levels, and deadness levels. We determine the computational complexity of these pricing rules and show that deadness levels actually satisfy an ex-post equilibrium, while winning levels do not allow for a strong game-theoretical solution concept. We investigate the relationship of deadness levels and the simple price update rules used in e cient ascending combinatorial auction formats. We show that ascending combinatorial auctions with deadness level pricing rules maintain a strong game theoretical solution concept and reduce the number of bids and rounds required at the expense of higher computational e ort. The calculation of exact deadness levels is a 2P -complete problem. Nevertheless, numerical experiments show that for mid-sized auctions this is a feasible approach. The paper provides a foundation for allocation constraints in combinatorial auctions and a theoretical framework for recent Information Systems contributions in this eld. ",information system
1753,A middleware-based approach to database caching,Advances in Databases and Information Systems,"Database caching supports declarative query processing close to the application. Using a full-fledged DBMS as cache manager, it enables the evaluation of specific project-select-join queries in the cache. In this paper, we propose significant improvements and optimizations – as compared to the well-known DBCache approach – that make our caching concept truly adaptive. Furthermore, we describe an adaptive constraint-based cache system (ACCache) relying on middleware components as a DBMS-independent realization of this approach.",information system
1754,Towards a standard upper ontology,Formal Ontology in Information Systems," - The Suggested Upper Merged Ontology (SUMO) is an upper level ontology that has been proposed as a starter document for The Standard Upper Ontology Working Group, an IEEE-sanctioned working group of collaborators from the fields of engineering, philosophy, and information science. The SUMO provides definitions for general-purpose terms and acts as a foundation for more specific domain ontologies. In this paper we outline the strategy used to create the current version of the SUMO, discuss some of the challenges that we faced in constructing the ontology, and describe in detail its most general concepts and the relations between them. Categories & Descriptors - I.2.4 [Knowledge Representation Formalisms and Methods]: Artificial Intelligence - representations (procedural and rule-based), semantic networks. ",information system
1755,Transfer learning for activity recognition: a survey,Knowledge and Information Systems,"Many intelligent systems that focus on the needs of a human require information about the activities being performed by the human. At the core of this capability is activity recognition, which is a challenging and well-researched problem. Activity recognition algorithms require substantial amounts of labeled training data yet need to perform well under very diverse circumstances. As a result, researchers have been designing methods to identify and utilize subtle connections between activity recognition datasets, or to perform transfer-based activity recognition. In this paper, we survey the literature to highlight recent advances in transfer learning for activity recognition. We characterize existing approaches to transfer-based activity recognition by sensor modality, by differences between source and target environments, by data availability, and by type of information that is transferred. Finally, we present some grand challenges for the community to consider as this field is further developed.",information system
1756,Periodic subgraph mining in dynamic networks,Knowledge and Information Systems," In systems of interacting entities such as social networks, interactions that occur regularly typically correspond to significant, yet often infrequent and hard to detect, interaction patterns. To identify such regular behavior in streams of dynamic interaction data, we propose a new mining problem of finding a minimal set of periodically recurring subgraphs to capture all periodic behavior in a dynamic network. We analyze the computational complexity of the problem and show that it is polynomial, unlike many related subgraph or itemset mining problems. We propose an efficient and scalable algorithm to mine all periodic subgraphs in a dynamic network. The algorithm makes a single pass over the data and is also capable of accommodating imperfect periodicity. We demonstrate the applicability of our approach on several real-world networks and extract interesting and insightful periodic interaction patterns. We also show that periodic subgraphs can be an effective way to uncover and characterize the natural periodicities in a system. ",information system
1757,Information System Use--Related Activity: An Expanded Behavioral Conceptualization of Individual-Level Information System Use,Information Systems Research,"Despite calls for improving current approaches to conceptualizing and measuring the construct of information system use, theoretical advances in this regard are still insufficient. The present paper proposes to expand the focus of existing conceptualizations that exclusively focus on technology interaction behaviors via the construct of IS use-related activity. Based on task-technology fit and activity theory IS use-related activity is conceptualized as a second-order aggregate construct that comprises both technology interaction behaviors, as well as activities users undertake to adapt the task-technology-individual system. A multiple-indicators and multiple-causes analysis of data collected from 190 users in 21 organizations is found to support the proposed conceptualization. © 2007 INFORMS.",information system
1758,An efficient graph-mining method for complicated and noisy data with real-world applications,Knowledge and Information Systems," In this paper, we present a novel graph database-mining method called APGM (APproximate Graph Mining) to mine useful patterns from noisy graph database. In our method, we designed a general framework for modeling noisy distribution using a probability matrix and devised an efficient algorithm to identify approximate matched frequent subgraphs. We have used APGM to both synthetic data set and real-world data sets on protein structure pattern identification and structure classification. Our experimental study demonstrates the efficiency and efficacy of the proposed method. ",information system
1759,Efficient and flexible anonymization of transaction data,Knowledge and Information Systems,"Transaction data are increasingly used in applications, such as marketing research and biomedical studies. Publishing these data, however, may risk privacy breaches, as they often contain personal information about individuals. Approaches to anonymizing transaction data have been proposed recently, but they may produce excessively distorted and inadequately protected solutions. This is because these approaches do not consider privacy requirements that are common in real-world applications in a realistic and flexible manner, and attempt to safeguard the data only against either identity disclosure or sensitive information inference. In this paper, we propose a new approach that overcomes these limitations. We introduce a rule-based privacy model that allows data publishers to express fine-grained protection requirements for both identity and sensitive information disclosure. Based on this model, we also develop two anonymization algorithms. Our first algorithm works in a top-down fashion, employing an efficient strategy to recursively generalize data with low information loss. Our second algorithm uses sampling and a combination of top-down and bottom-up generalization heuristics, which greatly improves scalability while maintaining low information loss. Extensive experiments show that our algorithms significantly outperform the state-of-the-art in terms of retaining data utility, while achieving good protection and scalability.",information system
1760,New Frontiers in business intelligence: distribution and personalization,Advances in Databases and Information Systems," To meet the new, more sophisticated needs of decision makers, a new generation of BI systems is emerging. In this paper we focus on two enabling technologies for this new generation, namely distribution and personalization. In particular, to support complex business scenarios where multiple partner companies cooperate towards a common goal, we outline a distributed architecture based on a network of collaborative, autonomous, and heterogeneous peers, each offering monitoring and decision support functionalities to the other peers. Then we discuss some issues related to OLAP query reformulation on peers, showing how it can be achieved using semantic mappings between the local multidimensional schemata of peers. Finally, as to personalization, we discuss the benefits of annotating OLAP queries with preferences, focusing in particular on how preferences enable peer heterogeneity in a distributed context to be overcome. ",information system
1761,Collaborative Filtering Using a Regression-Based Approach,Knowledge and Information Systems,"The task of collaborative filtering is to predict the preferences of an active user for unseen items given preferences of other users. These preferences are typically expressed as numerical ratings. In this paper, we propose a novel regression-based approach that first learns a number of experts describing relationships in ratings between pairs of items. Based on ratings provided by an active user for some of the items, the experts are combined by using statistical methods to predict the user’s preferences for the remaining items. The approach was designed to efficiently address the problem of data sparsity and prediction latency that characterise collaborative filtering. Extensive experiments on Eachmovie and Jester benchmark collaborative filtering data show that the proposed regression-based approach achieves improved accuracy and is orders of magnitude faster than the popular neighbour-based alternative. The difference in accuracy was more evident when the number of ratings provided by an active user was small, as is common for real-life recommendation systems. Additional benefits were observed in predicting items with large rating variability. To provide a more detailed characterisation of the proposed algorithm, additional experiments were performed on synthetic data with second-order statistics similar to that of the Eachmovie data. Strong experimental evidence was obtained that the proposed approach can be applied to data over a large range of sparsity scenarios and is superior to non-personalised predictors even when ratings data are very sparse.",information system
1762,Client-Side Dynamic Preprocessing of Transactions,Advances in Databases and Information Systems,"In a client-server relational database system the response time and server throughput can be improved by outsourcing workload to clients. As extention of client-side caching techniques, we propose to preprocess database transactions at the client-side. A client operates on secondary data and supports only a low degree of isolation. The main objective is to provide a framework where the amount of preprocessing at clients is variable and adapts dynamically at run-time. Thereby, the overall goal is to maximize the systems performance, e.g. response time and throughput. We make use of a two-phase transaction protocol that verifies and reprocesses client computations if necessary. By using execution statistics we show how the amount of preprocessing can be partially predicted for each client. Within an experiment we show the correspondence between amount of preprocessing, update frequency and response time.",information system
1763,Characterizing pattern preserving clustering,Knowledge and Information Systems,"This paper describes a new approach for clustering—pattern preserving clustering—which produces more easily interpretable and usable clusters. This approach is motivated by the following observation: while there are usually strong patterns in the data—patterns that may be key for the analysis and description of the data—these patterns are often split among different clusters by current clustering approaches. This is, perhaps, not surprising, since clustering algorithms have no built-in knowledge of these patterns and may often have goals that are in conflict with preserving patterns, e.g., minimize the distance of points to their nearest cluster centroids. In this paper, our focus is to characterize (1) the benefits of pattern preserving clustering and (2) the most effective way of performing pattern preserving clustering. To that end, we propose and evaluate two clustering algorithms, HIerarchical Clustering with pAttern Preservation (HICAP) and bisecting K-means Clustering with pAttern Preservation (K-CAP). Experimental results on document data show that HICAP can produce overlapping clusters that preserve useful patterns, but has relatively worse clustering performance than bisecting K-means with respect to the clustering evaluation criterion of entropy. By contrast, in terms of entropy, K-CAP can perform substantially better than the bisecting K-means algorithm when data sets contain clusters of widely different sizes—a common situation in the real-world. Most importantly, we also illustrate how patterns, if preserved, can aid cluster interpretation.",information system
1764,An unsupervised approach to modeling personalized contexts of mobile users,Knowledge and Information Systems,"Mobile context modeling is a process of recognizing and reasoning about contexts and situations in a mobile environment, which is critical for the success of context-aware mobile services. While there are prior work on mobile context modeling, the use of unsupervised learning techniques for mobile context modeling is still under-explored. Indeed, unsupervised techniques have the ability to learn personalized contexts which are difficult to be predefined. To that end, in this paper, we propose an unsupervised approach to modeling personalized contexts of mobile users. Along this line, we first segment the raw context data sequences of mobile users into context sessions where a context session contains a group of adjacent context records which are mutually similar and usually reflect the similar contexts. Then, we exploit topic models to learn personalized contexts in the form of probabilistic distributions of raw context data from the context sessions. Finally, experimental results on real-world data show that the proposed approach is efficient and effective for mining personalized contexts of mobile users.",information system
1765,Finding maximal homogeneous clique sets,Knowledge and Information Systems,"Many datasets can be encoded as graphs with sets of labels associated with the vertices. We consider this kind of graphs and we propose to look for patterns called maximal homogeneous clique sets, where such a pattern is a subgraph that is structured in several large cliques and where all vertices share enough labels. We present an algorithm based on graph enumeration to compute all patterns satisfying user-defined constraints on the number of separated cliques, on the size of these cliques, and on the number of labels shared by all the vertices. Our approach is tested on real datasets based on a social network of scientific collaborations and on a biological network of protein–protein interactions. The experiments show that the patterns are useful to exhibit subgraphs organized in several core modules of interactions. Performances are reported on real data and also on synthetic ones, showing that the approach can be applied on different kinds of large datasets.",information system
1766,Multi-source Materialized Views Maintenance: Multi-level Views,Advances in Databases and Information Systems,"In many information systems, the databases that make up the system are distributed in different modules or branch offices according to the requirements of the business enterprise. In these systems, it is often necessary to combine the information of all the organization’s databases in order to perform analysis and make decisions about the global operation. This is the case of Data Warehouse Systems. From a conceptual point of view, a Data Warehouse can be considered as a set of materialized views which are defined in terms of the tables stored in one or more databases. These materialized views store historical data that must be maintained in either real time or periodically by means of batch processes. During the maintenance process the systems must perform selections, projections, joins, etc. that can affect several databases. This is a complex problem since making a join among several tables requires (at least temporarily) having the information from these tables in the same place. This requires the Data Warehouse to store auxiliary materialized views that in many cases contain duplicated information. In this article, we study this problem, and we propose a method that minimizes the duplicated information in the auxiliary materialized views and also reduces the response time of the system.",information system
1767,Ubiquitous Microblogging: A Flow-Based Front-End for Information Logistics,Business Information Systems,"The success of information supply strongly depends on successful user adoption. This especially is the case for the integration of non-human information sources deriving from ubiquitous computing. To allow ordinary users to participate, there is a clear need for simple but yet powerful front-end technology. Therefore, we suggest leveraging existing and proven application patterns rather than building new concepts out of scratch. Especially Web 2.0 applications are designed for the management of millions of (human) networked information nodes and could be very useful in the context of Information Logistics. Beyond the Web 2.0 tool family, in particular microblogging could show a perfect match with Information Logistics scenarios due to its ad-hoc character and its simplicity. This paper discusses these possibilities and presents the vision of Ubiquitous Microblogging, which means a Twitter-like front-end for information from human and non-human information sources.",information system
1768,Emergence of New Project Teams from Open Source Software Developer Networks: Impact of Prior Collaboration Ties,Information Systems Research," Software development has traditionally been regarded as an activity that can only be effectively conducted and managed within a firm setting. However, contrary to such assertions, the open source software development (OSSD) approach, in which software developers in Internet-based communities coordinate to voluntarily contribute programming code, has recently emerged as a promising alternative to developing high quality software. Although many high profile cases of successful OSSD projects exist (e.g., Apache, OpenOffice, Emacs, PHP), the harsh reality is that the vast majority of OSS projects fail to take off and become abandoned. A commonly cited reason for the failure of OSS projects is the lack of developers in the project teams, or put differently, the inability of the software project to bring together a critical mass of developers. In this paper, we examine how OSSD project teams are formed. More specifically, we investigate whether prior collaborative ties impact OSSD team formation and developers' joining behaviors. Using software project data from real world OSSD projects, we empirically test the impact of previous collaborative ties on software team formation. Overall, we find that the existence and the amount of prior collaborative relations in the developer network do increase the probability that an OSS project will attract more developers and that a developer's prior relationships with a project initiator do increase the likelihood that a developer will join a project initiated by a past collaborator. We also explore the performance implications of early team formation behaviors. We discuss the implications of our results with respect to open source software development and software project management. ",information system
1769,Autonomic Databases: Detection of Workload Shifts with n-Gram-Models,Advances in Databases and Information Systems,"Autonomic databases are intended to reduce the total cost of ownership for a database system by providing self-management functionality. The self-management decisions heavily depend on the database workload, as the workload influences both the physical design and the DBMS configuration. In particular, a database reconfiguration is required whenever there is a significant change, i.e. shift, in the workload. In this paper we present an approach for continuous, light-weight workload monitoring in autonomic databases. Our concept is based on a workload model, which describes the typical workload of a particular DBS using n-Gram-Models. We show how this model can be used to detect significant workload changes. Additionally, a processing model for the instrumentation of the workload is proposed. We evaluate our approach using several workload shift scenarios.",information system
1770,Similarity measures for OLAP sessions,Knowledge and Information Systems,"OLAP queries are not normally formulated in isolation, but in the form of sequences called OLAP sessions. Recognizing that two OLAP sessions are similar would be useful for different applications, such as query recommendation and personalization; however, the problem of measuring OLAP session similarity has not been studied so far. In this paper, we aim at filling this gap. First, we propose a set of similarity criteria derived from a user study conducted with a set of OLAP practitioners and researchers. Then, we propose a function for estimating the similarity between OLAP queries based on three components: the query group-by set, its selection predicate, and the measures required in output. To assess the similarity of OLAP sessions, we investigate the feasibility of extending four popular methods for measuring similarity, namely the Levenshtein distance, the Dice coefficient, the tf–idf weight, and the Smith–Waterman algorithm. Finally, we experimentally compare these four extensions to show that the Smith–Waterman extension is the one that best captures the users’ criteria for session similarity.",information system
1771,Flattening the Metamodel for Object Databases,Advances in Databases and Information Systems,"A metamodel definition presents some important issues in the con- struction of an object database management system, whose rich data model inevitably increases the metamodel complexity. The required features of an object database metamodel are investigated. Roles of a metamodel in an object- oriented database management system are presented and compared to the pro- posal defined in the ODMG standard of object-oriented database management systems. After outlining the metamodel definition included in the standard, its main drawbacks are identified and several changes to the ODMG metamodel definition are suggested. The biggest conceptual change concerns flattening the metamodel to reduce complexity and to support extendibility.",information system
1772,On link privacy in randomizing social networks,Knowledge and Information Systems,"Many applications of social networks require relationship anonymity due to the sensitive, stigmatizing, or confidential nature of relationship. Recent work showed that the simple technique of anonymizing graphs by replacing the identifying information of the nodes with random IDs does not guarantee privacy since the identification of the nodes can be seriously jeopardized by applying subgraph queries. In this paper, we investigate how well an edge-based graph randomization approach can protect sensitive links. We show via theoretical studies and empirical evaluations that various similarity measures can be exploited by attackers to significantly improve their confidence and accuracy of predicted sensitive links between nodes with high similarity values. We also compare our similarity measure-based prediction methods with the low-rank approximation-based prediction in this paper.",information system
1773,GOSPL: a method and tool for fact-oriented hybrid ontology engineering,Advances in Databases and Information Systems,"In this paper we present GOSPL, which stands for Grounding Ontologies with Social Processes and Natural Language. GOSPL is a method and tool that supports stakeholders in iteratively interpreting and modeling their common hybrid ontologies using their own terminology for semantic interoperability between autonomously developed and maintained information systems. Hybrid ontologies are ontologies in which concepts are both formally and informally described with the help of a special linguistic resource called glossary. Social interactions between the community members drive the ontology evolution process and result in more stable and agreed upon ontologies.",information system
1774,QoS-aware web service selection with negative selection algorithm,Knowledge and Information Systems,"Web service selection, as an important part of web service composition, has direct influence on the quality of composite service. Many works have been carried out to find the efficient algorithms for quality of service (QoS)-aware service selection problem in recent years. In this paper, a negative selection immune algorithm (NSA) is proposed, and as far as we know, this is the first time that NSA is introduced into web service selection problem. Domain terms and operations of NSA are firstly redefined in this paper aiming at QoS-aware service selection problem. NSA is then constructed to demonstrate how to use negative selection principle to solve this question. Thirdly, an inconsistent analysis between local exploitation and global planning is presented, through which a local alteration of a composite service scheme can transfer to the global exploration correctly. It is a general adjusting method and independent to algorithms. Finally, extensive experimental results illustrate that NSA, especially for NSA with consistency weights adjusting strategy (NSA+), significantly outperforms particle swarm optimization and clonal selection algorithm for QoS-aware service selection problem. The superiority of NSA+ over others is more and more evident with the increase of component tasks and related candidate services.",information system
1775,Dynamic and fast processing of queries on large-scale RDF data,Knowledge and Information Systems,"As RDF data continue to gain popularity, we witness the fast growing trend of RDF datasets in both the number of RDF repositories and the size of RDF datasets. Many known RDF datasets contain billions of RDF triples (subject, predicate and object). One of the grant challenges for managing these huge RDF data is how to execute RDF queries efficiently. In this paper, we address the query processing problems against the billion triple challenges. We first identify some causes for the problems of existing query optimization schemes, such as large intermediate results, initial query cost estimation errors. Then, we present our block-oriented dynamic query plan generation approach powered with pipelining execution. Our approach consists of two phases. In the first phase, a near-optimal execution plan for queries is chosen by identifying the processing blocks of queries. We group the join patterns sharing a join variable into building blocks of the query plan since executing them first provides opportunities to reduce the size of intermediate results generated. In the second phase, we further optimize the initial pipelining for a given query plan. We employ optimization techniques, such as sideways information passing and semi-join, to further reduce the size of intermediate results, improve the query processing cost estimation and speed up the performance of query execution. Experimental results on several RDF datasets of over a billion triples demonstrate that our approach outperforms existing RDF query engines that rely on dynamic programming based static query processing strategies.",information system
1776,WiKi’mantics: interpreting ontologies with WikipediA,Knowledge and Information Systems,"In the context of the Semantic Web, many ontology-related operations can be boiled down to one fundamental task: finding as accurately as possible the semantics hiding beneath the superficial representation of ontological entities. This, however, is not an easy task due to the ambiguous nature of semantics and a lack of systematic engineering method to guide how we comprehend semantics. We acknowledge the gap between human cognition and knowledge representation formalisms: even though precise logic formulae can be used as the canonical representation of ontological entities, understanding of such formulae may vary. A feasible solution to juxtaposing semantics interpretation, therefore, is to reflect such cognitive variations. In this paper, we propose an approximation of semantics using sets of words/phrases, referred to as WɪKɪmantic vectors. These vectors are emerged through a set of well-tuned methods gradually surfacing the semantics that remain implicit otherwise. Given a concept, we first identify its conceptual niche amongst its neighbours in the graph representation of the ontology. We generate a natural language paraphrases of the isolated sub-graph and project this textual description upon a large document repository. WɪKɪmantic vectors are then drawn from the document repository. We evaluated each of the aforementioned steps by way of user study.",information system
1777,Design and Implementation of a Document Database Extension,Advances in Databases and Information Systems," Integration of text and documents into database management systems has been the subject of much research. However, most of the approaches are limited to data retrieval. Collaborative text editing, i.e. the ability for multiple users to work on a document instance simultaneously, is rarely supported. Also, documents mostly consist of plain text only, and support very limited meta data storage or search. We address the problem by proposing an extended definition of document data type which comprises not only the text itself but also structural information such as layout, template and semantics, as well as document creation meta data. We implemented a new collaborative data type Document which supports document manipulation via a text editing API and extended SQL syntax (TX SQL), as detailed in this work. We report also on the search capabilities of our document management system and present some of the future challenges for collaborative document management. ",information system
1778,Exploring heterogeneous information networks and random walk with restart for academic search,Knowledge and Information Systems,"In this paper, we explore heterogenous information networks in which each vertex represents one entity and the edges reflect linkage relationships. Heterogenous information networks contain vertices of several entity types, such as papers, authors and terms, and hence can fully reflect multiple linkage relationships among different entities. Such a heterogeneous information network is similar to a mixed media graph (MMG). By representing a bibliographic dataset as an MMG, the performance obtained when searching relevant entities (e.g., papers) can be improved. Furthermore, our academic search enables multiple-entity search, where a variety of entity search results are provided, such as relevant papers, authors and conferences, via a one-time query. Explicitly, given a bibliographic dataset, we propose a Global-MMG, in which a global heterogeneous information network is built. When a user submits a query keyword, we perform a random walk with restart (RWR) to retrieve papers or other types of entity objects. To reduce the query response time, algorithm Net-MMG (standing for NetClus-based MMG) is developed. Algorithm Net-MMG first divides a heterogeneous information network into a collection of sub-networks. Afterward, the Net-MMG performs a RWR on a set of selected relevant sub-networks. We implemented our academic search and conducted extensive experiments using the ACM Digital Library. The experimental results show that by exploring heterogeneous information networks and RWR, both the Global-MMG and Net-MMG achieve better search quality compared with existing academic search services. In addition, the Net-MMG has a shorter query response time while still guaranteeing good quality in search results. ÂŠ 2012 Springer-Verlag London Limited.",information system
1779,Graph Object Oriented Database for Semantic Image Retrieval,Advances in Databases and Information Systems,"This paper presents a new method for image retrieval using a graph object oriented database for processing the information extracted from the image through the segmentation process and through the semantic interpretation of this information. The object oriented database schema is structured as a classes hierarchy based on graph data structure. A graph structure is used in all phases of the image processing: image segmentation, image annotation, image indexing and image retrieval. The experiments showed that the retrieval can be conducted with good results and the method has a good time complexity.",information system
1780,A context-based enterprise ontology,Business Information Systems,"The main purpose of an enterprise ontology is to promote the common understanding between people across enterprises, as well as to serve as a communication medium between people and applications, and between different applications. This paper outlines a top-level ontology, called the context-based enterprise ontology, which aims to advance the understanding of the nature, purposes and meanings of things in enterprises with providing basic concepts for conceiving, structuring and representing things within contexts and/or as contexts. The ontology is based on the contextual approach according to which a context involves seven domains: purpose, actor, action, object, facility, location, and time. The concepts in the ontology are defined in English and presented in meta models in a UML-based ontology engineering language.",information system
1781,Aggregating multiple instances in relational database using semi-supervised genetic algorithm-based clustering technique.,Advances in Databases and Information Systems," In solving the classification problem in relational data mining, traditional methods, for example, the C4.5 and its variants, usually require data transformations from datasets stored in multiple tables into a single table. Unfortunately, we may loss some information when we join tables with a high degree of one-to-many association. Therefore, data transformation becomes a tedious trial-and-error work and the classification result is often not very promising especially when the number of tables and the degree of one-to -many association are large. In this paper, we propose a genetic semi-supervised clustering technique as a means of aggregating data in multiple tables for the classification problem in relational database. This algorithm is suitable for classification of datasets with a high degree of one-to-many associations. It can be used in two ways. One is user-controlled clustering, where the user may control the result of clustering by varying the compactness of the spherical cluster. The other is automatic clustering, where a non-overlap clustering strategy is applied. In this paper, we use the latter method to dynamically cluster multiple instances, as a means of aggregating them, and illustrate the effectiveness of this method using the semi-supervised genetic algorithm-based clustering technique. ",information system
1782,Weight-based consistent query answering over inconsistent {mathcal {SHIQ}} knowledge bases,Knowledge and Information Systems,"Non-standard query mechanisms that work under inconsistency are required in some important description logic (DL)-based applications, including those involving an inconsistent DL knowledge base ( KB) whose intensional knowledge is consistent but is violated by its extensional knowledge. This paper proposes a weight-based semantics for querying such an inconsistent KB. This semantics defines an answer of a conjunctive query posed upon an inconsistent KB as a tuple of individuals whose substitution for the variables in the query head makes the query body entailed by any subbase of the KB consisting of the intensional knowledge and a weight-maximally consistent subset of the extensional knowledge. A novel computational method for this semantics is proposed, which works for extensionally reduced({mathcal {SHIQ}}) KBs and conjunctive queries without non-distinguished variables. The method first compiles the given KB to a propositional program; then, for any given conjunctive query, it reduces the problem of computing all answers of the given query to a set of propositional satisfiability (SAT) problems with PB-constraints, which are then solved by SAT solvers. A decomposition-based framework for optimizing the method is also proposed. The feasibility of this method is demonstrated in our experiments.",information system
1783,Employee Competencies for Business Process Management,Business Information Systems,"Business process management (BPM) is an approach which empowers companies to react flexibly to new market situations. The main goal of BPM is to improve efficiency and effectiveness of value-adding business processes. The changes caused by globalization do not only concern organi-zation, technologies and processes, but also people. Employee competencies can be crucial competitive advantages. The need for new specialized and competent personnel in BPM becomes apparent from the definition of new roles, such as “Chief Process Officer” (CPO). Field reports and surveys reveal that role concepts of BPM have so far not been completely established, due to a lack of appropriate structures or due to resistance within the companies. This article considers and analyzes the success factor employee competencies in matters of the implementation of BPM in companies. For this purpose, competencies which are necessary for the roles in BPM are identified. Moreover, a classification method for the definition of role profiles is developed.",information system
1784,"Information, Technology and Information Worker Productivity: Task Level Evidence",International Conference on Information Systems," © 2007 by Sinan Aral, Erik Brynjolfsson, and Marshall Van Alstyne. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including © notice, is given to the source. ",information system
1785,Efficient mining of sequential patterns with time constraints by delimited pattern growth,Knowledge and Information Systems,"An active research topic in data mining is the discovery of sequential patterns, which finds all frequent subsequences in a sequence database. The generalized sequential pattern (GSP) algorithm was proposed to solve the mining of sequential patterns with time constraints, such as time gaps and sliding time windows. Recent studies indicate that the pattern-growth methodology could speed up sequence mining. However, the capabilities to mine sequential patterns with time constraints were previously available only within the Apriori framework. Therefore, we propose the DELISP (delimited sequential pattern) approach to provide the capabilities within the pattern-growth methodology. DELISP features in reducing the size of projected databases by bounded and windowed projection techniques. Bounded projection keeps only time-gap valid subsequences and windowed projection saves nonredundant subsequences satisfying the sliding time-window constraint. Furthermore, the delimited growth technique directly generates constraint-satisfactory patterns and speeds up the pattern growing process. The comprehensive experiments conducted show that DELISP has good scalability and outperforms the well-known GSP algorithm in the discovery of sequential patterns with time constraints.",information system
1786,REQUEST: A Query Language for Customizing Recommendations,Information Systems Research," Initially popularized by Amazon.com, recommendation technologies have become widespread over the past several years. However, the types of recommendations available to the users in these recommender systems are typically determined by the vendor and therefore are not flexible. In this paper we address this problem by presenting the recommendation query language REQUEST that allows users to customize recommendations by formulating them in the ways satisfying personalized needs of the users. REQUEST is based on the multidimensional model of recommender systems that supports additional contextual dimensions besides traditional User and Item dimensions and also OLAP-type aggregation and filtering capabilities. ",information system
1787,Complementarities in the Diffusion of Personal Computers and the Internet: Implications for the Global Digital Divide,Information Systems Research," irsbe isson ade itson co sbe Thow the diffusive interactions across these technologies affect the evolution of the global digital divide. his paper studies the cross-country diffusion of personal computers (PCs) and the Internet, and examines We adopt a generalized diffusion model that incorporates the impact of one technology's installed base on the diffusion of the other technology. We estimate the model on data from 26 developing and developed countries between 1991 and 2005. We find that the codiffusion effects between PCs and the Internet are complementary in nature and the impact of PCs on Internet diffusion is substantially stronger in developing countries as compared to developed ones. Furthermore, our results suggest that these codiffusive effects are a significant driver of the narrowing of the digital divide. We also examine the policy implications of our results, especially with respect to how complementarities in the diffusion of PC and Internet technologies might be harnessed to further accelerate the narrowing of the global digital divide. ",information system
1788,Iterative Process Models for Mobile Application Systems: A Framework,International Conference on Information Systems," The development of mobile application systems is usually realized on the basis of iterative process models of which there are many different variants. The selection of an appropriate process model is a crucial issue for the success of every system development project, particularly for systems in a highly volatile environment such as mobile application systems. This article is devoted to the idea of selecting and applying alternative iterative process models in a project-specific way. As a first step, an analysis of five predominant groups of mobile application systems has been conducted. On the basis of the results of this analysis, a three-dimensional classification scheme is developed. Applying this, we illustrate that mobile application systems can be classified, characterized, and differentiated over three dimensions: degree of innovation, speed of development, and risk. Based on these three dimensions, technical and organizational-personnel criteria are deduced and a typology of mobile application systems is developed. This framework allows the typification for a planned mobile application system according to eight defined types. Thus, recommendations concerning technical and organizational-personnel requirements can be generated and integrated into a specification in order to enable systems development teams to select an appropriate iterative process model. ",information system
1789,Using structural similarity for clustering XML documents,Knowledge and Information Systems,"In this paper, we describe a method for clustering XML documents. Its goal is to group documents sharing similar structures. Our approach is two-step. We first automatically extract the structure from each XML document to be classified. This extracted structure is then used as a representation model to classify the corresponding XML document. The idea behind the clustering is that if XML documents share similar structures, they are more likely to correspond to the structural part of the same query. Finally, for the experimentation purpose, we tested our algorithms on both real (ACM SIGMOD Record corpus) and synthetic data. The results clearly demonstrate the interest of our approach.",information system
1790,A distributed approach to enabling privacy-preserving model-based classifier training,Knowledge and Information Systems,"This paper proposes a novel approach for privacy-preserving distributed model-based classifier training. Our approach is an important step towards supporting customizable privacy modeling and protection. It consists of three major steps. First, each data site independently learns a weak concept model (i.e., local classifier) for a given data pattern or concept by using its own training samples. An adaptive EM algorithm is proposed to select the model structure and estimate the model parameters simultaneously. The second step deals with combined classifier training by integrating the weak concept models that are shared from multiple data sites. To reduce the data transmission costs and the potential privacy breaches, only the weak concept models are sent to the central site and synthetic samples are directly generated from these shared weak concept models at the central site. Both the shared weak concept models and the synthetic samples are then incorporated to learn a reliable and complete global concept model. A computational approach is developed to automatically achieve a good trade off between the privacy disclosure risk, the sharing benefit and the data utility. The third step deals with validating the combined classifier by distributing the global concept model to all these data sites in the collaboration network while at the same time limiting the potential privacy breaches. Our approach has been validated through extensive experiments carried out on four UCI machine learning data sets and two image data sets.",information system
1791,Energy conservation in wireless sensor networks : a rule-based approach.,Knowledge and Information Systems,"The research reported in this paper addresses the problem of energy conservation in wireless sensor networks (WSNs). It proposes concepts and techniques to extract environmental information that are useful for controlling sensor operations, in order to enable sensor nodes to conserve their energy, and consequently prolong the network lifetime. These concepts and techniques are consolidated in a generic framework we term CASE: Context Awareness in Sensing Environments framework. CASE targets energy conservation at the network level. A subset framework of CASE, we term CASE Compact, targets energy conservation at the sensor node level. In this paper, we elaborate on these two frameworks, elucidate the requirements for them to operate together within a WSN and evaluate the applications they can be applied to for energy conservation.",information system
1792,Generalized Affinity-Based Association Rule Mining for Multimedia Database Queries,Knowledge and Information Systems," The recent progress in high-speed communication networks and largecapacity storage devices has led to a tremendous increase in the number of databases and the volume of data in them. This has created a need to discover structural equivalence relationships from the databases since queries tend to access information from structurally equivalent media objects residing in di erent databases. The more databases there are, the more query-processing performance improvement can be achieved when the structural equivalence relationships are automatically discovered. In response to such a demand, association rule mining has emerged and proven to be a highly successful technique for discovering knowledge from large databases. In this paper, we explore a generalized a nity-based association rule mining approach to discover the quasi-equivalence relationships from a network of databases. The algorithm is implemented and two empirical studies on real databases are conducted. The results show that the proposed generalized a nity-based association rule mining approach not only correctly exploits the set of quasiequivalent media objects from the databases, but also outperforms the basic association rule mining approach in the discovery of the quasi-equivalent media object pairs. ",information system
1793,Toward the Study of Aesthetics in Information Technology,International Conference on Information Systems," This paper argues that an increasingly important dimension of the human-computer interaction is missing from the MIS and the HCI research agenda. This dimension-esthetics-plays a major role in our private, social, and business lives. It is argued that aesthetics is relevant to information technology research and practice for three theoretical reasons. (1) For many users, other aspects of the interaction hardly matter anymore. (2) Our evaluations of the environment are primarily visual, and the environment becomes increasingly replete with information technology. (3) Aesthetics satisfies basic human needs, and human needs are increasingly supplied by information technology. Aesthetics matters for a practical reason as well: it is here to stay. We propose a general framework for the study of aesthetics in information technology and provide some examples of research questions to illustrate the viability of this topic. ",information system
1794,ETL workflows: from formal specification to optimization,Advances in Databases and Information Systems,"In this paper, we present our work on a framework towards the modeling and optimization of Extraction-Transformation-Loading (ETL) workflows. The goal of this research was to facilitate, manage, and optimize the design and implementation of the ETL workflows both during the initial design and deployment stage, as well as, during the continuous evolution of a data warehouse. In particular, we present our results which include: (a) the provision of a novel conceptual model for the tracing of inter-attribute relationships and the respective ETL transformations in the early stages of a data warehouse project, along with an attempt to use ontology-based mechanisms to semi-automatically capture the semantics and the relationships among the various sources; (b) the provision of a novel logical model for the representation of ETL workflows with two main characteristics: genericity and customization; (c) the semi-automatic transition from the conceptual to the logical model for ETL workflows; and (d) the tuning of an ETL workflow for the optimization of the execution order of its operations. Finally, we discuss some issues on future work in the area that we consider important and a step towards the incorporation of the above research results to other areas as well.",information system
1795,Stream mining: a novel architecture for ensemble-based classification,Knowledge and Information Systems,"Mining data streams has become an important and challenging task for a wide range of applications. In these scenarios, data tend to arrive in multiple, rapid and time-varying streams, thus constraining data mining algorithms to look at data only once. Maintaining an accurate model, e.g. a classifier, while the stream goes by requires a smart way of keeping track of the data already passed away. Such a synthetic structure has to serve two purposes: distilling the most of information out of past data and allowing a fast reaction to concept drifting, i.e. to the change of the data trend that necessarily affects the model. The paper outlines novel data structures and algorithms to tackle the above problem, when the model mined out of the data is a classifier. The introduced model and the overall ensemble architecture are presented in details, even considering how the approach can be extended for treating numerical attributes. A large part of the paper discusses the experiments and the comparisons with several existing systems. The comparisons show that the performance of our system in general, and in particular with respect to the reaction to concept drifting, is at the top level.",information system
1796,A multi-colony ant algorithm for optimizing join queries in distributed database systems,Knowledge and Information Systems,"Distributed database systems provide a new data processing and storage technology for decentralized organizations of today. Query optimization, the process to generate an optimal execution plan for the posed query, is more challenging in such systems due to the huge search space of alternative plans incurred by distribution. As finding an optimal execution plan is computationally intractable, using stochastic-based algorithms has drawn the attention of most researchers. In this paper, for the first time, a multi-colony ant algorithm is proposed for optimizing join queries in a distributed environment where relations can be replicated but not fragmented. In the proposed algorithm, four types of ants collaborate to create an execution plan. Hence, there are four ant colonies in each iteration. Each type of ant makes an important decision to find the optimal plan. In order to evaluate the quality of the generated plan, two cost models are used—one based on the total time and the other on the response time. The proposed algorithm is compared with two previous genetic-based algorithms on chain, tree and cyclic queries. The experimental results show that the proposed algorithm saves up to about 80 % of optimization time with no significant difference in the quality of generated plans compared with the best existing genetic-based algorithm.",information system
1797,Visual transformation for interactive spatiotemporal data mining,Knowledge and Information Systems,"Analytical models intend to reveal inner structure, dynamics, or relationship of things. However, they are not necessarily intuitive to humans. Conventional scientific visualization methods are intuitive, but limited by depth, dimension, and resolution. The purpose of this study is to bridge the gap with transformation algorithms for mapping the data from an abstract space to an intuitive one, which include shape correlation, periodicity, multiphysics, and spatial Bayesian. We tested this approach with the oceanographic case study. We found that the interactive visualization increases robustness in object tracking and positive detection accuracy in object prediction. We also found that the interactive method enables the user to process the image data at less than 1 min per image versus 30 min per image manually. As a result, our test system can handle at least 10 times more data sets than traditional manual analyses. The results also suggest that minimal human interactions with appropriate computational transformations or cues may significantly increase the overall productivity.",information system
1798,Content based similarity of geographic classes organized as partition hierarchies,Knowledge and Information Systems,"In this paper we propose a method to measure the semantic similarity of geographic classes organized as partition hierarchies within Naive Geography. The contribution of this work consists in extending and integrating the information content approach, and the method for comparing concept attributes in the ontology management system SymOntos developed at IASI. As a result, this proposal allows us to address both the concept similarity within the partition hierarchy, and the attribute similarity of geographic classes and, therefore, to reduce the gap among the different similarity approaches defined in the literature. © Springer-Verlag London Limited 2008.",information system
1799,The Gr_Tree: The Use of Active Regions in G-Trees,Advances in Databases and Information Systems,"In this paper we present an efficient data structure, called Gr_tree, for organizing multidimensional data. The proposed structure combines the features of distance functions of metric spaces and G-trees. Although the Gr_tree requires distance computations and has the overhead of a small amount of storage space, due to the introduction of active regions inside the partitions of the data space, it reduces the accesses of partial match and range queries. We give algorithms for the dynamic operations of the Gr_tree, examine several types of queries and provide some comparative results.",information system
1800,Synthesis of the Canonical Models for Database Integration Preserving Semantics of the Value Inventive Data Models,Advances in Databases and Information Systems,"Recently families of sets of dependencies treated as the Datalog extensions were discovered for which the interpretation of queries becomes tractable. Such families are intended for inference of new, unknown values in the process of query answering. This paper considers such decidable classes of dependencies as the assets for creation of new data models (called in the paper the value inventive data models) analogously to axiomatic extension of the canonical model kernel used so far for unification of structured and object data models aimed at heterogeneous database integration. The paper examines the decidable classes of dependencies reflecting the semantics of value inventive data models considering such classes to be the extensions of the canonical data model kernel. The issue of identifying of decidable subsets of the canonical model extension is considered.",information system
1801,Turning a Corporate Folksonomy into a Lightweight Corporate Ontology,Business Information Systems,"Companies use company-specific terminology that may differ from the terminology used in existing corporate ontologies (e.g. Tove) and therefore need their own ontology. However, the current ontology engineering techniques are time-consuming and there exists a conceptual mismatch among developers and users. In contrast, folksonomies or the flat bottom-up taxonomies constituted by web users’ tags are rapidly created. In this paper, we present an approach that cost-efficiently derives a lightweight corporate ontology from a corporate folksonomy. We tested it on the folksonomy of a European company and first results are promising: it shows that it creates additional value to the company.",information system
1802,Making use of New Media for pan-European Crisis Communication,International Conference on Information Systems," Social or new media have over the past years become an integrated part of human communication, both as a means to establish and maintain social relationships, but also as a means of sharing and co-creating information. New media comes with an array of possibilities for individuals as well as organisations, corporations and authorities. Within the field of crisis communication new media possibilities, such as online sharing and social networking, has had an impact on the way crisis information is disseminated and updated. This paper addresses the issues related to using new media as a means of communicating crisis information and broadcasting alerting messages to the general population, and also discusses the role of new media in future pan-European alerting. It focuses on current and on-going research on social media for crisis communication. An extensive systematic literature review was done to identify factors that affect the use of social media for alerting and warning. These factors were mirrored in experiences, collected through interviews, in crisis communication organisations in three European regions (Sweden, Czech Republic and Spain). The factors finally form the basis for suggestions regarding the design of technological tools for both communication and information collection as part of a panEuropean alerting system. Crisis information and alerting of the population has one main motive: to save lives and prevent or limit the dangers and risk of damage caused by the on-going event/s. There are several different technical systems for crisis and emergency management in Europe, such as satellite based warning systems as well as sirens and automated emergency messages and high level strategic and organisational systems (Hill, 2010). The study presented in this paper was conducted in order to identify issues necessary for the design and development of a web screening tool (Artman et al., 2011). The research includes two separate data collection methods: a systematic literature review, and an interview study that investigates practitioner experiences of alerting. In order to identify, evaluate and interpret available research regarding the use of new and social media in crisis communication a literature review was conducted. The method was based on a systematic literature review process (Kitchenham and Charters, 2007). The general features of a pre-defined strategy include a review protocol, a defined search strategy in order to detect relevant literature, documentation of the strategy, and description of criteria for inclusion and exclusion of results. In order to ground the analysis empirically, information about the technologies used for alerting and communicating with the public during a crisis event was collected through interviews. Due to the size and variation of countries and languages in Europe, a representative sample selection of EU member states was chosen. Interviews were conducted with 12 emergency organisation officials from regional and local councils ",information system
1803,Learning element similarity matrix for semi-structured document analysis,Knowledge and Information Systems,"Capturing latent structural and semantic properties in semi-structured documents (e.g., XML documents) is crucial for improving the performance of related document analysis tasks. Structured Link Vector Mode (SLVM) is a representation recently proposed for modeling semi-structured documents. It uses an element similarity matrix to capture the latent relationships between XML elements—the constructing components of an XML document. In this paper, instead of applying heuristics to define the element similarity matrix, we propose to compute the matrix using the machine learning approach. In addition, we incorporate term semantics into SLVM using latent semantic indexing to enhance the model accuracy, with the element similarity learnability property preserved. For performance evaluation, we applied the similarity learning to k-nearest neighbors search and similarity-based clustering, and tested the performance using two different XML document collections. The SLVM obtained via learning was found to outperform significantly the conventional Vector Space Model and the edit-distance-based methods. Also, the similarity matrix, obtained as a by-product, can provide higher-level knowledge on the semantic relationships between the XML elements.",information system
1804,A Next Generation Information Gathering Agent,International Conference on Information Systems," The World Wide Web has become an invaluable information resource but the explosion of information available via the web has made web search a time consuming and complex process. Index-based search engines, such as AltaVista or Infoseek help, but they are not enough. This paper describes the rationale, architecture, and implementation of a next generation information gathering system - a system that integrates several areas of Artificial Intelligence (AI) research under a single umbrella. Our solution to the information explosion is an information gathering agent, BIG, that plans to gather information to support a decision process, reasons about the resource trade-offs of different possible gathering approaches, extracts information from both unstructured and structured documents, and uses the extracted information to refine its search and processing activities. ",information system
1805,Ex Ante Information and the Design of Keyword Auctions,Information Systems Research," Keyword advertising, including sponsored links and contextual advertising, powers many of today's online information services such as search engines and Internet-based emails. This paper examines the design of keyword auctions, a novel mechanism that keyword advertising providers such as Google and Yahoo! use to allocate advertising slots. In our keyword auction model, advertisers bid their willingness-to-pay per click on their advertisements, and the advertising provider can weigh advertisers' bids di®erently and require di®erent minimum bids based on advertisers' click-generating potential. We study the impact and design of such weighting schemes and minimumbids policies. We ¯nd that weighting scheme determines how advertisers with di®erent click-generating potential match in equilibrium. Minimum bids exclude low-valuation advertisers and at the same time may distort the equilibrium matching. The e±cient design of keyword auctions requires weighting advertisers' bids by their expected clickthrough-rates, and requires the same minimum weighted bids. The revenue-maximizing weighting scheme may or may not favor advertisers with low click-generating potential. The revenue-maximizing minimum-bid policy di®ers from those prescribed in the standard auction design literature. Keyword auctions that employ the revenue-maximizing weighting scheme and di®erential minimum bid policy can generate higher revenue than standard ¯xed-payment auctions. We draw managerial implications for pay-per-click and other pay-for-performance auctions and discuss potential applications to other areas. ",information system
1806,A Novel Particle Swarm Optimization Approach for Grid Job Scheduling,International Conference on Information Systems," This paper represents a Particle Swarm Optimization (PSO) algorithm, for grid job scheduling. PSO is a population-based search algorithm based on the simulation of the social behavior of bird flocking and fish schooling. Particles fly in problem search space to find optimal or near-optimal solutions. In this paper we used a PSO approach for grid job scheduling. The scheduler aims at minimizing makespan and flowtime simultaneously. Experimental studies show that the proposed novel approach is more efficient than the PSO approach reported in the literature. ",information system
1807,“Best K”: critical clustering structures in categorical datasets,Knowledge and Information Systems,"The demand on cluster analysis for categorical data continues to grow over the last decade. A well-known problem in categorical clustering is to determine the best K number of clusters. Although several categorical clustering algorithms have been developed, surprisingly, none has satisfactorily addressed the problem of best K for categorical clustering. Since categorical data does not have an inherent distance function as the similarity measure, traditional cluster validation techniques based on geometric shapes and density distributions are not appropriate for categorical data. In this paper, we study the entropy property between the clustering results of categorical data with different K number of clusters, and propose the BKPlot method to address the three important cluster validation problems: (1) How can we determine whether there is significant clustering structure in a categorical dataset? (2) If there is significant clustering structure, what is the set of candidate “best Ks”? (3) If the dataset is large, how can we efficiently and reliably determine the best Ks?",information system
1808,Script-based System for Monitoring Client-side Activity,Business Information Systems,"Gathering information on customer activity is a well established practice in e-commerce. So-called clickstreams consisting of a list of active elements clicked by website user during his session can be gathered and analyzed server-side, unfolding a pattern in users preferences [Andersen 2000]. Another notion comes from the field of application usage study [Hilbert 2000]. In this case, any kind of trackable user-triggered event may be recorded, and then processed in order to discover high-level actions, so that application user interface may be rearranged by removing unused capabilities and making frequent actions easier to be performed. This is also useful in e-commerce, as it is obvious that a well-designed website can generate much more profit than a badly designed one. But the detailed record of user actions also constitutes an important source of information for operational customer relationship management. It provides the sales force automation with a very precise description of customer behavior, and the customer service and support with thorough description of circumstances which led to posting an inquiry or a complaint. However, it requires a client-side monitoring as most of the low-level activity does not have any impact on the server. In todays world of malicious spyware, web users are reluctant to install any monitoring software. Thereinafter, we shall present a monitoring system which enables tracking many user actions that may be found useful in the study of improvement of an ecommerce website. The system does not require any additional software to be installed on the client, as all the tracking code is included in the website itself in the form of client-side scripts. Thus, a user has full knowledge on what is monitored-as the script source code is unhidden-and can easily turn off the monitoring just by changing web browser script-running options (though there is a subtler way of doing that embedded in the system).",information system
1809,Text document clustering using global term context vectors,Knowledge and Information Systems,"Despite the advantages of the traditional vector space model (VSM) representation, there are known deficiencies concerning the term independence assumption. The high dimensionality and sparsity of the text feature space and phenomena such as polysemy and synonymy can only be handled if a way is provided to measure term similarity. Many approaches have been proposed that map document vectors onto a new feature space where learning algorithms can achieve better solutions. This paper presents the global term context vector-VSM (GTCV-VSM) method for text document representation. It is an extension to VSM that: (i) it captures local contextual information for each term occurrence in the term sequences of documents; (ii) the local contexts for the occurrences of a term are combined to define the global context of that term; (iii) using the global context of all terms a proper semantic matrix is constructed; (iv) this matrix is further used to linearly map traditional VSM (Bag of Words—BOW) document vectors onto a ‘semantically smoothed’ feature space where problems such as text document clustering can be solved more efficiently. We present an experimental study demonstrating the improvement of clustering results when the proposed GTCV-VSM representation is used compared with traditional VSM-based approaches.",information system
1810,Manipulation of qualitative degrees to handle uncertainty: formal models and applications,Knowledge and Information Systems,"In this article, qualitative, symbolic and linguistic models for knowledge representation are presented as well as their applications. Such models are useful in decision making problems when information from the experts' knowledge is expressed through different heterogeneous types such as numerical, interval-valued, symbolic, linguistic, … The whole work proposed here takes place in a given many-valued logic. First, as an alternative to classic probabilities, a method using qualitative degrees is described and an application in supervised learning is proposed. Then we study the transformation of these degrees when they are subjected to a modification: thus we present the Generalized Symbolic Modifiers. These tools are defined as manipulations computed on a pair (degree, scale). They are grouped together into several families and thus offer many possibilities to handle uncertainty. An application in colorimetrics is described and shows the feasibility of the approach. The last point addressed in this article is the data combination. An operator called the Symbolic Weighted Median gives a summary of several qualitative degrees with associated weights. One particularity is that this median is constructed on the Generalized Symbolic Modifiers. Finally we explain how the Symbolic Weighted Median is exploited in the internal mechanism of the application in colorimetrics.",information system
1811,Spatio-Temporal Keyword Queries in Social Networks,Advances in Databases and Information Systems,"Due to the large amount of social network data produced at an ever growing speed and their complex nature, recent works have addressed the problem of efficiently querying such data according to social, temporal or spatial dimensions. In this work we propose a data model that keeps into account all these dimensions and we compare different approaches for efficient query execution on a large real dataset using standard relational technologies.",information system
1812,A framework for simulating real-time multi-agent systems,Knowledge and Information Systems,"In this paper, we describe an implementation of use in demonstrating the effectiveness of architectures for real-time multi-agent systems. The implementation provides a simulation of a simplified RoboCup Search and Rescue environment, with unexpected events, and includes a simulator for both a real-time operating system and a CPU. We present experimental evidence to demonstrate the benefit of the implementation in the context of a particular hybrid architecture for multi-agent systems that allows certain agents to remain fully autonomous, while others are fully controlled by a coordinating agent. In addition, we discuss the value of the implementation for testing any models for the construction of real-time multi-agent systems and include a comparison to related work.",information system
1813,Optimising Performance of Object-Oriented and Object-Relational Systems by Dynamic Method Materialisation,Advances in Databases and Information Systems," Efficient executions of object methods have a great impact on application response times. Optimising access to data returned by methods is an important issue in object-oriented programs, object-oriented and objectrelational systems, as well as in distributed object environments. Since methods are written in high-level programming languages, optimising their executions is difficult. In this paper we present a technique of reducing access time to data returned by methods by means of materialising method results. We have developed a prototype system where the software module, called the method analyser and optimiser is responsible for monitoring method access and gathering execution statistics. Based on the statistics, the module selects appropriate methods for materialisation. The experiments that we have conducted show that the overall system's response time decreases while using our optimisation technique. ",information system
1814,Tree-based partitioning of date for association rule mining,Knowledge and Information Systems,"The most computationally demanding aspect of Association Rule Mining is the identification and counting of support of the frequent sets of items that occur together sufficiently often to be the basis of potentially interesting rules. The task increases in difficulty with the scale of the data and also with its density. The greatest challenge is posed by data that is too large to be contained in primary memory, especially when high data density and/or low support thresholds give rise to very large numbers of candidates that must be counted. In this paper, we consider strategies for partitioning the data to deal effectively with such cases. We describe a partitioning approach which organises the data into tree structures that can be processed independently. We present experimental results that show the method scales well for increasing dimensions of data and performs significantly better than alternatives, especially when dealing with dense data and low support thresholds.",information system
1815,Knowledge-based vector space model for text clustering,Knowledge and Information Systems,"This paper presents a new knowledge-based vector space model (VSM) for text clustering. In the new model, semantic relationships between terms (e.g., words or concepts) are included in representing text documents as a set of vectors. The idea is to calculate the dissimilarity between two documents more effectively so that text clustering results can be enhanced. In this paper, the semantic relationship between two terms is defined by the similarity of the two terms. Such similarity is used to re-weight term frequency in the VSM. We consider and study two different similarity measures for computing the semantic relationship between two terms based on two different approaches. The first approach is based on the existing ontologies like WordNet and MeSH. We define a new similarity measure that combines the edge-counting technique, the average distance and the position weighting method to compute the similarity of two terms from an ontology hierarchy. The second approach is to make use of text corpora to construct the relationships between terms and then calculate their semantic similarities. Three clustering algorithms, bisecting k-means, feature weighting k-means and a hierarchical clustering algorithm, have been used to cluster real-world text data represented in the new knowledge-based VSM. The experimental results show that the clustering performance based on the new model was much better than that based on the traditional term-based VSM.",information system
1816,Against Idiosyncrasy in Ontology Development,Formal Ontology in Information Systems," The world of ontology development is full of mysteries. Recently, ISO Standard 15926 (“Lifecycle Integration of Process Plant Data Including Oil and Gas Production Facilities”), a data model initially designed to support the integration and handover of large engineering artefacts, has been proposed by its principal custodian for general use as an upper level ontology. As we shall discover, ISO 15926 is, when examined in light of this proposal, marked by a series of quite astonishing defects, which may however provide general lessons for the developers of ontologies in the future. ",information system
1817,A Probe-Based Technique to Optimize Join Queries in Distributed Internet Databases,Knowledge and Information Systems," An adaptive probe-based optimization technique is developed and demonstrated in the context of an Internet-based distributed database environment. More and more common are database systems which are distributed across servers communicating via the Internet where a query at a given site might require data from remote sites. Optimizing the response time of such queries is a challenging task due to the unpredictability of server performance and network tra c at the time of data shipment; this may result in the selection of an expensive query plan using a static query optimizer. We constructed an experimental setup consisting of two servers running the same database management system connected via the Internet. Concentrating on join queries, we demonstrate how a static query optimizer might choose an expensive plan by mistake. This is due to the lack of a priori knowledge of the run-time environment, inaccurate statistical assumptions in size estimation, and neglecting the cost of remote method invocation. These shortcomings are addressed collectively by proposing a probing mechanism. An implementation of our run-time optimization technique for join queries was constructed in the Java language and incorporated into an experimental setup. The results demonstrate the superiority of our probe-based optimization over a static optimization. ",information system
1818,Electronic roads: intelligent navigation through multi-contextual information,Knowledge and Information Systems,"This paper proposes a model for intelligent navigation through multi-contextual information that could form electronic roads in the information society. This paper aims to address the problem of electronic information roads, define their notion and the technical form they can take as well as present the tools developed for implementing such a system. The main objective of the proposed model is to give the traveler the capability of exploring the information space in a natural way where the information offered will remain continuously interesting. The system offers links to information in a dynamic and adaptive way. This is achieved by employing intelligent navigation techniques, which combine user profiling and meta-data. Electronic roads emphasize the presentation of multi-contextual information, i.e., information that is semantically related but of different nature at different locations and time. An electronic road is the user’s navigation path through a series of information units. Information units are the building blocks of the available cultural information content.",information system
1819,Combining deontic and action logics for collective agency,Knowledge and Information Systems, Lambèr Royakkers ,information system
1820,Behavioral Conformance of Artifact-Centric Process Models,Business Information Systems,"The use of process models in business information systems for analysis, execution, and improvement of processes assumes that the models describe reality. Conformance checking is a technique to validate how good a given process model describes recorded executions of the actual process. Recently, artifacts have been proposed as a paradigm to capture dynamic, and inter-organizational processes in a more natural way. In artifact-centric processes, several restrictions and assumptions of classical processes are dropped. This renders checking their conformance a more general problem. In this paper, we study the conformance problem of such processes. We show how to partition the problem into behavioral conformance of single artifacts and interaction conformance between artifacts, and solve behavioral conformance by a reduction to existing techniques.",information system
1821,Attribute summarization: a technique for wireless XML streaming,International Conference on Information Systems,"Recently, wireless mobile computing has been realized in the industry, where mobile clients communicate by using their handheld devices. Meanwhile, data broadcasting is an effective way for data dissemination due to its beneficial characteristics such as bandwidth efficiency, energy-efficiency, and scalability. In this paper, we propose an XML stream optimization method for time critical data, which is highly dependant to the time. To end this, we utilize structural index to integrate the elements of same path into one node. Furthermore, we propose an attribute summarization strategy to minimize the size of the XML steam by classifying attribute names and values. Experimental results show that our method outperforms the previous wireless XML broadcast methods in terms of energy and latency-efficiency. Copyright ÂŠ 2009 ACM.",information system
1822,Constructing of Mappings of Heterogeneous Information Models into the Canonical Models of Integrated Information Systems,Advances in Databases and Information Systems," The paper proposes an approach for semi-automatic construction of mappings of information models of heterogeneous information resources (such as databases, services, processes, ontologies) into the unifying, canonical models of the integrated interoperable information systems. The approach proposed is based on verifiable methods and tools of information model mapping preserving information and operations and synthesis of extensible canonical information models. An architecture of the Information Model Unifier that has been developed for supporting the methods is briefly described and illustrated by an example of mapping of a specific information model into the canonical one. A short overview of existing database schema mapping approaches and tools as well as their comparison with the approach developed in our project is provided. ",information system
1823,Horizontal Partitioning by Predicate Abstraction and Its Application to Data Warehouse Design,Advances in Databases and Information Systems,We propose a new method for horizontal partitioning of relations based on predicate abstraction by using a finite set of arbitrary predicates defined over the whole domains of relations. The method is formal and compositional: arbitrary fragments of relations can be partitioned with arbitrary number of predicates. We apply this partitioning to address the problem of finding suitable design for a relational data warehouse modeled using star schemas such that the performance of a given workload is optimized. We use a genetic algorithm to generate an appropriate solution for this optimization problem. The experimental results confirm effectiveness of our approach.,information system
1824,Spatio-Temporal Geographic Information Systems: A Causal Perspective,Advances in Databases and Information Systems,"In this paper approaches to conceptual modelling of spatio- temporal domains are identified and classified into five general cate- gories: location-based, object or feature-based, event-based, functional or behavioural and causal approaches. Much work has bee directed to- wards handling the problem from the first four view points, but less from a causal perspective. It is argued that more fundamental studies are needed of the nature of spatio-temporal objects and of their inter- actions and possible causal relationships, to support the development of spatio-temporal conceptual models. An analysis is carried out on the na- ture and type of spatio-temporal causation and a general classification is presented.",information system
1825,A systematic study on parameter correlations in large-scale duplicate document detection,Knowledge and Information Systems,"Although much work has been done on duplicate document detection (DDD) and its applications, we observe the absence of a systematic study on the performance and scalability of large-scale DDD algorithms. It is still unclear how various parameters in DDD correlate mutually, such as similarity threshold, precision/recall requirement, sampling ratio, and document size. This paper explores the correlations among several most important parameters in DDD and the impact of sampling ratio is of most interest since it heavily affects the accuracy and scalability of DDD algorithms. An empirical analysis is conducted on a million HTML documents from the TREC .GOV collection. Experimental results show that even when using the same sampling ratio, the precision of DDD varies greatly on documents with different sizes. Based on this observation, we propose an adaptive sampling strategy for DDD, which minimizes the sampling ratio with the constraint of a given precision requirement. We believe that the insights from our analysis are helpful for guiding the future large-scale DDD work. ÂŠ Springer-Verlag London Limited 2007.",information system
1826,Mining fuzzy association rules from uncertain data,Knowledge and Information Systems,"Association rule mining is an important data analysis method that can discover associations within data. There are numerous previous studies that focus on finding fuzzy association rules from precise and certain data. Unfortunately, real-world data tends to be uncertain due to human errors, instrument errors, recording errors, and so on. Therefore, a question arising immediately is how we can mine fuzzy association rules from uncertain data. To this end, this paper proposes a representation scheme to represent uncertain data. This representation is based on possibility distributions because the possibility theory establishes a close connection between the concepts of similarity and uncertainty, providing an excellent framework for handling uncertain data. Then, we develop an algorithm to mine fuzzy association rules from uncertain data represented by possibility distributions. Experimental results from the survey data show that the proposed approach can discover interesting and valuable patterns with high certainty.",information system
1827,Stones Falling in Water: When and How to Restructure a View–Based Relational Database,Advances in Databases and Information Systems,"Nowadays, one of the most important problems of software engineering continues to be the maintenance of both databases and applications. It is clear that any method that can reduce the impact that database modifications produce on application programs is valuable for software engineering processes. We have proposed such a method, by means of a database evolution architecture (MeDEA) that makes use of database views. By using views, changes in the structure of the database schema can be delayed until absolutely necessary. However, some conditions oblige modifications to be made. In this paper we present an approach to detect when the restructuring process must be realized and how to carry out this restructuring process.",information system
1828,Towards the object persistence via relational databases,Advances in Databases and Information Systems, ISBN: 3-540-76227-2 ,information system
1829,An abstract architecture for virtual organizations: The THOMAS approach,Knowledge and Information Systems,"Today, the need for architectures and computational models for large-scale open multi-agent systems is considered to be a key issue for the success of agent technology in real-world scenarios. This paper analyzes the significant unsolved problems that must be taken into account in order to develop real, open multi-agent systems. It identifies requirements and related open issues, discusses how some of these requirements have been tackled by current technologies, and explains how the THOMAS architecture is able to give support to these open issues. This paper also describes the THOMAS abstract architecture and computational model for large-scale open multi-agent systems based on a service-oriented approach that specifically addresses the design of virtual organizations. An application example for the management of a travel agency system, which demonstrates the new features of the proposal, is also presented.",information system
1830,Improving clustering by learning a bi-stochastic data similarity matrix,Knowledge and Information Systems,"An idealized clustering algorithm seeks to learn a cluster-adjacency matrix such that, if two data points belong to the same cluster, the corresponding entry would be 1; otherwise, the entry would be 0. This integer (1/0) constraint makes it difficult to find the optimal solution. We propose a relaxation on the cluster-adjacency matrix, by deriving a bi-stochastic matrix from a data similarity (e.g., kernel) matrix according to the Bregman divergence. Our general method is named the Bregmanian Bi-Stochastication (BBS) algorithm. We focus on two popular choices of the Bregman divergence: the Euclidean distance and the Kullback–Leibler (KL) divergence. Interestingly, the BBS algorithm using the KL divergence is equivalent to the Sinkhorn–Knopp (SK) algorithm for deriving bi-stochastic matrices. We show that the BBS algorithm using the Euclidean distance is closely related to the relaxed k-means clustering and can often produce noticeably superior clustering results to the SK algorithm (and other algorithms such as Normalized Cut), through extensive experiments on public data sets.",information system
1831,Machine learning and directional switching median-based filter for highly corrupted images,Knowledge and Information Systems,"In this paper, two-stage machine learning-based noise detection scheme has been proposed for identification of salt-and- pepper impulse noise which gives excellent detection results for highly corrupted images. In the first stage, a window of size (3",information system
1832,Concurrency Control in Distributed Object-Oriented Database Systems,Advances in Databases and Information Systems," Simulating distributed database systems is inherently difficult, as there are many factors that may influence the results. This includes architectural options as well as workload and data distribution. In this paper we present the DBsim simulator and some simulation results. The DBsim simulator architecture is extendible, and it is easy to change parameters and configuration. The simulation results in this paper is a comparison of performance and response times for two concurrency control algorithms, timestamp ordering and two-phase locking. The simulations have been run with different number of nodes, network types, data declustering and workloads. The results show that for a mix of small and long transactions, the throughput is significantly higher for a system with a timestamp ordering scheduler than for a system with a two-phase locking scheduler. With only short transactions, the performance of the two schedulers are almost identical. Long transactions are treated more fair by a two-phase locking scheduler, because a timestamp ordering scheduler has a very high abort rate for long transactions. ",information system
1833,"Trust and Satisfaction, Two Stepping Stones for Successful E-Commerce Relationships: A Longitudinal Exploration",Information Systems Research," Trust and satisfaction are essential ingredients for successful business relationships in business-toconsumer electronic commerce. Yet there is little research on trust and satisfaction in e-commerce that takes a longitudinal approach. Drawing on three primary bodies of literature, the theory of reasoned action, the extended valence framework, and expectation-confirmation theory, this study synthesizes a model of consumer trust and satisfaction in the context of electronic commerce. The model considers not only how consumers formulate their pre-purchase decisions, but also how they form their long-term relationships with the same website vendor by comparing their pre-purchase expectations to their actual purchase outcome. The results indicate that trust directly and indirectly affects a consumer's purchase decision in combination with perceived risk and perceived benefit, and also indicate that trust has a longer term impact on consumer e-loyalty through satisfaction. Thus, this study extends our understanding of consumer Internet transaction behavior as a three-fold (pre-purchase, purchase and, post-purchase) process, and it recognizes the crucial, multiple roles that trust plays in this process. Implications for theory and practice as well as limitations and future directions are discussed. ",information system
1834,Designing Triggers with Trigger-By-Example,Knowledge and Information Systems,"One of the obstacles that hinder database trigger systems from their wide deployment is the lack of tools that aid users in creating trigger rules. Similar to understanding and specifying database queries in SQL3, it is difficult to visualize the meaning of trigger rules. Furthermore, it is even more difficult to write trigger rules using such text-based trigger rule languages as SQL3. In this paper, we propose TBE (Trigger-By-Example) to remedy such problems in writing trigger rules visually by using QBE (Query-By-Example) ideas. TBE is a visual trigger rule composition system that helps the users understand and specify active database triggers. TBE retains benefits of QBE while extending features to support triggers. Hence, TBE is a useful tool for novice users to create simple triggers in a visual and intuitive manner. Further, since TBE is designed to hide the details of underlying trigger systems from users, it can be used as a universal trigger interface.",information system
1835,Merging local patterns using an evolutionary approach,Knowledge and Information Systems,"This paper describes a Decentralized Agent-based model for Theory Synthesis (DATS) implemented by MASETS, a Multi-Agent System for Evolutionary Theory Synthesis. The main contributions are the following: first, a method for the synthesis of a global theory from distributed local theories. Second, a conflict resolution mechanism, based on genetic algorithms, that deals with collision/contradictions in the knowledge discovered by different agents at their corresponding locations. Third, a system-level classification procedure that improves the results obtained from both: the monolithic classifier and the best local classifier. And fourth, a method for mining very large datasets that allows for divide-and-conquer mining followed by merging of discoveries. The model is validated with an experimental application run on 15 datasets. Results show that the global theory outperforms all the local theories, and the monolithic theory (obtained from mining the concatenation of all the available distributed data), in a statistically significant way.",information system
1836,Social Networks and the Diffusion of User-Generated Content: Evidence from YouTube,Information Systems Research,"This paper is motivated by the success of YouTube, which is attractive to content creators as well as corporations for its potential to rapidly disseminate digital content. The networked structure of interactions on YouTube and the tremendous variation in the success of videos posted online lends itself to an inquiry of the role of social influence. Using a unique data set of video information and user information collected from YouTube, we find that social interactions are influential not only in determining which videos become successful but also on the magnitude of that impact. We also find evidence for a number of mechanisms by which social influence is transmitted, such as (i) a preference for conformity and homophily and (ii) the role of social networks in guiding opinion formation and directing product search and discovery. Econometrically, the problem in identifying social influence is that individuals choices depend in great part upon the choices of other individuals, referred to as the reflection problem. Another problem in identification is to distinguish between social contagion and user heterogeneity in the diffusion process. Our results are in sharp contrast to earlier models of diffusion, such as the Bass model, that do not distinguish between different social processes that are responsible for the process of diffusion. Our results are robust to potential self-selection according to user tastes, temporal heterogeneity and the reflection problem. Implications for researchers and managers are discussed. ÂŠ 2012 INFORMS.",information system
1837,Clustering XML documents by patterns,Knowledge and Information Systems,"Now that the use of XML is prevalent, methods for mining semi-structured documents have become even more important. In particular, one of the areas that could greatly benefit from in-depth analysis of XML’s semi-structured nature is cluster analysis. Most of the XML clustering approaches developed so far employ pairwise similarity measures. In this paper, we study clustering algorithms, which use patterns to cluster documents without the need for pairwise comparisons. We investigate the shortcomings of existing approaches and establish a new pattern-based clustering framework called XPattern, which tries to address these shortcomings. The proposed framework consists of four steps: choosing a pattern definition, pattern mining, pattern clustering, and document assignment. The framework’s distinguishing feature is the combination of pattern clustering and document-cluster assignment, which allows to group documents according to their characteristic features rather than their direct similarity. We experimentally evaluate the proposed approach by implementing an algorithm called PathXP, which mines maximal frequent paths and groups them into profiles. PathXP was found to match, in terms of accuracy, other XML clustering approaches, while requiring less parametrization and providing easily interpretable cluster representatives. Additionally, the results of an in-depth experimental study lead to general suggestions concerning pattern-based XML clustering.",information system
1838,Indexing XML to Support Path Expressions,Advances in Databases and Information Systems," The extensible markup language (XML) is rapidly becoming a dominating technology in the area of data intensive applications. Although several implementations are already offered in commercial products, especially DBMSs, there are still open research issues related to efficiency of XML storage and retrieval. This paper introduces and analyses new index structures suitable for support of regular expressions over character data combined with path expressions in XML queries. The performance of these structures is analyzed and compared with performance of alternative approaches. In addition to usual criteria of I/O and CPU performance, a possibility to implemenent new index structure within existing DBMS engines is considered. ",information system
1839,An RMM-Based Methodology for Hypermedia Presentation Design,Advances in Databases and Information Systems,"Due to the rapid growth of the Web, there is an increasing need for methodologies that support the design of Web-based Information Systems (WIS). After investigating the application of existing hypermedia design methodologies in the context of automated hypermedia presentation design we propose a specification framework for this context. The framework considers the possibility of dynamically gathering information from a collection of structured, but also possibly heterogeneous sources (relational or object-oriented databases, XML repositories etc.). The methodology associated with the framework shows two levels of abstraction: the logical level, and the presentation level. At the logical level the application diagram captures the design of slices, thus specifying the content related grouping of data elements and their relationships. At the presentation level, the presentation diagram bridges the logical level and the actual implementation by specifying how the design of slices is translated into hypermedia mechanisms, e.g. hyperlinks.",information system
1840,An Efficient Storage Manager,Advances in Databases and Information Systems,"When dealing with large quantities of clauses, the use of persistent knowledge is inevitable, and indexing methods are essential to answer queries efficiently. We introduce PerKMan, a storage manager that uses G-trees and aims at efficient manipulation of large amount of persistent knowledge. PerKMan may be connected to Prolog systems that offer an external C language interface. As well as the fact that the storage manager allows different arguments of a predicate to share a common index dimension in a novel manner, it indexes rules and facts in the same manner. PerKMan handles compound terms efficiently and its data structures adapt their shape to large dynamic volumes of clauses, no matter what their distribution. The storage manager achieves fast clause retrieval and reasonable use of disk space.",information system
1841,Extracting Violent Events From On-Line News for Ontology Population,Business Information Systems,"This paper presents nexus, an event extraction system, developed at the Joint Research Center of the European Commission utilized for populating violent incident knowledge bases. It automatically extracts security-related facts from on-line news articles. In particular, the paper focuses on a novel bootstrapping algorithm for weakly supervised acquisition of extraction patterns from clustered news, cluster-level information fusion and pattern specification language. Finally, a preliminary evaluation of nexus on real-world data is given which revealed acceptable precision and a strong application potential.",information system
1842,Perceived Individual Collaboration Know-How Development Through Information Technology--Enabled Contextualization: Evidence from Distributed Teams,Information Systems Research," Ileverage their resources and address diverse markets. Individual members of structurally diverse distributed n today's global market environment, enterprises are increasingly turning to use of distributed teams to teams need to develop their collaboration know-how to work effectively with others on their team. The lack of face-to-face cues creates challenges in developing the collaboration know-how-challenges that can be overcome by communicating not just content, but also context. We derive a theoretical model from Te'eni's (2001) cognitive-affective model of communication to elaborate how information technology (IT) can support an individual's communication of context to develop collaboration know-how. Two hundred and sixty-three individuals working in structurally diverse distributed teams using a variety of virtual workspace technologies to support their communication needs were surveyed to test the model. Results indicate that when individuals perceive their task as nonroutine, there is a positive relationship between an individual's perceived degree of IT support for communicating context information and his collaboration know-how development. However, when individuals perceive their task as routine, partial IT support for contextualization is associated with lower levels of collaboration know-how development. This finding is attributed to individuals' misunderstanding of the conveyed context, or their struggling to utilize the context conveyed with partial IT support, making a routine task more prone to misunderstanding and leaving the user worse than if she had no IT support for contextualization. We end the paper by drawing theoretical and practical implications based on these findings. ",information system
1843,A high-performance distributed algorithm for mining association rules,Knowledge and Information Systems,"We present a new distributed association rule mining (D-ARM) algorithm that demonstrates superlinear speed-up with the number of computing nodes. The algorithm is the first D-ARM algorithm to perform a single scan over the database. As such, its performance is unmatched by any previous algorithm. Scale-up experiments over standard synthetic benchmarks demonstrate stable run time regardless of the number of computers. Theoretical analysis reveals a tighter bound on error probability than the one shown in the corresponding sequential algorithm. As a result of this tighter bound and by utilizing the combined memory of several computers, the algorithm generates far fewer candidates than comparable sequential algorithms—the same order of magnitude as the optimum.",information system
1844,Revisiting R-Tree Construction Principles,Advances in Databases and Information Systems,"Spatial indexing is a we researched field that benefited computer science with many outstanding results. Our effort in this paper can be seen as revisiting some outstanding contributions to spatial indexing, questioning some paradigms, and designing an access method with globally improved performance characteristics. In particular, we argue that dynamic R-tree construction is a typical clustering problem which can be addressed by incorporating existing clustering algorithms. As a working example, we adopt the well-known k-means algorithm. Further, we study the effect of relaxing the “two-way” split procedure and propose a “multi-way” split, which inherently is supported by clustering tech- niques. We compare our clustering approach to two prominent examples of spatial access methods, the R- and the R*-tree.",information system
1845,Multiple criteria optimization-based data mining methods and applications: a systematic survey,Knowledge and Information Systems,"Support Vector Machine, an optimization technique, is well known in the data mining community. In fact, many other optimization techniques have been effectively used in dealing with data separation and analysis. For the last 10 years, the author and his colleagues have proposed and extended a series of optimization-based classification models via Multiple Criteria Linear Programming (MCLP) and Multiple Criteria Quadratic Programming (MCQP). These methods are different from statistics, decision tree induction, and neural networks. The purpose of this paper is to review the basic concepts and frameworks of these methods and promote the research interests in the data mining community. According to the evolution of multiple criteria programming, the paper starts with the bases of MCLP. Then, it further discusses penalized MCLP, MCQP, Multiple Criteria Fuzzy Linear Programming (MCFLP), Multi-Class Multiple Criteria Programming (MCMCP), and the kernel-based Multiple Criteria Linear Program, as well as MCLP-based regression. This paper also outlines several applications of Multiple Criteria optimization-based data mining methods, such as Credit Card Risk Analysis, Classification of HIV-1 Mediated Neuronal Dendritic and Synaptic Damage, Network Intrusion Detection, Firm Bankruptcy Prediction, and VIP E-Mail Behavior Analysis. ÂŠ 2009 Springer-Verlag London Limited.",information system
1846,Scalable clustering methods for the name disambiguation problem,Knowledge and Information Systems,"When non-unique values are used as the identifier of entities, due to their homonym, confusion can occur. In particular, when (part of) “names” of entities are used as their identifier, the problem is often referred to as a name disambiguation problem, where goal is to sort out the erroneous entities due to name homonyms (e.g., If only last name is used as the identifier, one cannot distinguish “Masao Obama” from “Norio Obama”). In this paper, in particular, we study the scalability issue of the name disambiguation problem—when (1) a small number of entities with large contents or (2) a large number of entities get un-distinguishable due to homonyms. First, we carefully examine two of the state-of-the-art solutions to the name disambiguation problem and point out their limitations with respect to scalability. Then, we propose two scalable graph partitioning algorithms known as multi-level graph partitioning and multi-level graph partitioning and merging to solve the large-scale name disambiguation problem. Our claim is empirically validated via experimentation—our proposal shows orders of magnitude improvement in terms of performance while maintaining equivalent or reasonable accuracy compared to competing solutions.",information system
1847,Dynamic Integration of Classifiers in the Space of Principal Components,Advances in Databases and Information Systems,"Recent research has shown the integration of multiple classifiers to be one of the most important directions in machine learning and data mining. It was shown that, for an ensemble to be successful, it should consist of accurate and diverse base classifiers. However, it is also important that the integration procedure in the ensemble should properly utilize the ensemble diversity. In this paper, we present an algorithm for the dynamic integration of classifiers in the space of extracted features (FEDIC). It is based on the technique of dynamic integration, in which local accuracy estimates are calculated for each base classifier of an ensemble, in the neighborhood of a new instance to be processed. Generally, the whole space of original features is used to find the neighborhood of a new instance for local accuracy estimates in dynamic integration. In this paper, we propose to use feature extraction in order to cope with the curse of dimensionality in the dynamic integration of classifiers. We consider classical principal component analysis and two eigenvector-based supervised feature extraction methods that take into account class information. Experimental results show that, on some data sets, the use of FEDIC leads to significantly higher ensemble accuracies than the use of plain dynamic integration in the space of original features. As a rule, FEDIC outperforms plain dynamic integration on data sets, on which both dynamic integration works (it outperforms static integration), and considered feature extraction techniques are able to successfully extract relevant features.",information system
1848,Research Commentary: Workflow Management Issues in e-Business,Information Systems Research," T led to increasing interest in the field of workflow management. In this paper, we provide rends towards increased business process automation, e-commerce, and e-business have a perspective on the state of research in workflow management systems, and discuss possible future research directions in this area, with a particular emphasis on workflow systems in integrating interorganizational processes and enabling e-commerce solutions. (Workflow Systems; e-Business; Workflow Specification; Workflow Modeling; B-to-B Exchange; e-Hubs; e-Services; Composition) 1On leave from the University of Colorado, Boulder. ",information system
1849,A segment-based framework for modeling and mining data streams,Knowledge and Information Systems,"Data Streams have become ubiquitous in recent years because of advances in hardware technology which have enabled automated recording of large amounts of data. The primary constraint in the effective mining of streams is the large volume of data which must be processed in real time. In many cases, it is desirable to store a summary of the data stream segments in order to perform data mining tasks. Since density estimation provides a comprehensive overview of the probabilistic data distribution of a stream segment, it is a natural choice for this purpose. A direct use of density distributions can however turn out to be an inefficient storage and processing mechanism in practice. In this paper, we introduce the concept of cluster histograms, which provides an efficient way to estimate and summarize the most important data distribution profiles over different stream segments. These profiles can be constructed in a supervised or unsupervised way depending upon the nature of the underlying application. The profiles can also be used for change detection, anomaly detection, segmental nearest neighbor search, or supervised stream segment classification. Furthermore, these techniques can also be used for modeling other kinds of data such as text and categorical data. The flexibility of the tasks which can be performed from the cluster histogram framework follows from its generality in storing the historical density profile of the data stream. As a result, this method provides a holistic framework for density-based mining of data streams. We discuss and test the application of the cluster histogram framework to a variety of interesting data mining applications.",information system
1850,A note on proximity spaces and connection based mereology,Formal Ontology in Information Systems," Representation theorems for systems of regions have been of interest for some time, and various contexts have been used for this purpose: Mormann [17] has demonstrated the fruitfulness of the methods of continuous lattices to obtain a topological representation theorem for his formalisation of Whiteheadian ontological theory of space; similar results have been obtained by Roeper [20]. In this note, we prove a topological representation theorem for a connection based class of systems, using methods and tools from the theory of proximity spaces. The key novelty is a new proximity semantics for connection relations. Categories & Descriptors: I.2.4 [ARTIFICIAL INTELLIGENCE]: Knowledge Representation, Formalisms and Methods-Relation systems, representations Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. FOIS '01, October 17-19, 2001, Ogunquit, Maine, USA. Copyright 2001 ACM 1-58113-377-4/01/0010...$5.00. ",information system
1851,Clustering XML documents by structure,Advances in Databases and Information Systems,"Clustering of XML documents is an important data mining method, the aim of which is the grouping of similar XML documents. The issue of clustering XML documents by structure is being considered in this paper. Two different and independent methods of clustering XML documents by structure are being proposed. The first method represents a set of XML documents as a set of labels. The second method introduces a new representation of a set of XML documents, which is called the SuperTree. In this paper, it is suggested that the proposed methods may improve the accuracy of XML clustering by structure. Such thesis is based on the tests, the aim of which is to assess advantages of the proposals, as conducted respectively on the heterogeneous and homogenous sets of data.",information system
1852,A history-driven approach at evolving views under meta data changes,Knowledge and Information Systems,"Views over distributed information sources, such as data warehouses, rely on the stability of the schemas of underlying databases. In the event of meta data changes in the sources, such as the deletion of a table or column, such views may become undefined. Using meta data about information redundancy, views can be evolved as necessary to remain well defined after source meta data changes. Previous work in view synchronization focused only on deletions of schema elements. We now offer an approach that makes use of additions also. Our algorithm returns view definitions to previous versions by using knowledge about the history of views and meta data. This technology enables us to adapt views to temporary meta data changes by canceling out opposite changes. It also allows undo/redo operations on meta data. Last, in many cases, the resulting evolved views even have an improved information quality. In this paper, we give a formal taxonomy of schema and constraint changes and a full description of the proposed history-driven view-synchronization algorithm for this taxonomy. We also prove the history-driven view-synchronization algorithm to be correct. Our approach falls in the global-as-view category of data integration solutions but, unlike prior solutions in this category, it now also deals with changes in the information space rather than requiring source schemas to remain constant over time.",information system
1853,MotifMiner: Efficient discovery of common substructures in biochemical molecules,Knowledge and Information Systems,"Biochemical research often involves examining structural relationships in molecules since scientists strongly believe in the causal relationship between structure and function. Traditionally, researchers have identified these patterns, or motifs, manually using domain expertise. However, with the massive influx of new biochemical data and the ability to gather data for very large molecules, there is great need for techniques that automatically and efficiently identify commonly occurring structural patterns in molecules. Previous automated substructure discovery approaches have each introduced variations of similar underlying techniques and have embedded domain knowledge. While doing so improves performance for the particular domain, this complicates extensibility to other domains. Also, they do not address scalability or noise, which is critical for macromolecules such as proteins. In this paper, we present MotifMiner, a general framework for efficiently identifying common motifs in most scientific molecular datasets. The approach combines structure-based frequent-pattern discovery with search space reduction and coordinate noise handling. We describe both the framework and several algorithms as well as demonstrate the flexibility of our system by analyzing protein and drug biochemical datasets.",information system
1854,On the difficulty of automatically detecting irony: beyond a simple case of negation,Knowledge and Information Systems,"It is well known that irony is one of the most subtle devices used to, in a refined way and without a negation marker, deny what is literally said. As such, its automatic detection would represent valuable knowledge regarding tasks as diverse as sentiment analysis, information extraction, or decision making. The research described in this article is focused on identifying key values of components to represent underlying characteristics of this linguistic phenomenon. In the absence of a negation marker, we focus on representing the core of irony by means of three conceptual layers. These layers involve 8 different textual features. By representing four available data sets with these features, we try to find hints about how to deal with this unexplored task from a computational point of view. Our findings are assessed by human annotators in two strata: isolated sentences and entire documents. The results show how complex and subjective the task of automatically detecting irony could be.",information system
1855,A Hybrid Keyword Search across Peer-to-Peer Federated Databases,Advances in Databases and Information Systems," The need for Keyword search in databases is suggested both by Web integration with legacy database management system and by dynamic Web publication. However, it sacri¯ces the inherent meaning of database schema. Web search engines provide clues for resource location on the Web, but have similar semantic problems. The Semantic Web suggests an ideal solution for the semantic problem on the Web. But due to the need for sophisticated domain de¯nition and lack of uni¯ed de¯nitions, many Web pages are not part of the Semantic Web. We de¯ne a hybrid search to be a search combining semantic metadata and ",information system
1856,Towards Measuring Key Performance Indicators of Semantic Business Processes,Business Information Systems,"Business Activity Monitoring (BAM) enables continuous, real-time performance measurement of business processes based on key performance indicators (KPI). The performance information is employed by business users but prior support from IT engineers is required for setting up the BAM solution. Semantic Business Process Management (SBPM) tries to minimize the needed support from IT staff throughout the business process lifecycle. In this paper we introduce a framework for BAM as part of SBPM. We show how performance measurement related activities can be integrated into the semantic business process lifecycle. KPIs are modeled by business analysts exploiting semantic annotations of business processes. KPI models are automatically transformed to IT-level event-based models and used for real-time monitoring using reasoning technology.",information system
1857,"CaBMA: a case-based reasoning system for capturing, refining, and reusing project plans",Knowledge and Information Systems,"In this paper, we present CaBMA, a prototype of a knowledge-based system designed to assist with project planning tasks using case-based reasoning. CaBMA introduces a novel approach to project planning in that, for the first time, a knowledge layer is added on top of traditional project management software. Project management software provides editing and bookkeeping capabilities. CaBMA enhances these capabilities by automatically capturing project plans in the form of cases, refining these cases over time to avoid potential inconsistency between them, reusing these cases to generate plans for new projects, and indicating possible repairs for project plans when they derive away from existing knowledge. We will give an overview of the system, provide a detailed explanation on each component, and present an empirical study based on synthetic data.",information system
1858,Revisiting M-Tree Building Principles,Advances in Databases and Information Systems,The M-tree is a dynamic data structure designed to index metric datasets. In this paper we introduce two dynamic techniques of building the M-tree. The first one incorporates a multi-way object insertion while the second one exploits the generalized slim-down algorithm. Usage of these techniques or even combination of them significantly increases the querying performance of the M-tree. We also present comparative experimental results on large datasets showing that the new techniques outperform by far even the static bulk loading algorithm.,information system
1859,A Reduced Lattice Greedy Algorithm for Selecting Materialized Views,International Conference on Information Systems,"View selection generally deals with selecting an optimal set of beneficial views for materialization subject to constraints like space, response time, etc. The problem of view selection has been shown to be in NP. Several greedy view selection algorithms exist in literature, most of which are focused around algorithm HRU, which uses a multidimensional lattice framework to determine a good set of views to materialize. Algorithm HRU exhibits a high run time complexity. One reason for it may be the high number of re-computations of benefit values needed for selecting views for materialization. This problem has been addressed by the algorithm Reduced Lattice Greedy Algorithm (RLGA) proposed in this paper. Algorithm RLGA selects beneficial views greedily over a reduced lattice, instead of the complete lattice as in the case of HRU algorithm. The use of the reduced lattice, containing a reduced number of dependencies among views, would lead to overall reduction in the number of re-computations required for selecting materialized views. Further, it was also experimentally found that RLGA, in comparison to HRU, was able to select fairly good quality views with fewer re-computations and an improved execution time. ÂŠ 2009 Springer Berlin Heidelberg.",information system
1860,Query Rewriting Using Views in a Typed Mediator Environment,Advances in Databases and Information Systems,"Query rewriting method is proposed for the heterogeneous information integration infrastructure formed by the subject mediator environment. Local as View (LAV) approach treating schemas exported by sources as materialized views over virtual classes of the mediator is considered as the basis for the subject mediation infrastructure. In spite of significant progress of query rewriting with views, it remains unclear how to rewrite queries in the typed, object-oriented mediator environment. This paper embeds conjunctive views and queries into an advanced canonical object model of the mediator. The “selection-projection-join” (SPJ) conjunctive query semantics based on type specification calculus is introduced. The paper demonstrates how the existing query rewriting approaches can be extended to be applicable in such typed environment. The paper shows that refinement of the mediator class instance types by the source class instance types is the basic relationship required for establishing query containment in the object environment.",information system
1861,Generalizing the notion of confidence,Knowledge and Information Systems,"In this paper, we explore extending association analysis to non-traditional types of patterns and non-binary data by generalizing the notion of confidence. We begin by describing a general framework that measures the strength of the connection between two association patterns by the extent to which the strength of one association pattern provides information about the strength of another. Although this framework can serve as the basis for designing or analyzing measures of association, the focus in this paper is to use the framework as the basis for extending the traditional concept of confidence to error-tolerant itemsets (ETIs) and continuous data. To that end, we provide two examples. First, we (1) describe an approach to defining confidence for ETIs that preserves the interpretation of confidence as an estimate of a conditional probability, and (2) show how association rules based on ETIs can have better coverage (at an equivalent confidence level) than rules based on traditional itemsets. Next, we derive a confidence measure for continuous data that agrees with the standard confidence measure when applied to binary transaction data. Further analysis of this result exposes some of the important issues involved in constructing a confidence measure for continuous data.",information system
1862,An information gain-based approach for recommending useful product reviews,Knowledge and Information Systems,"Recently, many e-commerce Web sites, such as Amazon.com, provide platforms for users to review products and share their opinions, in order to help consumers make their best purchase decisions. However, the quality and the level of helpfulness of different product reviews are not disclosed to consumers unless they carefully analyze an immense number of lengthy reviews. Considering the large amount of available online product reviews, this is an impossible task for any consumer. Therefore, it is of vital importance to develop recommender systems that can evaluate online product reviews effectively to recommend the most useful ones to consumers. This paper proposes an information gain-based model to predict the helpfulness of online product reviews, with the aim of suggesting the most suitable products and vendors to consumers. Reviews are analyzed and ranked by our scoring model and reviews that help consumers better than others will be found. In addition, we also compare our model with several machine learning algorithms. Our experimental results show that our approach is effective in ranking and classifying online product reviews.",information system
1863,Impact of query intent and search context on clickthrough behavior in sponsored search,Knowledge and Information Systems,"Implicit feedback techniques may be used for query intent detection, taking advantage of user behavior to understand their interests and preferences. In sponsored search, a primary concern is the user’s interest in purchasing or utilizing a commercial service, or what is called online commercial intent. In this paper, we develop a methodology for employing the content of search engine result pages (SERPs), along with the information obtained from query strings, to study characteristics of query intent, with a particular focus on sponsored search. Our work represents a step toward the development and evaluation of an ontology for commercial search, considering queries that reference specific products, brands, and retailers. Characteristics of query categories are studied with respect to aggregated user clickthrough behavior on advertising links. We present a model for clickthrough behavior that considers the influence of such factors as the location of ads and the rank of ads, along with query category. We evaluate our work using a large corpus of clickthrough data obtained from a major commercial search engine. In addition, the impact of query intent is studied on clickthrough rate, where a baseline model and the query intent model are compared for the purpose of calculating an expected ad clickthrough rate. Our findings suggest that query-based features, along with the content of SERPs, are effective in detecting query intent. Factors such as query category, the rank of an ad, and the total number of ads displayed on a result page relate to the context of the ad, rather than its content. We demonstrate that these context-related factors can have a major influence on expected clickthrough rate, suggesting that these factors should be taken into consideration when the performance of an ad is evaluated.",information system
1864,Towards graphical models for text processing,Knowledge and Information Systems,"The rapid proliferation of the World Wide Web has increased the importance and prevalence of text as a medium for dissemination of information. A variety of text mining and management algorithms have been developed in recent years such as clustering, classification, indexing, and similarity search. Almost all these applications use the well-known vector-space model for text representation and analysis. While the vector-space model has proven itself to be an effective and efficient representation for mining purposes, it does not preserve information about the ordering of the words in the representation. In this paper, we will introduce the concept of distance graph representations of text data. Such representations preserve information about the relative ordering and distance between the words in the graphs and provide a much richer representation in terms of sentence structure of the underlying data. Recent advances in graph mining and hardware capabilities of modern computers enable us to process more complex representations of text. We will see that such an approach has clear advantages from a qualitative perspective. This approach enables knowledge discovery from text which is not possible with the use of a pure vector-space representation, because it loses much less information about the ordering of the underlying words. Furthermore, this representation does not require the development of new mining and management techniques. This is because the technique can also be converted into a structural version of the vector-space representation, which allows the use of all existing tools for text. In addition, existing techniques for graph and XML data can be directly leveraged with this new representation. Thus, a much wider spectrum of algorithms is available for processing this representation. We will apply this technique to a variety of mining and management applications and show its advantages and richness in exploring the structure of the underlying text documents.",information system
1865,Greedy Column Subset Selection for Large-scale Data Sets,Knowledge and Information Systems,"In today’s information systems, the availability of massive amounts of data necessitates the development of fast and accurate algorithms to summarize these data and represent them in a succinct format. One crucial problem in big data analytics is the selection of representative instances from large and massively distributed data, which is formally known as the Column Subset Selection problem. The solution to this problem enables data analysts to understand the insights of the data and explore its hidden structure. The selected instances can also be used for data preprocessing tasks such as learning a low-dimensional embedding of the data points or computing a low-rank approximation of the corresponding matrix. This paper presents a fast and accurate greedy algorithm for large-scale column subset selection. The algorithm minimizes an objective function, which measures the reconstruction error of the data matrix based on the subset of selected columns. The paper first presents a centralized greedy algorithm for column subset selection, which depends on a novel recursive formula for calculating the reconstruction error of the data matrix. The paper then presents a MapReduce algorithm, which selects a few representative columns from a matrix whose columns are massively distributed across several commodity machines. The algorithm first learns a concise representation of all columns using random projection, and it then solves a generalized column subset selection problem at each machine in which a subset of columns are selected from the sub-matrix on that machine such that the reconstruction error of the concise representation is minimized. The paper demonstrates the effectiveness and efficiency of the proposed algorithm through an empirical evaluation on benchmark data sets.",information system
1866,Ontology alignment design patterns,Knowledge and Information Systems,"Interoperability between heterogeneous ontological descriptions can be performed through ontology mediation techniques. At the heart of ontology mediation lies the alignment: a specification of correspondences between ontology entities. Ontology matching can bring some automation but are limited to finding simple correspondences. Design patterns have proven themselves useful to capture experience in design problems. In this article, we introduce ontology alignment patterns as reusable templates of recurring correspondences. Based on a detailed analysis of frequent ontology mismatches, we develop a library of common patterns. Ontology alignment patterns can be used to refine correspondences, either by the alignment designer or via pattern detection algorithms. We distinguish three levels of abstraction for ontology alignment representation, going from executable transformation rules, to concrete correspondences between two ontologies, to ontology alignment patterns at the third level. We express patterns using an ontology alignment representation language, making them ready to use in practical mediation tasks. We extract mismatches from vocabularies associated with data sets published as linked open data, and we evaluate the ability of correspondence patterns to provide proper alignments for these mismatches. Finally, we describe an application of ontology alignment patterns for an ontology transformation service.",information system
1867,Beyond Databases: An Asset Language for Conceptual Content Management,Advances in Databases and Information Systems,"Innovative information systems such as content management systems and information brokers are designed to organize a complex mixture of media content – texts, images, maps, videos, ... – and to present it through domain specific conceptual models, for example, on sports, stock exchange, or art history. In this paper we extend the currently dominating computational container models into a coherent content-concept model intended to capture more of the meaning – thereby improving the value – of content. Integrated content-concept views on entities are modeled using the notion of assets, and the rationale of our asset language is based on arguments for open language expressiveness [19] and dynamic system responsiveness [8]. In addition, we discuss our experiences with a component-based implementation technology which substantially simplifies the implementation of open and dynamic asset management systems.",information system
1868,The Basic Tools of Formal Ontology,Formal Ontology in Information Systems," The term 'formal ontology' was first used by the philosopher Edmund Husserl in his Logical investigations to signify the study of those formal structures and relations - above all relations of part and whole - which are exemplified in the subject-matters of the different material sciences. We follow Husserl in presenting the basic concepts of formal ontology as falling into three groups: the theory of part and whole, the theory of dependence, and the theory of boundary, continuity and contact. These basic concepts are presented in relation to the problem of providing an account of the formal ontology of the mesoscopic realm of everyday experience, and specifically of providing an account of the concept of individual substance. ",information system
1869,Semantic trajectory based event detection and event pattern mining,Knowledge and Information Systems,"Video event detection is an effective way to automatically understand the semantic content of the video. However, due to the mismatch between low-level visual features and high-level semantics, the research of video event detection encounters a number of challenges, such as how to extract the suitable information from video, how to represent the event, how to build up reasoning mechanism to infer the event according to video information. In this paper, we propose a novel event detection method. The method detects the video event based on the semantic trajectory, which is a high-level semantic description of the moving object’s trajectory in the video. The proposed method consists of three phases to transform low-level visual features to middle-level raw trajectory information and then to high-level semantic trajectory information. Event reasoning is then carried out with the assistance of semantic trajectory information and background knowledge. Additionally, to release the users’ burden in manual event definition, a method is further proposed to automatically discover the event-related semantic trajectory pattern from the sample semantic trajectories. Furthermore, in order to effectively use the discovered semantic trajectory patterns, the associative classification-based event detection framework is adopted to discover the possibly occurred event. Empirical studies show our methods can effectively and efficiently detect video events.",information system
1870,Towards the Semantic Web - An Approach to Reverse Engineering of Relational Databases to Ontologies,Advances in Databases and Information Systems," We propose a novel approach to reverse engineering of relational databases to ontologies. Our approach incorporates two main sources of semantics: HTML pages and a relational schema. This incorporation results in that: (1) only minimal information about a relational database is required to build an ontology; and (2) the ontology is no longer “impaired” by bad-database design, and by optimization and de-normalization of the relational schema. Our approach can be used for migrating HTML pages (especially those that are dynamically generated from a relational database) to the ontology-based Semantic Web. The main reason for this migration is to make the relational database information that is available on the Web machine-processable. ",information system
1871,Term Extraction and Mining of Term Relations from Unrestricted Texts in the Financial Domain,Business Information Systems,  ,information system
1872,Consensus-based evaluation framework for distributed information retrieval systems,Knowledge and Information Systems,"Multi-agent systems have been attacking the challenges of information retrieval tasks on distributed environment. In this paper, we propose a consensus choice selection method based framework to evaluate the performance of cooperative information retrieval tasks of the multiple agents. Thereby, two well-known measurements, precision and recall, are extended to handle consensual closeness (i.e., local and global consensus) between the sets of retrieved results. We show that in a motivating example the proposed criteria are prone to solve the rigidity problem of classical precision and recall. More importantly, the retrieved results can be ranked with respect to the consensual score, and the ranking mechanism has been verified to be more reasonable.",information system
1873,Towards a model for the multidimensional analysis of field data,Advances in Databases and Information Systems,"Integration of spatial data into multidimensional models leads to the concept of Spatial OLAP (SOLAP). Usually, SOLAP models exploit discrete spatial data. Few works integrate continuous field data into dimensions and measures. In this paper, we provide a multidimensional model that supports measures and dimension as continuous field data, independently of their implementation.",information system
1874,Software Versioning and Quality Degradation? An Exploratory Study of the Evidence,International Conference on Information Systems," We present a framework for measuring software quality using pricing and demand data, and empirical estimates that quantify the extent of quality degradation associated with software versioning. Using a 7-month, 108-product panel of software sales from Amazon.com, we document the extent to which quality varies across different software versions, estimating quality degradation that ranges from as little as 8% to as much as 56% below that of the corresponding flagship version. Consistent with prescriptions from the theory of vertical differentiation, we also find that an increase in the total number of versions is associated with an increase in the difference in quality between the highest and lowest quality versions, and a decrease in the quality difference between ""neighboring"" versions. We compare our estimates with those derived from two sets of subjective measures of quality, based on CNET editorial ratings and Amazon.com user reviews, and discuss competing interpretations of the significant differences that emerge from this comparison. As the first empirical study of software versioning that is based on both subjective and econometrically estimated measures of quality, this paper provides a framework for testing a wide variety of results in IS that are based on related models of vertical differentiation, and its findings have important implications for studies that treat web-based user ratings as cardinal data. ",information system
1875,Exponential family tensor factorization: an online extension and applications,Knowledge and Information Systems,"In this paper, we propose a new probabilistic model of heterogeneously attributed multi-dimensional arrays. The model can manage heterogeneity by employing individual exponential family distributions for each attribute of the tensor array. Entries of the tensor are connected by latent variables and share information across the different attributes through the latent variables. The assumption of heterogeneity makes a Bayesian inference intractable, and we cast the EM algorithm approximated by the Laplace method and Gaussian process. We also extended the proposal algorithm for online learning. We apply our method to missing-values prediction and anomaly detection problems and show that our method outperforms conventional approaches that do not consider heterogeneity.",information system
1876,Combining Efficient XML Compression with Query Processing,Advances in Databases and Information Systems,"This paper describes a new XML compression scheme that offers both high compression ratios and short query response time. Its core is a fully reversible transform featuring substitution of every word in an XML document using a semi-dynamic dictionary, effective encoding of dictionary indices, as well as numbers, dates and times found in the document, and grouping data within the same structural context in individual containers. The results of conducted tests show that the proposed scheme attains compression ratios rivaling the best available algorithms, and fast compression, decompression, and query processing.",information system
1877,Security-aware intermediate data placement strategy in scientific cloud workflows,Knowledge and Information Systems,"Massive computation power and storage capacity of cloud computing systems allow scientists to deploy data-intensive applications without the infrastructure investment, where large application datasets can be stored in the cloud. Based on the pay-as-you-go model, data placement strategies have been developed to cost-effectively store large volumes of generated datasets in the scientific cloud workflows. As promising as it is, this paradigm also introduces many new challenges for data security when the users outsource sensitive data for sharing on the cloud servers, which are not within the same trusted domain as the data owners. This challenge is further complicated by the security constraints on the potential sensitive data for the scientific workflows in the cloud. To effectively address this problem, we propose a security-aware intermediate data placement strategy. First, we build a security overhead model to reasonably measure the security overheads incurred by the sensitive data. Second, we develop a data placement strategy to dynamically place the intermediate data for the scientific workflows. Finally, our experimental results show that our strategy can effectively improve the intermediate data security while ensuring the data transfer time during the execution of scientific workflows.",information system
1878,"POTMiner: mining ordered, unordered, and partially-ordered trees",Knowledge and Information Systems," Non-linear data structures are becoming more and more common in data mining problems. Trees, in particular, are amenable to efficient mining techniques. In this paper, we introduce a scalable and parallelizable algorithm to mine partially-ordered trees. Our algorithm, POTMiner, is able to identify both induced and embedded subtrees in such trees. As special cases, it can also handle both completely ordered and completely unordered trees. ",information system
1879,Optimization of Storage Structures of Complex Types in Object-Relational Database Systems,Advances in Databases and Information Systems,"Modern relational DBMS use more and more object-relational features to store complex objects with nested structures and collection-valued attributes. Thus evolving towards object-relational database management systems. This paper presents results of the project “Object-Relational Database Features and Extensions: Model and Physical Aspects"" of the Jena Database Group. It introduces an approach to optimize the physical representation of complex types with respect to the actual workload, mainly based on two concepts: First, different variants of physical representation of complex objects can be described and controlled by a new Physical Representation Definition Language (PRDL). Second a method based on workload capturing is suggested that allows to detect the need for physical restructuring, to evaluate alternative storage structures with respect to better performance and lower execution costs and to get well-founded improvement estimations.",information system
1880,"An adaptive, memory-efficient and fast algorithm for Huffman decoding and its implementation",International Conference on Information Systems,This paper proposes an adaptive algorithm for Huffman decoding and its implementation based on Single-side Growing Huffman Coding approach which provides a memory efficient and highspeed decoding algorithm. The search time of the proposed algorithm for finding a symbol is [CL/4] where CL is the code length of the corresponding symbol. This algorithm is applied for MP3 decoding. The result shows that the proposed algorithm is applicable to all applications using Huffman decoding. Copyright ÂŠ 2009 ACM.,information system
1881,Efficient algorithms for discovering high utility user behavior patterns in mobile commerce environments,Knowledge and Information Systems,"Mining user behavior patterns in mobile environments is an emerging topic in data mining fields with wide applications. By integrating moving paths with purchasing transactions, one can find the sequential purchasing patterns with the moving paths, which are called mobile sequential patterns of the mobile users. Mobile sequential patterns can be applied not only for planning mobile commerce environments but also for analyzing and managing online shopping websites. However, unit profits and purchased numbers of the items are not considered in traditional framework of mobile sequential pattern mining. Thus, the patterns with high utility (i.e., profit here) cannot be found. In view of this, we aim at integrating mobile data mining with utility mining for finding high-utility mobile sequential patterns in this study. Two types of algorithms, namely level-wise and tree-based methods, are proposed for mining high-utility mobile sequential patterns. A series of analyses and comparisons on the performance of the two different types of algorithms are conducted through experimental evaluations. The results show that the proposed algorithms outperform the state-of-the-art mobile sequential pattern algorithms and that the tree-based algorithms deliver better performance than the level-wise ones under various conditions.",information system
1882,Applying language modeling to session identification from database trace logs,Knowledge and Information Systems,"A database session is a sequence of requests presented to the database system by a user or an application to achieve a certain task. Session identification is an important step in discovering useful patterns from database trace logs. The discovered patterns can be used to improve the performance of database systems by prefetching predicted queries, rewriting the current query or conducting effective cache replacement. In this paper, we present an application of a new session identification method based on statistical language modeling to database trace logs. Several problems of the language modeling based method are revealed in the application, which include how to select values for the parameters of the language model, how to evaluate the accuracy of the session identification result and how to learn a language model without well-labeled training data. All of these issues are important in the successful application of the language modeling based method for session identification. We propose solutions to these open issues. In particular, new methods for determining an entropy threshold and the order of the language model are proposed. New performance measures are presented to better evaluate the accuracy of the identified sessions. Furthermore, three types of learning methods, namely, learning from labeled data, learning from semi-labeled data and learning from unlabeled data, are introduced to learn language models from different types of training data. Finally, we report experimental results that show the effectiveness of the language model based method for identifying sessions from the trace logs of an OLTP database application and the TPC-C Benchmark.",information system
1883,Spectral clustering in multi-agent systems,Knowledge and Information Systems,"We examine the application of spectral clustering for breaking up the behavior of a multi-agent system in space and time into smaller, independent elements. We propose clustering observations of individual entities in order to identify significant changes in the parameter space (like spatial position) and detect temporal alterations of behavior within the same framework. Available knowledge of important interactions (events) between entities is also considered. We describe a novel algorithm utilizing iterative subdivisions where clusters are pre-processed at each step to counter spatial scaling, rotation, replay speed, and varying sampling frequency. A method is presented to balance spatial and temporal segmentation based on the expected group size, and a validity measure is introduced to determine the optimal number of clusters. We demonstrate our results by analyzing the outcomes of computer games and compare our algorithm to K-means and traditional spectral clustering.",information system
1884,A Paradigmatic Analysis Contrasting Information Systems Development Approaches and Methodologies,Information Systems Research," T formation systems development (ISD) approaches: the interactionist approach, the speech his paper analyses the fundamental philosophical assumptions of five “contrasting” inact-based approach, the soft systems methodology approach, the trade unionist approach, and the professional work practice approach. These five approaches are selected for analysis because they illustrate alternative philosophical assumptions from the dominant “orthodoxy” identified in the research literature. The paper also proposes a distinction between “approach” and “methodology.” The analysis of the five approaches is organized around four basic questions: What is the assumed nature of an information system (ontology)? What is human knowledge and how can it be obtained (epistemology)? What are the preferred research methods for continuing the improvement of each approach (research methodology)? and what are the implied values of information system research (ethics)? Each of these questions is explored from the internal perspective of the particular ISD approach. The paper addresses these questions through a conceptual structure which is based on a paradigmatic framework for analyzing ISD approaches. (Paradigms; Paradigmatic Analysis; Information Systems Development; Information Systems Development Methodologies and Approaches; Assumption Analysis) ",information system
1885,"Honey, I Shrunk the Cube",Advances in Databases and Information Systems,"Information flooding may occur during an OLAP session when the user drills down her cube up to a very fine-grained level, because the huge number of facts returned makes it very hard to analyze them using a pivot table. To overcome this problem we propose a novel OLAP operation, called shrink, aimed at balancing data precision with data size in cube visualization via pivot tables. The shrink operation fuses slices of similar data and replaces them with a single representative slice, respecting the constraints posed by dimension hierarchies, until the result is smaller than a given threshold. We present a greedy agglomerative clustering algorithm that at each step fuses the two slices yielding the minimum increase in the total approximation, and discuss some experimental results that show its efficiency and effectiveness.",information system
1886,Static Analysis of Structural Recursion in Semistructured Databases and Its Consequences,Advances in Databases and Information Systems,"Structural recursion is a graph traversing and restructuring operation in UnQL [7], [8], a query language for semistructured data. In this paper we consider satisfiability questions mainly in the presence of schema graphs [2], [9], which are used for describing the structure of semistructured data. We introduce a new kind of simulation between schema graphs, with which the relationships can be represented in more subtle ways. By means of operational graphs we also develop a new way for defining the semantics of structural recursions. Our results give us algorithms for checking whether a given query will satisfy the restrictions imposed by schema graphs and techniques with which these can be involved in queries. Query optimizing methods are also developed.",information system
1887,Pushing Predicates into Recursive SQL Common Table Expressions,Advances in Databases and Information Systems,"A recursive SQL-1999 query consists of a recursive CTE (Common Table Expression) and a query which uses it. If such a recursive query is used in a context of a selection predicate, this predicate can possibly be pushed into the CTE thus limiting the breadth and/or depth of the recursive search. This can happen e.g. after the definition of a view containing recursive query has been expanded in place. In this paper we propose a method of pushing predicates and other query operators into a CTE. This allows executing the query with smaller temporary data structures, since query operators external w.r.t. the CTE can be computed on the fly together with the CTE. Our method is inspired on the deforestation (a.k.a. program fusion) successfully applied in functional programming languages.",information system
1888,Intelligent systems in the automotive industry: applications and trends,Knowledge and Information Systems,"There is a common misconception that the automobile industry is slow to adapt new technologies, such as artificial intelligence (AI) and soft computing. The reality is that many new technologies are deployed and brought to the public through the vehicles that they drive. This paper provides an overview and a sampling of many of the ways that the automotive industry has utilized AI, soft computing and other intelligent system technologies in such diverse domains like manufacturing, diagnostics, on-board systems, warranty analysis and design.",information system
1889,Real-time classification of variable length multi-attribute motions,Knowledge and Information Systems,"Multi-attribute motion data can be generated in many applications/ devices, such as motion capture devices and animations. It can have dozens of attributes, thousands of rows, and even similar motions can have different durations and different speeds at corresponding parts. There are no row-to-row correspondences between data matrices of two motions. To be classified and recognized, multi-attribute motion data of different lengths are reduced to feature vectors by using the properties of singular value decomposition (SVD) of motion data. The reduced feature vectors of similar motions are close to each other, while reduced feature vectors are different from each other if their motions are different. By applying support vector machines (SVM) to the feature vectors, we efficiently classify and recognize real-world multi-attribute motion data. With our data set of more than 300 motions with different lengths and variations, SVM outperforms classification by related similarity measures, in terms of accuracy and CPU time. The performance of our approach shows its feasibility of real-time applications to real-world data.",information system
1890,Norm negotiation in online multi-player games,Knowledge and Information Systems,"In this paper, we introduce an agent communication protocol and speech acts for norm negotiation. The protocol creates individual or contractual obligations to fulfill goals of the agents based on the so-called social delegation cycle. First, agents communicate their individual goals and powers. Second, they propose social goals which can be accepted or rejected by other agents. Third, they propose obligations and sanctions to achieve the social goal, which can again be accepted or rejected. Finally, the agents accept the new norm by indicating which of their communicated individual goals the norm achieves. The semantics of the speech acts is based on a commitment to public mental attitudes. The norm negotiation model is illustrated by an example of norm negotiation in multi-player online gaming.",information system
1891,A graph-based approach for extracting terminological properties from information sources with heterogeneous formats,Knowledge and Information Systems,"The problem of handling both the integration and the cooperation of a large number of information sources characterised by heterogeneous representation formats is a challenging issue. In this context, a central role can be played by the knowledge about the semantic relationships holding between concepts belonging to different information sources (intersource properties). In this paper, we propose a semiautomatic approach for extracting two kinds of intersource properties, namely synonymies and homonymies, from heterogeneous information sources. In order to carry out the extraction task, we introduce both a conceptual model, for representing involved sources, and a metrics, for measuring the strength of the semantic relationships holding among concepts represented within the same source.",information system
1892,Voting techniques for expert search,Knowledge and Information Systems," In an expert search task, the users' need is to identify people who have relevant expertise to a topic of interest. An expert search system predicts and ranks the expertise of a set of candidate persons with respect to the users' query. In this paper, we propose a novel approach for predicting and ranking candidate expertise with respect to a query, called the Voting Model for Expert Search. In the Voting Model, we see the problem of ranking experts as a voting problem. We model the voting problem using 12 various voting techniques, which are inspired from the data fusion field. We investigate the effectiveness of the Voting Model and the associated voting techniques across a range of document weighting models, in the context of the TREC 2005 and TREC 2006 Enterprise tracks. The evaluation results show that the voting paradigm is very effective, without using any query or collection-specific heuristics. Moreover, we show that improving the quality of the underlying document representation can significantly improve the retrieval performance of the voting techniques on an expert search task. In particular, we demonstrate that applying field-based weighting models improves the ranking of candidates. Finally, we demonstrate that the relative performance of the voting techniques for the proposed approach is stable on a given task regardless of the used weighting models, suggesting that some of the proposed voting techniques will always perform better than other voting techniques. ",information system
1893,Keyword search in relational databases,Knowledge and Information Systems,"This paper surveys research on enabling keyword search in relational databases. We present fundamental characteristics and discuss research dimensions, including data representation, ranking, efficient processing, query representation, and result presentation. Various approaches for developing the search system are described and compared within a common framework. We discuss the evolution of new research strategies to resolve the issues associated with probabilistic models, efficient top-k query processing, and schema analysis in relational databases.",information system
1894,Reasoning About Web Information Systems Using Story Algebras,Advances in Databases and Information Systems,"As web information systems (WIS) tend to become large, it becomes decisive that the underlying application story is well designed. Such stories can be expressed by a process algebra. In this paper we show that such WIS-oriented process algebras lead to many-sorted Kleene algebras with tests, where the sorts correspond to scenes in the story space. As Kleene algebras with tests subsume propositional Hoare logic, they are an ideal candidate for reasoning about the story space. We show two applications for this: (1) the personalisation of the story space to the preferences of a particular user, and (2) the satisfaction of particular information needs of a WIS user.",information system
1895,Automatic Derivation of Service Candidates from Business Process Model Repositories,Business Information Systems,"Although several approaches for service identification have been defined in research and practice, there is a notable lack of automatic analysis techniques. In this paper we take the integrated approach by Kohlborn et al. as a starting point, and combine different analysis techniques in a novel way. Our contribution is an automated approach for the identification and detailing of service candidates. Its output is meant to provide a transparent basis for making decisions about which services to implement with which priority. The approach has been implemented and evaluated for an industry collection of process models.",information system
1896,Learning accurate and concise naïve Bayes classifiers from attribute value taxonomies and data,Knowledge and Information Systems," In many application domains, there is a need for learning algorithms that can effectively exploit attribute value taxonomies (AVT)-hierarchical groupings of attribute values-to learn compact, comprehensible and accurate classifiers from data-including data that are partially specified. This paper describes AVTNBL, a natural generalization of the na¨ıve Bayes learner (NBL), for learning classifiers from AVT and data. Our experimental results show that AVT-NBL is able to generate classifiers that are substantially more compact and more accurate than those produced by NBL on a broad range of data sets with different percentages of partially specified values. We also show that AVT-NBL is more efficient in its use of training data: AVT-NBL produces classifiers that outperform those produced by NBL using substantially fewer training examples. ",information system
1897,Optimizing Pattern Queries for Web Access Logs,Advances in Databases and Information Systems,"Web access logs, usually stored in relational databases, are commonly used for various data mining and data analysis tasks. The tasks typically consist in searching the web access logs for event sequences that support a given sequential pattern. For large data volumes, this type of searching is extremely time consuming and is not well optimized by traditional indexing techniques. In this paper we present a new index structure to optimize pattern search queries on web access logs. We focus on its physical structure, maintenance and performance issues.",information system
1898,A markov prediction model for data-driven semi-structured business processes,Knowledge and Information Systems,"In semi-structured case-oriented business processes, the sequence of process steps is determined by case workers based on available document content associated with a case. Transitions between process execution steps are therefore case specific and depend on independent judgment of case workers. In this paper, we propose an instance-specific probabilistic process model (PPM) whose transition probabilities are customized to the semi-structured business process instance it represents. An instance-specific PPM serves as a powerful representation to predict the likelihood of different outcomes. We also show that certain instance-specific PPMs can be transformed into a Markov chain under some non-restrictive assumptions. For instance-specific PPMs that contain parallel execution of tasks, we provide an algorithm to map them to an extended space Markov chain. This way existing Markov techniques can be leveraged to make predictions about the likelihood of executing future tasks. Predictions provided by our technique could generate early alerts for case workers about the likelihood of important or undesired outcomes in an executing case instance. We have implemented and validated our approach on a simulated automobile insurance claims handling semi-structured business process. Results indicate that an instance-specific PPM provides more accurate predictions than other methods such as conditional probability. We also show that as more document data become available, the prediction accuracy of an instance-specific PPM increases.",information system
1899,Adaptive learning of dynamic Bayesian networks with changing structures by detecting geometric structures of time series,Knowledge and Information Systems," It reads “If F >= F ” in Sect. 3.3 in listing (4) at the end of the section. This is wrong, it ",information system
1900,Large-scale k-means clustering with user-centric privacy-preservation,Knowledge and Information Systems,"A k-means clustering with a new privacy-preserving concept, user-centric privacy preservation, is presented. In this framework, users can conduct data mining using their private information by storing them in their local storage. After the computation, they obtain only the mining result without disclosing private information to others. In most cases, the number of parties that can join conventional privacy-preserving data mining has been assumed to be only two. In our framework, we assume large numbers of parties join the protocol; therefore, not only scalability but also asynchronism and fault-tolerance is important. Considering this, we propose a k-mean algorithm combined with a decentralized cryptographic protocol and a gossip-based protocol. The computational complexity is O(log n) with respect to the number of parties n, and experimental results show that our protocol is scalable even with one million parties. © 2009 Springer-Verlag London Limited.",information system
1901,Multiple-camera people localization in an indoor environment,Knowledge and Information Systems,"With the rapid proliferation of video cameras in public places, the ability to identify and track people and other objects creates tremendous opportunities for business and security applications. This paper presents the Multiple Camera Indoor Surveillance project which is devoted to using multiple cameras, agent-based technology and knowledge-based techniques to identify and track people and summarize their activities. We also describe a people localization system, which identifies and localizes people in an indoor environment. The system uses low-level color features – a color histogram and average vertical color – for building people models and the Bayesian decision-making approach for people localization. The results of a pilot experiment that used 32 h of data (4 days × 8 h) showed the average recall and precision values of 68 and 59% respectively. Augmenting the system with domain knowledge, such as location of working places in cubicles, doors and passages, increased the average recall to 87% and precision to 73%.",information system
1902,Internet Exchanges for Used Books: An Empirical Analysis of Product Cannibalization and Welfare Impact,Information Systems Research," Idramatically wider selection, lower search costs, and lower prices than their brick-and-mortar counterparts nformation systems and the Internet have facilitated the creation of used-product markets that feature a do. The increased viability of these used-product markets has caused concern among content creators and distributors, notably the Association of American Publishers and Author's Guild, who believe that used-product markets will significantly cannibalize new product sales. This proposition, while theoretically possible, is based on speculation as opposed to empirical evidence. In this paper, we empirically analyze the degree to which used products cannibalize new-product sales for books-one of the most prominent used-product categories sold online. To do this, we use a unique data set collected from Amazon.com's new and used book marketplaces to measure the degree to which used products cannibalize new-product sales. We then use these estimates to measure the resulting first-order changes in publisher welfare and consumer surplus. Our analysis suggests that used books are poor substitutes for new books for most of Amazon's customers. The cross-price elasticity of new-book demand with respect to used-book prices is only 0.088. As a result, only 16% of used-book sales at Amazon cannibalize new-book purchases. The remaining 84% of used-book sales apparently would not have occurred at Amazon's new-book prices. Further, our estimates suggest that this increase in book readership from Amazon's used-book marketplace increases consumer surplus by approximately $67.21 million annually. This increase in consumer surplus, together with an estimated $45.05 million loss in publisher welfare and a $65.76 million increase in Amazon's profits, leads to an increase in total welfare to society of approximately $87.92 million annually from the introduction of used-book markets at Amazon.com. ",information system
1903,Using step-wise refinement to build a flexible lightweight storage manager,Advances in Databases and Information Systems,"In recent years the deployment of embedded systems has increased dramatically, e.g. in the domains of sensor networks or ubiquitous computing. At the same time the amount of data that have to be managed by embedded systems is growing rapidly. For this reason an adequate data management support is urgently needed. Current database technologies are not able to cope with the requirements specific to embedded environments. Especially the extreme resource constraints and the diversity of hardware plattforms and operating systems are challenging. To overcome this tension we argue that embedded database functionality has to be tailored to the application scenario as well as to the target platform. This reduces the resource consumption and customizes the data management to the characteristices of the plattform and the application scenarion. We show that component techniques and feature-oriented programming help to face the mentioned limitations without focusing on special-purpose software. We present the design and the implementation of a database storage manager family. We discuss how feature-oriented domain analysis and feature-oriented programming help to do this task. Our evaluation criteria are the number of features and the flexibility to combine these features in different valid variants.",information system
1904,An Index Structure for Data Mining and Clustering,Knowledge and Information Systems," In this paper we present an index structure, called M etricM ap, that takes a set of objects and a distance metric and then maps those objects to a k-dimensional space in such a way that the distances among objects are approximately preserved. The index structure is a useful tool for clustering and visualization in data intensive applications, because it replaces expensive distance calculations by sum-of-square calculations. This can make clustering in large databases with expensive distance metrics practical. We compare the index structure with another data mining index structure, F astM ap, recently proposed by Faloutsos and Lin, according to two criteria: relative error and clustering accuracy. For relative error, we show that (i) F astM ap gives a lower relative error than M etricM ap for Euclidean distances, (ii) M etricM ap gives a lower relative error than F astM ap for non-Euclidean distances (i.e., general distance metrics), and (iii) combining the two reduces the error yet further. A similar result is obtained when comparing the accuracy of clustering. These results hold for different data sizes. The main qualitative conclusion is that these two index structures capture complementary information about distance metrics and therefore This work was supported in part by the National Science Foundation under grant numbers IRI-9531548 and IRI-9531554, and by the Natural Sciences and Engineering Research Council of Canada under grant number OGP0046373. A preliminary version of this paper was presented in the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining held in San Diego, California in August 1999. Part of the work of this author was done while visiting Courant Institute of Mathematical Sciences, New York University. ",information system
1905,Anonymizing graphs: measuring quality for clustering,Knowledge and Information Systems,"Anonymization of graph-based data is a problem, which has been widely studied last years, and several anonymization methods have been developed. Information loss measures have been carried out to evaluate the noise introduced in the anonymized data. Generic information loss measures ignore the intended anonymized data use. When data has to be released to third-parties, and there is no control on what kind of analyses users could do, these measures are the standard ones. In this paper we study different generic information loss measures for graphs comparing such measures to the cluster-specific ones. We want to evaluate whether the generic information loss measures are indicative of the usefulness of the data for subsequent data mining processes.",information system
1906,Modified algorithms for synthesizing high-frequency rules from different data sources,Knowledge and Information Systems,"Because of the rapid growth in information and communication technologies, a company’s data may be spread over several continents. For an effective decision-making process, knowledge workers need data, which may be geographically spread in different locations. In such circumstances, multi-database mining plays a major role in the process of extracting knowledge from different data sources. In this paper, we have proposed a new methodology for synthesizing high-frequency rules from different data sources, where data source weight has been calculated on the basis of their transaction population. We have also proposed a new method for calculating global confidence. Our goal in synthesizing local patterns to obtain global patterns is that, the support and confidence of synthesized patterns must be very nearly same if all the databases are integrated and mono-mining has been done. Experiments conducted clearly establish that the proposed method of synthesizing high-frequency rules fairly meets the stipulation.",information system
1907,A CONVERSION PROCESS FROM FLICKR TAGS TO RDF DESCRIPTIONS,Business Information Systems," The recent evolution of the Web, now designated by the term Web 2.0, has seen the appearance of a huge number of resources created and annotated by users. However the annotations consist only in simple tags that are gathered in unstructured sets called folksonomies. The use of more complex languages to annotate resources and to define semantics according to the vision of the Semantic Web, would improve the understanding by machines and programs, like search engines, of what is on the Web. Indeed tags expressivity is very low compared to the representation standards of the Semantic Web, like RDF and OWL. But users appear to be still reluctant to annotate resources with RDF, and it should be recognized that Semantic Web, contrary to Web 2.0, is still not a reality of today's Web. One way to take advantage of Semantic Web capabilities right now, without waiting for a change of the annotation usages, would be to be able to generate RDF annotations from tags. As a first step toward this direction, this paper presents a tentative to automatically convert a set of tags into a RDF description in the context of photos on Flickr. Such a method exploits some specificity of tags used on Flickr, some basic natural language processing tools and some semantic resources, in order to relate semantically tags describing a given photo and build a pertinent RDF annotation for this photo. ",information system
1908,"Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study",Knowledge and Information Systems,"Semi-supervised classification methods are suitable tools to tackle training sets with large amounts of unlabeled data and a small quantity of labeled data. This problem has been addressed by several approaches with different assumptions about the characteristics of the input data. Among them, self-labeled techniques follow an iterative procedure, aiming to obtain an enlarged labeled data set, in which they accept that their own predictions tend to be correct. In this paper, we provide a survey of self-labeled methods for semi-supervised classification. From a theoretical point of view, we propose a taxonomy based on the main characteristics presented in them. Empirically, we conduct an exhaustive study that involves a large number of data sets, with different ratios of labeled data, aiming to measure their performance in terms of transductive and inductive classification capabilities. The results are contrasted with nonparametric statistical tests. Note is then taken of which self-labeled models are the best-performing ones. Moreover, a semi-supervised learning module has been developed for the Knowledge Extraction based on Evolutionary Learning software, integrating analyzed methods and data sets.",information system
1909,Processing Sliding Window Join Aggregate in Continuous Queries over Data Streams,Advances in Databases and Information Systems,"Processing continuous queries over unbounded streams require unbounded memory. A common solution to this issue is to restrict the range of continuous queries into a sliding window that contains the most recent data of data streams. Sliding window join aggregates are often-used queries in data stream applications. The processing method to date is to construct steaming binary operator tree and pipeline execute. This method consumes a great deal of memory in storing the sliding window join results, therefore it isn’t suitable for stream query processing. To handle this issue, we present a set of novel sliding window join aggregate operators and corresponding realized algorithms, which achieve memory-saving and efficient performance. Because the performances of proposed algorithms vary with the states of data streams, a scheduling strategy is also investigated to maximize the processing efficiency. The algorithms in this paper not only can process the complex sliding window join aggregate, but also can process the multi-way sliding window join aggregate.",information system
1910,"Potential Research Space in MIS: A Framework for Envisioning and Evaluating Research Replication, Extension, and Generation",Information Systems Research," R belief to accepted knowledge. Given the espoused importance of replications to the exeplications are an important component of scientific method in that they convert tentative traction of knowledge from research, there is surprisingly little evidence of its practice or discussion of its importance in the management information systems literature. In this article we develop a framework within which to systematize the conceptualization of replications; we review and illustrate how some key information systems research fits into the framework and examine the factors that influence the selection of a research strategy. Our framework includes a conceptualization of the relationship among replication, extension, and generation in IS research. The concept of “research space” is defined and a framework is developed that delineates eight possible research strategies. Finally, the benefits of our framework to salient stakeholders in the research process are outlined. (Research Issues; Research Methodology; Research Models; Research Status; Research; Theoretical Evaluation) ",information system
1911,Learning User Real-Time Intent for Optimal Dynamic Web Page Transformation,Information Systems Research," A. Program Estimation We used a hierarchical Bayesian approach (Monte Carlo Markov chain) to estimate the models. We ran the models coded in C++ for 50,000 iterations. The first 40,000 iterations served as the ""burn-in"" period to ensure convergence, and the last 10,000 iterations provided input for our parameter inference. We conducted Geweke's (1992) convergence test and Heidelberger and Welch's (1983) stationary test to determine the convergence of the chains; all the nodes in our analysis pass both tests, so our estimation is reasonably stable and convergent. Readers interested in the detailed estimation procedure should see Li et al. (2011) and Netzer et al. (2008). ",information system
1912,Towards an Exhaustive Set of Rewriting Rules for XQuery Optimization: BizQuery Experience,Advances in Databases and Information Systems, Today it is wildly recognized that optimization based on rewriting leads to faster query execution. The role of a query rewriting grows significantly when a query defined in terms of some view is processed. Using views is a good idea for building flexible virtual data integration systems with declarative query support. At present time such systems tend to be based on the XML data model and use XML as the internal data representation for processing query over heterogeneous data. Hence an elaborated algorithm of query rewriting is of great importance for efficient processing of XML declarative queries. This paper describes the query rewriting techniques for the XQuery language that is implemented as part of the BizQuery virtual data integration system. The goals of XQuery rewriting are stated. An algebra for rewriting is proposed. Besides query rewriting rules for FLWR expressions the rules for XQuery functions and recursive XQuery functions are presented. Also the role of the XML schema in query rewriting is discussed. ,information system
1913,Threshold conditions for arbitrary cascade models on arbitrary networks,Knowledge and Information Systems,"Given a network of who-contacts-whom or who-links-to-whom, will a contagious virus/product/meme spread and ‘take over’ (cause an epidemic) or die out quickly? What will change if nodes have partial, temporary or permanent immunity? The epidemic threshold is the minimum level of virulence to prevent a viral contagion from dying out quickly and determining it is a fundamental question in epidemiology and related areas. Most earlier work focuses either on special types of graphs or on specific epidemiological/cascade models. We are the first to show the G2-threshold (twice generalized) theorem, which nicely de-couples the effect of the topology and the virus model. Our result unifies and includes as special case older results and shows that the threshold depends on the first eigenvalue of the connectivity matrix, (a) for any graph and (b) for all propagation models in standard literature (more than 25, including H.I.V.). Our discovery has broad implications for the vulnerability of real, complex networks and numerous applications, including viral marketing, blog dynamics, influence propagation, easy answers to ‘what-if’ questions, and simplified design and evaluation of immunization policies. We also demonstrate our result using extensive simulations on real networks, including on one of the biggest available social-contact graphs containing more than 31 million interactions among more than 1 million people representing the city of Portland, Oregon, USA.",information system
1914,Three Layer Evolution Model for XML Stored in Relational Databases,Advances in Databases and Information Systems," XML-relational systems with well de ned XML and relational schemas are widely used in industry. In the presence of rapidly changing requirements both schemas of such a model need continuous evolution, which can be performed by a formal framework that describes the evolution of the mutually dependent schema pairs of the system as sequences of elementary schema transformations. There are a number of common tasks, like relational schema performance tuning, XML syntax clean-up, etc that stand apart from changes of semantics of the schemas. We discuss a framework with three layers of operations: the changes in XML schema only, the semantic changes that affect both schemas, and the changes that affect relational schema only. We show how the layered framework allows to formalize and address the above mentioned tasks. We consider the formal quality of the solutions of the tasks that may be achieved inside the framework. ",information system
1915,A Novel Integrated Classifier for Handling Data Warehouse Anomalies,Advances in Databases and Information Systems," Within databases employed in various commercial sectors, anomalies continue to persist and hinder the overall integrity of data. Typically, Duplicate, Wrong and Missed observations of spatial-temporal data causes the user to be not able to accurately utilise recorded information. In literature, different methods have been mentioned to clean data which fall into the category of either deterministic and probabilistic approaches. However, we believe that to ensure the maximum integrity, a data cleaning methodology must have properties of both of these categories to effectively eliminate the anomalies. To realise this, we have proposed a method which relies both on integrated deterministic and probabilistic classifiers using fusion techniques. We have empirically evaluated the proposed concept with state-of-the-art techniques and found that our approach improves the integrity of the resulting data set. ",information system
1916,Basic Concepts of Formal Ontology,Formal Ontology in Information Systems," The term 'formal ontology' was first used by the philosopher Edmund Husserl in his Logical Investigations to signify the study of those formal structures and relations - above all relations of part and whole - which are exemplified in the subject-matters of the different material sciences. We follow Husserl in presenting the basic concepts of formal ontology as falling into three groups: the theory of part and whole, the theory of dependence, and the theory of boundary, continuity and contact. These basic concepts are presented in relation to the problem of providing an account of the formal ontology of the mesoscopic realm of everyday experience, and specifically of providing an account of the concept of individual substance. ",information system
1917,Logical comparison of inconsistent perspectives using scoring functions,Knowledge and Information Systems,"The language for describing inconsistency is underdeveloped. If a database (a set of formulae) is inconsistent, there is usually no qualification of that inconsistency. Yet, it would seem useful to be able to say how inconsistent a database is, or to say whether one database is “more inconsistent” than another database. In this paper, we provide a more general characterization of inconsistency in terms of a scoring function for each database Δ. A scoring function S is from the power set of Δ into the natural numbers defined so that S(Γ) gives the number of minimally inconsistent subsets of Δ that would be eliminated if the subset Γ was removed from Δ. This characterization offers an expressive and succinct means for articulating, in general terms, the nature of inconsistency in a set of formulae. We then compare databases using their scoring functions. This gives an intuitive ordering relation over databases that we can describe as “more inconsistent than”. These techniques are potentially useful in a wide range of problems including monitoring progress in negotiations between a number of participants, and in comparing heterogeneous sources of information.",information system
1918,Demystifying The Internet-Based Software Artefact,International Conference on Information Systems, There is a growing interest in information systems implemented as Internet-based software artefacts. Little attention has been paid to a comprehensive picture of such artefacts and their difference in relationship to traditional software artefacts. This paper presents an analysis of the Internet-based software artefact in order to expose differences from and similarities to traditional information systems. The analysis is based on Organizational Semiotics and the concept of actability focuses on user requirements as a basis for information systems development. ,information system
1919,Microeconomic analysis using dominant relationship analysis,Knowledge and Information Systems,"The concept of dominance has recently attracted much interest in the context of skyline computation. Given an N-dimensional data set S, a point p is said to dominate q if p is better than q in at least one dimension and equal to or better than it in the remaining dimensions. In this article, we propose extending the concept of dominance for business analysis from a microeconomic perspective. More specifically, we propose a new form of analysis, called Dominant Relationship Analysis (DRA), which aims to provide insight into the dominant relationships between products and potential buyers. By analyzing such relationships, companies can position their products more effectively while remaining profitable. To support DRA, we propose a novel data cube called DADA (Data Cube for Dominant Relationship Analysis), which captures the dominant relationships between products and customers. Three types of queries called Dominant Relationship Queries (DRQs) are consequently proposed for analysis purposes: (1) Linear Optimization Queries (LOQ), (2) Subspace Analysis Queries (SAQ), and (3) Comparative Dominant Queries (CDQ). We designed efficient algorithms for computation, compression and incremental maintenance of DADA as well as for answering the DRQs using DADA. We conducted extensive experiments on various real and synthetic data sets to evaluate the technique of DADA and report results demonstrating the effectiveness and efficiency of DADA and its associated query-processing strategies.",information system
1920,A clustering approach for sampling data streams in sensor networks,Knowledge and Information Systems,"The growing usage of embedded devices and sensors in our daily lives has been profoundly reshaping the way we interact with our environment and our peers. As more and more sensors will pervade our future cities, increasingly efficient infrastructures to collect, process and store massive amounts of data streams from a wide variety of sources will be required. Despite the different application-specific features and hardware platforms, sensor network applications share a common goal: periodically sample and store data collected from different sensors in a common persistent memory. In this article, we present a clustering approach for rapidly and efficiently computing the best sampling rate which minimizes the Sum of Square Error for each particular sensor in a network. In order to evaluate the efficiency of the proposed approach, we carried out experiments on real electric power consumption data streams provided by EDF (Électricité de France).",information system
1921,A collaborative filtering method based on associative memory model,International Conference on Information Systems,"Recommender systems are intelligent systems that help consumers by recommending products they are likely to appreciate or purchase. These recommendations are based on the users own purchasing, searching or browsing history and also that of other consumers with similar interests. These systems are often embedded in e-commerce applications with the aim to provide efficient personalized recommendations that are of mutual value to both the buyer and the seller. This paper presents a novel neural network based approach that employs associative memory model to make recommendations for purchase to consumers. Associative memory models are inherently able to solve pattern completion problem. This intrinsic property is of immense value in building efficient recommender systems for e-commerce applications that present consumers with recommendations they are likely to have a higher acceptance. The results of experiments based on this model compare favorably with those from the standard user-based algorithm. ÂŠ 2013 IEEE.",information system
1922,Environmental scanning on the Internet,International Conference on Information Systems," This study investigates the important organizational task of environmental scanning in an Internet context. A theoretical model relating potential causal factors to effectiveness of environmental scanning was formulated based on a synthesis of environmental scanning literature that took into consideration the Internet context. A questionnaire was developed for data collection. Responses from 105 organizations were tested for convergent and discriminant validity before the theoretical model was assessed using PLS analysis. Results showed that smaller organizations tend to scan more frequently on the Internet, both use of external consultants and volatility of competitor sector tend to cause organizations to scan more frequently on the Internet, and both use of external consultants and scanning frequency on the Internet tend to result in effectiveness of environmental scanning. Additional insights on these results were obtained through telephone interviews with 10 respondents. ",information system
1923,Intelligent agents for retrieving chinese Web financial news,International Conference on Information Systems," As the popularity of World Wide Web increases, many newspapers expand their services by providing news information on the Web in order to be competitive and increase benefit. The Web provides real time dissemination of financial news to investors. However, most investors find it difficult to search for the financial information of interest from the huge Web information space. Most of the commercial search engines are not user friendly and do not provide any tailor-made intelligent agents to search for relevant Web documents on behalf of users. Users have to exert a lot of effort to submit an appropriate query to obtain the information they want. Intelligent agents that learn user preferences and monitor the postings of Web information providers are desired. In this paper, we present an intelligent agent that utilizes user profiles and user feedback to search for the Chinese Web financial news articles on behalf of users. A Chinese indexing component is developed to index the continuously fetched Chinese financial news articles. User profiles capture the basic knowledge of user preferences based on the sources of news articles, the regions of the news reported, categories of industries related, the listed companies, and user specified keywords. User feedback captures the semantics of the user rated news articles. The search engine will rank the top 20 news articles that users are most interested in based on these inputs. Experiments were conducted to measure the performance of the agents based on the inputs from user profile and user feedback. ",information system
1924,Resource Allocation Policies for Personalization in Content Delivery Sites,Information Systems Research," One of the distinctive features of sites on the Internet is their ability to gather enormous amounts of information regarding their visitors, and use this information to enhance a visitor's experience by providing personalized information or recommendations. In providing personalized services a website is typically faced with the following tradeoff: when serving a visitor's request, it can deliver an optimally personalized version of the content to the visitor possibly with a long delay due to the computational effort needed, or it can deliver a suboptimal version of the content more quickly. This problem becomes more complex when several requests are waiting for information from a server. The website then needs to trade-off the benefit from providing more personalized content to each user with the negative externalities associated with higher waiting costs for all other visitors that have requests pending. We examine several deterministic resource allocation policies in such personalization contexts. We identify an optimal policy for the above problem when requests to be scheduled are batched, and show that the policy can be very efficiently implemented in practice. We provide an experimental approach to determine optimal batch lengths, and demonstrate that it performs favorably when compared with viable queuing approaches. ",information system
1925,The ClusTree: indexing micro-clusters for anytime stream mining,Knowledge and Information Systems,"Clustering streaming data requires algorithms that are capable of updating clustering results for the incoming data. As data is constantly arriving, time for processing is limited. Clustering has to be performed in a single pass over the incoming data and within the possibly varying inter-arrival times of the stream. Likewise, memory is limited, making it impossible to store all data. For clustering, we are faced with the challenge of maintaining a current result that can be presented to the user at any given time. In this work, we propose a parameter-free algorithm that automatically adapts to the speed of the data stream. It makes best use of the time available under the current constraints to provide a clustering of the objects seen up to that point. Our approach incorporates the age of the objects to reflect the greater importance of more recent data. For efficient and effective handling, we introduce the ClusTree, a compact and self-adaptive index structure for maintaining stream summaries. Additionally we present solutions to handle very fast streams through aggregation mechanisms and propose novel descent strategies that improve the clustering result on slower streams as long as time permits. Our experiments show that our approach is capable of handling a multitude of different stream characteristics for accurate and scalable anytime stream clustering.",information system
1926,Query Rewriting and Search in CROQUE,Advances in Databases and Information Systems,"In query optimization, a given query will be transformed by rewrite rules into an equivalent execution plan that is cheaper than the straightforwardly assigned plan according to some cost model. Finding the cheapest of all equivalent execution plans is a challenge since the rewriting of complex queries on the basis of a large set of rewriting rules may potentially span a very large space of equivalent plans. Consequently, one has to either use search strategies to explore (parts of) the search space or some heuristics to prune this space thus making it efficiently traversable. This paper presents the use of search strategies in the CROQUE project. The adaptation of some common strategies led to the development of a simple but powerful heuristics which is demonstrated by examples executed in the CROQUE prototype. The proposed heuristics can support any random-based search strategy or can be used stand-alone. It may be integrated seamlessly into most of the present query optimizers without almost any effort.",information system
1927,OntoMap: portal for upper-level ontologies,Formal Ontology in Information Systems," Currently the evaluation of feasibility of general-purpose ontologies and upperlevel models is expensive mostly because of technical problems such as different representation formalisms and terminologies used. Additionally, there are no formal mappings between the upper-level ontologies that could ease any kind of studies and comparisons. We present the OntoMap Project (http://www.OntoMap.org), a project with the pragmatic goal to facilitate the access, understanding, and reuse of such resources. A semantic framework on conceptual level is implemented that is small and easy enough to be learned on-the-fly. We tried to design the framework so that it captures most of the semantics usually encoded in upperlevel models. Technically, OntoMap is a web-site providing access to several upper-level ontologies and manual mapping between them. Categories & Descriptors: I.2.4 [Artificial Intelligence ]: Knowledge Representation Formalisms and Methods ",information system
1928,Business Process Query Language - a Way to Make Workflow Processes More Flexible,Advances in Databases and Information Systems," Many requirements for a business process depend on the workflow execution data that includes common data for all the population of processes, state of resources, state of processes, etc. The natural way to specify and implement such requirements is to put them into the process definition. In order to do it, we need: (1) a generalised workflow metamodel that includes data on the workflow environment, process definitions, and process execution; (2) a powerful and flexible query language addressing the metamodel; (3) integration of a query language with a business process definition language. In this paper the mentioned workflow metamodel together with the business process query language BPQL is presented. BPQL is integrated with the XML Process Definition Language (XPDL) increasing significantly its expressiveness and flexibility. We also present practical results for application of the proposed language in the OfficeObjects® WorkFlow system. ",information system
1929,Extension of TOPSIS method for 2-tuple linguistic multiple attribute group decision making with incomplete weight information,Knowledge and Information Systems,"With respect to linguistic multiple attribute group decision making problems with incomplete weight information, a new method is proposed. In the method, the 2-tuple linguistic representation developed in recent years is used to aggregate the linguistic assessment information. In order to get the weight vector of the attribute, we establish an optimization model based on the basic ideal of traditional technique for order performance by similarity to ideal solution, by which the attribute weights can be determined. Then, the optimal alternative(s) is determined by calculating the shortest distance from the 2-tuple linguistic positive ideal solution, and on the other side, the farthest distance of the 2-tuple linguistic negative ideal solution. The method has exact characteristic in linguistic information processing. It avoided information distortion and losing, which occur formerly in the linguistic information processing. Finally, a numerical example is used to illustrate the use of the proposed method. The result shows the approach is simple, effective, and easy to calculate.",information system
1930,Analyzing topics and authors in chat logs for crime investigation,Knowledge and Information Systems,"Cybercriminals have been using the Internet to accomplish illegitimate activities and to execute catastrophic attacks. Computer-Mediated Communication such as online chat provides an anonymous channel for predators to exploit victims. In order to prosecute criminals in a court of law, an investigator often needs to extract evidence from a large volume of chat messages. Most of the existing search tools are keyword-based, and the search terms are provided by an investigator. The quality of the retrieved results depends on the search terms provided. Due to the large volume of chat messages and the large number of participants in public chat rooms, the process is often time-consuming and error-prone. This paper presents a topic search model to analyze archives of chat logs for segregating crime-relevant logs from others. Specifically, we propose an extension of the Latent Dirichlet Allocation-based model to extract topics, compute the contribution of authors in these topics, and study the transitions of these topics over time. In addition, we present a special model for characterizing authors-topics over time. This is crucial for investigation because it provides a view of the activity in which authors are involved in certain topics. Experiments on two real-life datasets suggest that the proposed approach can discover hidden criminal topics and the distribution of authors to these topics.",information system
1931,SMOTE-RSB *: a hybrid preprocessing approach based on oversampling and undersampling for high imbalanced data-sets using SMOTE and rough sets theory,Knowledge and Information Systems,"Imbalanced data is a common problem in classification. This phenomenon is growing in importance since it appears in most real domains. It has special relevance to highly imbalanced data-sets (when the ratio between classes is high). Many techniques have been developed to tackle the problem of imbalanced training sets in supervised learning. Such techniques have been divided into two large groups: those at the algorithm level and those at the data level. Data level groups that have been emphasized are those that try to balance the training sets by reducing the larger class through the elimination of samples or increasing the smaller one by constructing new samples, known as undersampling and oversampling, respectively. This paper proposes a new hybrid method for preprocessing imbalanced data-sets through the construction of new samples, using the Synthetic Minority Oversampling Technique together with the application of an editing technique based on the Rough Set Theory and the lower approximation of a subset. The proposed method has been validated by an experimental study showing good results using C4.5 as the learning algorithm.",information system
1932,Semi-supervised learning with an imperfect supervisor,Knowledge and Information Systems,"Real-life applications may involve huge data sets with misclassified or partially classified training data. Semi-supervised learning and learning in the presence of label noise have recently emerged as new paradigms in the machine learning community to cope with this kind of problems. This paper describes a new discriminant algorithm for semi-supervised learning. This algorithm optimizes the classification maximum likelihood (CML) of a set of labeled–unlabeled data, using a discriminant extension of the Classification Expectation Maximization algorithm. We further propose to extend this algorithm by modeling imperfections in the estimated class labels for unlabeled data. The parameters of this label-error model are learned together with the semi-supervised classifier parameters. We demonstrate the effectiveness of the approach using extensive experiments on different datasets.",information system
1933,Integration of multiple network views in Wikipedia,Knowledge and Information Systems,One of the challenges in network data analysis is the determination of the most informative perspective on the network to use in analysis. This is particularly an issue when the network is dynamic and is defined by events that occur over time. We present an example of such a scenario in the analysis of edit networks in Wikipedia—the networks of editors interacting on Wikipedia pages. We propose the prediction of article quality as a task that allows us to quantify the informativeness of alternative network views. We present three fundamentally different views on the data that attempt to capture structural and temporal aspects of the edit networks. We demonstrate that each view captures information that is unique to that view and propose a strategy for integrating the different sources of information.,information system
1934,Using Probabilistic Topic Models in Enterprise Social Software,Business Information Systems,"Enterprise social software (ESS) systems are open and flexible corporate environments which utilize Web 2.0 technologies to stimulate participation through informal interactions and aggregate these interactions into collective structures. A challenge in these systems is to discover, organize and manage the knowledge model of topics found within the enterprise. In this paper we aim to enhance the search and recommendation functionalities of ESS by extending their folksonomies and taxonomies with the addition of underlying topics through the use of probabilistic topic models. We employ Latent Dirichlet Allocation in order to elicit latent topics and use the latter to assess similarities in resource and tag recommendation as well as for the expansion of query results. As an application of our approach we extend the search and recommendation facilities of the Organik enterprise social system.",information system
1935,Beyond topical similarity: a structural similarity measure for retrieving highly similar documents,Knowledge and Information Systems,"Accurately measuring document similarity is important for many text applications, e.g. document similarity search, document recommendation, etc. Most traditional similarity measures are based only on “bag of words” of documents and can well evaluate document topical similarity. In this paper, we propose the notion of document structural similarity, which is expected to further evaluate document similarity by comparing document subtopic structures. Three related factors (i.e. the optimal matching factor, the text order factor and the disturbing factor) are proposed and combined to evaluate document structural similarity, among which the optimal matching factor plays the key role and the other two factors rely on its results. The experimental results demonstrate the high performance of the optimal matching factor for evaluating document topical similarity, which is as well as or better than most popular measures. The user study shows the good ability of the proposed overall measure with all three factors to further find highly similar documents from those topically similar documents, which is much better than that of the popular measures and other baseline structural similarity measures.",information system
1936,Monitoring Continuous Location Queries Using Mobile Agents,Advances in Databases and Information Systems,"Nowadays the number of mobile device users is continuously increasing. However the available data services for those users are rare and usually provide an inefficient performance. More particularly, a growing interest is arising around location-based services but the processing of location-dependent queries is still a subject of research in the new mobile computing environment. Special difficulties arise when considering the need of keeping the answer to these queries up-to-date, due to the mobility of involved objects. In this paper we introduce a new approach for processing location-dependent queries that presents the following features: 1) it deals with scenarios where users issuing queries as well as objects involved in such queriescan change their location, 2) it dealswit h continuous queries and so answers are updated with a certain frequency, 3) it provides a completely decentralised solution and 4) it optimises wireless communication costs by using mobile agents. We focus on the way in which data presented to the user must be refreshed in order to show an up-to-date answer but optimising communications effort.",information system
1937,THE VIRTUAL COMMONS: WHY FREE- RIDING CAN BE TOLERATED IN FILE SHARING NETWORKS,International Conference on Information Systems," Peer-to-peer networks have emerged as a popular alternative to traditional client-server architectures for the distribution of information goods. Recent academic studies have observed high levels of free-riding in various peer-to-peer networks, leading some to suggest the imminent collapse of these communities as a viable information sharing mechanism. Our research develops an analytic model to analyze the behavior of P2P networks in the presence of free-riding. In contrast to previous predictions we find that P2P networks can operate effectively in the presence of significant free-riding. In future work we plan to explore how much peerto-peer network performance could be improved if free-riding were eliminated and discuss both the costs and benefits of managerial mechanisms to limit free-riding. ",information system
1938,Goals in a Formal Theory of Commonsense Psychology,Formal Ontology in Information Systems," In the context of developing formal theories of commonsense psychology, or how peole think they think, we have developed a formal theory of goals. In it we explicate and axiomatize, among others, the goal-related notions of trying, success, failure, functionality, intactness, and importance. ",information system
1939,Constructing Bodies and their Qualities from Observations,Formal Ontology in Information Systems," The principle challenge for information semantics lies in the degrees of freedom to interpret symbols in terms of thoughts and experiences which leads to incompatible views on the world. Consequently, incompatible information ontologies and interpretations of the described data will remain. Even though there is usually a common experiential ground, it stays often unknown to users of semantically annotated data. This symbol grounding problem is a bottleneck of information semantics, which remains largely unsolved in ontological practice. In this paper, we suggest - in the spirit of Jeremy Bentham - to introduce formal primitives which are directly grounded in inter-subjective experience, and which serve to expose and construct complex qualities in information ontologies. ",information system
1940,Realism for scientific ontologies,Formal Ontology in Information Systems," Science aims to develop an accurate understanding of reality through a variety of rigorously empirical and formal methods. Ontologies are used to formalize the meaning of terms within a domain of discourse. The Basic Formal Ontology (BFO) is an ontology of particular importance in the biomedical domains, where it provides the top-level for numerous ontologies, including those admitted as part of the OBO Foundry collection. The BFO requires that all classes in an ontology are actually instantiated in reality. Despite the fact that it is hard to show whether entities of some kind exist or do not exist in reality (especially for unobservable entities like elementary particles), this criterion fails to satisfy the need of scientists to communicate their findings and theories unambiguously. We discuss the problems that arise due to the BFO's realism criterion and suggest viable alternatives. ",information system
1941,Adverse Selection and Reputation Systems in Online Auctions: Evidence fom eBay Motors,International Conference on Information Systems," While the possibility of adverse selection is present in many transactional settings, online auctions appear to be especially susceptible to the problem. Unlike buyers in most traditional settings, online auction shoppers are physically unable to inspect the products for sale and must rely on pictures and descriptions provided by the seller. If buyers cannot distinguish quality until after the purchase has been made, there is no incentive for sellers to provide high quality products. As a result, buyers will be unwilling to pay a quality premium, the average quality in the market will decline, and the level of trade will fall to a level below what is socially optimal. Accordingly, if adverse selection exists in online auctions, the quality of items traded would be expected to be subaverage. In addition, we would expect the decreases in online prices to be larger than in offline prices as the variance in the condition of items increases. Using data from completed eBay Motors vehicle auctions, we test both assumptions and examine the ability of online reputation systems to offset the effects of adverse selection. Our results suggest that adverse selection is more pronounced in online auctions compared to traditional marketplaces for used goods, and that reputation systems reduce, but do not fully eliminate, the problem. Consistent with theoretical predictions, we find that as vehicle age and mileage increase (i.e., as the variance of a car's condition increases), the price that eBay buyers are willing to pay for the vehicle decreases by amounts greater than we would expect offline. In addition, we find that newer cars and those with low mileage are less likely to be sold on eBay. Further, sellers of higher quality vehicles and those with poorer reputations are more likely to protect themselves from the effects of adverse selection by setting reserve prices, and sellers with better reputations are more likely to sell their vehicles and receive higher price premiums. ",information system
1942,Beyond concepts: Ontology as reality representation,Formal Ontology in Information Systems," There is an assumption commonly embraced by ontological engineers, an assumption which has its roots in the discipline of knowledge representation, to the effect that it is concepts which form the subject-matter of ontology. The term 'concept' is hereby rarely precisely defined, and the intended role of concepts within ontology is itself subject to a variety of conflicting (and sometimes intrinsically incoherent) interpretations. It seems, however, to be widely accepted that concepts are in some sense the products of human cognition. The present essay is devoted to the application of ontology in support of research in the natural sciences. It defends the thesis that ontologies developed for such purposes should be understood as having as their subject matter, not concepts, but rather the universals and particulars which exist in reality and are captured in scientific laws. We outline the benefits of a view along these lines by showing how it yields rigorous formal definitions of the foundational relations used in many influential ontologies, illustrating our results by reference to examples drawn from the domain of the life sciences. ",information system
1943,Resource Mining: Applying Process Mining to Resource-Oriented Systems,Business Information Systems,"Service Oriented Architecture is an increasingly popular approach to implement complex distributed systems. It enables implementing complex functionality just by composing simple services into so called business processes. Unfortunately, such composition of services may lead to some incorrect system behavior. In order discover such depreciances and fix them, process mining methods may be used. Unfortunately, the current state of the art focuses only on SOAP-based Web Services leaving RESTful Web Service (resource-oriented) unsupported. In this article the relevance of adapting the Web Service Mining methods to new resource-oriented domain is introduced with initial work on process discovery in such systems.",information system
1944,A Query Language for Similarity-Based Retrieval of Multimedia Data,Advances in Databases and Information Systems, ISBN: 3-540-76227-2 ,information system
1945,Information Overload and the Message Dynamics of Online Interaction Spaces: A Theoretical Model and Empirical Exploration,Information Systems Research," Otional, and economic importance. In this paper, a theoretical model and associated unobtrusive method are nline spaces that enable shared public interpersonal communications are of significant social, organizaproposed for researching the relationship between online spaces and the behavior they host. The model focuses on the collective impact that individual information-overload coping strategies have on the dynamics of open, interactive public online group discourse. Empirical research was undertaken to assess the validity of both the method and the model, based on the analysis of over 2.65 million postings to 600 Usenet newsgroups over a 6-month period. Our findings support the assertion that individual strategies for coping with “information overload” have an observable impact on large-scale online group discourse. Evidence was found for the hypotheses that: (1) users are more likely to respond to simpler messages in overloaded mass interaction; (2) users are more likely to end active participation as the overloading of mass interaction increases; and (3) users are more likely to generate simpler responses as the overloading of mass interaction grows. The theoretical model outlined offers insight into aspects of computer-mediated communication tool usability, technology design, and provides a road map for future empirical research. ",information system
1946,Extended MDL principle for feature-based inductive transfer learning,Knowledge and Information Systems,"Transfer learning provides a solution in real applications of how to learn a target task where a large amount of auxiliary data from source domains are given. Despite numerous research studies on this topic, few of them have a solid theoretical framework and are parameter-free. In this paper, we propose an Extended Minimum Description Length Principle (EMDLP) for feature-based inductive transfer learning, in which both the source and the target data sets contain class labels and relevant features are transferred from the source domain to the target one. Unlike conventional methods, our encoding measure is based on a theoretical background and has no parameter. To obtain useful features to be used in the target task, we design an enhanced encoding length by adopting a code book that stores useful information obtained from the source task. With the code book that builds connections between the source and the target tasks, our EMDLP is able to evaluate the inferiority of the results of transfer learning with the add sum of the code lengths of five components: those of the corresponding two hypotheses, the two data sets with the help of the hypotheses, and the set of the transferred features. The proposed method inherits the nice property of the MDLP that elaborately evaluates the hypotheses and balances the simplicity of the hypotheses and the goodness-of-the-fit to the data. Extensive experiments using both synthetic and real data sets show that the proposed method provides a better performance in terms of the classification accuracy and is robust against noise.",information system
1947,2-D Spatial Indexing Scheme in Optimal Time,Advances in Databases and Information Systems,"We consider the 2-dimensional space with integer coordinates in the range [1, N] × [1, N]. We present the MPST (Modified Priority Search Tree) index structure which reports the k points that lie inside the quadrant query range (, b] × (, c] in optimal O(k) time. Our Index Scheme is simple, fast and it can be used in various geometric or spatial applications such as: (1) 2 D dominance reporting on a grid (2) 2D maximal elements on a grid. Then, based on structures of Gabow et al. [6] and Beam and Fich [31] we describe an index scheme, which handles in an efficient way window queries in spatial database applications. In the general case in which the plane has real coordinates the above time results are slowed down by adding a logarithmic factor due to the normalization technique.",information system
1948,Multirelational classification: a multiple view approach,Knowledge and Information Systems,"Multirelational classification aims at discovering useful patterns across multiple inter-connected tables (relations) in a relational database. Many traditional learning techniques, however, assume a single table or a flat file as input (the so-called propositional algorithms). Existing multirelational classification approaches either “upgrade” mature propositional learning methods to deal with relational presentation or extensively “flatten” multiple tables into a single flat file, which is then solved by propositional algorithms. This article reports a multiple view strategy—where neither “upgrading” nor “flattening” is required—for mining in relational databases. Our approach learns from multiple views (feature set) of a relational databases, and then integrates the information acquired by individual view learners to construct a final model. Our empirical studies show that the method compares well in comparison with the classifiers induced by the majority of multirelational mining systems, in terms of accuracy obtained and running time needed. The paper explores the implications of this finding for multirelational research and applications. In addition, the method has practical significance: it is appropriate for directly mining many real-world databases.",information system
1949,Relative Reduct-Based Estimation of Relevance for Stylometric Features,Advances in Databases and Information Systems,"In rough set theory characteristic features, which describe classified objects, correspond to conditional attributes. A relative reduct is such an irreducible subset of attributes that preserves the quality of approximation of a complete decision table. For a decision table a single reduct or many reducts may exist. In typical processing one reduct is selected for the subsequent generation of decision rules, while others can be discarded. Yet when the set of reducts is analysed as a whole, observations and conclusions drawn can be used to evaluate relevance of attributes, which in turn can be employed in reduction of features not only for rule-based but also connectionist classifiers. The paper describes the steps of such procedure applied in the domain of stylometric processing of literary texts.",information system
1950,Multiversion Spatio-temporal Telemetric Data Warehouse,Advances in Databases and Information Systems,"One of the crucial problems characterizing current data warehouses is the implicit assumption of dimension invariance with respect to the time dimension. This assumption inhibits the proper treatment of changes in dimension data. Meanwhile, we can give examples indicating that it is necessary to take into consideration the modifications of dimension data - ignoring such changes leads to incorrect analysis which then results in wrong decisions. This article describes the implemented temporal telemetric data warehouse system, which provides the user with the ability to query about the time interval embracing many structure versions. The system also informs the user about modifications which occurred in separated structure versions.",information system
1951,A Hidden Markov Model of Developer Learning Dynamics in Open Source Software Projects,Information Systems Research, This research was supported by a grant from the Center for Organizational Learning and ,information system
1952,Multivariate discretization for set mining,Knowledge and Information Systems," Many algorithms in data mining can be formulated as a set-mining problem where the goal is to nd conjunctions (or disjunctions) of terms that meet user-speci ed constraints. Set-mining techniques have been largely designed for categorical or discrete data where variables can only take on a xed number of values. However, many datasets also contain continuous variables and a common method of dealing with these is to discretize them by breaking them into ranges. Most discretization methods are univariate and consider only a single feature at a time (sometimes in conjunction with a class variable). We argue that this is a suboptimal approach for knowledge discovery as univariate discretization can destroy hidden patterns in data. Discretization should consider the e ects on all variables in the analysis and that two regions X and Y should only be in the same interval after discretization if the instances in those regions have similar multivariate distributions (Fx Fy) across all variables and combinations of variables. We present a bottom-up merging algorithm to discretize continuous variables based on this rule. Our experiments indicate that the approach is feasible, that it will not destroy hidden patterns and that it will generate meaningful intervals. ",information system
1953,A probabilistic model to resolve diversity---accuracy challenge of recommendation systems,Knowledge and Information Systems,"Recommendation systems have wide-spread applications in both academia and industry. Traditionally, performance of recommendation systems has been measured by their precision. By introducing novelty and diversity as key qualities in recommender systems, recently increasing attention has been focused on this topic. Precision and novelty of recommendation are not in the same direction, and practical systems should make a trade-off between these two quantities. Thus, it is an important feature of a recommender system to make it possible to adjust diversity and accuracy of the recommendations by tuning the model. In this paper, we introduce a probabilistic structure to resolve the diversity–accuracy dilemma in recommender systems. We propose a hybrid model with adjustable level of diversity and precision such that one can perform this by tuning a single parameter. The proposed recommendation model consists of two models: one for maximization of the accuracy and the other one for specification of the recommendation list to tastes of users. Our experiments on two real datasets show the functionality of the model in resolving accuracy–diversity dilemma and outperformance of the model over other classic models. The proposed method could be extensively applied to real commercial systems due to its low computational complexity and significant performance.",information system
1954,S*-Tree: An Improved S+-Tree for Coloured Images,Advances in Databases and Information Systems,"In this paper we propose and analyze a new spatial access method, namely the S*-tree, for the efficient secondary memory encoding and manipulation of images containing multiple non-overlapping features (i.e., coloured images). We show that the S*-tree is more space efficient than its precursor, namely the S+-tree, which was explicitly designed for binary images, and whose straightforward extension to coloured images can lead to large space wastage. Moreover, we tested time efficiency of the S*-tree in answering classical window queries, comparing it against a previous efficient access method, namely the HL-quadtree [7]. Our experiments show that the S*-tree can reach up to a 30% of time saving.",information system
1955,High-dimensional clustering: a clique-based hypergraph partitioning framework,Knowledge and Information Systems,"Hypergraph partitioning has been considered as a promising method to address the challenges of high-dimensional clustering. With objects modeled as vertices and the relationship among objects captured by the hyperedges, the goal of graph partitioning is to minimize the edge cut. Therefore, the definition of hyperedges is vital to the clustering performance. While several definitions of hyperedges have been proposed, a systematic understanding of desired characteristics of hyperedges is still missing. To that end, in this paper, we first provide a unified clique perspective of the definition of hyperedges, which serves as a guide to define hyperedges. With this perspective, based on the concepts of shared (reverse) nearest neighbors, we propose two new types of clique hyperedges and analyze their properties regarding purity and size issues. Finally, we present an extensive evaluation using real-world document datasets. The experimental results show that, with shared (reverse) nearest neighbor-based hyperedges, the clustering performance can be improved significantly in terms of various external validation measures without the need for fine tuning of parameters.",information system
1956,Accelerating EM clustering to find high-quality solutions,Knowledge and Information Systems,"Clustering is one of the most important techniques used in data mining. This article focuses on the EM clustering algorithm. Two fundamental aspects are studied: achieving faster convergence and finding higher quality clustering solutions. This work introduces several improvements to the EM clustering algorithm, being periodic M steps during initial iterations, reseeding of low-weight clusters and splitting of high-weight clusters the most important. These improvements lead to two important parameters. The first parameter is the number of M steps per iteration and the second one, a weight threshold to reseed low-weight clusters. Experiments show how frequently the M step must be executed and what weight threshold values make EM reach higher quality solutions. In general, the improved EM clustering algorithm finds higher quality solutions than the classical EM algorithm and converges in fewer iterations.",information system
1957,CARDAP: A Scalable Energy-Efficient Context Aware Distributed Mobile Data Analytics Platform for the Fog,Advances in Databases and Information Systems,"Distributed online data analytics has attracted significant research interest in recent years with the advent of Fog and Cloud computing. The popularity of novel distributed applications such as crowdsourcing and crowdsensing have fostered the need for scalable energy-efficient platforms that can enable distributed data analytics. In this paper, we propose CARDAP, a (C)ontext (A)ware (R)eal-time (D)ata (A)nalytics (P)latform. CARDAP is a generic, flexible and extensible, component- based platform that can be deployed in complex distributed mobile analytics applications e.g. sensing activity of citizens in smart cities. CARDAP incorporates a number of energy efficient data delivery strategies using real-time mobile data stream mining for data reduction and thus less data transmission. Extensive experimental evaluations indicate the CARDAP platform can deliver significant benefits in energy efficiency over naive approaches. Lessons learnt and future work conclude the paper.",information system
1958,The Effect of Formal Representation Formats on the Quality of Legal Decision-Making,Knowledge and Information Systems," Accessibility of legal sources is crucial to both jurists and citizens. As the development of E-government is speeded up the number of legal information systems including knowledge-based services is likely to increase accordingly. In the Program for an Ontology-based Working Environment for Rules and legislation (POWER) the Dutch Tax and Customs Administration (Belastingdienst) developed a formal modelling approach for modelling legal sources. The results of applying this modelling process are formal models expressed in UML/OCL which can be used as the basis for amongst others verification, simulation and application generation. Since the legal knowledge to be applied in the operational units and on the Web is represented in these formal models, their quality needs serious attention. To be able to determine their quality, inspection by (legal) experts should be possible (validation). In most cases however these experts are not able to read these UML/OCL-models. Therefore a representation is required that can easily be derived from these models and that is easy to understand. Based on previous research we compared two different knowledge representations and tested their learnability and usability. Furthermore we examined the effect of representation on legal decision-making. The two representation-forms used are production rules and scenarios. The results of our experiment show that the performance of scenarios is significantly better then the performance of production rules. ",information system
1959,Non-unique cluster numbers determination methods based on stability in spectral clustering,Knowledge and Information Systems,"Recently, a large amount of work has been devoted to the study of spectral clustering—a simple yet powerful method for finding structure in a data set using spectral properties of an associated pairwise similarity matrix. Most of the existing spectral clustering algorithms estimate only one cluster number or estimate non-unique cluster numbers based on eigengap criterion. However, the number of clusters not always exists one, and eigengap criterion lacks theoretical justification. In this paper, we propose non-unique cluster numbers determination methods based on stability in spectral clustering (NCNDBS). We first utilize the multiway normalized cut spectral clustering algorithm to cluster data set for a candidate cluster number (k). Then the ratio value of the multiway normalized cut criterion of the obtained clusters and the sum of the leading eigenvalues (descending sort) of the stochastic transition matrix is chosen as a standard to decide whether the (k) is a reasonable cluster number. At last, by varying the scaling parameter in the Gaussian function, we judge whether the reasonable cluster number (k) is also a stability one. By three stages, we can determine non-unique cluster numbers of a data set. The Lumpability theorem concluded by Meil(reve{a}) and Xu provides a theoretical base for our methods. NCNDBS can estimate non-unique cluster numbers of the data set successfully by illustrative experiments.",information system
1960,Improving Analysis Pattern Reuse in Conceptual Design: Augmenting Automated Processes with Supervised Learning,Information Systems Research," Cpatterns can greatly benefit this phase because they capture abstractions of situations onceptual design is an important, but difficult, phase of systems development. Analysis that occur frequently in conceptual modeling. Naïve approaches to automate conceptual design with reuse of analysis patterns have had limited success because they do not emulate the learning that occurs over time. This research develops learning mechanisms for improving analysis pattern reuse in conceptual design. The learning mechanisms employ supervised learning techniques to support the generic reuse tasks of retrieval, adaptation, and integration, and emulate expert behaviors of analogy making and designing by assembly. They are added to a naïve approach and the augmented methodology implemented as an intelligent assistant to a designer for generating an initial conceptual design that a developer may refine. To assess the potential of the methodology to benefit practice, empirical testing is carried out on multiple domains and tasks of different sizes. The results suggest that the methodology has the potential to benefit practice. (Reuse; Design Automation; Object-Oriented Systems; Learning Mechanisms; Conceptual Design; Analysis Patterns; APSARA; Software Development ) ",information system
1961,MOLAP cube based on parallel scan algorithm,Advances in Databases and Information Systems,"This paper describes a new approach to multidimensional OLAP cubes implementation by employing a massively parallel scan operation. This task requires dedicated data structures, setting up and querying algorithms. A prototype implementation is evaluated in aspects of robustness and scalability for both time and storage.",information system
1962,Distributed Searching of k-Dimensional Data with Almost Constant Costs,Advances in Databases and Information Systems,"In this paper we consider the dictionary problem in the scalable distributed data structure paradigm introduced by Litwin, Neimat and Schneider and analyze costs for insert and exact searches in an amortized framework. We show that both for the 1-dimensional and the k- dimensional case insert and exact searches have an amortized almost constant costs, namely O (log(1+A-) n) messages, where n is the total number of servers of the structure, b is the capacity of each server, and ( A = ",information system
1963,Integration of decision support systems to improve decision support performance,Knowledge and Information Systems,"Decision support system (DSS) is a well-established research and development area. Traditional isolated, stand-alone DSS has been recently facing new challenges. In order to improve the performance of DSS to meet the challenges, research has been actively carried out to develop integrated decision support systems (IDSS). This paper reviews the current research efforts with regard to the development of IDSS. The focus of the paper is on the integration aspect for IDSS through multiple perspectives, and the technologies that support this integration. More than 100 papers and software systems are discussed. Current research efforts and the development status of IDSS are explained, compared and classified. In addition, future trends and challenges in integration are outlined. The paper concludes that by addressing integration, better support will be provided to decision makers, with the expectation of both better decisions and improved decision making processes.",information system
1964,The Social Construction of Meaning: An Alternative Perspective on Information Sharing,Information Systems Research," R on content relevant to a decision. We propose and examine an alternate function of inesearch on information sharing has viewed this activity as essential for informing groups formation sharing, i.e., the social construction of meaning. To accomplish this goal, we turn to social construction, social presence, and task closure theories. Drawing from these theories, we hypothesize relationships among the meeting environment, breadth and depth of information shared during a meeting, and decision quality. We explore these relationships in terms of the effects of both the media environment in which the group is situated and the medium that group members choose to utilize for their communication. Our study of 32 five- and six-person groups supports our belief that interpretation underlies information sharing and is necessary for favorable decision outcomes. It also supports the proposed negative effect of low social presence media on interpretation in terms of depth of information sharing; a low social presence medium, however, promotes information sharing breadth. Finally, the findings indicate that when in multimedia environments and faced with a relatively complex task, choosing to utilize an electronic medium facilitates closure and, therefore, favorable outcomes. (Communication Media; Group Support Systems; Social Construction of Meaning; Intersubjective Interpretation; Social Presence; Information Sharing; Decision Quality; Task Closure) ",information system
1965,Artificial intelligence methodologies for agile refining: an overview,Knowledge and Information Systems,"Agile manufacturing is the capability to prosper in a competitive environment of continuous and unpredictable changes by reacting quickly and effectively to the changing markets and other exogenous factors. Agility of petroleum refineries is determined by two factors – ability to control the process and ability to efficiently manage the supply chain. In this paper, we outline some challenges faced by refineries that seek to be lean, nimble, and proactive. These problems, which arise in supply chain management and operations management are seldom amenable to traditional, monolithic solutions. As discussed here using several examples, methodologies drawn from artificial intelligence – software agents, pattern recognition, expert systems – have a role to play in this path toward agility.",information system
1966,Clustering spatial data with a geographic constraint: exploring local search,Knowledge and Information Systems,"Spatial data objects that possess attributes in the optimization domain and the geographic domain are now widely available. For example, sensor data are one kind of spatial data objects. The location of a sensor is an attribute in the geographic domain, while its reading is an attribute in the optimization domain. Previous studies discuss dual clustering problems that attempt to partition spatial data objects into several groups, such that objects in the same group have similar values in their optimization attributes and form a compact region in the geographic domain. However, previous studies do not clearly define compact regions. Therefore, this paper formulates a connective dual clustering problem with an explicit connected constraint given. Objects with a geographic distance smaller than or equal to the connected constraint are connected. The goal of the connective dual clustering problem is to derive clusters that contain objects with similar values in the optimization domain and are connected in the geographic domain. This study further proposes an algorithm CLS (Clustering with Local Search) to efficiently derive clusters. This algorithm consists of two phases: the ConGraph (standing for Connective Graph) transformation phase and the clustering phase. In the ConGraph transformation phase, CLS first transforms the data objects into a ConGraph that captures geographic constraints among data objects and selects initial seeds for clustering. Then, the initial seeds selected nearby data objects and formed coarse clusters by exploring local search in the clustering phase. Moreover, coarse clusters are merged and finely turned. Experiments show that CLS algorithm is more efficient and scalable than existing methods.",information system
1967,A review on the use of action research in information systems studies,International Conference on Information Systems," This paper examines the use of action research in information systems (IS) studies reported in literature over the last twenty-five years. Thirty such field studies and discussion papers on information technology, system design/use or socio-technical systems were reviewed and compared with those from social science. Evolving patterns are noted among these IS studies in terms of their underlying assumptions, study designs and presentation styles. A contemporary IS action research framework is proposed as a conceptual foundation and practical guide for researchers and practitioners interested in action research for IS studies. Its implications in IS research and practice are discussed. ",information system
1968,Predictive Join Processing between Regions and Moving Objects,Advances in Databases and Information Systems,"The family of R-trees is suitable for indexing various kinds of multidimensional objects. TPR*-trees are R-tree based structures that have been proposed for indexing a moving object database, e.g. a data-base of moving boats. Region Quadtrees are suitable for indexing 2-dimensional regional data and their linear variant (Linear Region Quadtrees) is used in many Geographical Information Systems (GIS) for this purpose, e.g. for the representation of stormy, or sunny regions. Although, both are tree structures, the organization of data space, the types of spatial data stored and the search algorithms applied on them are different in R-trees and Region Quadtrees. In this paper, we examine a spatio-temporal problem that appears in many practical applications: processing of predictive joins between moving objects and regions (e.g. discovering the boats that will enter a storm), using these two families of data structures as storage and indexing mechanisms, and taking into account their similarities and differences. With a thorough experimental study, we show that the use of a synchronous Depth-First traversal order has the best performance balance (on average), taking into account the I/O activity and response time as performance measurements.",information system
1969,A general measure of similarity for categorical sequences,Knowledge and Information Systems,"Measuring the similarity between categorical sequences is a fundamental process in many data mining applications. A key issue is extracting and making use of significant features hidden behind the chronological and structural dependencies found in these sequences. Almost all existing algorithms designed to perform this task are based on the matching of patterns in chronological order, but such sequences often have similar structural features in chronologically different order. In this paper we propose SCS, a novel, effective and domain-independent method for measuring the similarity between categorical sequences, based on an original pattern matching scheme that makes it possible to capture chronological and non-chronological dependencies. SCS captures significant patterns that represent the natural structure of sequences, and reduces the influence of those which are merely noise. It constitutes an effective approach to measuring the similarity between data in the form of categorical sequences, such as biological sequences, natural language texts, speech recognition data, certain types of network transactions, and retail transactions. To show its effectiveness, we have tested SCS extensively on a range of data sets from different application fields, and compared the results with those obtained by various mainstream algorithms. The results obtained show that SCS produces results that are often competitive with domain-specific similarity approaches.",information system
1970,A multi-phase correlation search framework for mining non-taxonomic relations from unstructured text,Knowledge and Information Systems,"Over the last decade, ontology engineering has been pursued by “learning” the ontology from domain-specific electronic documents. Most of the research works are focused on extraction of concepts and taxonomic relations. The extraction of non-taxonomic relations is often neglected and not well researched. In this paper, we present a multi-phase correlation search framework to extract non-taxonomic relations from unstructured text. Our framework addresses the two main problems in any non-taxonomic relations extraction: (a) the discovery of non-taxonomic relations and (b) the labelling of non-taxonomic relations. First, our framework is capable of extracting correlated concepts beyond ordinary search window size of a single sentence. Interesting correlations are then filtered using association rule mining with lift interestingness measure. Next, our framework distinguishes non-taxonomic concept pairs from taxonomic concept pairs based on existing domain ontology. Finally, our framework features the usage of domain related verbs as labels for the non-taxonomic relations. Our proposed framework has been tested with the marine biology domain. Results have been validated by domain experts showing reliable results as well as demonstrate significant improvement over traditional association rule approach in search of non-taxonomic relations from unstructured text.",information system
1971,A Transaction Model For Handling Composite Events,Advances in Databases and Information Systems, British Computer Society CBS ,information system
1972,Decision trees for uplift modeling with single and multiple treatments,Knowledge and Information Systems,"Most classification approaches aim at achieving high prediction accuracy on a given dataset. However, in most practical cases, some action such as mailing an offer or treating a patient is to be taken on the classified objects, and we should model not the class probabilities themselves, but instead, the change in class probabilities caused by the action. The action should then be performed on those objects for which it will be most profitable. This problem is known as uplift modeling, differential response analysis, or true lift modeling, but has received very little attention in machine learning literature. An important modification of the problem involves several possible actions, when for each object, the model must also decide which action should be used in order to maximize profit. In this paper, we present tree-based classifiers designed for uplift modeling in both single and multiple treatment cases. To this end, we design new splitting criteria and pruning methods. The experiments confirm the usefulness of the proposed approaches and show significant improvement over previous uplift modeling techniques.",information system
1973,An new immune genetic algorithm based on uniform design sampling,Knowledge and Information Systems,"The deficiencies of keeping population diversity, prematurity and low success rate of searching the global optimal solution are the shortcomings of genetic algorithm (GA). Based on the bias of samples in the uniform design sampling (UDS) point set, the crossover operation in GA is redesigned. Using the concentrations of antibodies in artificial immune system (AIS), the chromosomes concentration in GA is defined and the clonal selection strategy is designed. In order to solve the maximum clique problem (MCP), an new immune GA (UIGA) is presented based on the clonal selection strategy and UDS. The simulation results show that the UIGA provides superior solution quality, convergence rate, and other various indices to those of the simple and good point GA when solving MCPs.",information system
1974,Online Information Privacy: Measuring the Cost-Benefit Trade-Off,International Conference on Information Systems," Concern over information privacy is widespread and rising. However, prior research is silent about the value of information privacy and the benefit of privacy protection. We conducted a conjoint analysis to explore individuals trade-offs between the benefits and costs of providing personal information to Websites. We find that economic incentives (monetary reward and future convenience) do affect individuals preferences over Websites with differing privacy policies. For instance, the disallowance of secondary use of personal information is worth between $39.83 and $49.78. Surprisingly, we find that cost-benefit trade-offs did not vary with personal characteristics including gender, contextual knowledge, individualism, and trust propensity. ",information system
1975,Shared-memory and Shared-nothing Stochastic Gradient Descent Algorithms for Matrix Completion,Knowledge and Information Systems,"We provide parallel algorithms for large-scale matrix completion on problems with millions of rows, millions of columns, and billions of revealed entries. We focus on in-memory algorithms that run either in a shared-memory environment on a powerful compute node or in a shared-nothing environment on a small cluster of commodity nodes; even very large problems can be handled effectively in these settings. Our ASGD, DSGD-MR, DSGD++, and CSGD algorithms are novel variants of the popular stochastic gradient descent (SGD) algorithm, with the latter three algorithms based on a new “stratified SGD” approach. All of the algorithms are cache-friendly and exploit thread-level parallelism, in-memory processing, and asynchronous communication. We investigate the performance of both new and existing algorithms via a theoretical complexity analysis and a set of large-scale experiments. The results show that CSGD is more scalable, and up to 60 % faster, than the best-performing alternative method in the shared-memory setting. DSGD++ is superior in terms of overall runtime, memory consumption, and scalability in the shared-nothing setting. For example, DSGD++ can solve a difficult matrix completion problem on a high-variance matrix with 10M rows, 1M columns, and 10B revealed entries in around 40 min on 16 compute nodes. In general, algorithms based on SGD appear to perform better than algorithms based on alternating minimizations, such as the PALS and DALS alternating least-squares algorithms.",information system
1976,Subspace sums for extracting non-random data from massive noise,Knowledge and Information Systems,"An algorithm is introduced that distinguishes relevant data points from randomly distributed noise. The algorithm is related to subspace clustering based on axis-parallel projections, but considers membership in any projected cluster of a given side length, as opposed to a particular cluster. An aggregate measure is introduced that is based on the total number of points that are close to the given point in all possible 2d projections of a d-dimensional hypercube. No explicit summation over subspaces is required for evaluating this measure. Attribute values are normalized based on rank order to avoid making assumptions on the distribution of random data. Effectiveness of the algorithm is demonstrated through comparison with conventional outlier detection on a real microarray data set as well as on time series subsequence data.",information system
1977,Dimensionality Reduction for Fast Similarity Search in Large Time Series Databases,Knowledge and Information Systems," The problem of similarity search in large time series databases has attracted much attention recently. It is a non-trivial problem because of the inherent high dimensionality of the data. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a spatial access method. Three major dimensionality reduction techniques have been proposed, Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and more recently the Discrete Wavelets Transform (DWT). In this work we introduce a new dimensionality reduction technique which we call PAA (Piecewise Aggregate Approximation). We theoretically and empirically compare it to the other techniques and demonstrate its superiority. In addition to being competitive with or faster than the other methods our approach has numerous advantages. It is simple to understand and implement, allows more flexible distance measures including weighted Euclidean queries and the index can be built in linear time. ",information system
1978,Expressive reasoning with horn rules and fuzzy description logics,Knowledge and Information Systems,"In this paper, we describe fuzzy CARIN, a knowledge representation language combining fuzzy Description Logics with Horn rules. Fuzzy CARIN integrates the management of fuzzy logic into the non-recursive CARIN language. We introduce the decision problems of answering to conjunctive queries, unions of conjunctive queries and the existential entailment problem and provide a sound and complete algorithm that permits reasoning with the DL fuzzy ({mathcal{ALCNR}}) extended with non-recursive Horn rules. This extension is most useful in realistic applications that handle uncertain or imprecise data such as multimedia processing and medical applications.",information system
1979,Leveraging the storage layer to support XML similarity joins in XDBMSs,Advances in Databases and Information Systems,"XML is widely applied to describe semi-structured data commonly generated and used by modern information systems. XML database management systems (XDBMSs) are thus essential platforms in this context. Most XDBMS architectures proposed so far aim at reproducing functionalities found in relational systems. As such, these architectures inherit the same deficiency of traditional systems in dealing with less-structured data. What is badly needed is efficient support of common database operations under the similarity matching paradigm. In this paper, we present an engineering approach to incorporating similarity joins into XDBMSs, which exploits XDBMS components—the storage layer in particular—to design efficient algorithms. We experimentally confirm the accuracy, performance, and scalability of our approach.",information system
1980,Data Organization Issues for Location-Dependent Queries in Mobile Computing,Advances in Databases and Information Systems,"We consider queries which originate from a mobile unit and whose result depends on the location of the user who initiates the query. Example of such a query is How many people are living in the region I am currently in?” We execute such queries based on location-dependent data involved in their processing. We build concept hierarchies based on the location data. These hierarchies define mapping among different granularities of locations. One such hierarchy is to generate domain knowledge about the cities that belong to a state. The hierarchies are used as distributed directories to assist in finding the database or relation that contains the values of the location-dependent attribute in a particular location. We extend concept hierarchies to include spatial indexes on the location-dependent attributes. Finally, we discuss how to partition and replicate relations based on the location to process the queries efficiently. We briefly discuss the implementation issues.",information system
1981,Intent mining in search query logs for automatic search script generation,Knowledge and Information Systems,"Capturing users’ information needs is essential in decreasing the barriers in information access. This paper mines sequences of actions called search scripts from search query logs which keep large-scale users’ search experiences. Search scripts can be applied to guide users to satisfy their information needs, improve the search effectiveness of retrieval systems, recommend advertisements at suitable places, and so on. Information quality, query ambiguity, topic diversity, and document relevancy are four major challenging issues in search script mining. In this paper, we determine the relevance of URLs for a query, adopt the Open Directory Project (ODP) categories to disambiguate queries and URLs, explore various features and clustering algorithms for intent clustering, identify critical actions from each intent cluster to form a search script, generate a nature language description for each action, and summarize a topic for each search script. Experiments show that the complete link hierarchical clustering algorithm with the features of query terms, relevant URLs, and disambiguated ODP categories performs the best. Applying the intent clusters created by the best model to intent boundary identification achieves an (F) score of  0.6666. The intent clusters then are applied to generate search scripts.",information system
1982,Efficient greedy feature selection for unsupervised learning,Knowledge and Information Systems,"Reducing the dimensionality of the data has been a challenging task in data mining and machine learning applications. In these applications, the existence of irrelevant and redundant features negatively affects the efficiency and effectiveness of different learning algorithms. Feature selection is one of the dimension reduction techniques, which has been used to allow a better understanding of data and improve the performance of other learning tasks. Although the selection of relevant features has been extensively studied in supervised learning, feature selection in the absence of class labels is still a challenging task. This paper proposes a novel method for unsupervised feature selection, which efficiently selects features in a greedy manner. The paper first defines an effective criterion for unsupervised feature selection that measures the reconstruction error of the data matrix based on the selected subset of features. The paper then presents a novel algorithm for greedily minimizing the reconstruction error based on the features selected so far. The greedy algorithm is based on an efficient recursive formula for calculating the reconstruction error. Experiments on real data sets demonstrate the effectiveness of the proposed algorithm in comparison with the state-of-the-art methods for unsupervised feature selection.",information system
1983,A survey of the state of the art in learning the kernels,Knowledge and Information Systems,"In recent years, the machine learning community has witnessed a tremendous growth in the development of kernel-based learning algorithms. However, the performance of this class of algorithms greatly depends on the choice of the kernel function. Kernel function implicitly represents the inner product between a pair of points of a dataset in a higher dimensional space. This inner product amounts to the similarity between points and provides a solid foundation for nonlinear analysis in kernel-based learning algorithms. The most important challenge in kernel-based learning is the selection of an appropriate kernel for a given dataset. To remedy this problem, algorithms to learn the kernel have recently been proposed. These methods formulate a learning algorithm that finds an optimal kernel for a given dataset. In this paper, we present an overview of these algorithms and provide a comparison of various approaches to find an optimal kernel. Furthermore, a list of pivotal issues that lead to efficient design of such algorithms will be presented.",information system
1984,Time Management in Workflow Systems,Business Information Systems,"Management of workflow processes is more than just enactment of process activities according to business rules. Time management functionality should be provided to control the lifecycle of processes. Time management should address planning of workflow executions in time, provide various estimates about activity execution durations, avoid violations of deadlines assigned to activities and the entire process, and react to deadline violations when they occur. In this paper we describe how time information can be captured in the workflow definition, and we propose a technique for calculating internal activity deadlines with the goal to meet the overall deadlines during process execution.",information system
1985,Design and Implementation of a Novel Approach to Keyword Searching in Relational Databases,Advances in Databases and Information Systems,"The majority of the tools available for browsing and searching the Web is based on extracting information from structured documents. However, as information on the Web increasingly comes out of a database, it is crucial to be able to search databases when working with the Web. Due to the highly dynamic nature of the Web, it is unlikely ever to know the underlying schemata of those databases. We remedy this situation by introducing an extension of SQL called Reflective SQL (RSQL) which treats data and queries uniformly. Queries are stored in specific program relations and can be evaluated by a LISP-like operator called eval. Program relations cannot only be constructed for given que ries, but their contents can also be generated dynamically based on the current contents of the underlying database. RSQL serves as a basis for a keyword-based search which renders it possible to formulate queries to databases in the absence of schema-knowledge. It is shown how this language can be exploited as a Web search engine that works on databases instead of documents.",information system
1986,A Mobile Robot Platform for Assistance and Entertainment,Information Systems Research," A new generation of mobile robots has recently been designed based on the successful hardware and software architecture of Care-O-bot™ [2]. The robots have been created to communicate with and to entertain visitors in a museum. Their tasks include welcoming visitors, leading a guided tour in the museum, or playing with a ball. ",information system
1987,Mining Constraints for Artful Processes,Business Information Systems,"Artful processes are informal processes typically carried out by those people whose work is mental rather than physical (managers, professors, researchers, engineers, etc.), the so called “knowledge workers”. MailOfMine is a tool, the aim of which is to automatically build, on top of a collection of email messages, a set of workflow models that represent the artful processes laying behind the knowledge workers activities. After an outline of the approach and the tool, this paper focuses on the mining algorithm, able to efficiently compute the set of constraints describing the artful process. Finally, an experimental evaluation of it is reported.",information system
1988,Integrating multiple document features in language models for expert finding,Knowledge and Information Systems,"We argue that expert finding is sensitive to multiple document features in an organizational intranet. These document features include multiple levels of associations between experts and a query topic from sentence, paragraph, up to document levels, document authority information such as the PageRank, indegree, and URL length of documents, and internal document structures that indicate the experts’ relationship with the content of documents. Our assumption is that expert finding can largely benefit from the incorporation of these document features. However, existing language modeling approaches for expert finding have not sufficiently taken into account these document features. We propose a novel language modeling approach, which integrates multiple document features, for expert finding. Our experiments on two large scale TREC Enterprise Track datasets, i.e., the W3C and CSIRO datasets, demonstrate that the natures of the two organizational intranets and two types of expert finding tasks, i.e., key contact finding for CSIRO and knowledgeable person finding for W3C, influence the effectiveness of different document features. Our work provides insights into which document features work for certain types of expert finding tasks, and helps design expert finding strategies that are effective for different scenarios. Our main contribution is to develop an effective formal method for modeling multiple document features in expert finding, and conduct a systematic investigation of their effects. It is worth noting that our novel approach achieves better results in terms of MAP than previous language model based approaches and the best automatic runs in both the TREC2006 and TREC2007 expert search tasks, respectively.",information system
1989,Intelligent data structures selection using neural networks,Knowledge and Information Systems,"It is well known that abstract data types represent the core for any software application, and a proper use of them is an essential requirement for developing a robust and efficient system. Data structures are essential in obtaining efficient algorithms, having a major importance in the software development process. Selecting and creating the appropriate data structure for implementing an abstract data type can greatly impact the performance and the efficiency of the software systems. It is not a trivial problem for a software developer, as it is hard to anticipate all the use scenarios of the deployed application, and a static selection before the system’s execution is, generally, not accurate. In this paper, we are focusing on the problem of dynamic selection of efficient data structures for abstract data types implementation using a supervised learning approach. In order to dynamically select the most suitable representation for an aggregate according to the software system’s current execution context, a neural network will be used. We experimentally evaluate the proposed technique on a case study, emphasizing the advantages of the proposed model in comparison with existing similar approaches.",information system
1990,Diverse dimension decomposition for itemset spaces,Knowledge and Information Systems,"We introduce the problem of diverse dimension decomposition in transactional databases, where a dimension is a set of mutually exclusive itemsets. The problem we consider requires to find a decomposition of the itemset space into dimensions, which are orthogonal to each other and which provide high coverage of the input database. The mining framework we propose can be interpreted as a dimensionality-reducing transformation from the space of all items to the space of orthogonal dimensions. Relying on information-theoretic concepts, we formulate the diverse dimension decomposition problem with a single objective function that simultaneously captures constraints on coverage, exclusivity, and orthogonality. We show that our problem is NP-hard, and we propose a greedy algorithm exploiting the well-known FP-tree data structure. Our algorithm is equipped with strategies for pruning the search space deriving directly from the objective function. We also prove a property that allows assessing the level of informativeness for newly added dimensions, thus allowing to define criteria for terminating the decomposition. We demonstrate the effectiveness of our solution by experimental evaluation on synthetic datasets with known dimension and three real-world datasets, flickr, del.icio.us and dblp. The problem we study is largely motivated by applications in the domain of collaborative tagging; however, the mining task we introduce in this paper is useful in other application domains as well.",information system
1991,Reference Process Models and Systems for Inter-Organizational Ad-Hoc Coordination - Supply Chain Management in Humanitarian Operations,International Conference on Information Systems," In this work we present a general framework for process-oriented coordination and collaboration in humanitarian operations. Process management has been proven useful in many business domains, but humanitarian operations and disaster response management in general require different process management approaches. Related work has only recently introduced traditional process management approaches for emergency management. These traditional approaches have several limitations with respect to the domain of humanitarian operations and disaster management. Our approach points to design, run-time and monitoring of inter-organizational humanitarian logistics processes. It consists of two parts: A reference model for humanitarian logistics tasks and a system for ad-hoc process management of these tasks. We discuss how they can be integrated to provide additional benefits. ",information system
1992,A two-stage gene selection scheme utilizing MRMR filter and GA wrapper,Knowledge and Information Systems,"Gene expression data usually contain a large number of genes, but a small number of samples. Feature selection for gene expression data aims at finding a set of genes that best discriminates biological samples of different types. In this paper, we propose a two-stage selection algorithm for genomic data by combining MRMR (Minimum Redundancy-Maximum Relevance) and GA (Genetic Algorithm). In the first stage, MRMR is used to filter noisy and redundant genes in high-dimensional microarray data. In the second stage, the GA uses the classifier accuracy as a fitness function to select the highly discriminating genes. The proposed method is tested for tumor classification on five open datasets: NCI, Lymphoma, Lung, Leukemia and Colon using Support Vector Machine (SVM) and Naïve Bayes (NB) classifiers. The comparison of the MRMR-GA with MRMR filter and GA wrapper shows that our method is able to find the smallest gene subset that gives the most classification accuracy in leave-one-out cross-validation (LOOCV). © 2010 Springer-Verlag London Limited.",information system
1993,Risk Management and Optimal Pricing in Online Storage Grids,Information Systems Research,"Online storage service providers such as Amazon S3 grant a way for companies, particularly startups, to avoid spending resources on maintaining their own in-house storage infrastructure and thereby allowing them to focus on their core business activities. These providers however follow a fixed, posted pricing strategy which charges the same price in each time period and thus bear all the risk arising out of demand uncertainties faced by their client companies. We examine the effects of providing a spot market with dynamic prices and forward contracts to hedge against future revenue uncertainty. We derive revenue-maximizing spot and forward prices for a single seller facing a known set of buyers. We perform a simulation study using publicly available traffic data regarding Amazon S3 clients from Alexa.com to validate our analytical results. Our field study supports our analysis and indicates that spot markets alone can enhance revenues to Amazon but this comes at the cost of increased risks, due to the increased market share in the spot markets. Furthermore, adding a forward contract feature to the spot markets can reduce risks while still providing the benefits of enhanced revenues. While the buyers incur an increase in costs in the spot market, adding a forward contract does not cause any additional cost increase while transferring the risk to the buyers. Thus storage grid providers can greatly benefit by applying a forward contract alongside the spot market.",information system
1994,The consultancy Game,Knowledge and Information Systems," . The Legal Consultancy Game is the game of giving legal advice professionally. It aims to be a description in general terms of the predictable patterns in an advice transaction between a consultant and a client. It is also a description of what the client demands of good legal advice. There are good reasons to describe consultancy as a game-like activity. Firstly, it shows how legal consultancy relates to legal argumentation. Secondly, it is attractive for very practical reasons to describe legal consultancy as a game that can be played by a computer. It provides us with a prescriptive theory of what legal information retrieval (IR) systems are trying to achieve through various technologies. ",information system
1995,Research Commentary: Rethinking “Diversity” in Information Systems Research,Information Systems Research," T a decade: (a) diversity in the problems addressed; (b) diversity in the theoretical foimdations hree types of diversity have been prominent in the Information Systems discipline for over and reference disciplines used to account for IS phenomena; and (c) diversity in the methods used to collect, analyze, and interpret data. History has played a major part in encouraging IS researchers to use diversity as a means of coimtering criticisms of their discipline and increasing their research rigor and productivity. In particular, frequent recourse to reference disciplines has underpiru\ed much of the research that has been undertaken since the early 1980s. There are now signs, however, that the level of diversity that currently exists in IS research may be problematic. In this paper, we consider some of the benefits and costs of allowing diversity to reign in the IS discipline. We also propose a structure that we hope will facilitate discourse on the benefits and costs of diversity and on the role that diversity should now play in the IS discipline. {Diversity, IS Research, History, Reference Disciplines, Paradigms, Ethics) ",information system
1996,Reconceptualizing System Usage: An Approach and Empirical Test,Information Systems Research,"Although DeLone, McLean, and others insist that system usage is a key variable in information systems research, the system usage construct has received little theoretical scrutiny, boasts no widely accepted definition, and has been operationalized by a diverse set of unsystematized measures. In this article, we present a systematic approach for reconceptualizing the system usage construct in particular nomological contexts. Comprising two stages, definition and selection, the approach enables researchers to develop clear and valid measures of system usage for a given theoretical and substantive context. The definition stage requires that researchers define system usage and explicate its underlying assumptions. In the selection stage, we suggest that system usage be conceptualized in terms of its structure and function. The structure of system usage is tripartite, comprising a user, system, and task, and researchers need to justify which elements of usage are most relevant for their study. In terms of function, researchers should choose measures for each element (i.e., user, system, and/or task) that tie closely to the other constructs in the researchers nomological network. To provide evidence of the viability of the approach, we undertook an empirical investigation of the relationship between system usage and short-run task performance in cognitively engaging tasks. The results support the benefits of the approach and show how an inappropriate choice of usage measures can lead researchers to draw opposite conclusions in an empirical study. Together, the approach and the results of the empirical investigation suggest new directions for research into the nature of system usage, its antecedents, and its consequences. ÂŠ 2006 INFORMS.",information system
1997,Correlation-Based Web Document Clustering for Adaptive Web Interface Design,Knowledge and Information Systems,"A great challenge for web site designers is how to ensure users' easy access to important web pages efficiently. In this paper we present a clustering-based approach to address this problem. Our approach to this challenge is to perform efficient and effective correlation analysis based on web logs and construct clusters of web pages to reflect the co-visit behavior of web site users. We present a novel approach for adapting previous clustering algorithms that are designed for databases in the problem domain of web page clustering, and show that our new methods can generate high-quality clusters for very large web logs when previous methods fail. Based on the high-quality clustering results, we then apply the data-mined clustering knowledge to the problem of adapting web interfaces to improve users' performance. We develop an automatic method for web interface adaptation: by introducing index pages that minimize overall user browsing costs. The index pages are aimed at providing short cuts for users to ensure that users get to their objective web pages fast, and we solve a previously open problem of how to determine an optimal number of index pages. We empirically show that our approach performs better than many of the previous algorithms based on experiments on several realistic web log files.",information system
1998,Architecture of Pattern Management Software System,Advances in Databases and Information Systems, Patterns and models are artifacts that are assets to their owners. Storing them in a repository and using common interfaces for their management simplifies their usage. This article proposes architecture of an extensible repository system that is called Pattern Management Software System (PMSS). It permits metamodel engineer to define abstract syntax of a new modeling or pattern writing language using a metamodel. System registers metamodel in the object-relational database and creates database objects for recording corresponding artifacts and their metadata. Views provide to the users different ways of looking at the data in the database. New views are continuously created. How can user find appropriate views? This article proposes that one component of PMSS must be the subsystem that permits searching and execution of the views. ,information system
1999,Efficient string matching with wildcards and length constraints,Knowledge and Information Systems,"This paper defines a challenging problem of pattern matching between a pattern P and a text T, with wildcards and length constraints, and designs an efficient algorithm to return each pattern occurrence in an online manner. In this pattern matching problem, the user can specify the constraints on the number of wildcards between each two consecutive letters of P and the constraints on the length of each matching substring in T. We design a complete algorithm, SAIL that returns each matching substring of P in T as soon as it appears in T in an O(n+klmg) time with an O(lm) space overhead, where n is the length of T, k is the frequency of P's last letter occurring in T, l is the user-specified maximum length for each matching substring, m is the length of P, and g is the maximum difference between the user-specified maximum and minimum numbers of wildcards allowed between two consecutive letters in P.",information system
2000,Towards Variability Modelling for Reuse in Hypermedia Engineering,Advances in Databases and Information Systems,"In this paper we discuss variability modelling for hypermedia applications. Inspired by domain engineering, we propose a domain engi- neering based method for hypermedia development. Since several adap- tive hypermedia become more and more popular to incorporate different information views for different audience or environments, we believe that it is important to move variability capturing to modelling phases. Several established modelling views of hypermedia application are discussed from the variability point of view. We also explain modelling techniques by means of examples for the application domain view, the navigation view, the presentation view and discuss importance of the user/environment view for parametrisation of components.",information system
2001,PowerDB-IR – Scalable Information Retrieval and Storage with a Cluster of Databases,Knowledge and Information Systems,"Our objective is a scalable infrastructure for information retrieval (IR) with up-to-date retrieval results in the presence of updates. Timely processing of updates is important with novel application domains such as e-commerce. These issues are challenging, given the additional requirement that the system must scale well. We have built PowerDB-IR, a system that has the characteristics sought. This article describes its design, implementation, and evaluation. We follow a three-tier architecture with a database cluster as the bottom layer for storage management. The rationale for a database cluster is to ‘scale out’, i.e., to add further cluster nodes, whenever necessary for better performance. The middle tier provides IR-specific retrieval and update services. We deploy state-of-the-art middleware software to coordinate the cluster and to invoke IR-specific components. PowerDB-IR extends the middleware layer with service decomposition and parallelisation. PowerDB-IR has the following features: It supports state-of-the-art retrieval models such as vector space retrieval. It allows documents to be inserted and retrieved concurrently and ensures up-to-date retrieval results with almost no overhead. PowerDB-IR ensures the correctness of global concurrency and recovery. Alternative physical data organisation schemes and respective query processing techniques provide adequate performance for different workloads and database sizes. Scaling out the database cluster yields higher throughput and lower response times. We have run extensive experiments with PowerDB-IR using several commercial database systems as well as different middleware products. Further experiments have quantified the effect of transactional guarantees on performance. The main result is that PowerDB-IR shows surprisingly good scalability and low response times.",information system
2002,Economics of Free Under Perpetual Licensing: Implications for the Software Industry,Information Systems Research," In this paper, we explore the economics of free under perpetual licensing. In particular, we focus on two emerging software business models that involve a free component: feature-limited freemium (F LF ) and uniform seeding (S). Under F LF , the firm offers the basic software version for free, while charging for premium features. Under S, the firm gives away for free the full product to a percentage of the addressable market uniformly across consumer types. We benchmark their performance against a conventional business model under which software is sold as a bundle (labeled as “charge-for-everything” or CE) without free offers. In the context of consumer bounded rationality and information asymmetry, we develop a unified two-period consumer valuation learning framework that accounts for both word-of-mouth (WOM) effects and experience-based learning, and use it to compare and contrast the three business models. Under both constant and dynamic pricing, for moderate strength of WOM signals, we derive the equilibria for each model and identify optimality regions. In particular, S is optimal when consumers significantly underestimate the value of functionality and cross-module synergies are weak. When either cross-module synergies are stronger or initial priors are higher, the firm decides between CE and F LF . Furthermore, we identify nontrivial switching dynamics from one optimality region to another depending on the initial consumer beliefs about the value of the embedded functionality. For example, there are regions where, ceteris paribus, F LF is optimal when the prior on premium functionality is either relatively low or high, but not in between. We also demonstrate the robustness of our findings with respect to various parameterizations of cross-module synergies, strength of WOM effects, and number of periods. We find that stronger WOM effects or more periods lead to an expansion of the seeding optimality region in parallel with a decrease in the seeding ratio. Moreover, under CE and dynamic pricing, second period price may be decreasing in the initial consumer valuation beliefs when WOM effects are strong and the prior is relatively low. However, this is not the case under weak WOM effects. We also discuss regions where price skimming and penetration pricing are optimal. Our results provide key managerial insights that are useful to firms in their business model search and implementation. ",information system
2003,A hierarchical approach for the redesign of chemical processes,Knowledge and Information Systems,"An approach to improve the management of complexity during the redesign of technical processes is proposed. The approach consists of two abstract steps. In the first step, model-based reasoning is used to generate automatically alternative representations of an existing process at several levels of abstraction. In the second step, process alternatives are generated through the application of case-based reasoning. The key point of our framework is the modeling approach, which is an extension of the Multimodeling and Multilevel Flow Modeling methodologies. These, together with a systematic design methodology, are used to represent a process hierarchically, thus improving the identification of analogous equipment/sections from different processes. The hierarchical representation results in sets of equipment/sections organized according to their functions and intentions. A case-based reasoning system then retrieves from a library of cases similar equipment/sections to the one selected by the user. The final output is a set of equipment/sections ordered according to their similarity. Human intervention is necessary to adapt the most promising case within the original process.",information system
2004,Context-based information analysis for the Web environment,Knowledge and Information Systems,"Finding the relevant set of information that satisfies an information request of a Web user in the availability of today’s vast amount of digital data is becoming a challenging problem. Currently, available Information Retrieval (IR) Systems are designed to return long lists of results, only a few of which are relevant for a specific user. In this paper, an IR method called Context-Based Information Analysis (CONIA) that investigates the context information of the user and user’s information request to provide relevant results for the given domain users is introduced. In this paper, relevance is measured by the semantics of the information provided in the documents. The information extracted from lexical and domain ontologies is integrated by the user’s interest information to expand the terms entered in the request. The obtained set of terms is categorized by a novel approach, and the relations between the categories are obtained from the ontologies. This categorization is used to improve the quality of the document selection by going beyond checking the availability of the words in the document by analyzing the semantic composition of the mapped terms.",information system
2005,Data discretization unification,Knowledge and Information Systems,"Data discretization is defined as a process of converting continuous data attribute values into a finite set of intervals with minimal loss of information. In this paper, we prove that discretization methods based on informational theoretical complexity and the methods based on statistical measures of data dependency are asymptotically equivalent. Furthermore, we define a notion of generalized entropy and prove that discretization methods based on Minimal description length principle, Gini index, AIC, BIC, and Pearson’s X2 and G2 statistics are all derivable from the generalized entropy function. We design a dynamic programming algorithm that guarantees the best discretization based on the generalized entropy notion. Furthermore, we conducted an extensive performance evaluation of our method for several publicly available data sets. Our results show that our method delivers on the average 31% less classification errors than many previously known discretization methods.",information system
2006,A general framework for designing a fuzzy rule-based classifier,Knowledge and Information Systems,"This paper presents a general framework for designing a fuzzy rule-based classifier. Structure and parameters of the classifier are evolved through a two-stage genetic search. To reduce the search space, the classifier structure is constrained by a tree created using the evolving SOM tree algorithm. Salient input variables are specific for each fuzzy rule and are found during the genetic search process. It is shown through computer simulations of four real world problems that a large number of rules and input variables can be eliminated from the model without deteriorating the classification accuracy. By contrast, the classification accuracy of unseen data is increased due to the elimination.",information system
2007,Fast and memory efficient mining of high-utility itemsets from data streams: with and without negative item profits,Knowledge and Information Systems,"Mining utility itemsets from data steams is one of the most interesting research issues in data mining and knowledge discovery. In this paper, two efficient sliding window-based algorithms, MHUI-BIT (Mining High-Utility Itemsets based on BITvector) and MHUI-TID (Mining High-Utility Itemsets based on TIDlist), are proposed for mining high-utility itemsets from data streams. Based on the sliding window-based framework of the proposed approaches, two effective representations of item information, Bitvector and TIDlist, and a lexicographical tree-based summary data structure, LexTree-2HTU, are developed to improve the efficiency of discovering high-utility itemsets with positive profits from data streams. Experimental results show that the proposed algorithms outperform than the existing approaches for discovering high-utility itemsets from data streams over sliding windows. Beside, we also propose the adapted approaches of algorithms MHUI-BIT and MHUI-TID in order to handle the case when we are interested in mining utility itemsets with negative item profits. Experiments show that the variants of algorithms MHUI-BIT and MHUI-TID are efficient approaches for mining high-utility itemsets with negative item profits over stream transaction-sensitive sliding windows.",information system
2008,Ontologies for Knowledge Management: An Information Systems Perspective,Knowledge and Information Systems,"Knowledge management research focuses on concepts, methods, and tools supporting the management of human knowledge. The main objective of this paper is to survey basic concepts that have been used in computer science for the representation of knowledge and summarize some of their advantages and drawbacks. A secondary objective is to relate these techniques to information science theory and practice. The survey classifies the concepts used for knowledge representation into four broad ontological categories. Static ontologies describe static aspects of the world, i.e., what things exist, their attributes and relationships. A dynamic ontology, on the other hand, describes the changing aspects of the world in terms of states, state transitions and processes. Intentional ontologies encompass the world of things agents believe in, want, prove or disprove, and argue about. Finally, social ontologies cover social settings – agents, positions, roles, authority, permanent organizational structures or shifting networks of alliances and interdependencies.",information system
2009,Prototypical Implementation of a Pragmatic Approach to Semantic Web Service Discovery during Process Execution,Business Information Systems,"The usage of semantic web services in enterprise computing promises many advantages. Instead of hard-wiring web services in business processes during design-time, business experts just specify the expected functionality and matching semantic web services are bound automatically during runtime. However, the introduction of semantic web services requires the replacement or extension of existing middleware and standards. We suggest a more realistic approach based on existing standards like BPEL. We provide a prototypical implementation as a proof of concept. Our solution helps to accelerate the adoption of semantic technologies in business process management and provides researchers with early feedback.",information system
2010,Learning Feature Weights from Customer Return-Set Selections,Knowledge and Information Systems," This paper describes LCW, a procedure for learning customer preferences represented as feature weights by observing customers' selections from return sets. An empirical evaluation on simulated customer behavior indicated that uninformed hypotheses about customer weights lead to low ranking accuracy unless customers place some importance on almost all features or the total number of features is quite small. In contrast, LCW's estimate of the mean preferences of a customer population improved as the number of customers increased, even for larger numbers of features of widely differing importance. This improvement in the estimate of mean customer preferences led to improved prediction of individual customer's rankings, irrespective of the extent of variation among customers and whether a single or multiple retrievals were permitted. The experimental results suggest that the return set that optimizes benefit may be smaller for customer populations with little variation than for customer populations with wide variation. ",information system
2011,Mining Various Patterns in Sequential Data in an SQL-like Manner,Advances in Databases and Information Systems," One of the most important data mining tasks is discovery of frequently occurring patterns in sequences of events. Many algorithms for finding various patterns in sequential data have been proposed recently. Researchers concentrated on different classes of patterns, which resulted in many different models and formulations of the problem. In this paper a uniform formulation of the problem of mining frequent patterns in sequential data is provided together with an SQL-like language capable of expressing queries concerning all classes of patterns. An issue of materializing discovered patterns for further selective analysis is also addressed by introducing a concept of knowledge snapshots. ",information system
2012,Emergence of Cooperative Internet Server Sharing Among Internet Search Agents Caught in the n -Person Prisoner’s Dilemma Game,Knowledge and Information Systems,"Information on the Internet can be collected by autonomous agents that send out queries to the servers that may have the information sought. From a single agent’s perspective, sending out as many queries as possible maximizes the chances of finding the information sought. However, if every agent does the same, the servers will be overloaded. The first major contribution of this paper is proving mathematically that the agents situated in such environments play the n-Person Prisoner’s Dilemma Game. The second is mathematically deriving the notion of effectiveness of cooperation among the agents in such environments and then presenting the optimal interval for the number of information sites for a given number of information-seeking agents. When the optimal interval is satisfied, cooperation among agents is effective, meaning that resources (e.g., servers) are optimally shared. Experimental results suggest that agents can better share available servers through the kinship-based cooperation without explicitly knowing about the entire environment. This paper also identifies difficulties of promoting cooperation in such environments and presents possible solutions. The long-term goal of this research is to elucidate the understanding of massively distributed multiagent environments such as the Internet and to identify valuable design principles of software agents in similar environments.",information system
2013,A weighted voting framework for classifiers ensembles,Knowledge and Information Systems,"We propose a probabilistic framework for classifier combination, which gives rigorous optimality conditions (minimum classification error) for four combination methods: majority vote, weighted majority vote, recall combiner and the naive Bayes combiner. The framework is based on two assumptions: class-conditional independence of the classifier outputs and an assumption about the individual accuracies. The four combiners are derived subsequently from one another, by progressively relaxing and then eliminating the second assumption. In parallel, the number of the trainable parameters increases from one combiner to the next. Simulation studies reveal that if the parameter estimates are accurate and the first assumption is satisfied, the order of preference of the combiners is: naive Bayes, recall, weighted majority and majority. By inducing label noise, we expose a caveat coming from the stability-plasticity dilemma. Experimental results with 73 benchmark data sets reveal that there is no definitive best combiner among the four candidates, giving a slight preference to naive Bayes. This combiner was better for problems with a large number of fairly balanced classes while weighted majority vote was better for problems with a small number of unbalanced classes.",information system
2014,Data Mining: How Research Meets Practical Development?,Knowledge and Information Systems,"Abstract. At the 2001 IEEE International Conference on Data Mining in San Jose, California, on November 29 to December 2, 2001, there was a panel discussion on how data mining research meets practical development. One of the motivations for organizing the panel discussion was to provide useful advice for industrial people to explore their directions in data mining development. Based on the panel discussion, this paper presents the views and arguments from the panel members, the Conference Chair and the Program Committee Co-Chairs. These people as a group have both academic and industrial experiences in different data mining related areas such as databases, machine learning, and neural networks. We will answer questions such as (1) how far data mining is from practical development, (2) how data mining research differs from practical development, and (3) what are the most promising areas in data mining for practical development.",information system
2015,Window update patterns in stream operators,Advances in Databases and Information Systems,"Continuous queries applied over nonterminating data streams usually specify windows in order to obtain an evolving –yet restricted– set of tuples and thus provide timely results. Among other typical variants, sliding windows are mostly employed in stream processing engines and several advanced techniques have been suggested for their incremental evaluation. In this paper, we set out to study the existence of monotonic-related semantics in windowing constructs towards a more efficient maintenance of their changing contents. We investigate update patterns observed in common window variants as well as their impact on windowed adaptations of typical operators (like selection, join or aggregation), offering more insight towards design and implementation of stream processing mechanisms. Finally, to demonstrate its significance, this framework is validated for several windowed operations against streaming datasets with simulations at diverse arrival rates and window sizes.",information system
2016,Designing Persistence for Real-Time Distributed Object Systems,Advances in Databases and Information Systems,"An implementation of persistent object store for real-time systems with strict processing time constraints is a challenging task, because many traditional database techniques, e.g. transaction management schemes, are not applicable for such systems. This paper examines technical and business requirements for one particular class of such systems and describes an architecture based on distributed shared virtual memory. The major contributions are: use of distributed dynamic hashing to achieve load balancing and tight coupling of transaction and virtual memory management, which allows local scheduling of read-only transactions.",information system
2017,An ontology-mediated validation process of software models,International Conference on Information Systems," When errors in software modelling activities propagate to later phases of software development lifecycle, they become costlier to fix and lower the quality of the final product. Early validation of software models can prevent rework and incorrect development non-compliant with client's specification. In this paper we advocate the use of ontologies to validate and improve the quality of software models as they are being developed, at the same time bridging the traditional gap between developers and clients. We propose a general ontology-mediated process to validate software models that can be adapted in a broad range of software development projects. We illustrate this for Multi-Agent Systems (MAS) development providing early evidence of the soundness of our approach. We successfully validate and improve the quality of MAS models for a real-life development project, illustrating the ontology-mediated models validation in a commercial setting. ",information system
2018,KEMM: A Knowledge Engineering Methodology in the Medical Domain,Formal Ontology in Information Systems, THESEUS-MEDICO ,information system
2019,Summarization – compressing data into an informative representation,Knowledge and Information Systems,"In this paper, we formulate the problem of summarization of a dataset of transactions with categorical attributes as an optimization problem involving two objective functions - compaction gain and information loss. We propose metrics to characterize the output of any summarization algorithm. We investigate two approaches to address this problem. The first approach is an adaptation of clustering and the second approach makes use of frequent item sets from the association analysis domain. We illustrate one application of summarization in the field of network data where we show how our technique can be effectively used to summarize network traffic into a compact but meaningful representation. Specifically, we evaluate our proposed algorithms on the 1998 DARPA Off-line Intrusion Detection Evaluation data and network data generated by SKAION Corp for the ARDA information assurance program.",information system
2020,On clustering massive text and categorical data streams,Knowledge and Information Systems," In this paper, we will study the data stream clustering problem in the context of text and categorical data domains. While the clustering problem has been studied recently for numeric data streams, the problems of text and categorical data present different challenges because of the large and un-ordered nature of the corresponding attributes. Therefore, we will propose algorithms for text and categorical data stream clustering. We will propose a condensation based approach for stream clustering which summarizes the stream into a number of fine grained cluster droplets. These summarized droplets can be used in conjunction with a variety of user queries to construct the clusters for different input parameters. Thus, this provides an online analytical processing approach to stream clustering. We also study the problem of detecting noisy and outlier records in real time. We will test the approach for a number of real and synthetic data sets, and show the effectiveness of the method over the baseline OSKM algorithm for stream clustering. ",information system
2021,A Review of Delegation and Break-Glass Models for Flexible Access Control Management,Business Information Systems,"Access control models provide important means for the systematic specification and management of the permissions in a business information system. While there are may well-known access control models (e.g., RBAC), standard access control models are often not suited for handling exceptional situations. The demand to increase the flexibility of access management has been approached mainly via the development of delegation models and break-glass models. This paper presents the results of a literature review of 329 delegation and break-glass approaches. We give an overview on the existing body of scientific literature in these two areas and compare 35 selected approaches in detail. We reveal different ways of providing delegation and break-glass concepts in general as well as in the context of business process management. Moreover, we identify different sub-topics that have not yet been addressed in detail and thus provide opportunities for future research.",information system
2022,Use of RDF for expertise matching within academia,Knowledge and Information Systems,"Organisations have realized that effective development and management of their organisational knowledge base is critical to survival in today’s competitive business environment. Employees, as a special knowledge asset, also attract the interest of many researchers because only through people communicating with one another can they really share their tacit knowledge and skills, which are often more valuable than sharing explicit documentations. The need to be able to quickly locate experts among the heterogeneous data sources stored in the organisational memory has been recognized by many researchers. This paper examines the advantages of using RDF (resource description framework) for expertise matching—the process of finding an individual with the required knowledge and skills. The major challenge is to semantically integrate multiple expertise indications from heterogeneous data sources stored in the organisational memory in order to facilitate users to locate the right expert(s). To understand the issues, a case study has been performed that involves building a RDF-based expertise broker that helps Ph.D. applicants locate potential supervisors before they formally apply to a university. An evaluation of the brokering system has been conducted through an experiment and the key results are presented. An extended brokering system that would support expertise matching in a multidisciplinary context is also described and further research identified.",information system
2023,Faceted Wikipedia Search,Business Information Systems,"Wikipedia articles contain, besides free text, various types of structured information in the form of wiki markup. The type of wiki content that is most valuable for search are Wikipedia infoboxes, which display an article’s most relevant facts as a table of attribute-value pairs on the top right-hand side of the Wikipedia page. Infobox data is not used by Wikipedia’s own search engine. Standard Web search engines like Google or Yahoo also do not take advantage of the data. In this paper, we present Faceted Wikipedia Search, an alternative search interface for Wikipedia, which facilitates infobox data in order to enable users to ask complex questions against Wikipedia knowledge. By allowing users to query Wikipedia like a structured database, Faceted Wikipedia Search helps them to truly exploit Wikipedia’s collective intelligence.",information system
2024,Query-dependent cross-domain ranking in heterogeneous network,Knowledge and Information Systems,"Traditional learning-to-rank problem mainly focuses on one single type of objects. However, with the rapid growth of the Web 2.0, ranking over multiple interrelated and heterogeneous objects becomes a common situation, e.g., the heterogeneous academic network. In this scenario, one may have much training data for some type of objects (e.g. conferences) while only very few for the interested types of objects (e.g. authors). Thus, the two important questions are: (1) Given a networked data set, how could one borrow supervision from other types of objects in order to build an accurate ranking model for the interested objects with insufficient supervision? (2) If there are links between different objects, how can we exploit their relationships for improved ranking performance? In this work, we first propose a regularized framework called HCDRank to simultaneously minimize two loss functions related to these two domains. Then, we extend the approach by exploiting the link information between heterogeneous objects. We conduct a theoretical analysis to the proposed approach and derive its generalization bound to demonstrate how the two related domains could help each other in learning ranking functions. Experimental results on three different genres of data sets demonstrate the effectiveness of the proposed approaches.",information system
2025,Reopening the Black Box of Technology Artifacts and Human Agency,International Conference on Information Systems," The argument presented in this article is that the premises governing human-technology interaction partly derive from the distinctive ways by which each technology defines a domain of reference, and organizes and codifies knowledge and experience within it. While social in its origins and its implications, technology constitutes a distinct realm of human experience that is not reducible to social or institutional relations. Drawing on Goodmans (1976, 1978) cognitive philosophy the article proposes a scheme for analyzing the very architecture of items and relations underlying the constitution of cognition-based artifacts. Such an analysis is used as a basis for inferring the malleability and negotiability of technologies and the forms by which they admit human involvement and participation. ",information system
2026,Cost estimation for queries experiencing multiple contention states in dynamic multidatabase environments,Knowledge and Information Systems," Accurate query cost estimation is crucial to query optimization in a multidatabase system. Several estimation techniques for a static environment have been suggested in the literature. To develop a cost model for a dynamic environment, we recently introduced a multistate query-sampling method. It has been shown that this technique is promising in estimating the cost of a query run in any given contention state for a dynamic environment. In this paper, we study a new problem on how to estimate the cost of a large query that may experience multiple contention states. Following the discussion of limitations for two simple approaches, i.e., single state analysis and average cost analysis, we propose two novel techniques to tackle this challenge. The rst one, called fractional analysis, is suitable for a gradually and smoothly changing environment, while the second one, called the probabilistic approach, is developed for a rapidly and randomly changing environment. The former estimates a query cost by analyzing its fractions, and the latter estimates a query cost based on Markov chain theory. The related issues including cost formula development, error analysis, and comparison among di erent approaches are discussed. Experiments demonstrate that the proposed techniques are quite promising in solving the new problem. ",information system
2027,A binary decision diagram based approach for mining frequent subsequences,Knowledge and Information Systems,"Sequential pattern mining is an important problem in data mining. State of the art techniques for mining sequential patterns, such as frequent subsequences, are often based on the pattern-growth approach, which recursively projects conditional databases. Explicitly creating database projections is thought to be a major computational bottleneck, but we will show in this paper that it can be beneficial when the appropriate data structure is used. Our technique uses a canonical directed acyclic graph as the sequence database representation, which can be represented as a binary decision diagram (BDD). In this paper, we introduce a new type of BDD, namely a sequence BDD (SeqBDD), and show how it can be used for efficiently mining frequent subsequences. A novel feature of the SeqBDD is its ability to share results between similar intermediate computations and avoid redundant computation. We perform an experimental study to compare the SeqBDD technique with existing pattern growth techniques, that are based on other data structures such as prefix trees. Our results show that a SeqBDD can be half as large as a prefix tree, especially when many similar sequences exist. In terms of mining time, it can be substantially more efficient when the support is low, the number of patterns is large, or the input sequences are long and highly similar.",information system
2028,Investigating Weblogs in Small and Medium Enterprises: An Exploratory Case Study,Business Information Systems," Contrary to a Wiki where the opinion of the individual user disappears in favor of a more impartial 'collective intelligence', a weblog is author-centered, expressing the author's subjective point of view. This particular property of weblogs played a fundamental role for the popularity weblogs gained for making implicit knowledge explicit in an unsolicited, selforganized way. However, empirical studies from academia exploring internal corporate weblogs remain scarce, especially when they focus on small and medium enterprises (SMEs) which make up the majority of all enterprises worldwide. To counteract this lack of research, we investigate an internal corporate weblog in an ICT SME from a knowledge management perspective. We derive both research questions and hypotheses to test within future studies. Furthermore, we consider already gained findings from corporate weblog research and investigate their immediate applicability in the context of SMEs. ",information system
2029,Counting triangles in real-world networks using projections,Knowledge and Information Systems," Triangle counting is an important problem in graph mining. Two frequently used metrics in complex network analysis which require the count of triangles are the clustering coefficients and the transitivity ratio of the graph. Triangles have been used successfully in several real-world applications, such as detection of spamming activity, uncovering the hidden thematic structure of the web and link recommendation in online social networks. Furthermore, the count of triangles is a frequently used network statistic in exponential random graph models. However, counting the number of triangles in a graph is computationally expensive. In this paper, we propose the EigenTriangle and EigenTriangleLocal algorithms to estimate the number of triangles in a graph. The efficiency of our algorithms is based on the special spectral properties of real-world networks, which allow us to approximate accurately the number of triangles. We verify the efficacy of our method experimentally in almost 160 experiments using several Web Graphs, social, co-authorship, information and Internet networks where we obtain significant speedups with respect to a straight-forward triangle counting algorithm. Furthermore, we propose FastSVD, an algorithm which allows us to apply the core idea of the EigenTriangle algorithm on graphs which do not fit in the main memory. The main idea is a simple node sampling process according to which node i is selected di where di is the degree of node i and m is the total number of edges with probability 2m in the graph. Our theoretical contributions also include a theorem which gives a closed formula for the number of triangles in Kronecker graphs, a model of networks which mimics several properties of real-world networks. ",information system
2030,Approximate Functional Dependencies for XML Data,Advances in Databases and Information Systems," Functional dependencies (FDs) are an integral part of database theory since they are used in integrity enforcement and in database design. Recently, functional dependencies satis¯ed by XML data (XFDs) have been introduced. In this work approximate functional dependencies that are XFDs approximately satis¯ed by a considerable part of the XML database are de¯ned and the problem of inferring such XFDs is addressed. ",information system
2031,Using Transaction Prices to Re-Examine Price Dispersion in Electronic Markets,Information Systems Research," irsbe isson Ppotential to reduce transaction and search costs, thereby creating more efficient, “frictionless” markets, as rice dispersion is an important indicator of market efficiency. Internet-based electronic markets have the predicted by theories in information economics. However, earlier work has reported significant levels of price dispersion on the Internet, which is in contrast to theoretical predictions. A key feature of the existing stream of work has been its use of posted prices to estimate price dispersion. In theory, this can lead to an overestimation of price dispersion because a sale may not have occurred at the posted price. In this research, we use a unique data set of actual transaction prices collected from both the electronic and offline markets of buyers in a businessto-business market to evaluate the extent of price dispersion. We find that price dispersion in the electronic market is as low as 0.22%, which is substantially less than that reported in the existing literature. This near-zero price dispersion suggests that in some electronic markets the “law of one price” can prevail when we consider transaction prices, instead of posted prices. We further develop a theoretical framework that identifies several new drivers of price dispersion using transaction data. In particular, we focus on four product-level and market-level attributes-product cost, order cycle time, own price elasticity, and transaction quantity, and we estimate their impact on price dispersion. We also examine the electronic market's moderating role in the relationship between these drivers and price dispersion. Finally, we estimate the efficiency gains that accrue from transactions in the relatively friction-free market and find that the electronic market can enhance consumer surplus by as much as $97.92 million per year. ",information system
2032,Extending the Gemstone Smalltalk Interface Debugger to cope with Active Database Components,Advances in Databases and Information Systems, British Computer Society CBS ,information system
2033,Hiding sensitive knowledge without side effects,Knowledge and Information Systems,"Sensitive knowledge hiding in large transactional databases is one of the major goals of privacy preserving data mining. However, it is only recently that researchers were able to identify exact solutions for the hiding of knowledge, depicted in the form of sensitive frequent itemsets and their related association rules. Exact solutions allow for the hiding of vulnerable knowledge without any critical compromises, such as the hiding of nonsensitive patterns or the accidental uncovering of infrequent itemsets, amongst the frequent ones, in the sanitized outcome. In this paper, we highlight the process of border revision, which plays a significant role towards the identification of exact hiding solutions, and we provide efficient algorithms for the computation of the revised borders. Furthermore, we review two algorithms that identify exact hiding solutions, and we extend the functionality of one of them to effectively identify exact solutions for a wider range of problems (than its original counterpart). Following that, we introduce a novel framework for decomposition and parallel solving of hiding problems, which are handled by each of these approaches. This framework improves to a substantial degree the size of the problems that both algorithms can handle and significantly decreases their runtime. Through experimentation, we demonstrate the effectiveness of these approaches toward providing high quality knowledge hiding solutions.",information system
2034,Integrating Bibliographical Data from Heterogeneous Digital Libraries,Advances in Databases and Information Systems," The integration of bibliographical data today is considered one of the most important tasks in the area of digital libraries. Various available sources of bibliographical information vary widely in terms of data representation and access interfaces. To overcome this heterogeneity during the last years attempts were made to apply methods developed for information system integration, like federated databases and mediators. In this paper we describe our approach using the loosely coupled federated system FRAQL. Furthermore, we present a generic adapter that can be used in highly distributed scenarios which uses XML and related technology for transfer and homogenization of data. As an application scenario we describe global citation linking for integrated digital libraries. ",information system
2035,Special issue on big data research in China,Knowledge and Information Systems," On March 5-7, 2013, the National Natural Science Foundation of China (NSFC) organized the 89th Shuangqing Forum in Tongji University (Shanghai, China) on “Challenging Scientific Problems in Big Data Technologies and Applications” [1]. Big data refers to dynamic information that is generated in complex systems and has the characteristics of huge quantity, continuous sampling, multiple sources, and sparse values [2]. Big data has attracted tremendous attention from academia, industry, and the government both within China and internationally. Big data research seeks to extract useful information from massive data and to use it to facilitate our decision making [3]. In the future, big data technologies are expected to make a full use of public data resources to realize digital and intelligent transformations in areas such as traffic management, logistics, health care, and education [4]. However, the research on big data technologies still has many challenges. Therefore, this Springer KAIS special issue invites original research work on Big Data in China from both the invited speakers from the above NSFC forum and other established researchers in this field. We aim to bring together innovative designs, revolutionary ideas, and emerging applications of big data efforts. For this special issue, from all invited and regular submissions, only nine were accepted for publication. The first paper, by Xu et al., proposed an automatic annotating technique based on the analysis of streaming social interactions of media content. This method first iteratively loads the streaming records to build the preference-sensitive subgraphs, then extracts static ",information system
2036,"Location privacy: going beyond K-anonymity, cloaking and anonymizers",Knowledge and Information Systems,"With many location-based services, it is implicitly assumed that the location server receives actual users locations to respond to their spatial queries. Consequently, information customized to their locations, such as nearest points of interest can be provided. However, there is a major privacy concern over sharing such sensitive information with potentially malicious servers, jeopardizing users’ private information. The anonymity- and cloaking-based approaches proposed to address this problem cannot provide stringent privacy guarantees without incurring costly computation and communication overhead. Furthermore, they require a trusted intermediate anonymizer to protect user locations during query processing. This paper proposes a fundamental approach based on private information retrieval to process range and K-nearest neighbor queries, the prevalent queries used in many location-based services, with stronger privacy guarantees compared to those of the cloaking and anonymity approaches. We performed extensive experiments on both real-world and synthetic datasets to confirm the effectiveness of our approaches.",information system
2037,Parallel randomized sampling for support vector machine (SVM) and support vector regression (SVR),Knowledge and Information Systems,"A parallel randomized support vector machine (PRSVM) and a parallel randomized support vector regression (PRSVR) algorithm based on a randomized sampling technique are proposed in this paper. The proposed PRSVM and PRSVR have four major advantages over previous methods. (1) We prove that the proposed algorithms achieve an average convergence rate that is so far the fastest bounded convergence rate, among all SVM decomposition training algorithms to the best of our knowledge. The fast average convergence bound is achieved by a unique priority based sampling mechanism. (2) Unlike previous work (Provably fast training algorithm for support vector machines, 2001) the proposed algorithms work for general linear-nonseparable SVM and general non-linear SVR problems. This improvement is achieved by modeling new LP-type problems based on Karush–Kuhn–Tucker optimality conditions. (3) The proposed algorithms are the first parallel version of randomized sampling algorithms for SVM and SVR. Both the analytical convergence bound and the numerical results in a real application show that the proposed algorithm has good scalability. (4) We present demonstrations of the algorithms based on both synthetic data and data obtained from a real word application. Performance comparisons with SVMlight show that the proposed algorithms may be efficiently implemented.",information system
2038,Models of distributed data clustering in peer-to-peer environments,Knowledge and Information Systems,"Distributed data mining applies techniques to mine distributed data sources by avoiding the need to first collect the data into a central site. This has a significant appeal when issues of communication cost and privacy put a restriction on traditional centralized methods. Although there has been development on many fronts in distributed data mining, we are still lacking models that abstract the process by showing similarities and contrasts between the different methods. In this paper, we introduce two abstract models for distributed clustering in peer-to-peer environments with different goals. The first is the Locally optimized Distributed Clustering (LDC) model, which aims toward achieving better local clusters at each node, and is facilitated by collaboration through sharing of summarized cluster information. The second is the Globally optimized Distributed Clustering (GDC) model, which aims toward achieving one global clustering solution that is an approximation of centralized clustering. We also report on concrete realizations of the two models that show their benefits, through application in text mining. The LDC model is realized through the Collaborative P2P Clustering algorithm, while the GDC model is realized through the Hierarchically distributed P2P Clustering algorithm. In the former, we show that peer collaboration results in significant increase in local clustering quality. The process utilizes cluster summarization to exchange information between peers. In the latter, we target scalability by structuring the P2P network hierarchically and devise a distributed variant of the k-means algorithm to compute one set of clusters across the hierarchy. We demonstrate through experimental results the effectiveness of both methods and make recommendation on when to use each method.",information system
2039,Privacy-preserving hybrid collaborative filtering on cross distributed data,Knowledge and Information Systems,"Data collected for collaborative filtering (CF) purposes might be cross distributed between two online vendors, even competing companies. Such corporations might want to integrate their data to provide more precise and reliable recommendations. However, due to privacy, legal, and financial concerns, they do not desire to disclose their private data to each other. If privacy-preserving measures are introduced, they might decide to generate predictions based on their distributed data collaboratively. In this study, we investigate how to offer hybrid CF-based referrals with decent accuracy on cross distributed data (CDD) between two e-commerce sites while maintaining their privacy. Our proposed schemes should prevent data holders from learning true ratings and rated items held by each other while still allowing them to provide accurate CF services efficiently. We perform real data-based experiments to evaluate our proposals in terms of accuracy. The results show that the proposed methods are able to provide precise predictions. Moreover, we analyze our schemes in terms of privacy and supplementary costs. We demonstrate that our schemes are secure, and online overhead costs due to privacy concerns are insignificant.",information system
2040,A non-parametric semi-supervised discretization method,Knowledge and Information Systems,"Semi-supervised classification methods aim to exploit labeled and unlabeled examples to train a predictive model. Most of these approaches make assumptions on the distribution of classes. This article first proposes a new semi-supervised discretization method, which adopts very low informative prior on data. This method discretizes the numerical domain of a continuous input variable, while keeping the information relative to the prediction of classes. Then, an in-depth comparison of this semi-supervised method with the original supervised MODL approach is presented. We demonstrate that the semi-supervised approach is asymptotically equivalent to the supervised approach, improved with a post-optimization of the intervals bounds location.",information system
2041,Information Technology and Firm Boundaries: Evidence From Panel Data,Information Systems Research," Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions This article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org. ",information system
2042,Specifying Separation of Duty Constraints in BPEL4People Processes,Business Information Systems,"Security issues have to be carefully considered for information systems that support the business processes of an organization, in particular, when these systems build on open interfaces such as web services. In this paper, we examine the new BPEL extension BPEL4People from an access control perspective. In particular, we discuss the importance of “separation of duty” constraints and identify options to specify such constraints in BPEL4People processes. Moreover, we identify and discuss shortcomings of the BPEL4People specifications that complicate and/or impede separation of duty enforcement. In addition, we suggest solutions which can be introduced into future versions of BPEL4People to mitigate those shortcomings.",information system
2043,An agent oriented ontology of social reality,Formal Ontology in Information Systems," In this paper we introduce an ontology based on the notion of agent to represent and reason about social reality. We model social constructions as agents, for example, groups, organizations, normative systems, and roles, and we attribute mental attitudes to them. Roughly, we define obligations or regulative norms as goals of the normative system, constitutive norms as beliefs of the normative system, joint, shared, mutual and social beliefs, desires and goal as beliefs, desires and goals of group, responsibilities of an agent as goals of the role he plays, and the required expertise of an agent as beliefs and actions of the role he plays. In this way, we achieve a uniform framework for a large variety of concepts using a small vocabulary, and, in particular, basing it on notions, like mental attitudes, which are commonly used in agent theories. The proposed ontology is modelled using a description logic. ",information system
2044,Social Capital and Knowledge Integration in Digitally Enabled Teams,Information Systems Research," Tteams, we studied 46 teams who had a history and a future working together. All three dimensions of their o understand the impact of social capital on knowledge integration and performance within digitally enabled social capital (structural, relational, and cognitive) were measured prior to the team performing two tasks in a controlled setting, one face-to-face and the other through a lean digital network. Structural and cognitive capital were more important to knowledge integration when teams communicated through lean digital networks than when they communicated face-to-face; relational capital directly impacted knowledge integration equally, regardless of the communication media used by the team. Knowledge integration, in turn, impacted team decision quality, suggesting that social capital influences team performance in part by increasing a team's ability to integrate knowledge. These results suggest that team history may be necessary but not sufficient for teams to overcome the problems with the use of lean digital networks as a communication environment. However, team history may present a window of opportunity for social capital to develop, which in turn allows teams to perform just as well as in either communication environment. ",information system
2045,Rule-based composite event queries: the language XChangeEQ and its semantics,Knowledge and Information Systems,"Web systems, Web services, and Web-based publish/subscribe systems communicate events as XML messages and in many cases, require composite event detection: it is not sufficient to react to single event messages, but events have to be considered in relation to other events that are received over time. This entails a need for expressive, high-level languages for querying composite events. Emphasizing language design and formal semantics, we describe the rule-based composite event query language XChangeEQ. XChangeEQ is designed to completely cover and integrate the four complementary querying dimensions: event data, event composition, temporal relationships, and event accumulation. Semantics are provided as a model theory with accompanying fixpoint theory, an approach that is established for rule languages but has not been applied to event queries so far. Because they are highly declarative, thus easy to understand and well suited for query optimization, such semantics are desirable for event queries.",information system
2046,Approximate data instance matching: a survey,Knowledge and Information Systems,"Approximate data matching is a central problem in several data management processes, such as data integration, data cleaning, approximate queries, similarity search and so on. An approximate matching process aims at defining whether two data represent the same real-world object. For atomic values (strings, dates, etc), similarity functions have been defined for several value domains (person names, addresses, and so on). For matching aggregated values, such as relational tuples and XML trees, approaches alternate from the definition of simple functions that combine values of similarity of record attributes to sophisticated techniques based on machine learning, for example. For complex data comparison, including structured and semistructured documents, existing approaches use both structure and data for the comparison, by either considering or not considering data semantics. This survey presents terminology and concepts that base approximated data matching, as well as discusses related work on the use of similarity functions in such a subject.",information system
2047,Kingfisher: an efficient algorithm for searching for both positive and negative dependency rules with statistical significance measures,Knowledge and Information Systems,"Statistical dependency analysis is the basis of all empirical science. A commonly occurring problem is to find the most significant dependency rules, which describe either positive or negative dependencies between categorical attributes. In medical science, for example, one is interested in genetic factors, which can either predispose or prevent diseases. The requirement of statistical significance is essential, because the discoveries should hold also in future data. Typically, the significance is estimated either by Fisher’s exact test or the χ2-measure. The problem is computationally very difficult, because the number of all possible dependency rules increases exponentially with the number of attributes. As a solution, different kinds of restrictions and heuristics have been applied, but a general, scalable search method has been missing. In this paper, we introduce an efficient algorithm, called Kingfisher, for searching for the best non-redundant dependency rules with statistical significance measures. The rules can express either positive or negative dependencies between a set of positive attributes and a single consequent attribute. The algorithm itself is independent from the used goodness measure, but we concentrate on Fisher’s exact test and the χ2-measure. The algorithm is based on an application of the branch-and-bound search strategy, supplemented by several pruning properties. Especially, we prove a new lower bound for Fisher’s p and introduce a new effective pruning principle. According to our experiments on classical benchmark data, the algorithm is well scalable and can efficiently handle even dense and high-dimensional data sets. An interesting observation was that Fisher’s exact test did not only produce more reliable rules than the χ2-measure, but it also performed the search much faster.",information system
2048,Risks and Risk Management in ERP Project - Cases in SME Context,Business Information Systems," Enterprise resource planning (ERP) projects are considered to be expensive, time-consuming, difficult to manage and risky. This paper presents how companies should consider and manage the risks in their ERP project. The focus in a qualitative case study is on small and medium-sized enterprises (SMEs) and it illustrates how the risks in the case firms are identified, analyzed and managed. ",information system
2049,Characterizing and Mining the Citation Graph of the Computer Science Literature,Knowledge and Information Systems,"Citation graphs representing a body of scientific literature convey measures of scholarly activity and productivity. In this work we present a study of the structure of the citation graph of the computer science literature. Using a web robot we built several topic-specific citation graphs and their union graph from the digital library ResearchIndex. After verifying that the degree distributions follow a power law, we applied a series of graph theoretical algorithms to elicit an aggregate picture of the citation graph in terms of its connectivity. We discovered the existence of a single large weakly-connected and a single large biconnected component, and confirmed the expected lack of a large strongly-connected component. The large components remained even after removing the strongest authority nodes or the strongest hub nodes, indicating that such tight connectivity is widespread and does not depend on a small subset of important nodes. Finally, minimum cuts between authority papers of different areas did not result in a balanced partitioning of the graph into areas, pointing to the need for more sophisticated algorithms for clustering the graph.",information system
2050,Geographic knowledge extraction and semantic similarity in OpenStreetMap,Knowledge and Information Systems,"In recent years, a web phenomenon known as Volunteered Geographic Information (VGI) has produced large crowdsourced geographic data sets. OpenStreetMap (OSM), the leading VGI project, aims at building an open-content world map through user contributions. OSM semantics consists of a set of properties (called ‘tags’) describing geographic classes, whose usage is defined by project contributors on a dedicated Wiki website. Because of its simple and open semantic structure, the OSM approach often results in noisy and ambiguous data, limiting its usability for analysis in information retrieval, recommender systems and data mining. Devising a mechanism for computing the semantic similarity of the OSM geographic classes can help alleviate this semantic gap. The contribution of this paper is twofold. It consists of (1) the development of the OSM Semantic Network by means of a web crawler tailored to the OSM Wiki website; this semantic network can be used to compute semantic similarity through co-citation measures, providing a novel semantic tool for OSM and GIS communities; (2) a study of the cognitive plausibility (i.e. the ability to replicate human judgement) of co-citation algorithms when applied to the computation of semantic similarity of geographic concepts. Empirical evidence supports the usage of co-citation algorithms—SimRank showing the highest plausibility—to compute concept similarity in a crowdsourced semantic network.",information system
2051,Mining incomplete survey data through classification,Knowledge and Information Systems,"Data mining with incomplete survey data is an immature subject area. Mining a database with incomplete data, the patterns of missing data as well as the potential implication of these missing data constitute valuable knowledge. This paper presents the conceptual foundations of data mining with incomplete data through classification which is relevant to a specific decision making problem. The proposed technique generally supposes that incomplete data and complete data may come from different sub-populations. The major objective of the proposed technique is to detect the interesting patterns of data missing behavior that are relevant to a specific decision making, instead of estimation of individual missing value. Using this technique, a set of complete data is used to acquire a near-optimal classifier. This classifier provides the prediction reference information for analyzing the incomplete data. The data missing behavior concealed in the missing data is then revealed. Using a real-world survey data set, the paper demonstrates the usefulness of this technique.",information system
2052,A countably infinite mixture model for clustering and feature selection,Knowledge and Information Systems,"Mixture modeling is one of the most useful tools in machine learning and data mining applications. An important challenge when applying finite mixture models is the selection of the number of clusters which best describes the data. Recent developments have shown that this problem can be handled by the application of non-parametric Bayesian techniques to mixture modeling. Another important crucial preprocessing step to mixture learning is the selection of the most relevant features. The main approach in this paper, to tackle these problems, consists on storing the knowledge in a generalized Dirichlet mixture model by applying non-parametric Bayesian estimation and inference techniques. Specifically, we extend finite generalized Dirichlet mixture models to the infinite case in which the number of components and relevant features do not need to be known a priori. This extension provides a natural representation of uncertainty regarding the challenging problem of model selection. We propose a Markov Chain Monte Carlo algorithm to learn the resulted infinite mixture. Through applications involving text and image categorization, we show that infinite mixture models offer a more powerful and robust performance than classic finite mixtures for both clustering and feature selection.",information system
2053,A clustering approach to generalized pattern identification based on multi-instanced objects with DARA,Advances in Databases and Information Systems," Clustering is an essential data mining task with various types of applications. Traditional clustering algorithms are based on a vector space model representation. A relational database system often contains multirelational information spread across multiple relations (tables). In order to cluster such data, one would require to restrict the analysis to a single representation, or to construct a feature space comprising all possible representations from the data stored in multiple tables. In this paper, we present a data summarization approach, borrowed from the Information Retrieval theory, to clustering in multi-relational environment. We find that the data summarization technique can be used here to capture the typical high volume of multiple instances and numerous forms of patterns. Our experiments demonstrate a technique to cluster data in a multi-relational environment and show the evaluation results on the mutagenesis dataset. In addition, the effect of varying the number of features considered in clustering on the classification performance is also evaluated. ",information system
2054,Optimal Query Mapping in Mobile OLAP,Advances in Databases and Information Systems,"Query mapping to aggregation lattices is used in order to exploit sub-cube dependencies in multidimensional databases. It is employed in mobile OLAP dissemination systems, in order to reduce the number of handled data items and thus optimize their scheduling and dissemination process. This paper analyzes the impact of choosing between mapping to the data cube lattice or alternatively to the respective hierarchical data cube lattice. We analyze the involved tradeoffs and identify the exploitation degree of sub-cube derivability as the deciding factor. We therefore introduce an analytical framework which computes derivability related probabilities and thus facilitates the quantification of this degree. The information provided by the framework is consistent with experimental results of state of the art mobile OLAP dissemination systems.",information system
2055,Product development and pricing strategy for information goods under heterogeneous outside opportunities,Information Systems Research," This paper considers a two-stage development problem for information goods with costless quality degradation. In our model, a seller of information goods faces customers that are heterogeneous with regard to both the marginal willingness to pay for quality and the outside opportunity. In the development stage, the seller determines the quality limit of the product. In the second stage, the seller's problem is to design the price schedule corresponding to different quality levels, taking into account production and distribution costs. We show that versioning is optimal for the seller when customers have multiple outside options, or more generally, convex reservation utilities. In addition, we show that in the optimal solution, the seller discards both low-end and high-end customers. Among those that are served, the seller offers a continuum of (inferior) versions to customers with relatively low willingness to pay, and extracts full information rent from each of them. A common version with the quality limit is offered to the rest. We further prove that the seller should offer a single version when reservation utilities are either concave or linear. Through numerical experiments, we study the sensitivity of our results to changes in the cost structure and customer utilities. ",information system
2056,Designing a flash-aware two-level cache,Advances in Databases and Information Systems,"The random read efficiency of flash memory, combined with its growing density and dropping price, make it well-suited for use as a read cache. We explore how a system can use flash memory as a cache layer between the main memory buffer pool and the magnetic disk. We study the problem of deciding which data pages to cache on flash and propose alternatives that serve different purposes. We give an analytical model to decide the optimal caching scheme for any workload, taking into account the physical properties of the flash disk used. We discuss implementation issues such as the effect of the flash cache block size on performance. Our experimental evaluation shows that questions on systems with flash-resident caches cannot be given universal answers that hold across all flash disks and workloads. Rather, our cost model should be applied per case to provide an optimal setup with confidence.",information system
2057,On Integrating Data Mining into Business Processes,Business Information Systems,"Integrating data mining into business processes becomes crucial for business today. Modern business process management frameworks provide great support for flexible design, deployment and management of business processes. However, integrating complex data mining services into such frameworks is not trivial due to unclear definitions of user roles and missing flexible data mining services as well as missing standards and methods for the deployment of data mining solutions. This work contributes an integrated view on the definition of user roles for business, IT and data mining and discusses the integration of data mining in business processes and its evaluation in the context of BPR.",information system
2058,An Operator-Stream-Based Scheduling Engine for Effective GPU Coprocessing,Advances in Databases and Information Systems,"Since a decade, the database community researches opportunities to exploit graphics processing units to accelerate query processing. While the developed GPU algorithms often outperform their CPU counterparts, it is not beneficial to keep processing devices idle while over utilizing others. Therefore, an approach is needed that effectively distributes a workload on available (co-)processors while providing accurate performance estimations for the query optimizer. In this paper, we extend our hybrid query-processing engine with heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach doubles the throughput compared to our previous solution and state-of-the-art approaches, because of nearly equal device utilization while preserving accurate performance estimations.",information system
2059,"When the Wait Isn’t So Bad: The Interacting Effects of Website Delay, Familiarity, and Breadth",Information Systems Research," Awhen moving from one page to another. This experimental study examined whether delay and two other lthough its popularity is widespread, the Web is well known for one particular drawback: its frequent delay website design variables (site breadth and content familiarity) have interaction effects on user performance, attitudes, and behavioral intentions. The three experimental factors (delay, familiarity, and breadth) collectively impact the cognitive costs and penalties that users incur when making choices in their search for target information. An experiment was conducted with 160 undergraduate business majors in a completely counterbalanced, fully factorial design that exposed them to two websites and asked them to browse the sites for nine pieces of information. Results showed that all three factors have strong direct impacts on performance and user attitudes, in turn affecting behavioral intentions to return to the site, as might be expected. A significant three-way interaction was found between all three factors indicating that these factors not only individually impact a user's experiences with a website, but also act in combination to either increase or decrease the costs a user incurs. Two separate analyses support an assertion that attitudes mediate the relationship of the three factors on behavioral intentions. The implications of these results for both researchers and practitioners are discussed. Additional research is needed to discover other factors that mitigate or accentuate the effects of delay, other effects of delay, and under what amounts of delay these effects occur. ",information system
2060,The analysis and management of non-canonical requirement specifications through a belief integration game,Knowledge and Information Systems,"Non-canonical requirement specifications refer to a set of software requirements that is either inconsistent, vague or incomplete. In this paper, we provide a correspondence between requirement specifications and annotated propositional belief bases. Through this analogy, we are able to analyze the contents of a given set of requirement collections known as viewpoints and specify whether they are incomplete, incoherent, or inconsistent under a closed-world reasoning assumption. Based on the requirement collections’ properties introduced in this paper, we define a viewpoint integration game through which the inconsistencies of non-canonical requirement specifications are resolved. The game consists of several rounds of negotiation and is performed by two main functions, namely choice and enhancement functions. The outcome of this game is a set of inconsistency-free requirement collections that can be integrated to form a unique fair representative of the given requirement collections.",information system
2061,"Robustness, stability, recoverability, and reliability in constraint satisfaction problems",Knowledge and Information Systems,"Many real-world problems in Artificial Intelligence (AI) as well as in other areas of computer science and engineering can be efficiently modeled and solved using constraint programming techniques. In many real-world scenarios the problem is partially known, imprecise and dynamic such that some effects of actions are undesired and/or several unforeseen incidences or changes can occur. Whereas expressivity, efficiency, and optimality have been the typical goals in the area, there are several issues regarding robustness that have a clear relevance in dynamic Constraint Satisfaction Problems (CSPs). However, there is still no clear and common definition of robustness-related concepts in CSPs. In this paper, we propose two clearly differentiated definitions for robustness and stability in CSP solutions. We also introduce the concepts of recoverability and reliability, which arise in temporal CSPs. All these definitions are based on related well-known concepts, which are addressed in engineering and other related areas.",information system
2062,The Productivity of Information Technology Investments: New Evidence from IT Labor Data,Information Systems Research," Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions This article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org. ",information system
